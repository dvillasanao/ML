---
title: "Machine Learning (Apuntes) "
author: "Diana Villasana Ocampo"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  Apuntes personales
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---
```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```

```{r, echo = FALSE, include = FALSE}
require(dplyr)
require(gt)
require(caret)
```

# Machine Learning {.unnumbered}

El machine learning ha experimentado un crecimiento significativo durante la última década. Este desarrollo se atribuye a tres factores fundamentales: el **incremento sustancial en la disponibilidad de datos** (Big Data), la **evolución de las capacidades computacionales** y el **perfeccionamiento de algoritmos avanzados**. En la actualidad, el machine learning constituye un elemento transformador en diversos sectores: desde aplicaciones médicas para el diagnóstico de enfermedades, hasta la optimización de estrategias financieras. Su capacidad analítica y de procesamiento de datos lo posiciona como un recurso esencial para la planificación estratégica, la optimización de procesos y el desarrollo de soluciones personalizadas.

**¿Cuándo Implementar Machine Learning?**

La implementación del machine learning resulta particularmente efectiva en los siguientes contextos:

-   **Disponibilidad de datos a gran escala:** La eficacia del modelo se incrementa proporcionalmente con la cantidad de datos relevantes disponibles.\
-   **Presencia de relaciones complejas entre variables:** En situaciones donde la multiplicidad de variables dificulta la definición de reglas convencionales.\
-   **Necesidad de adaptación dinámica:** Los sistemas de machine learning permiten una optimización continua mediante la incorporación de nueva información.\
-   **Requerimientos de automatización avanzada:** Facilita la ejecución de tareas complejas, desde el análisis visual hasta la generación de pronósticos predictivos.

```{r echo=FALSE, fig.align='center', out.width="80%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/ML Categories_Page_1.png"))
```

## 📌 Cuadro {.unnumbered}

```{r, echo = FALSE}
algoritmos_ml <- data.frame(
                            Tipo = c(
                              "Regressión",
                              "Árboles / Decision Tree",
                              "Ensambles",
                              "Deep Learning",
                              "Reducción de Dim.",
                              "Bayesianos",
                              "Regularización",
                              "Instance-Based",
                              "Clustering",
                              "Rule-Based"
                            ),
                            Problema_tipico = c(
                              "Valores numéricos",
                              "Clasificación, regresión",
                              "Clasificación, regresión",
                              "Imágenes, texto, audio",
                              "Visualización, preprocesamiento",
                              "Clasificación rápida",
                              "Evitar overfitting",
                              "Clasificación",
                              "Agrupamiento no supervisado",
                              "Interpretabilidad"
                            ),
                            Ventajas = c(
                              "Simplicidad",
                              "Interpretabilidad",
                              "Precisión",
                              "Modelos complejos",
                              "Mejora eficiencia",
                              "Velocidad",
                              "Generalización",
                              "Simple, no requiere entrenamiento",
                              "Descubrir estructuras ocultas",
                              "Lógica clara"
                            ),
                            Cuando_usarlo = c(
                              "Relaciones lineales",
                              "Datos tabulares",
                              "Alto rendimiento, Kaggle",
                              "Datos grandes y no estructurados",
                              "Datos con muchas variables",
                              "Texto, spam detection",
                              "Modelos lineales con muchas variables",
                              "Pocos datos y relaciones claras",
                              "Segmentación sin etiquetas",
                              "Reglas conocidas, decisiones explicables"
                            ),
                            stringsAsFactors = FALSE
                          )

require(gt)

algoritmos_ml %>% 
 gt() %>%
  tab_header(title = "Modelos y cuando usarlos") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     fmt_integer(columns = names(data)[4:22], 
                sep_mark = " ") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Cuando_") ~ px(300),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```

## Modelos de Machine Learning {.unnumbered}

Modelos disponibles en la paquetería `caret` en R.

**Enlace**: <https://topepo.github.io/caret/model-training-and-tuning.html>

```{r list_setup, include=FALSE}
library(DT)
library(caret)
mods <- getModelInfo()

num_param <- unlist(lapply(mods, function(x) paste(as.character(x$param$parameter), sep = "", collapse = ", ")))
num_param[num_param == "parameter"] <- "None"
num_param <- data.frame(model = names(num_param), num_param = num_param)

mod_type <- unlist(lapply(mods, function(x) paste(sort(x$type), sep = "", collapse = ", ")))
mod_type <- data.frame(model = names(mod_type), type = mod_type)

libs <- unlist(lapply(mods, function(x) paste(x$library, sep = "", collapse = ", ")))
libs <- data.frame(model = names(libs), libraries = libs)

mod_name <- unlist(lapply(mods, function(x) x$label))
mod_name <- data.frame(model = names(mod_name), name = mod_name)

model_info <- merge(mod_name, mod_type, all = TRUE)
model_info <- merge(model_info, libs, all = TRUE)
model_info <- merge(model_info, num_param, all = TRUE)
model_info <- model_info[, c('name', 'model', 'type', 'libraries', 'num_param')]
model_info <- model_info[order(model_info$name),]
```

::: {style="height:400px;overflow:auto;"}
```{r list_table, echo = FALSE}
model_info %>%
 gt() %>%
  tab_header(title = "Modelos de Machine Learning",
             subtitle = "Disponibles en la paquetería caret") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("name") ~ px(350),
                   starts_with("num_param") ~ px(350),
                   everything() ~ px(200)) %>%
         cols_label(name = md("**Model**"),
                    model = md("**Method Value**"),
                    type = md("**Type**"),
                    libraries = md("**Libraries**"),
                    num_param = md("**Tunning Parameters**")) %>%
         as_raw_html()
```
:::

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 🔍 1. Regresión {-}   

**Ejemplos:** Regresión Lineal Simple, Regresión Ridge, Regresión Lasso.  
**Uso:** Ideal para **predecir valores numéricos continuos** (como precios o temperaturas) y cuando esperas **relaciones lineales** entre tus variables.   
**Ventajas:** Es un modelo **simple** de entender y altamente **interpretable**.  
**Limitaciones:** Su desempeño es bajo cuando las relaciones entre las variables son **no lineales** o muy complejas.  

---

## Ordinary Least Squares Regression (`OLSR`) {-}    

[Ordinary Least Squares Regression (`OLSR`) en R](https://dvillasanao.github.io/ML_Examples/Output/Regression/01_01.OLSR.html)  
[Ordinary Least Squares Regression (`OLSR`) en Python](https://dvillasanao.github.io/ML_Examples/R/Regression/01_01_OLSR_py.html)   

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/01_image_OLSR.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/OLSR.png"))
```


La **Regresión por Mínimos Cuadrados Ordinarios (OLS)** es una técnica fundamental en estadística y Machine Learning para modelar la **relación lineal** entre una **variable dependiente** (a predecir) y una o más **variables independientes**. Su objetivo es encontrar la **línea (o hiperplano) que mejor se ajusta** a los datos, minimizando la **suma de los cuadrados de las diferencias** entre los valores reales y los predichos por el modelo. Es decir, busca los coeficientes que hacen que la distancia (al cuadrado) de los puntos a la línea sea mínima.

Los coeficientes de OLS se pueden calcular directamente con una fórmula matemática, sin necesidad de procesos iterativos complejos, bajo ciertos supuestos como la linealidad de la relación y la independencia de los errores.

En el contexto del **aprendizaje global vs. local**, OLS es un ejemplo claro de un modelo de **aprendizaje global**. OLS busca una **única ecuación** o un conjunto de coeficientes que describan la relación entre las variables para **todo el conjunto de datos**. La línea o hiperplano que encuentra es una solución global que se aplica de manera uniforme en todo el espacio de características. Esto la hace muy interpretable y computacionalmente eficiente, pero limitada si la relación real entre las variables no es estrictamente lineal en todo el dominio de los datos.


## Linear Regression {.unnumbered}   


La **Regresión Lineal** es uno de los algoritmos más fundamentales y ampliamente utilizados en el campo del **Machine Learning y la estadística**. Es un modelo **supervisado** que busca establecer una **relación lineal** entre una **variable de respuesta (o dependiente)** continua y una o más **variables predictoras (o independientes)**.

**Concepto y Ecuación:**

La idea central de la regresión lineal es encontrar la **línea (o hiperplano en múltiples dimensiones)** que mejor se ajusta a los datos, de modo que se pueda predecir el valor de la variable dependiente basándose en los valores de las variables predictoras.

* **Regresión Lineal Simple:** Implica una única variable predictora. La ecuación de la línea es:
    $$Y = \beta_0 + \beta_1 X + \epsilon$$  
    
Donde:
    * $Y$ es la variable de respuesta.
    * $X$ es la variable predictora.
    * $\beta_0$ es el **intercepto** (el valor de $Y$ cuando $X$ es 0).
    * $\beta_1$ es el **coeficiente de la pendiente** (cuánto cambia $Y$ por cada unidad de cambio en $X$).
    * $\epsilon$ es el **término de error** o residual (la parte de $Y$ que el modelo no puede explicar).

* **Regresión Lineal Múltiple:** Implica dos o más variables predictoras. La ecuación se extiende a:   
    $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \epsilon$$  
    
Donde:
    * $X_1, X_2, ..., X_p$ son las $p$ variables predictoras.
    * $\beta_1, \beta_2, ..., \beta_p$ son los coeficientes de cada variable predictora.

**Cómo Funciona (Mínimos Cuadrados Ordinarios - OLS):**

El método más común para estimar los coeficientes ($\beta$s) en la regresión lineal es el de **Mínimos Cuadrados Ordinarios (OLS)**. OLS funciona minimizando la **suma de los cuadrados de los residuos**. Un residuo es la diferencia entre el valor real de $Y$ y el valor predicho por el modelo ($\hat{Y}$).

$$\text{Minimizar: } \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} (Y_i - (\beta_0 + \beta_1 X_{i1} + ... + \beta_p X_{ip}))^2$$

Al minimizar esta suma de cuadrados, OLS encuentra los coeficientes que hacen que la línea de regresión esté lo más cerca posible de la mayoría de los puntos de datos.

**Supuestos Clave:**  

La validez de los resultados de la regresión lineal tradicional se basa en varios supuestos:

* **Linealidad:** La relación entre las variables $X$ y $Y$ es lineal.
* **Independencia:** Las observaciones son independientes entre sí.
* **Normalidad de los Residuos:** Los residuos se distribuyen normalmente.
* **Homocedasticidad:** La varianza de los residuos es constante a lo largo de todos los niveles de las variables predictoras.
* **No Multicolinealidad Perfecta:** Las variables predictoras no deben estar perfectamente correlacionadas entre sí.

**Uso y Limitaciones:**

La regresión lineal es popular por su **simplicidad, interpretabilidad** y por ser un buen punto de partida para muchos problemas de predicción. Sin embargo, su principal limitación es que solo puede modelar **relaciones lineales**. Si la relación subyacente entre las variables es no lineal, una regresión lineal puede no capturarla adecuadamente y dar resultados inexactos.

**Aprendizaje Global vs. Local:**

La Regresión Lineal es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** La Regresión Lineal aprende un **único conjunto de coeficientes** que define una **línea (o hiperplano) global** que se aplica a todo el espacio de características. Esta línea busca representar la **relación lineal promedio o general** entre las variables predictoras y la variable de respuesta a lo largo de todo el rango de los datos. La predicción para cualquier nueva instancia se realiza utilizando la misma ecuación lineal, sin importar en qué parte del espacio de características se encuentre. No hay adaptaciones o modelos separados para diferentes subregiones de los datos; el modelo es una función que describe una tendencia general y global.

* **Rigidez de la Linealidad:** Debido a su naturaleza global y lineal, la regresión lineal no puede capturar relaciones **no lineales o interacciones complejas** entre las variables predictoras de forma inherente. Si la relación real en los datos es no lineal, el modelo lineal intentará ajustarla con la mejor línea recta posible, lo que podría llevar a un bajo rendimiento.


## Regresión Logística (Logit) {.unnumbered}

```{r, echo = FALSE,out.width='50%', fig.align='center', fig.cap="Elaboración propia"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Logit.png"))
```

La **Regresión Logística** es un modelo estadístico usado principalmente para problemas de **clasificación binaria**, donde el objetivo es predecir la **probabilidad** de que una instancia pertenezca a una de dos clases (por ejemplo, "sí" o "no", "0" o "1"). A pesar de su nombre, no predice un valor continuo, sino una probabilidad.

Este modelo utiliza una **función sigmoide (o logística)** para transformar una combinación lineal de las variables de entrada en un valor entre 0 y 1, que se interpreta como una probabilidad. Los coeficientes del modelo se aprenden maximizando la verosimilitud de observar los datos, generalmente a través de algoritmos como el descenso de gradiente.

En el contexto del **aprendizaje global vs. local**, la Regresión Logística es un modelo de **aprendizaje global**. Esto significa que busca un **único conjunto de coeficientes** que definen una frontera de decisión (un hiperplano) que se aplica a todo el espacio de características para separar las clases. Asume una relación lineal entre las variables de entrada y el logaritmo de la probabilidad, y una vez entrenado, usa esta relación global para hacer predicciones en cualquier parte del espacio de datos. Si bien es eficiente y muy interpretable, su naturaleza global puede limitar su rendimiento en casos donde las fronteras de decisión son inherentemente no lineales o muy complejas.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "✅ Supervisado",
  "✅ Categórica binaria (0/1)",
  "✅ Numéricas y categóricas",
  "✅ Lineal entre log-odds y predictores",
  "❌ No es requisito",
  "✅ Necesaria",
  "✅ Deseable",
  "⚠️ Sí",
  "⚠️ Puede afectar",
  "✅ Alta (coeficientes interpretables)",
  "✅ Alta",
  "✅ Compatible",
  "❌ Respuesta no binaria o multiclase sin ajuste"
)
detalles <- c(
  "Clasificación binaria",
  "Ej. éxito/fracaso, sí/no",
  "Convertir categóricas a dummies",
  "Relación entre log(p/(1-p)) y X debe ser lineal",
  "No se exige normalidad en errores",
  "Independencia entre observaciones",
  "Idealmente varianza constante",
  "Outliers pueden alterar los coeficientes",
  "Usar VIF y regularización si hay problema",
  "Coeficientes en términos de odds/log-odds",
  "Rápido y estable para datasets medianos",
  "K-fold funciona muy bien",
  "Evitar si hay multiclase sin ajuste"
)
tabla_logit <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

require(gt)
tabla_logit %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir logit",
             subtitle = "Regresión logística") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Generalized Linear Model (GLM) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/GLM.png"))
```

## Least Angle Regression (LARS)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/LARS.png"))
```

**Least Angle Regression (LARS)** es un algoritmo de **regresión lineal** desarrollado por Bradley Efron, Trevor Hastie, Iain Johnstone y Robert Tibshirani. Es particularmente interesante porque puede considerarse como una **versión más eficiente y paso a paso de LASSO** (Least Absolute Shrinkage and Selection Operator) y es útil para **seleccionar características** y manejar datos de alta dimensión.

A diferencia de OLS, que calcula todos los coeficientes de una vez, o de Lasso, que requiere optimización más compleja, LARS opera de manera incremental. Su idea central es avanzar los coeficientes de forma que su ángulo con el vector de residuos sea siempre el mismo y que sea el "más pequeño" posible.

El proceso de LARS se puede resumir así:

1.  **Inicio:** Todos los coeficientes se inicializan en cero.
2.  **Identificación del Predictor más Correlacionado:** El algoritmo encuentra la variable predictora que está más correlacionada con la variable de respuesta (o con el residuo actual).
3.  **Movimiento en la Dirección del Predictor:** El coeficiente de esa variable predictora se mueve gradualmente desde cero en la dirección del signo de su correlación. A medida que el coeficiente se mueve, el residuo cambia.
4.  **Activación de Nuevos Predictores:** Cuando otra variable predictora alcanza la misma correlación con el residuo actual que la variable que ya está activa, el algoritmo cambia de dirección. Ahora, los coeficientes de *ambas* variables activas se mueven juntas en un "ángulo equiestadístico" de tal manera que permanecen igualmente correlacionadas con el residuo.
5.  **Proceso Iterativo:** Este proceso continúa, añadiendo nuevas variables al conjunto de variables "activas" (es decir, aquellas con coeficientes distintos de cero) a medida que estas alcanzan la misma correlación con el residuo. Los coeficientes se mueven de forma coordinada.
6.  **Criterio de Parada:** El algoritmo se detiene cuando todos los predictores han sido incluidos en el modelo, o cuando se alcanza un número predefinido de pasos o de variables.

**Relación con otros modelos:**
* Si LARS se detiene cuando los coeficientes de las variables no activas son menores o iguales a la correlación actual de las variables activas (y los coeficientes de las variables no activas se fijan en cero si su correlación es menor), entonces genera la **solución completa del camino de LASSO**.
* También puede generar el camino de soluciones para la **Ridge Regression** si se modifica ligeramente.

LARS es eficiente porque solo requiere un número de pasos igual al número de variables, o menos si se detiene antes.

**Aprendizaje Global vs. Local:**

Least Angle Regression (LARS) es un modelo de **aprendizaje global**.

* **Aspecto Global:** LARS construye un **modelo lineal global** paso a paso. Aunque el algoritmo añade variables una por una y ajusta sus coeficientes de manera incremental, el modelo resultante en cada paso es una ecuación de regresión lineal que se aplica a todo el conjunto de datos. La decisión de qué variable añadir y cómo ajustar los coeficientes se basa en las correlaciones globales entre las variables predictoras y la respuesta (o el residuo). La finalidad es encontrar los coeficientes óptimos para una función de regresión que se aplica a todo el espacio de características.

* **Selección de Características Globalmente:** La capacidad de LARS para realizar selección de características (al igual que LASSO) es un proceso global. Se identifican las variables más influyentes en el contexto de todo el conjunto de datos, y su inclusión en el modelo contribuye a la formación de una relación global entre los predictores y la respuesta. No se construyen modelos separados para diferentes subregiones de los datos; en cambio, se construye un único modelo global de manera progresiva.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (regresión)",
  "✅ Numérica continua",
  "✅ Numéricas (requiere estandarización)",
  "✅ Lineal",
  "⚠️ Deseable para inferencia clásica",
  "✅ Necesaria",
  "✅ Supuesto importante",
  "⚠️ Afectado por valores extremos",
  "✅ Maneja bien multicolinealidad como LASSO",
  "✅ Muy interpretable (secuencia de modelos anidados)",
  "✅ Muy eficiente, especialmente con muchas variables",
  "✅ Útil para elegir número de variables con validación cruzada",
  "❌ Datos ruidosos o no lineales; ❌ si hay muchas interacciones no capturadas"
)

detalles <- c(
  "Algoritmo de regresión eficiente que selecciona variables secuencialmente como alternativa a LASSO.",
  "Busca predecir una variable continua usando múltiples predictores.",
  "Usa variables numéricas estandarizadas; es sensible a escala.",
  "Asume relación lineal entre predictores y respuesta.",
  "No exige normalidad para predicción, pero sí para inferencia estadística.",
  "Errores deben ser independientes entre sí.",
  "Supone varianza constante de los errores.",
  "Puede verse afectado si hay valores extremos en las variables.",
  "Muy útil cuando los predictores están correlacionados; elige uno a la vez.",
  "Produce una ruta de modelos fácilmente interpretable con selección progresiva.",
  "Más rápido que LASSO al generar trayectorias de coeficientes.",
  "Puede aplicarse validación cruzada para seleccionar el mejor modelo en la secuencia.",
  "No captura relaciones no lineales o interacciones sin modificación previa del modelo."
)

tabla_lars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lars %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir LARS",
             subtitle = "Least Angle Regression (LARS)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Locally Estimated Scatterplot Smoothing (`LOESS`) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
knitr::include_graphics(paste0(here::here(), "/img/Regression/LOESS.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/LOESS_1.png"))
```

**LOESS (Locally Estimated Scatterplot Smoothing)**, o LOWESS, es una técnica de **regresión no paramétrica** para crear una curva suave que se ajusta a los datos en un diagrama de dispersión. Su gran ventaja es que **no asume una forma funcional global** específica para la relación entre las variables, lo que la hace muy flexible para identificar tendencias y patrones no lineales.

El principio de LOESS es simple: para estimar el valor suavizado en un punto, se seleccionan los **puntos de datos cercanos** (definido por un parámetro de **"span"** o ancho de banda), se les asignan **pesos** (dando más peso a los puntos más cercanos), y luego se ajusta un **polinomio de bajo grado** (comúnmente lineal o cuadrático) a esos puntos usando mínimos cuadrados ponderados. Este proceso se repite para cada punto de interés para construir la curva.

En el contexto del **aprendizaje global vs. local**, LOESS es un modelo de **aprendizaje puramente local**. Su flexibilidad reside en que **ajusta múltiples modelos simples y locales** (regresiones ponderadas) en diferentes vecindarios de los datos. No busca una única ecuación global que describa la relación en todo el conjunto de datos. Esto le permite adaptarse maravillosamente a las variaciones en las relaciones y curvaturas de los datos, lo que es especialmente útil cuando los datos no se distribuyen linealmente. Sin embargo, su naturaleza local implica que no produce una fórmula explícita del modelo, y puede ser computacionalmente más intensivo para conjuntos de datos muy grandes.  


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "✅ Supervisado",
  "✅ Continua",
  "✅ Numéricas (usualmente 1 o 2 predictores)",
  "✅ No lineal y suave",
  "❌ No necesaria",
  "✅ Deseable",
  "✅ Deseable",
  "⚠️ Muy sensible",
  "❌ No aplica (pocos predictores)",
  "✅ Muy interpretable gráficamente",
  "⚠️ Lento en grandes volúmenes de datos",
  "✅ Puede usarse para elegir 'span'",
  "❌ Datos grandes o con ruido fuerte"
)
detalles <- c(
  "Modelo no paramétrico local",
  "Regresión para valores continuos",
  "Generalmente 1 o 2 variables numéricas",
  "Ajuste por vecindad, suaviza la curva",
  "No asume distribución específica",
  "Supuesto deseable si hay dependencias temporales",
  "Ideal si la varianza no cambia mucho localmente",
  "Altamente afectado por outliers locales",
  "No es una técnica multivariable compleja",
  "La curva ajustada se interpreta visualmente",
  "Computacionalmente costoso con datos grandes",
  "Ayuda a seleccionar el mejor 'span'",
  "Poco eficaz en alta dimensión o datos muy dispersos"
)
tabla_loess <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
tabla_loess %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir LOESS",
             subtitle = "Locally Estimated Scatterplot Smoothing (LOESS)") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Multivariate Adaptive Regression Splines (`MARS`) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/MARS.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/MARS_1.png"))
```

**Multivariate Adaptive Regression Splines (MARS)** es un algoritmo de **regresión no paramétrica** que extiende los modelos lineales para manejar relaciones no lineales y complejas. Desarrollado por Jerome Friedman, MARS construye su modelo al **dividir el espacio de entrada en múltiples regiones y ajustar una función lineal simple (o de orden superior) a cada región**.

El proceso de MARS consta de dos fases: una **fase de adelante** que añade iterativamente **funciones base** (pares de funciones "hinge" o bisagra) y **nudos** (puntos de corte) para capturar no linealidades e interacciones entre variables, y una **fase de atrás** que poda las funciones base menos significativas utilizando criterios como la **Validación Cruzada Generalizada (GCV)** para prevenir el sobreajuste. Esto permite a MARS ser adaptable a las particularidades de los datos.

En el contexto del **aprendizaje global vs. local**, MARS se sitúa como un modelo de **aprendizaje adaptativo que combina aspectos globales y locales**. Es "local" en el sentido de que sus funciones base y nudos dividen el espacio de datos en regiones, y dentro de cada región se aplica una relación simple. Sin embargo, es "global" porque la suma de todas estas funciones base forma una única ecuación que describe la relación en todo el conjunto de datos y se aplica de forma consistente. Esto significa que si los datos no se distribuyen linealmente, MARS puede aprender y modelar estas relaciones complejas de forma adaptativa, encontrando automáticamente dónde y cómo las relaciones cambian, ofreciendo una solución que es tanto flexible como interpretable.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "✅ Supervisado",
  "✅ Continua o categórica (binaria con extensión)",
  "✅ Numéricas (categóricas con dummies)",
  "✅ No lineal (automático)",
  "❌ No requerida",
  "✅ Deseable",
  "✅ Deseable",
  "⚠️ Sí (aunque algo robusto)",
  "⚠️ Puede afectar",
  "⚠️ Media (modelo tipo caja negra)",
  "✅ Razonable para tamaños medianos",
  "✅ Recomendado (ej. repeated k-fold)",
  "❌ Relación puramente lineal o muchos factores irrelevantes"
)
detalles <- c(
  "Regresión flexible no lineal",
  "Ideal para regresión continua (también clasificación con `earth`)",
  "Crea automáticamente 'splines' por variable",
  "Crea funciones por tramos con 'nudos'",
  "No exige distribución específica de errores",
  "Mejor si los datos no están correlacionados temporalmente",
  "Idealmente errores con varianza constante",
  "Puede ser sensible a valores extremos",
  "Detecta interacciones, pero VIF sigue siendo útil",
  "Coeficientes no tan interpretables como OLS",
  "Más lento que OLS pero más flexible",
  "CV ayuda a elegir número óptimo de términos",
  "Tiene riesgo de sobreajuste si no se controla bien"
)
tabla_mars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_mars %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir MARS",
             subtitle = "Splines de Regresión Adaptativa Multivariante (MARS)") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Polynomial Regression {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Polynomial Regression.png"))
```

## Quantile Regression {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Quantile Regression.png"))
```

## Stepwise Regression {.unnumbered}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/StepW.png"))
```

La **Regresión por Pasos (Stepwise Regression)** es una técnica para construir un modelo de regresión lineal (o a veces otros modelos lineales generalizados) seleccionando las variables predictoras de forma iterativa y automática. Su objetivo es encontrar un subconjunto óptimo de variables que mejore la capacidad predictiva del modelo sin incluir variables irrelevantes o redundantes. Esto ayuda a simplificar el modelo, mejorar la interpretabilidad y reducir el riesgo de sobreajuste.

Existen tres estrategias principales para la regresión por pasos:

1.  **Selección Hacia Adelante (Forward Selection):**
    * Comienza con un modelo que no incluye ninguna variable predictora (solo el intercepto).
    * En cada paso, evalúa todas las variables predictoras disponibles que aún no están en el modelo.
    * Añade al modelo la variable que, al ser incluida, produce la mayor mejora estadística (generalmente medida por un valor p bajo, un R-cuadrado ajustado mayor, o un criterio de información como AIC o BIC).
    * El proceso continúa hasta que ninguna de las variables restantes mejora el modelo por encima de un umbral predefinido.

2.  **Eliminación Hacia Atrás (Backward Elimination):**
    * Comienza con un modelo que incluye **todas** las variables predictoras posibles.
    * En cada paso, evalúa las variables predictoras que actualmente están en el modelo.
    * Elimina del modelo la variable que es menos significativa estadísticamente (generalmente medida por un valor p alto, o una reducción en el R-cuadrado ajustado o un aumento en AIC/BIC).
    * El proceso continúa hasta que la eliminación de cualquier variable empeoraría significativamente el modelo.

3.  **Híbrida (Mixed / Bidirectional Stepwise):**
    * Combina la selección hacia adelante y la eliminación hacia atrás.
    * En cada paso, el algoritmo puede tanto añadir una variable si mejora el modelo, como eliminar una variable que ya está en el modelo si se vuelve redundante o no significativa. Esto permite que el modelo reconsidere variables que fueron añadidas o eliminadas en pasos anteriores. Es el enfoque más común y robusto.

**Criterios de Selección:**  

La decisión de añadir o eliminar una variable en cada paso se basa en criterios estadísticos, siendo los más comunes:
* **Valores p:** Umbrales para la significancia estadística de los coeficientes.
* **$R^2$ ajustado:** Mide la proporción de varianza explicada por el modelo, penalizando la inclusión de variables innecesarias.
* **Criterio de Información de Akaike (AIC):** Penaliza la complejidad del modelo (número de parámetros) en relación con su bondad de ajuste.
* **Criterio de Información Bayesiano (BIC):** Similar al AIC, pero con una penalización más fuerte por la complejidad.

**Ventajas y Desventajas:**

* **Ventajas:** Puede ayudar a construir modelos más parsimoniosos, mejorar la interpretabilidad y reducir la multicolinealidad.
* **Desventajas:**
    * **Sobreajuste:** Puede llevar a sobreajuste si se usa de forma acrítica, ya que el algoritmo se optimiza para los datos de entrenamiento.
    * **Problemas de Significancia Estadística:** Los valores p y otras métricas pueden no ser confiables debido a la selección de características basada en los datos.
    * **Inestabilidad:** El conjunto de variables seleccionadas puede ser muy sensible a pequeñas perturbaciones en los datos o a la elección del criterio de selección.
    * **Ignora el Conocimiento del Dominio:** Puede seleccionar variables que son estadísticamente significativas pero que carecen de sentido práctico o causal.
    * **No Maneja Interacciones Complejas:** Es fundamentalmente un método para seleccionar variables para un modelo lineal y no está diseñado para descubrir relaciones no lineales o interacciones complejas.

Debido a sus desventajas, la regresión por pasos se utiliza con más cautela hoy en día. A menudo se prefieren métodos de regularización (como Lasso o Elastic Net) para la selección de características, ya que son más estables y realizan la selección de forma más robusta.

**Aprendizaje Global vs. Local:**

La Regresión por Pasos es un modelo de **aprendizaje global**.

* **Aspecto Global:** La regresión por pasos construye un **único modelo de regresión lineal global** que busca explicar la relación entre las variables predictoras y la respuesta en todo el conjunto de datos. La selección de variables se realiza para optimizar el rendimiento de este modelo global. Los coeficientes finales que se obtienen definen una función lineal que se aplica de manera consistente a cualquier nueva observación, sin importar en qué parte del espacio de características se encuentre.

* **Proceso de Selección (Global):** Aunque el proceso es iterativo y añade/elimina variables, la decisión en cada paso se basa en cómo esa adición/eliminación afecta la bondad de ajuste o la complejidad del modelo en **todo el conjunto de datos**. No se ajustan modelos separados o locales para diferentes regiones.



## Support Vector Machine (SVM) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/SVM.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/SVM_1.png"))
```


**Support Vector Machine (SVM)** es un potente y versátil algoritmo de **Machine Learning** que se utiliza tanto para tareas de **clasificación** como de **regresión**, aunque es más conocido por su aplicación en clasificación. Su objetivo principal es encontrar el **hiperplano óptimo** que separe las clases en el espacio de características con el **margen** más grande posible. Los puntos de datos más cercanos a este hiperplano se llaman **vectores de soporte**, y son cruciales para definir la frontera de decisión.

Para manejar datos que no son linealmente separables, SVM utiliza el **"truco del kernel"**. Este truco permite a SVM mapear implícitamente los datos a un espacio de mayor dimensión donde las clases podrían ser linealmente separables, sin necesidad de calcular explícitamente las coordenadas. Funciones kernel comunes como el **Radial Basis Function (RBF) o Gaussiano** permiten a SVM modelar fronteras de decisión no lineales complejas en el espacio original de baja dimensión.

En el contexto del **aprendizaje global vs. local**, SVM se clasifica principalmente como un modelo de **aprendizaje global**. Esto se debe a que busca un **único hiperplano óptimo** (o una frontera de decisión no lineal definida por el kernel) que se aplica a la totalidad del espacio de características. Una vez entrenado, el modelo predice evaluando la posición de un nuevo punto con respecto a esta frontera global. Sin embargo, hay un matiz "local" en su funcionamiento: la determinación de este hiperplano depende **críticamente solo de los vectores de soporte**, que son los puntos de datos "más difíciles" cercanos a la frontera. Los puntos que están lejos del margen no influyen en la definición del modelo. Aunque la frontera de decisión es una función global que se aplica en todas partes, su construcción está influenciada por estos puntos localmente relevantes, permitiendo a SVM adaptar su aproximación incluso cuando las relaciones en los datos no se distribuyen linealmente, al encontrar la separación óptima en un espacio transformado.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "✅ Supervisado",
  "✅ Categórica o Continua",
  "✅ Numéricas (categorías deben codificarse)",
  "✅ Capta relaciones no lineales (kernel)",
  "❌ No requiere",
  "✅ Idealmente sí",
  "❌ No es requisito",
  "⚠️ Sí, especialmente sin margen amplio",
  "✅ Puede manejarla bien",
  "❌ Baja (modelo es una caja negra)",
  "⚠️ Lento con muchos datos o predictores",
  "✅ Esencial para elegir kernel y parámetros",
  "❌ Datos con mucho ruido o solapamiento entre clases"
)
detalles <- c(
  "Modelo supervisado que maximiza el margen entre clases",
  "Clasificación binaria, multiclase o regresión (SVR)",
  "Requiere escalar o estandarizar las variables numéricas",
  "Puede usar kernel para resolver problemas no lineales",
  "No requiere supuestos clásicos como normalidad",
  "Mejor si los datos son independientes",
  "Puede usarse aunque haya heterocedasticidad",
  "Los outliers cercanos al margen afectan el modelo",
  "Los kernels pueden reducir el efecto de multicolinealidad",
  "Difícil de explicar; es un modelo de tipo caja negra",
  "Puede ser costoso computacionalmente con datos grandes",
  "Parámetros como C y gamma se ajustan vía validación cruzada",
  "No es ideal si hay ruido o datos mal etiquetados"
)
tabla_svm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_svm %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir SVM",
             subtitle = "Support Vector Machine (SVM) ") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

<!--chapter:end:01-regression.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 🌲 2. Árboles de Decisión y Derivados {-}   

**Ejemplos:** Árbol de Decisión, Random Forest, Gradient Boosting.   
**Uso:** Excelentes para **datos tabulares** con relaciones no lineales, incluyendo variables categóricas y numéricas. Son una buena opción cuando la **interpretabilidad** es clave.   
**Ventajas:** Pueden manejar diversos tipos de datos y, los árboles individuales, son **fáciles de interpretar**.   
**Limitaciones:** Los árboles simples pueden **sobreajustarse**, y su rendimiento baja con datos muy ruidosos si no se usan métodos de ensamble.   

---


## C4.5  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/C4.5.png"))
```

**C4.5** es una extensión del algoritmo **ID3**, también desarrollado por Ross Quinlan, y es uno de los algoritmos de **árboles de decisión** más influyentes y ampliamente utilizados para tareas de **clasificación**. Fue diseñado para abordar algunas de las limitaciones de su predecesor, ID3, y se ha convertido en un estándar de facto en el aprendizaje automático para construir modelos predictivos interpretables.

Al igual que ID3, C4.5 construye un árbol de clasificación seleccionando en cada nodo el atributo que mejor divide el conjunto de datos. Sin embargo, en lugar de usar solo la **ganancia de información**, C4.5 utiliza la **relación de ganancia** (Gain Ratio). La relación de ganancia normaliza la ganancia de información por la entropía intrínseca del atributo, lo que ayuda a mitigar el sesgo de ID3 hacia atributos con muchos valores. Además, C4.5 introduce varias mejoras significativas:

* **Manejo de atributos continuos:** Puede discretizar atributos numéricos continuos dividiendo el rango en intervalos.
* **Manejo de valores faltantes:** Puede manejar datos con valores ausentes asignando una probabilidad fraccionada a cada rama posible.
* **Poda del árbol:** Implementa una técnica de poda para reducir el sobreajuste, lo que implica eliminar ramas del árbol que no aportan significativamente a la clasificación o que representan ruido en los datos.

En el contexto del **aprendizaje global vs. local**, C4.5, al igual que ID3 y CART, opera como un sistema de **aprendizaje local**. La construcción del árbol se logra a través de decisiones de división que se optimizan localmente en cada nodo, buscando la máxima homogeneidad o pureza en los subconjuntos resultantes. Esto le permite a C4.5 manejar eficazmente relaciones no lineales entre las variables independientes y dependientes. La idea es que, si los datos no se distribuyen linealmente, el concepto de regresión (o clasificación) se puede aplicar de forma efectiva mediante esta **regresión ponderada localmente**, donde el algoritmo divide el problema de aprendizaje global en múltiples problemas de aprendizaje más pequeños y simples. Al centrarse en divisiones óptimas a nivel de subconjuntos, C4.5 ofrece una alternativa robusta a los métodos de aproximación de funciones globales, que a veces pueden fallar en proporcionar una buena aproximación cuando la relación entre las variables no es lineal.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica (clasificación)",
  "✅ Categóricas y numéricas",
  "✅ Captura relaciones no lineales",
  "❌ No requiere",
  "✅ Deseable, pero no obligatorio",
  "❌ No es relevante",
  "⚠️ Moderadamente (puede hacer overfitting con ruido)",
  "✅ Robusto a multicolinealidad",
  "✅ Alta (árbol interpretable)",
  "✅ Relativamente rápido",
  "✅ Recomendable para evitar sobreajuste",
  "❌ Demasiadas categorías o ruido en datos"
)

detalles <- c(
  "Modelo supervisado tipo árbol de decisión",
  "Clasifica variables categóricas en ramas lógicas",
  "Divide por puntos de corte para variables numéricas",
  "No asume forma funcional entre predictores y respuesta",
  "No necesita normalidad de errores",
  "Mejor si las observaciones son independientes",
  "No requiere varianzas constantes",
  "Datos ruidosos pueden afectar las ramas",
  "No se ve afectado por correlaciones entre predictores",
  "Salida fácil de visualizar y explicar",
  "Escala bien para tamaños de muestra medianos",
  "Evita sobreajuste con poda y validación",
  "Muchas clases con pocos datos pueden sobreajustar"
)

tabla_c45 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_c45 %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir C4.5") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## C5.0  {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/C5.0.png"))
```

**C5.0** es la versión más reciente y avanzada de los algoritmos de árboles de decisión desarrollados por Ross Quinlan, sucediendo a ID3 y C4.5. Es un algoritmo propietario (aunque se ofrece una versión de código abierto bajo ciertas licencias) y es ampliamente reconocido por su **rapidez**, **precisión** y **eficiencia** en la construcción de **árboles de decisión** y **reglas de clasificación** para tareas de **clasificación**.

Al igual que sus predecesores, C5.0 construye un árbol de clasificación mediante la división recursiva de los datos en subconjuntos más homogéneos. Sin embargo, C5.0 incorpora mejoras significativas que lo hacen superior en muchos aspectos:

* **Velocidad y eficiencia:** Es notablemente más rápido y más eficiente en el uso de memoria que C4.5, lo que le permite manejar conjuntos de datos mucho más grandes.
* **Impulso (Boosting):** C5.0 puede usar la técnica de **boosting** (específicamente, una variante de AdaBoost) para crear múltiples árboles de decisión y combinarlos para producir una predicción más robusta y precisa. Esto reduce significativamente los errores de clasificación y mejora la generalización.
* **Poda mejorada:** Ofrece técnicas de poda más sofisticadas para evitar el sobreajuste y producir árboles más pequeños y comprensibles.
* **Manejo de valores faltantes y atributos continuos:** Al igual que C4.5, maneja de manera efectiva valores faltantes y atributos numéricos continuos.
* **Generación de reglas:** Además de árboles de decisión, C5.0 puede generar conjuntos de **reglas de clasificación** concisas, que a menudo son más fáciles de interpretar que un árbol completo.

En el contexto de la **regresión localmente ponderada**, C5.0, como los demás algoritmos de árboles de decisión, opera bajo la premisa de un **aprendizaje local**. La construcción del árbol implica tomar decisiones de división óptimas en cada nodo, basándose en la información local de ese subconjunto de datos. Si los datos no se distribuyen linealmente, el concepto de regresión (o clasificación, que es su enfoque principal) se puede aplicar eficazmente al dividir el problema de aprendizaje global en múltiples problemas de aprendizaje más pequeños y simples. Cada división en el árbol se puede ver como una forma de **regresión ponderada localmente**, donde el algoritmo se enfoca en aproximar la relación dentro de un subespacio específico del conjunto de datos. Esto convierte a C5.0 en una potente alternativa a los métodos de aproximación de funciones globales, especialmente cuando la relación entre las variables independientes y dependientes no es lineal y se busca un modelo interpretable y robusto.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica (clasificación)",
  "✅ Categóricas y numéricas",
  "✅ Captura relaciones no lineales",
  "❌ No requiere",
  "✅ Deseable, pero no obligatorio",
  "❌ No es relevante",
  "⚠️ Moderadamente (puede generar ramas excesivas)",
  "✅ Robusto a multicolinealidad",
  "✅ Alta (árbol fácil de visualizar)",
  "✅ Relativamente rápido en training",
  "✅ Recomendable para evitar sobreajuste",
  "❌ Clases muy desbalanceadas sin ajuste"
)

detalles <- c(
  "Algoritmo de árbol de decisión avanzado basado en C4.5",
  "Clasifica en múltiples categorías (también multiclase)",
  "Divide automáticamente variables numéricas con puntos de corte",
  "No asume función lineal: usa ganancia de información y boosting",
  "No exige normalidad de errores",
  "Mejor si las instancias son independientes",
  "No requiere varianzas constantes",
  "Los valores extremos pueden influir en ramas profundas",
  "No se ve afectado por correlación alta entre predictores",
  "Salida clara con reglas y pesos de boosting",
  "Más rápido que C4.5 y con opciones de boosting",
  "Usar k-fold o repeated CV para determinar parámetros óptimos",
  "Muchos atributos irrelevantes pueden generar sobreajuste"
)

tabla_c50 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_c50 %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir C5.0") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

``` 


## Classification and Regression Tree (CART)  {-} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CART.png"))
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CART_1.png"))
```

**Classification and Regression Tree (CART)** es un método no paramétrico que se utiliza para construir **árboles de decisión** tanto para problemas de **clasificación** como de **regresión**. La idea central es dividir recursivamente el espacio de las características en regiones más pequeñas y manejables, creando así un modelo con forma de árbol que es fácil de interpretar.

A diferencia de los modelos lineales o algunos algoritmos de aprendizaje global, CART no asume una relación lineal entre las variables. En su lugar, el algoritmo identifica los mejores **puntos de división** en las variables predictoras para maximizar la **homogeneidad** de las respuestas dentro de cada región resultante. Para problemas de clasificación, esto se mide comúnmente con métricas como la **impureza Gini** o la **ganancia de información**, mientras que para la regresión, se busca minimizar la **suma de los cuadrados de los residuos**.

Mientras que muchos algoritmos (como las redes neuronales clásicas o las máquinas de vectores de soporte) son sistemas de **aprendizaje global** que buscan minimizar una función de pérdida única para todo el conjunto de datos, CART se puede considerar más como un sistema de **aprendizaje local**. Construye el modelo tomando decisiones de división locales en cada nodo del árbol, lo que le permite capturar relaciones complejas y no lineales en los datos. Esto es particularmente útil cuando una aproximación de función global única podría no ser suficiente para modelar la relación entre las variables. Una de las ventajas de CART es su capacidad para manejar diferentes tipos de datos (numéricos y categóricos) y su interpretabilidad, ya que la ruta desde la raíz hasta una hoja del árbol representa un conjunto de reglas de decisión.



```{r, echo =FALSE}

criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica o Continua",
  "✅ Numéricas y Categóricas",
  "✅ No lineal y con interacciones",
  "❌ No requiere",
  "⚠️ Puede verse afectado",
  "⚠️ No necesario pero deseable",
  "⚠️ Sí, en puntos de corte",
  "✅ No se ve afectado",
  "✅ Alta (gráfico del árbol)",
  "✅ Rápido en datasets medianos",
  "✅ Muy usado para poda y ajuste",
  "❌ Muy profundo (overfitting), datos muy ruidosos"
)

detalles <- c(
  "Algoritmo basado en divisiones binarias",
  "Puede predecir clases o valores continuos",
  "Acepta todo tipo de variables predictoras",
  "Captura relaciones complejas y no lineales",
  "No requiere distribución normal",
  "Idealmente los errores deben ser independientes",
  "La varianza constante mejora resultados",
  "Puede generar divisiones extremas por valores atípicos",
  "No necesita preocuparse por colinealidad",
  "Fácil de entender, especialmente con árboles pequeños",
  "Escalable pero no óptimo en grandes volúmenes sin poda",
  "Usa poda y validación cruzada para evitar sobreajuste",
  "Tiende al sobreajuste si no se poda o se regulariza"
)

tabla_cart <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

require(gt) 

tabla_cart %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir CART",
             subtitle = "Classification and Regression Tree (CART)") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Chi-squared Automatic Interaction Detection (CHAID)  {-}    

```{r echo=FALSE, fig.show="hold", out.width="48%", fig.cap="https://select-statistics.co.uk/blog/chaid-chi-square-automatic-interaction-detector/"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CHAID.png"))
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CHAID_1.png"))
```



**Chi-squared Automatic Interaction Detection (CHAID)** es un algoritmo de **árboles de decisión** utilizado principalmente para tareas de **clasificación** y, en menor medida, para la **regresión** (aunque se aplica más comúnmente a variables dependientes categóricas). La idea fundamental de CHAID es construir un árbol de decisión al encontrar las mejores divisiones en las variables predictoras que maximicen la significancia estadística de la relación con la variable dependiente.

A diferencia de ID3, C4.5 o CART, que utilizan medidas de impureza como la entropía o el índice Gini, CHAID se basa en pruebas estadísticas de **chi-cuadrado ($\chi^2$)** para identificar las divisiones óptimas. Cuando la variable dependiente es nominal o ordinal, CHAID evalúa cada variable predictora para encontrar la combinación de categorías que sea más significativamente diferente de otras combinaciones en términos de la distribución de la variable dependiente. El algoritmo fusiona las categorías de una variable predictora si no son significativamente diferentes, y luego selecciona la variable predictora y la división que resultan en el valor más bajo de $p$ (es decir, la mayor significancia estadística) de la prueba $\chi^2$. Para variables dependientes continuas, se utiliza una prueba F.

En el contexto del **aprendizaje global vs. local**, CHAID opera como un sistema de **aprendizaje local**. La construcción del árbol es un proceso iterativo y recursivo donde las decisiones de división se toman en cada nodo basándose en la significancia estadística local de la interacción entre las variables predictoras y la variable dependiente. Esto le permite a CHAID descubrir relaciones complejas y no lineales en los datos. La idea es que, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresión (o clasificación) de manera efectiva mediante lo que se denomina **regresión ponderada localmente**. Esto se logra al dividir el problema de aprendizaje global en múltiples problemas de aprendizaje más pequeños y simples, donde cada rama del árbol representa una región del espacio de características donde las interacciones son evaluadas y modeladas localmente. Esto hace de CHAID una alternativa robusta a los métodos de aproximación de funciones globales, especialmente cuando se busca un modelo interpretable y se quieren identificar las interacciones entre las variables de una manera estadísticamente rigurosa.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica (clasificación multinivel)",
  "✅ Categóricas nativas (numéricas requieren binarización o discretización)",
  "✅ No lineal (explora interacciones automáticas con χ²)",
  "❌ No requiere",
  "✅ Deseable, pero no obligatorio",
  "❌ No aplica",
  "⚠️ Moderadamente (outliers categóricos pueden crear nodos muy pequeños)",
  "✅ Robusto a multicolinealidad (usa χ², no varianzas)",
  "✅ Media (árboles con muchos nodos pueden resultar complejos)",
  "⚠️ Razonablemente rápido en datasets moderados, lento si hay muy altas cardinalidades",
  "✅ Recomendable para determinar profundidad y grado de interacción",
  "❌ Variable objetivo continua o muchos niveles con pocas observaciones"
)

detalles <- c(
  "Construye un árbol de decisión usando pruebas χ² para detectar interacciones entre predictores y variable objetivo.",
  "Diseñado para clasificar en múltiples categorías sin orden; puede manejar targets con más de dos niveles.",
  "Funciona mejor con predictores categóricos; las variables numéricas deben transformarse en categorías mediante binning.",
  "No asume ninguna forma funcional; detecta automáticamente relaciones complejas basadas en χ².",
  "No depende de supuestos de normalidad de errores ni de forma de distribución de residuos.",
  "Las instancias deben ser independientes; no es ideal para datos con fuerte dependencia temporal sin procesar.",
  "Homoscedasticidad no se evalúa, ya que no se basa en un término de error paramétrico como OLS.",
  "Los valores extremos en variables categóricas con pocas observaciones pueden crear ramas muy específicas, pero CHAID maneja cardinalidades moderadas.",
  "Al basarse en χ², CHAID no se ve afectado directamente por colinealidad, aunque variables muy correlacionadas pueden crear redundancia en las divisiones.",
  "Cada división se basa en pruebas de χ²; el árbol resultante puede interpretarse visualmente, pero muchos niveles pueden reducir claridad.",
  "La creación recursiva de nodos por agrupación de categorías es eficiente para conjuntos de datos moderados; puede volverse lento si hay muchas categorías de predictores.",
  "Se usa validación cruzada para podar el árbol y elegir el nivel óptimo de interacción, equilibrando sesgo y varianza.",
  "No es adecuado si la variable objetivo es continua (sin discretizar) o si hay demasiados niveles con muy pocos casos en cada uno."
)

tabla_chaid <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_chaid %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir CHAID",
             subtitle = "Chi-squared Automatic Interaction Detection (CHAID)") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Conditional Decision Trees (Conditional Inference Trees - CITs)  {-}   

**Conditional Decision Trees**, often referred to as **Conditional Inference Trees (CITs)**, represent a class of **árboles de decisión** que abordan una limitación importante de los algoritmos de árboles de decisión tradicionales como CART, ID3, y C4.5: el **sesgo en la selección de variables**. Mientras que los algoritmos tradicionales pueden favorecer variables predictoras con muchas categorías o valores continuos (debido a que estas variables tienen más "oportunidades" de generar una división que parezca óptima), los CITs emplean un enfoque basado en **pruebas estadísticas** para seleccionar la mejor división.

La idea fundamental de los Conditional Decision Trees es que cada división en el árbol se basa en la **significancia estadística** de la asociación entre las variables predictoras y la variable de respuesta. En lugar de seleccionar el atributo que maximiza una medida de impureza (como la ganancia de información o la impureza Gini), los CITs realizan una serie de **pruebas de inferencia condicional** (típicamente **pruebas de permutación**).

El algoritmo opera de la siguiente manera:
1.  En cada nodo, se evalúa una **hipótesis nula** de independencia entre cada variable predictora y la variable de respuesta.
2.  Se calcula el valor de $p$ para cada variable predictora.
3.  La variable predictora con el valor de $p$ más pequeño (es decir, la asociación más estadísticamente significativa) es seleccionada para la división, siempre y cuando este valor de $p$ sea menor que un umbral de significancia predefinido.
4.  Una vez seleccionada la variable, se encuentra la mejor división binaria (generalmente) dentro de esa variable para ese nodo.
5.  Este proceso se repite recursivamente hasta que no haya más variables significativas para dividir o se alcance un criterio de parada.

En el contexto del **aprendizaje global vs. local**, los Conditional Decision Trees se pueden considerar como un enfoque de **aprendizaje local** con un fuerte respaldo estadístico. Aunque el árbol resultante es un modelo global, cada decisión de división se toma localmente basándose en la inferencia estadística sobre la relación entre las variables en ese subconjunto de datos. Esto significa que si los datos no se distribuyen linealmente, el concepto de regresión (o clasificación) se aplica de forma efectiva mediante lo que se denomina **regresión ponderada localmente**. Al utilizar pruebas de significancia para las divisiones, los CITs evitan el problema de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en una única aproximación global, ya que las divisiones se determinan por la evidencia estadística local. Esto los convierte en una alternativa robusta que ofrece una selección de variables menos sesgada y modelos con una mayor interpretabilidad estadística.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Continua (regresión) o Categórica (clasificación)",
  "✅ Numéricas y categóricas",
  "✅ No lineal, usa tests condicionales para particionar",
  "❌ No requiere",
  "✅ Deseable, pero no obligatorio",
  "❌ No relevante",
  "⚠️ Moderadamente (consume tests basados en permutaciones)",
  "✅ Robusto a colinealidad",
  "✅ Alta (cada división está basada en criterios estadísticos claros)",
  "⚠️ Más lento que CART en datasets grandes",
  "✅ Recomendable para podar y evitar sobreajuste",
  "❌ Datos muy pequeños por nodo o variables irrelevantes"
)

detalles <- c(
  "Construye árboles basados en test de independencia condicional (ctree).",
  "Permite tanto regresión (valor continuo) como clasificación multinivel.",
  "Acepta variables numéricas y categóricas sin necesidad de dummies.",
  "Detecta relaciones complejas y no lineales usando tests basados en permutaciones.",
  "No exige que los residuos sigan una distribución específica.",
  "Ideal si las observaciones no están correlacionadas en el tiempo.",
  "No requiere homoscedasticidad porque no se basa en un modelo paramétrico de error.",
  "Los outliers pueden afectar el cálculo de los tests, aunque es más robusto que CART.",
  "El algoritmo ctree no se ve afectado por predictores altamente correlacionados.",
  "Los árboles generados son fáciles de visualizar y explicar.",
  "Para cada división realiza múltiples tests, por lo que es más lento en datos muy grandes.",
  "Usar k-fold o repeated CV para elegir la profundidad y evitar sobreajuste.",
  "No es apto si tienes muy pocas observaciones en cada parto o muchas variables irrelevantes."
)

tabla_ctree <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_ctree %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir ctree",
             subtitle = "Conditional Decision Trees") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```

## Cubist  {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Cubist.png"))
```

**Cubist** es un algoritmo de **Machine Learning** desarrollado por RuleQuest Research (autores de C4.5 y See5/C5.0), principalmente para tareas de **regresión**. Es una extensión de los modelos de **árboles de decisión** que combina la simplicidad de las reglas con la precisión de los modelos locales, lo que lo hace muy potente para datos complejos con muchas características.

En esencia, Cubist construye un **modelo de reglas con un modelo lineal adjunto a cada regla**. Opera en dos fases principales:

1.  **Construcción del Árbol de Reglas:**
    * Similar a un árbol de decisión, Cubist construye una estructura de árbol dividiendo los datos en subconjuntos basados en los valores de las características.
    * Sin embargo, en lugar de hojas que contienen un valor constante (como en los árboles de regresión tradicionales), cada hoja de este árbol se transforma en un **conjunto de reglas**.
    * A cada regla se le asocia un **modelo lineal multivariado local** (o un "modelo de comité" de reglas, donde varias reglas contribuyen a la predicción). Este modelo lineal se entrena solo con los datos que satisfacen las condiciones de esa regla.

2.  **Ajuste del Modelo de Reglas y Predicción:**
    * Para cada nueva instancia de predicción, Cubist identifica las reglas que se aplican a esa instancia.
    * La predicción final se calcula combinando las predicciones de los modelos lineales de las reglas que se aplican, y luego se ajusta un poco esa predicción mediante un **"comité" de vecinos** (ajustes locales adicionales basados en ejemplos similares), si está configurado para ello. Esta etapa de ajuste lo hace aún más robusto.

Cubist es valorado por su capacidad para manejar **relaciones complejas y no lineales** en los datos. Proporciona un modelo que es más interpretable que una "caja negra" (como una red neuronal profunda) debido a su base en reglas, pero mucho más preciso que los modelos lineales o los árboles de regresión simples, gracias a sus modelos lineales locales y ajustes.


**Aprendizaje Global vs. Local:**

Cubist es un algoritmo que combina de manera muy efectiva aspectos de **aprendizaje global y local**.

* **Aspecto Global (Estructura de Reglas):** La fase de construcción del árbol y la derivación de las reglas crean una **estructura global** que divide el espacio de características. Este conjunto de reglas abarca todo el dominio de los datos y determina qué modelo local se aplicará a una instancia. Es una forma de particionar el espacio de características de manera jerárquica para establecer un marco de predicción general.

* **Aspecto Local (Modelos Lineales y Ajustes):** Aquí es donde Cubist brilla en su capacidad de aprendizaje local:
    * **Modelos Lineales Locales:** Cada regla tiene asociado un **modelo lineal que se entrena solo con los datos que caen dentro de esa regla**. Esto permite a Cubist capturar **relaciones locales y no lineales** de manera precisa. En lugar de una única relación lineal global, el modelo se adapta a las particularidades de diferentes subregiones de los datos.
    * **Ajuste Basado en Vecinos:** Si se activa la opción de "comité" o el ajuste basado en vecinos (conocido como `committees` o `neighbors`), el modelo refina aún más su predicción incorporando la información de los ejemplos de entrenamiento más cercanos al punto de consulta. Esto es una forma de **"regresión ponderada localmente"**, donde la predicción final se ajusta en función de los patrones observados en el vecindario inmediato del punto de interés.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (regresión basada en reglas)",
  "✅ Numérica (regresión)",
  "✅ Numéricas y categóricas",
  "✅ Modelo aditivo basado en reglas y ajustes lineales locales",
  "⚠️ Requiere análisis de residuos, no siempre normalidad estricta",
  "⚠️ Asume independencia, como otros modelos supervisados",
  "⚠️ Puede tener heteroscedasticidad",
  "⚠️ Moderadamente sensible a outliers",
  "⚠️ Puede manejar correlación, pero multicolinealidad puede afectar interpretabilidad",
  "✅ Moderada: reglas explican el modelo, pero menos transparente que modelos lineales",
  "⚠️ Relativamente rápido, pero depende del número de reglas",
  "✅ Compatible con validación cruzada para evaluar rendimiento",
  "❌ No funciona bien con datos muy pequeños o ruido extremo"
)

detalles <- c(
  "Combina técnicas de árboles de decisión con modelos lineales locales para predicción precisa.",
  "Predice variables continuas mediante reglas que dividen el espacio y ajustes lineales en cada región.",
  "Puede manejar variables predictoras mixtas (numéricas y categóricas).",
  "Modelo flexible que ajusta múltiples reglas para capturar relaciones no lineales y locales.",
  "Evaluar residuos para verificar supuestos; no es tan rígido como OLS.",
  "Como modelo supervisado, se espera independencia entre observaciones.",
  "Puede tolerar algo de heteroscedasticidad, pero afecta precisión de intervalos.",
  "Outliers pueden afectar algunas reglas locales y coeficientes.",
  "Multicolinealidad puede dificultar interpretación de coeficientes locales.",
  "Las reglas pueden interpretarse, pero el modelo global puede ser complejo.",
  "La velocidad depende del tamaño del dataset y número de reglas generadas.",
  "Se usa validación cruzada para seleccionar parámetros y validar modelo.",
  "No es ideal para datasets muy pequeños o con mucho ruido no estructurado."
)

tabla_cubist <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_cubist %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir cubist",
             subtitle = "Cubist")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Decision Stump  {-} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Decision Stump.png"))
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Decision Stump_1.png"))
```

Un **Decision Stump** es el tipo de **árbol de decisión** más simple y fundamental, compuesto por un **único nodo de decisión (la raíz)** que se conecta directamente a los **nodos hoja**. La idea es que un *decision stump* toma una decisión de clasificación o regresión basándose en una sola característica o atributo de entrada.

Aunque parece demasiado simple, la lógica es que, a pesar de su simplicidad, un *decision stump* identifica el mejor umbral o categoría dentro de una única variable para separar los datos de la manera más efectiva posible. Para problemas de clasificación, esto significa encontrar la característica que, por sí sola, maximice alguna medida de **pureza** (como la ganancia de información, la impureza Gini, o la significancia chi-cuadrado) o minimice el error de clasificación. Para regresión, buscará el punto de división en una sola característica que minimice la suma de los cuadrados de los errores.

En el contexto del **aprendizaje local vs. global**, un *decision stump* es inherentemente un sistema de **aprendizaje local**. Su "aprendizaje" se limita a encontrar la mejor división dentro de una única variable, lo que es una forma extrema de **regresión ponderada localmente**. Si los datos no se distribuyen linealmente, un *decision stump* no puede por sí mismo modelar relaciones complejas. Sin embargo, su valor no reside en ser un modelo predictivo robusto por sí mismo, sino en ser un **"clasificador débil"** o **"regresor débil"** que puede ser combinado en **conjuntos de modelos (ensembles)** más potentes. Por ejemplo, los *decision stumps* son los bloques de construcción más comunes para algoritmos de **boosting** como **AdaBoost**. En estos casos, múltiples *decision stumps* se entrenan secuencialmente, cada uno enfocándose en los errores que cometieron los *stumps* anteriores, sumando sus "aprendizajes locales" para formar un modelo global más preciso. Esto contrarresta la limitación de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica (clasificación) o Continua (regresión simplificada)",
  "✅ Numéricas y/o categóricas",
  "⚠️ Captura solo una división (muy simple, un solo nodo interno)",
  "❌ No requiere",
  "✅ Deseable, pero no obligatorio",
  "❌ No relevante",
  "✅ Relativamente robusto (poca complejidad)",
  "✅ Ignora colinealidad (usa solo una variable)",
  "✅ Muy alta (un solo umbral para dividir)",
  "✅ Extremadamente rápido",
  "✅ Se puede usar k-fold para evaluar estabilidad",
  "❌ No funciona bien si la relación es compleja o no hay un buen umbral único"
)

detalles <- c(
  "Modelo de árbol con un solo nivel de decisión (un umbral en una sola variable).",
  "En clasificación predice una clase binaria; en regresión, un valor medio para cada división.",
  "Selecciona la mejor variable con el punto de corte que maximiza ganancia (clasificación) o reduce varianza (regresión).",
  "Solo ajusta un umbral, por lo que no modela interacciones ni no linealidades complejas.",
  "No hay supuestos paramétricos de distribución de errores.",
  "Mejor si las instancias no están correlacionadas (por ejemplo, no aplica a series de tiempo sin agrupar).",
  "La varianza constante no se evalúa, pues el modelo es no paramétrico y muy simple.",
  "Un solo punto de corte es menos sensible a outliers en comparación con árboles profundos, pero aún puede verse afectado si un outlier define el umbral.",
  "Como solo usa una variable, no se ve afectado por correlaciones altas entre predictores.",
  "El modelo entero es resumido en un único umbral; fácil de explicar.",
  "Muy rápido de entrenar y predecir, pues solo se evalúa un umbral en un predictor.",
  "Es útil para comprobar si hay una única variable con gran poder predictivo; k-fold ayuda a validar que el umbral se mantenga estable.",
  "No sirve si el problema requiere varias divisiones, interacciones o relaciones no lineales profundas."
)

tabla_stump <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_stump %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir Decision Stump",
             subtitle = "Decision Stump") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Guided Trees / Hybrid Trees {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Hybrid Trees.png"))
```

## Iterative Dichotomiser 3 (ID3)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/ID3.png"))
```

**Iterative Dichotomiser 3 (ID3)** es un algoritmo clásico para construir **árboles de decisión**, diseñado principalmente para tareas de **clasificación**. Fue uno de los primeros algoritmos de árboles de decisión desarrollados por Ross Quinlan. La idea central de ID3 es construir un árbol de clasificación seleccionando en cada nodo del árbol el atributo que mejor divide el conjunto de datos en subconjuntos más puros y homogéneos.

ID3 opera de forma **iterativa** y **dicotómica** (aunque puede manejar atributos con más de dos categorías), dividiendo el conjunto de datos en cada paso basándose en el atributo más informativo. La selección del "mejor" atributo se basa en métricas de **teoría de la información**, principalmente la **ganancia de información** (Information Gain). La ganancia de información mide la reducción en la **entropía** (una medida de la impureza o desorden de un conjunto de datos) que se logra al dividir los datos según un atributo particular. El atributo con la mayor ganancia de información es elegido como el nodo de decisión en cada nivel del árbol.

A diferencia de los sistemas de aprendizaje global que buscan minimizar funciones de pérdida globales (como el error cuadrático medio), ID3 es un algoritmo de **aprendizaje local** en el sentido de que toma decisiones de división óptimas en cada nodo basándose en la información disponible en ese subconjunto de datos. Aunque la construcción del árbol es un proceso global, cada paso de la división se optimiza localmente para maximizar la pureza de los subconjuntos resultantes. Esto le permite a ID3 capturar relaciones no lineales entre las variables, ya que no asume una distribución lineal de los datos. En esencia, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresión (o clasificación, en este caso) de manera ponderada localmente al dividir el espacio de características en regiones más manejables. Sin embargo, una desventaja de ID3 es que tiende a favorecer atributos con muchos valores y puede ser propenso al sobreajuste.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica (clasificación)",
  "✅ Principalmente categóricas (numéricas requieren discretización)",
  "✅ No lineal (basado en ganancia de información)",
  "❌ No requiere",
  "✅ Deseable, pero no obligatorio",
  "❌ No aplica",
  "⚠️ Moderadamente (valores atípicos pueden generar ramas poco representativas)",
  "✅ Robusto a multicolinealidad",
  "✅ Alta (árbol simple de interpretar)",
  "✅ Rápido con datos moderados y discretizados",
  "✅ Recomendable para equilibrar datos y evitar overfitting",
  "❌ Respuesta continua, muchos valores faltantes o ruido elevado"
)

detalles <- c(
  "Construye un árbol de decisión dividiendo por ganancia de información (entropía).",
  "Clasifica muestras en categorías discretas, ej. Sí/No, A/B/C.",
  "Mejor con variables categóricas nativas; las numéricas deben transformarse en rangos.",
  "No asume ninguna relación funcional: usa particiones basadas en criterios de información.",
  "No hay residuos en el sentido paramétrico; no exige distribución normal.",
  "Las instancias deben ser independientes; no orientado a series temporales.",
  "No requiere varianzas constantes porque no hay término de error paramétrico.",
  "Los outliers categóricos pueden crear nodos muy pequeños no representativos.",
  "ID3 ignora correlaciones altas, pero demasiadas variables correlacionadas pueden ralentizar la búsqueda de mejores divisiones.",
  "Cada nodo muestra la regla de división; el árbol global es fácil de visualizar para pocos niveles.",
  "La construcción recursiva es eficiente para datos discretizados; se vuelve lento si hay muchas categorías o atributos.",
  "Se usa para podar y seleccionar profundidad óptima del árbol, equilibrando sesgo y varianza.",
  "No es recomendable si la variable objetivo es continua o si hay mucho ruido sin transformar."
)

tabla_id3 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_id3 %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir IDE3",
             subtitle = "Iterative Dichotomiser 3 (ID3)") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## M5 (Model Tree) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/M5 (Model Tree).png"))
```

**M5**, a menudo referida como **M5'** o **M5P** (su implementación en el software Weka), es un algoritmo de **árboles de decisión** específicamente diseñado para **tareas de regresión**, es decir, para predecir valores numéricos continuos. Desarrollado por Ross Quinlan en 1992 y luego mejorado por Wang y Witten en 1997, M5 se destaca de los árboles de regresión tradicionales (como los de CART que solo tienen valores constantes en las hojas) al incorporar **modelos de regresión lineal** en sus nodos hoja.

La idea fundamental de M5 es combinar la interpretabilidad de un árbol de decisión con la capacidad predictiva de los modelos de regresión lineal. Funciona en dos etapas principales:

1.  **Construcción del Árbol:** M5 construye un árbol de decisión de forma recursiva, similar a otros algoritmos de árboles. Sin embargo, en lugar de usar medidas de impureza para clasificación, utiliza la **reducción de la desviación estándar (SDR)** como criterio de división. El algoritmo selecciona el atributo y el punto de división que maximizan la reducción de la desviación estándar del valor objetivo en los subconjuntos resultantes. Este proceso continúa hasta que el número de instancias en un nodo es muy pequeño o la desviación estándar es muy baja.

2.  **Poda y Suavizado:** Una vez construido el árbol inicial, M5 lo **poda** para evitar el sobreajuste. En lugar de reemplazar los nodos con un valor constante, los nodos hoja (y a veces nodos internos) son reemplazados por **modelos de regresión lineal multivariados**. Estos modelos lineales se construyen utilizando los atributos relevantes para esa rama del árbol. Además, M5 aplica un proceso de **suavizado** para compensar las discontinuidades bruscas que podrían surgir entre las predicciones de modelos lineales adyacentes. Este suavizado ajusta el valor predicho en una hoja basándose en las predicciones de los modelos en los nodos a lo largo de la ruta desde la raíz hasta esa hoja.

En el contexto del **aprendizaje global vs. local**, M5 es un híbrido interesante. Por un lado, la construcción del árbol se basa en decisiones de división **locales**, buscando la mejor reducción de la desviación estándar en cada nodo. Esto permite a M5 modelar relaciones no lineales, ya que "si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresión de manera ponderada localmente". El árbol divide el problema de regresión global en múltiples subproblemas más pequeños. Por otro lado, al tener **modelos de regresión lineal** en las hojas, M5 incorpora un componente de **aproximación de función local** más sofisticado que un simple valor constante. Estos modelos lineales son "locales" para la región de datos que representa esa hoja, pero internamente son modelos globales para esa subregión. Esto permite a M5 ofrecer una alternativa potente a las aproximaciones de funciones puramente globales, especialmente cuando las relaciones entre las variables son complejas y se benefician de una combinación de particionamiento del espacio y modelado lineal dentro de esas particiones.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Continua",
  "✅ Numéricas (categóricas procesar como dummies)",
  "✅ Lineal por segmentos (árbol + regresión en hojas)",
  "❌ No requiere estrictamente",
  "✅ Deseable, pero no obligatorio",
  "❌ No es requisito",
  "⚠️ Moderadamente (outliers pueden distorsionar regresiones locales)",
  "✅ Relativamente robusto (regresión en hojas mitiga algo la colinealidad)",
  "⚠️ Media (árbol complejo, hojas lineales más interpretables)",
  "⚠️ Moderado (depende de número de nodos y atributos)",
  "✅ Recomendable para optimizar número de nodos y hojas",
  "❌ Muchos nodos con pocos casos o ruido elevado"
)

detalles <- c(
  "Modelo de árbol de regresión con ajustes lineales en cada hoja.",
  "Predice valores continuos, p. ej., precio, consumo, etc.",
  "Requiere que variables categóricas se conviertan a indicadores antes de ajuste.",
  "Combina particiones basadas en atributos con regresiones múltiples en hojas.",
  "No exige que los residuos en cada hoja sean normales, aunque mejora inferencia.",
  "Ideal si las observaciones son independientes; en series de tiempo hay que agrupar.",
  "La varianza constante no es crítica, cada hoja ajusta localmente.",
  "Los extremos pueden afectar las regresiones locales; poda puede mitigar esto.",
  "El método divide el espacio antes de ajustar, reduciendo efectos de colinealidad.",
  "El árbol completo puede ser grande, pero cada hoja contiene una función lineal clara.",
  "Construcción y poda del árbol más costosas que OLS, pero razonables para tamaños medianos.",
  "Ayuda a determinar número óptimo de hojas y complejidad del árbol.",
  "Si hay muy pocas observaciones por hoja o ruido demasiado alto, las regresiones locales fallan."
)

tabla_m5 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_m5 %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir M5",
             subtitle = "M5 model tree algorithm") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Oblique Decision Trees {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Oblique Decision Trees.png"))
```

## Specific Implementations/Libraries {-}  


<!--chapter:end:02-decision_tree.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 🌟 3. Métodos de Ensamble {-}   

**Ejemplos:** Random Forest, Gradient Boosting (XGBoost, LightGBM, AdaBoost).   
**Uso:** Excelentes para **clasificación y regresión en datos tabulares**, especialmente en **competencias de datos** por su alto rendimiento.   
**Ventajas:** Ofrecen **alta precisión** y son muy **robustos**.   
**Limitaciones:** Pueden ser **difíciles de interpretar** y suelen ser **computacionalmente más costosos**.   

---

## Adaptive Boosting (AdaBoost)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/AdaBoost.png"))
```


**AdaBoost (Adaptive Boosting)** es uno de los algoritmos de **boosting** más influyentes y el primero en ser propuesto con éxito, desarrollado por Yoav Freund y Robert Schapire en 1995. Es una técnica de **aprendizaje conjunto (ensemble learning)** utilizada principalmente para **clasificación**, aunque sus principios pueden extenderse a la regresión. La idea fundamental de AdaBoost es construir un modelo fuerte combinando secuencialmente las predicciones de múltiples **clasificadores "débiles" o "base"**, y lo hace prestando más atención a los ejemplos que los modelos anteriores clasificaron incorrectamente.

El funcionamiento de AdaBoost se basa en un sistema de **re-ponderación de datos** en cada iteración:

1.  **Inicialización de Pesos:** Se asigna un peso inicial igual a cada ejemplo de entrenamiento.
2.  **Entrenamiento del Clasificador Débil:** En cada iteración, se entrena un clasificador débil (a menudo un **Decision Stump**, que es un árbol de decisión de un solo nivel) en el conjunto de datos actual. Este clasificador se enfoca en minimizar el error ponderado.
3.  **Cálculo del Error Ponderado:** Se calcula el error del clasificador débil, teniendo en cuenta los pesos de los ejemplos. Los ejemplos mal clasificados tienen un mayor impacto en este error.
4.  **Actualización de Pesos de Datos:** Los pesos de los ejemplos mal clasificados por el clasificador actual son **aumentados**, mientras que los pesos de los ejemplos correctamente clasificados son **disminuidos**. Esto asegura que el siguiente clasificador débil se enfoque más en los ejemplos que son difíciles de clasificar.
5.  **Cálculo del Peso del Clasificador:** Se asigna un peso (o "contribución") al clasificador débil actual en función de su precisión. Los clasificadores más precisos reciben un peso mayor en la predicción final del conjunto.
6.  **Combinación de Predicciones:** Las predicciones finales del modelo AdaBoost se obtienen mediante una **suma ponderada** de las predicciones de todos los clasificadores débiles.

En el contexto del **aprendizaje global vs. local**, AdaBoost es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada clasificador débil que se entrena en una iteración puede verse como una forma de **regresión ponderada localmente** (o, más precisamente, clasificación ponderada localmente), ya que ajusta su enfoque basándose en los ejemplos que el modelo combinado anterior no pudo clasificar bien. Al iterar y ajustar los pesos de los datos, AdaBoost se enfoca progresivamente en las regiones del espacio de características donde el modelo actual tiene un rendimiento deficiente. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de clasificación (y por extensión, las ideas de regresión) de manera altamente adaptativa. La capacidad de AdaBoost para concentrarse en los "errores" más difíciles aborda directamente la desventaja de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo. El resultado es un clasificador global muy preciso y robusto, capaz de modelar relaciones complejas y no lineales, que es una combinación ponderada de muchas decisiones locales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica (clasificación) o Continua (regresión adaptada)",
  "✅ Numéricas y categóricas (requiere codificación para algunas implementaciones)",
  "✅ Captura no linealidades e interacciones mediante reponderación iterativa",
  "❌ No requiere supuestos de normalidad en los residuos",
  "✅ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "❌ No asume homoscedasticidad",
  "⚠️ Moderadamente (outliers pueden obtener demasiado peso durante iteraciones)",
  "✅ Robusto (reduce colinealidad al iterar sobre subconjuntos ponderados)",
  "⚠️ Baja (modelo resultante es especie de ’caja negra’)",
  "⚠️ Lento con muchas iteraciones o datos grandes",
  "✅ Recomendable para ajustar tasa de aprendizaje y número de iteraciones",
  "❌ No es ideal con datos muy ruidosos o clases extremadamente desbalanceadas sin técnicas adicionales"
)

detalles <- c(
  "Ensamble supervisado que combina varios modelos débiles (ej. árboles simples) ajustando pesos según errores anteriores.",
  "En clasificación ajusta pesos para mal clasificados; en regresión, adapta predicción por minimización de pérdida.",
  "Puede trabajar con datos mixtos; para variables categóricas suele usar codificación de dummies.",
  "Cada iteración repondera observaciones difíciles, enfocándose en patrones que previos modelos no capturaron.",
  "No impone distribución de errores; se basa en función de pérdida, no en supuestos paramétricos.",
  "Funciona mejor si cada observación es independiente; sensible a dependencias temporales si no se corrige.",
  "No requiere varianza constante, ya que funciona sobre el error iterativo en lugar de residuos tradicionales.",
  "Outliers difíciles de clasificar tienden a recibir mayor peso, lo que puede sesgar el ensamble si no se controla el learning rate.",
  "La reponderación de muestras atenúa el efecto de predictores correlacionados, pues cada iteración puede focalizarse en subconjuntos distintos.",
  "Es complejo desentrañar la contribución de cada modelo débil; se pueden usar métricas de importancia o SHAP para interpretación.",
  "Cada iteración entrena un modelo débil; muchas iteraciones o modelos complejos pueden ralentizar el entrenamiento.",
  "K-fold o repeated CV ayudan a elegir tasa de aprendizaje (learning rate) y número de iteraciones (trials).",
  "No conviene con instancias altamente ruidosas: se puede sobreajustar rápidamente si la tasa de aprendizaje es alta o no se regula iteraciones."
)

tabla_adaboost <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_adaboost %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir adaboost",
             subtitle = "AdaBoost") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Boosting  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Boosting.png"))
```

**Boosting** es una técnica de **aprendizaje conjunto (ensemble learning)** que busca transformar un conjunto de **modelos "débiles" o "base"** en un **modelo "fuerte" o "preciso"**. La idea fundamental es construir modelos de forma **secuencial** e **iterativa**, donde cada nuevo modelo se centra en corregir los errores o deficiencias de los modelos construidos en las iteraciones anteriores. A diferencia del *bagging* (como en Random Forest), donde los modelos se entrenan de forma independiente, el *boosting* es intrínsecamente secuencial y adaptativo.

El concepto clave de Boosting radica en la asignación de **pesos** o en el enfoque en los **errores residuales**:

1.  **Iteraciones Secuenciales:** El proceso comienza con un modelo base inicial (a menudo simple, como un *decision stump*).
2.  **Enfoque en los Errores:** En cada iteración subsiguiente, el algoritmo presta más atención a los ejemplos que fueron clasificados (o predichos) incorrectamente por los modelos anteriores, o a los errores residuales no explicados. Esto se logra ya sea **re-ponderando** los datos (dando más peso a los ejemplos mal clasificados) o **ajustando** el nuevo modelo para que prediga los residuos de los modelos anteriores.
3.  **Combinación Ponderada:** Las predicciones de todos los modelos débiles se combinan, generalmente a través de una suma ponderada, donde los modelos más precisos reciben un mayor peso en la predicción final.

La fuerza del boosting radica en su capacidad para reducir el **sesgo** y la **varianza** del modelo final, al construir un modelo complejo a partir de componentes simples que se complementan entre sí.

En el contexto del **aprendizaje global vs. local**, Boosting es una estrategia de **aprendizaje global** que opera construyendo una serie de aproximaciones **locales**. Cada modelo "débil" que se entrena en una iteración puede verse como una forma de **regresión ponderada localmente** (o clasificación ponderada localmente), ya que se enfoca en una parte específica del espacio de las características o en los datos con mayor error. El proceso iterativo de Boosting busca corregir estos errores localizados. Si los datos no se distribuyen linealmente, el boosting permite que el concepto de regresión (o clasificación) se aplique de manera muy flexible y potente. La capacidad de concentrarse en los "errores" residuales aborda directamente la desventaja de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo. Al ensamblar muchos modelos débiles que se adaptan a los errores de los anteriores, Boosting construye un modelo final que es una aproximación de función global altamente adaptable y precisa. Algoritmos como AdaBoost y Gradient Boosting Machines (GBM) son ejemplos prominentes de esta técnica.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica (clasificación) o Continua (regresión)",
  "✅ Numéricas y categóricas (requiere codificación para algunas implementaciones)",
  "✅ Captura no linealidades e interacciones complejas mediante aprendizaje secuencial",
  "❌ No requiere supuestos de normalidad",
  "✅ Deseable, aunque no obligatorio para muchos algoritmos",
  "❌ No se asume homoscedasticidad",
  "⚠️ Moderadamente (puede ajustar demasiado a outliers si no se controla)",
  "✅ Robusto (cada iteración utiliza un subconjunto ponderado de datos)",
  "⚠️ Baja (modelo en su conjunto es tipo caja negra)",
  "⚠️ Lento con muchos árboles o altas iteraciones",
  "✅ Recomendable con k-fold o repeated CV para ajustar tasa de aprendizaje y número de iteraciones",
  "❌ Si se tienen pocos datos, alto ruido o target muy desbalanceado sin ajuste"
)

detalles <- c(
  "Ensamble supervisado que combina varios modelos débiles (ej. árboles pequeños) de forma secuencial",
  "En clasificación se usan votaciones ponderadas; en regresión se suman predicciones graduadas",
  "Acepta variables mixtas; algunas bibliotecas requieren convertir categóricas en dummies",
  "Construye modelos débiles en cada iteración, enfocándose en muestras mal clasificadas o con alto residuo",
  "No exige distribución de errores, ya que se basa en función de pérdida sin supuestos paramétricos",
  "Mejor si los ejemplos son independientes; puede usar técnicas especiales para datos correlacionados",
  "No asume varianza constante, usa función de pérdida directa para optimizar",
  "Los outliers pueden recibir peso excesivo en iteraciones posteriores, por lo que es necesario regularizar o usar robust loss",
  "La selección de variables se hace implícitamente, reduciendo el impacto de colinealidad",
  "Difícil de interpretar directamente; se pueden usar métricas de importancia, SHAP o partial dependence para explicación",
  "Cada iteración entrena un modelo débil, por lo que puede ser costoso si el número de iteraciones es alto",
  "CV ayuda a determinar la tasa de aprendizaje (learning rate), número de iteraciones y complejidad de base learners",
  "No es adecuado si hay muy pocos ejemplos, alta dimensionalidad con poco señal o target extremadamente desequilibrado"
)

tabla_boosting <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_boosting %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir Boosting",
             subtitle = "Boosting") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Bootstrapped Aggregation (Bagging)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Bagging.png"))
```

**Bootstrapped Aggregation (Bagging)** es una técnica de **aprendizaje conjunto (ensemble learning)** diseñada para mejorar la estabilidad y precisión de los algoritmos de aprendizaje automático, particularmente para reducir la **varianza** en los modelos. Fue introducida por Leo Breiman en 1996 y es la base de algoritmos muy populares como **Random Forest**. La idea fundamental de Bagging es entrenar múltiples versiones de un mismo modelo base en diferentes subconjuntos del conjunto de datos original y luego combinar sus predicciones.

El proceso central de Bagging implica dos pasos clave:

1.  **Muestreo Bootstrap:** En lugar de entrenar un único modelo en todo el conjunto de datos de entrenamiento, Bagging crea **múltiples conjuntos de datos de arranque (bootstrap samples)**. Cada muestra de arranque se crea seleccionando aleatoriamente, **con reemplazo**, un número de observaciones igual al tamaño del conjunto de datos original. Esto significa que algunos puntos de datos pueden aparecer varias veces en una muestra de arranque, mientras que otros pueden no aparecer en absoluto. Este muestreo aleatorio introduce diversidad entre los conjuntos de entrenamiento para cada modelo.

2.  **Agregación (Aggregation):** Una vez que se han entrenado **múltiples modelos base independientes** (por ejemplo, árboles de decisión) en cada una de estas muestras de arranque, sus predicciones se combinan. Para tareas de **clasificación**, la combinación se realiza mediante **votación por mayoría** (la clase más votada). Para tareas de **regresión**, las predicciones se promedian. Esta agregación de predicciones de modelos diversos reduce la varianza y, por lo tanto, hace que el modelo final sea más robusto y menos propenso al sobreajuste que un solo modelo entrenado en todo el conjunto de datos.

En el contexto del **aprendizaje global vs. local**, Bagging es una estrategia que combina las ventajas de los modelos de **aprendizaje local** para construir una **aproximación de función global** más estable. Cada modelo base (ej. un árbol de decisión) que se entrena en una muestra de arranque puede considerarse un sistema de aprendizaje local, ya que toma decisiones basadas en el subconjunto de datos que le ha sido asignado. Sin embargo, al entrenar estos múltiples modelos en paralelo y luego agregarlos, Bagging construye un modelo final que es una aproximación de función global altamente adaptable. La ventaja principal es que, si los datos no se distribuyen linealmente, el concepto de regresión (o clasificación) se puede aplicar eficazmente mediante esta forma de **regresión ponderada localmente** (donde los "pesos" son implícitos a través de la agregación de predicciones de modelos entrenados en subconjuntos aleatorios de datos). Bagging aborda el problema de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo al promediar o votar las predicciones de múltiples modelos, lo que reduce la varianza y mejora la generalización.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica o Continua",
  "✅ Numéricas y categóricas",
  "✅ Captura relaciones no lineales al promediar múltiples modelos",
  "❌ No requiere",
  "✅ Deseable, pero no obligatorio",
  "❌ No se asume homoscedasticidad",
  "✅ Robusto (cada bootstrap reduce el impacto de outliers)",
  "✅ Robusto (la agregación mitiga colinealidad)",
  "⚠️ Moderada (difícil interpretar conjunto de modelos)",
  "⚠️ Moderado (depende del número de árboles y tamaño del dataset)",
  "✅ Recomendable usar k-fold",
  "❌ No es ideal con muy pocos datos o si los base learners son demasiado simples"
)

detalles <- c(
  "Ensamble supervisado que ajusta varios modelos (usualmente árboles) sobre muestras bootstrap y promedia predicciones.",
  "En clasificación predice la clase más votada; en regresión, promedia los valores predichos.",
  "Acepta todo tipo de variables; las categóricas deben codificarse adecuadamente.",
  "Al promediar múltiples modelos, reduce varianza y captura no linealidades implícitamente.",
  "No impone supuestos sobre la distribución de errores.",
  "Los datos deben ser independientes; funciona peor en datos con fuerte autocorrelación sin ajuste.",
  "No requiere varianza constante puesto que se basa en agregación de múltiples predicciones.",
  "Cada muestra bootstrap y árbol es menos sensible a valores extremos; la agregación aumenta robustez.",
  "La selección aleatoria de subconjuntos y bootstrap reduce el efecto de predictores correlacionados.",
  "El modelo final es un conjunto de muchos árboles, lo que dificulta su explicación directa.",
  "Entrenar cientos de árboles toma tiempo, pero es paralelizable; la predicción es relativamente rápida.",
  "CV ayuda a ajustar parámetros como número de árboles y profundidad máxima de cada árbol.",
  "Con pocos ejemplos, los bootstrap no aportan diversidad suficiente; si los base learners son muy simples, no capturan bien patrones complejos."
)

tabla_bagging <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bagging %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir bagging",
             subtitle = "Bootstrapped Aggregation (Bagging) ") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## CatBoost {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/CatBoost.png"))
```

## Extreme Gradient Boosting (XGBoost)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/XGBoost.png"))
```

**XGBoost (Extreme Gradient Boosting)** es una implementación optimizada y altamente eficiente del algoritmo de **Gradient Boosting Machines (GBM)**, ampliamente reconocida por su **velocidad**, **rendimiento** y **escalabilidad** en problemas de **clasificación** y **regresión**. Ganó una inmensa popularidad debido a su éxito en numerosas competiciones de *machine learning* (como Kaggle). Aunque se basa en los principios de GBM, XGBoost introduce varias mejoras clave que lo hacen superior en muchos escenarios.

La idea fundamental de XGBoost, al igual que GBM, es construir un modelo aditivo de forma **secuencial**, donde cada nuevo árbol intenta corregir los errores residuales del conjunto de árboles previos. Sin embargo, XGBoost optimiza este proceso con las siguientes características:

1.  **Paralelización:** Aunque el *boosting* es inherentemente secuencial, XGBoost permite la paralelización de la construcción de los árboles individuales. Por ejemplo, en el paso de búsqueda de la mejor división, puede evaluar las posibles divisiones en paralelo a través de múltiples núcleos de CPU.
2.  **Regularización:** Incorpora términos de **regularización L1 (Lasso)** y **L2 (Ridge)** en la función de costo para controlar la complejidad del modelo y evitar el sobreajuste. Esto es crucial para la generalización.
3.  **Manejo de Valores Faltantes:** Tiene una capacidad incorporada para manejar valores faltantes en los datos, permitiendo al algoritmo aprender la mejor dirección para los valores ausentes.
4.  **Poda por Profundidad (Depth-First Search):** A diferencia de muchos algoritmos de árboles que crecen nivel por nivel, XGBoost puede usar un enfoque de poda por profundidad, lo que a menudo resulta en árboles más eficientes.
5.  **Caché-Aware Computing:** Optimiza el acceso a la memoria para manejar grandes conjuntos de datos de manera eficiente.
6.  **Flexibilidad de Función de Pérdida:** Permite el uso de funciones de pérdida personalizadas, lo que lo hace adaptable a una amplia gama de problemas.

En el contexto del **aprendizaje global vs. local**, XGBoost es una poderosa estrategia de **aprendizaje global** que se construye iterativamente a partir de componentes de **aprendizaje local**. Cada árbol de regresión (o clasificación) individual es un "aprendiz débil" que se enfoca en las deficiencias del modelo acumulado. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresión (o clasificación) de manera altamente sofisticada mediante esta **regresión ponderada localmente**. Al centrarse en los errores residuales y optimizar el proceso de manera rigurosa, XGBoost aborda de manera excepcional la desventaja de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo. Su combinación de precisión, velocidad y capacidad para manejar grandes conjuntos de datos lo ha convertido en uno de los algoritmos más populares y efectivos en la práctica del *machine learning*.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Continua (regresión) o Categórica (clasificación)",
  "✅ Numéricas y categóricas (requiere codificación para dummies o label encoding)",
  "✅ Captura no linealidades e interacciones complejas vía árboles en boosting",
  "❌ No requiere supuestos de normalidad",
  "✅ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "❌ No asume homoscedasticidad",
  "⚠️ Moderadamente (puede sobreajustar a outliers si no regula)",
  "✅ Robusto (reduce efecto de colinealidad al usar árboles secuenciales)",
  "⚠️ Baja (modelo complejo y tipo ’caja negra’)",
  "✅ Muy rápido y escalable (implementación optimizada, paralelizable)",
  "✅ Recomendable usar k-fold o repeated CV para ajustar hiperparámetros",
  "❌ No es ideal con datos muy pequeños, ruido alto o target extremadamente desbalanceado sin ajuste"
)

detalles <- c(
  "Ensamble supervisado que combina múltiples árboles débiles optimizados con gradiente descendente acelerado.",
  "En regresión predice valores continuos; en clasificación combina probabilidades o clases mediante log-loss o multiclass objectives.",
  "Acepta variables mixtas; las categóricas deben convertirse a formatos compatibles (p. ej. factor numerico, one-hot encoding).",
  "Cada iteración ajusta un nuevo árbol enfocándose en los residuos del modelo anterior, capturando patrones complejos.",
  "No impone distribución paramétrica de errores, ya que optimiza funciones de pérdida directamente.",
  "Funciona mejor si cada muestra es independiente; sensible a series de tiempo sin preparación apropiada.",
  "No requiere varianza constante, porque basa la optimización en gradientes del loss, no en supuestos de error.",
  "Los outliers difíciles de predecir pueden recibir demasiado peso en iteraciones sucesivas; usar _learning_rate_ bajo y _max_depth_ pequeño para regular.",
  "Los árboles reducen el impacto de variables altamente correlacionadas, aunque múltiples iteraciones pueden aún privilegiar características correlacionadas.",
  "Difícil interpretar directamente cada árbol; se utilizan métricas de importancia, SHAP values o partial dependence plots para explicación.",
  "Implementación en C++ altamente optimizada (CPU/GPU), permite entrenamiento muy rápido incluso con millones de filas.",
  "Validación cruzada anidada o simple ayuda a elegir hiperparámetros como `eta` (learning_rate), `nrounds` (número de árboles), `max_depth`, `subsample`, `colsample_bytree`.",
  "No conviene con datasets muy pequeños, ya que puede sobreajustar; tampoco si el ruido es muy alto y no se regula bien la complejidad."
)

tabla_xgboost <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_xgboost %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir xgboost",
             subtitle = "XGBoost")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Gradient Boosting Machines (GBM)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/GBM.png"))
```

**Gradient Boosting Machines (GBM)** es un algoritmo de **aprendizaje conjunto (ensemble learning)** extremadamente potente y versátil, utilizado para **clasificación**, **regresión** y otras tareas predictivas. A diferencia de Random Forest que construye árboles de forma independiente en paralelo (bagging), GBM construye los árboles de forma **secuencial** y aditiva. La idea central es que cada nuevo árbol en el conjunto intenta corregir los errores residuales (residuos) del conjunto de árboles construidos previamente.

El concepto fundamental detrás de GBM es el **impulso (boosting)**, donde los modelos "débiles" (generalmente árboles de decisión, a menudo árboles poco profundos o "stumps") se combinan para formar un modelo "fuerte". GBM logra esto de una manera específica:

1.  **Modelo Inicial:** Comienza con una predicción inicial para todos los datos (por ejemplo, el valor promedio para regresión o la probabilidad logarítmica para clasificación).
2.  **Cálculo de Residuos (Pseudo-residuos):** En cada iteración, el algoritmo calcula los **residuos** (o más precisamente, los "pseudo-residuos" o gradientes negativos de la función de pérdida) entre los valores reales y las predicciones actuales del modelo. Estos residuos representan los "errores" que el modelo actual no ha podido capturar.
3.  **Entrenamiento de un Nuevo Árbol:** Se entrena un nuevo árbol de decisión para **predecir estos residuos**. Este árbol es típicamente pequeño y débil, diseñado para centrarse en las áreas donde el modelo actual tiene los mayores errores.
4.  **Actualización del Modelo:** La predicción de este nuevo árbol se añade a la predicción acumulada del modelo existente, multiplicada por una **tasa de aprendizaje (learning rate)**. Esta tasa de aprendizaje controla el tamaño del paso de cada árbol, evitando que el modelo se sobreajuste rápidamente.
5.  **Iteración:** Este proceso se repite para un número predefinido de iteraciones, o hasta que una métrica de rendimiento deje de mejorar. Cada nuevo árbol contribuye a reducir los errores restantes.

En el contexto del **aprendizaje global vs. local**, GBM es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada árbol individual en el proceso de boosting es un sistema de aprendizaje local (como los *decision stumps* o árboles poco profundos) que se enfoca en una parte específica del error. Sin embargo, la combinación aditiva y secuencial de estos modelos "débiles" produce un modelo predictivo global altamente sofisticado y preciso. Si los datos no se distribuyen linealmente, GBM aplica el concepto de regresión (o clasificación) mediante una forma incremental y adaptativa de **regresión ponderada localmente**. Al centrarse en los errores residuales, GBM aborda directamente la desventaja de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo. Su capacidad para minimizar la función de pérdida de forma gradual y dirigida lo hace excepcionalmente eficaz para modelar relaciones complejas y no lineales, a menudo logrando un rendimiento superior en muchos problemas del mundo real.


```{r, echo = FALSE}

criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica o Continua",
  "✅ Numéricas y categóricas (requiere codificación)",
  "✅ Captura no linealidades e interacciones complejas vía boosting",
  "❌ No requiere",
  "✅ Deseable pero no obligatorio",
  "❌ No se asume homoscedasticidad",
  "⚠️ Moderadamente (los outliers pueden influir en árboles individuales)",
  "✅ Robusto (los arboles manejan colinealidad localmente)",
  "⚠️ Baja (modelo de tipo caja negra con varios árboles secuenciales)",
  "⚠️ Lento con muchos árboles, datos grandes o parámetros altos",
  "✅ Recomendable con k-fold o repeated CV para ajuste de hiperparámetros",
  "❌ Si se tienen pocos datos, muchas categorías o ruido alto"
)

detalles <- c(
  "Ensamble de árboles secuenciales donde cada árbol corrige errores del anterior.",
  "En clasificación se combinan probabilidades; en regresión se promedian predicciones.",
  "Funciona con muchas variables y aprende la importancia automáticamente.",
  "Construye árboles débiles que se enfocan en los errores residuales previos.",
  "No impone supuestos en la distribución de los errores.",
  "Los datos deben ser observaciones independientes; sensible a dependencias temporales.",
  "No requiere varianza constante puesto que se basa en árboles.",
  "Los outliers pueden exagerar los gradientes y forzar ajustes extremos en árboles individuales.",
  "Los árboles reducen impacto de colinealidad, pero múltiples árboles pueden still complicarla.",
  "Difícil de interpretar el conjunto; se pueden usar importance plots o SHAP para explicación.",
  "La construcción secuencial de cientos de árboles puede ser costosa en tiempo y memoria.",
  "La validación cruzada ayuda a determinar tasa de aprendizaje, número de árboles y profundidad.",
  "No es ideal cuando se tienen muy pocos datos o categorías con pocos ejemplos."
)

tabla_gbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_gbm %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir GBM",
             subtitle = "Gradient Boosting Machines (GBM)") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## Gradient Boosted Regression Trees (GBRT)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/GBRT.png"))
```


**Gradient Boosted Regression Trees (GBRT)**, a menudo conocida como **Gradient Boosting Machines (GBM)** cuando los modelos base son árboles de decisión de regresión, es una técnica de **aprendizaje conjunto (ensemble learning)** extremadamente potente y ampliamente utilizada para tareas de **regresión** (predicción de valores numéricos continuos) y también puede adaptarse para **clasificación**. Su fortaleza radica en su capacidad para construir un modelo predictivo robusto y preciso mediante la combinación secuencial de múltiples árboles de decisión "débiles".

La idea central de GBRT se basa en el principio de **boosting**, donde cada nuevo árbol en el conjunto se entrena para **corregir los errores residuales** (la diferencia entre los valores reales y las predicciones acumuladas del modelo hasta ese momento) de los árboles construidos en las iteraciones anteriores. Este proceso es iterativo y aditivo:

1.  **Modelo Inicial:** El proceso comienza con una predicción inicial simple para todos los datos, a menudo el valor promedio de la variable objetivo.
2.  **Cálculo de Pseudo-Residuos:** En cada iteración, GBRT calcula los "pseudo-residuos", que son los **gradientes negativos de la función de pérdida** con respecto a la predicción actual. Para la pérdida cuadrática media (común en regresión), estos pseudo-residuos son simplemente los errores tradicionales (valor real - predicción).
3.  **Entrenamiento de un Árbol de Regresión:** Se entrena un nuevo **árbol de decisión de regresión** (que es un "aprendiz débil", a menudo un árbol poco profundo o un *decision stump*) para **predecir estos pseudo-residuos**. El árbol busca los mejores puntos de división para reducir estos errores.
4.  **Actualización del Modelo:** La predicción de este nuevo árbol de regresión se añade a la predicción acumulada del modelo existente, pero se escala por una **tasa de aprendizaje (learning rate)**. Esta tasa de aprendizaje es un hiperparámetro crucial que controla la "contribución" de cada nuevo árbol y ayuda a prevenir el sobreajuste.
5.  **Iteración:** Los pasos 2 a 4 se repiten para un número predefinido de iteraciones. Cada nuevo árbol se enfoca en las deficiencias del modelo combinado anterior, refinando gradualmente la predicción.

En el contexto del **aprendizaje global vs. local**, GBRT es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada árbol de regresión individual es un sistema de aprendizaje local que divide el espacio de características y aprende patrones en subregiones. Sin embargo, el proceso de boosting, al combinar estos árboles secuencialmente para reducir los errores residuales globales, construye una **aproximación de función global** altamente flexible y precisa. La clave es que, si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresión de manera muy efectiva a través de esta **regresión ponderada localmente**. Al centrarse en los errores que el modelo actual no puede explicar, GBRT aborda directamente la desventaja de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo. Es excepcionalmente potente para capturar relaciones complejas y no lineales, y es ampliamente utilizado en diversas aplicaciones, desde la predicción de precios hasta la optimización de rutas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Continua (regresión)",
  "✅ Numéricas y categóricas (requiere codificación para ciertas implementaciones)",
  "✅ Captura no linealidades e interacciones complejas mediante boosting de árboles",
  "❌ No requiere supuestos de normalidad en residuos",
  "✅ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "❌ No asume homoscedasticidad",
  "⚠️ Moderadamente (outliers pueden influir en árboles individuales)",
  "✅ Robusto (los árboles reducen el impacto de colinealidad localmente)",
  "⚠️ Baja (modelo en su conjunto es “caja negra”)",
  "⚠️ Lento con muchos árboles o datos extensos",
  "✅ Recomendable usar k-fold o repeated CV para ajuste de hiperparámetros",
  "❌ No es ideal si hay muy pocos datos o ruido excesivo"
)

detalles <- c(
  "Ensamble de árboles de regresión secuenciales donde cada árbol corrige errores del anterior mediante gradiente.",
  "Predice valores continuos sumando las predicciones ponderadas de múltiples árboles débiles.",
  "Funciona con variables mixtas; las categóricas suelen transformarse en dummies.",
  "Cada nuevo árbol se enfoca en los residuos del modelo anterior, capturando patrones complejos.",
  "No impone distribución normal porque optimiza una función de pérdida (por ejemplo, MSE) directamente.",
  "Mejor si las observaciones no están correlacionadas en el tiempo; ajustar para series si es necesario.",
  "No requiere varianza constante puesto que se basa en árboles, no en un modelo paramétrico de errores.",
  "Los valores extremos pueden provocar ajustes excesivos en árboles individuales; usar tasa de aprendizaje baja ayuda a mitigar.",
  "Los árboles reducen el impacto de variables correlacionadas, aunque múltiples iteraciones pueden complicar interpretaciones.",
  "Difícil de interpretar directamente; se puede usar importancia de variables o herramientas como SHAP para explicación.",
  "Cada iteración entrena un árbol nuevo; muchos árboles o gran profundidad de árbol incrementan el tiempo de entrenamiento.",
  "CV ayuda a determinar parámetros como tasa de aprendizaje (`learning_rate`), número de árboles (`n.trees`) y profundidad máxima (`max_depth`).",
  "No es adecuado cuando el dataset es muy pequeño o extremadamente ruidoso, ya que puede sobreajustar fácilmente."
)

tabla_gbrt <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_gbrt %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir GBRT",
             subtitle = "Gradient Boosted Regression Trees (GBRT)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Isolation Forest {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Isolation Forest.png"))
```

## Light Gradient Boosting Machine (LightGBM)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/LightGBM.png"))
```

**LightGBM (Light Gradient Boosting Machine)** es otro algoritmo de **Gradient Boosting Machines (GBM)** de alto rendimiento, desarrollado por Microsoft. Está diseñado para ser **extremadamente rápido** y **eficiente** en el uso de memoria, especialmente con grandes conjuntos de datos, sin sacrificar una precisión significativa. Al igual que XGBoost, ha ganado popularidad en competiciones de *machine learning* por su velocidad y capacidad para manejar grandes volúmenes de datos.

La idea fundamental de LightGBM es la misma que la de otros algoritmos de boosting: construir un modelo aditivo de forma **secuencial**, donde cada nuevo árbol intenta corregir los errores residuales del modelo combinado anterior. Sin embargo, LightGBM introduce varias optimizaciones clave para lograr su notable eficiencia:

1.  **Gradient-based One-Side Sampling (GOSS):** A diferencia de XGBoost que usa todas las instancias para cada iteración, GOSS se enfoca en las instancias que tienen un **mayor gradiente** (es decir, las que contribuyen más al error). Descarta las instancias con gradientes pequeños o las muestrea con menos frecuencia, lo que acelera el entrenamiento sin perder demasiada precisión.
2.  **Exclusive Feature Bundling (EFB):** EFB agrupa características mutuamente exclusivas (es decir, características que rara vez toman valores distintos de cero al mismo tiempo) en un solo "bundle". Esto reduce el número de características y acelera el cálculo del histograma sin afectar la precisión.
3.  **Histogram-based Algorithm:** En lugar de construir árboles en una forma de pre-orden que es común en muchos algoritmos (lo que puede ser lento al enumerar todos los puntos de división), LightGBM utiliza un **algoritmo basado en histogramas**. Convierte los valores de las características continuas en *bins* discretos. Esto acelera significativamente el proceso de búsqueda del mejor punto de división.
4.  **Leaf-wise (Best-first) Tree Growth:** A diferencia de la mayoría de los árboles de decisión que crecen nivel por nivel (como en XGBoost), LightGBM crece el árbol **"hoja por hoja" (leaf-wise)**. Esto significa que en cada paso, selecciona la hoja con la mayor reducción de pérdida y la divide. Este enfoque puede llevar a árboles más profundos y asimétricos que pueden ser más precisos para el mismo número de nodos, aunque puede ser más propenso al sobreajuste (lo cual se mitiga con la regularización).

En el contexto del **aprendizaje global vs. local**, LightGBM, al igual que otros algoritmos de boosting, es una estrategia de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada árbol que se entrena es un "aprendiz débil" que se enfoca en las deficiencias residuales del modelo acumulado. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresión (o clasificación) de manera muy eficiente mediante esta **regresión ponderada localmente**. Al centrarse en los errores y optimizar los cálculos, LightGBM aborda de manera sobresaliente la desventaja de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo. Su énfasis en la velocidad y la eficiencia lo hace ideal para conjuntos de datos muy grandes o escenarios donde el tiempo de entrenamiento es una preocupación crítica.  



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Continua (regresión) o Categórica (clasificación)",
  "✅ Numéricas y categóricas (requiere codificación apropiada)",
  "✅ Captura no linealidades e interacciones mediante árboles en boosting",
  "❌ No requiere supuestos de normalidad",
  "✅ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "❌ No asume homoscedasticidad",
  "⚠️ Moderadamente (los outliers pueden influir en pesos de hojas)",
  "✅ Robusto (usa histogram-based split que atenúa colinealidad)",
  "⚠️ Baja (modelo complejo tipo ‘caja negra’)",
  "✅ Muy rápido y escalable (optimized gradient-based)",
  "✅ Recomendable usar k-fold o repeated CV para ajustar hiperparámetros",
  "❌ No conviene con datos muy pequeños o muy ruidosos sin regularización"
)

detalles <- c(
  "Ensamble supervisado que entrenan árboles de decisión usando histogram-based gradient boosting.",
  "En regresión predice valores continuos; en clasificación maximiza log-loss u otras funciones objetivo.",
  "Acepta variables mixtas; las categóricas deben convertirse a formato numérico o usar encoding interno.",
  "Cada iteración ajusta un árbol enfocándose en los residuos del anterior, capturando patrones complejos.",
  "No impone distribución paramétrica de errores; optimiza la función de pérdida directamente.",
  "Funciona mejor si las muestras son independientes; sensible a series de tiempo sin preparación adecuada.",
  "No requiere varianza constante, dado que es un método basado en árbol, no en supuestos de error.",
  "Los valores extremos pueden afectar el cálculo de gradientes y splits; usar regularización y parámetros de manejo de outliers.",
  "La división basada en histogramas reduce el impacto de predictores altamente correlacionados.",
  "Difícil interpretar cada árbol; se utilizan métricas de importancia y herramientas como SHAP para explicación.",
  "Implementación en C++ altamente optimizada que permite entrenamiento muy rápido incluso con grandes volúmenes de datos.",
  "CV ayuda a elegir parámetros como `learning_rate`, `num_leaves`, `max_depth`, `feature_fraction`, `bagging_fraction`.",
  "No es ideal si el dataset es muy pequeño, pues el boosting puede sobreajustar; tampoco con mucho ruido sin regularización adecuada."
)

tabla_lightgbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lightgbm %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir LightGBM",
             subtitle = "LightGBM")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Random Forest  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Bagging.png"))
```

**Random Forest** es un algoritmo de **aprendizaje conjunto (ensemble learning)** altamente popular y potente, utilizado tanto para tareas de **clasificación** como de **regresión**. Fue desarrollado por Leo Breiman en 2001 y se basa en la idea de combinar las predicciones de múltiples **árboles de decisión** para lograr una mayor precisión y robustez que un solo árbol. La fuerza de Random Forest reside en dos conceptos clave: **bagging (bootstrap aggregation)** y la **aleatoriedad en la selección de características**.

La idea fundamental detrás de Random Forest es construir un "bosque" de árboles de decisión de una manera específica:

1.  **Bagging (Bootstrap Aggregation):** En lugar de entrenar un solo árbol en todo el conjunto de datos, Random Forest entrena cada árbol en una **muestra de arranque (bootstrap sample)** diferente. Una muestra de arranque es un subconjunto del conjunto de datos original, muestreado con reemplazo. Esto significa que algunos puntos de datos pueden aparecer varias veces en una muestra, mientras que otros pueden no aparecer en absoluto. Este muestreo genera diversidad entre los árboles.

2.  **Aleatoriedad en la Selección de Características:** Cuando cada árbol se construye, en cada paso de división (nodo), Random Forest no considera todas las características disponibles. En cambio, solo considera un **subconjunto aleatorio de características** para encontrar la mejor división. Esta aleatoriedad adicional (además del muestreo de arranque) descorrelaciona aún más los árboles, lo que es crucial para el rendimiento del algoritmo. Si los árboles estuvieran altamente correlacionados, el error de un árbol promedio no se reduciría al promediar.

Una vez que se han construido numerosos árboles (típicamente cientos o miles), las predicciones se combinan: para **clasificación**, se utiliza la **votación por mayoría** (la clase más votada por los árboles individuales); para **regresión**, se calcula el **promedio** de las predicciones de todos los árboles.

En el contexto del **aprendizaje global vs. local**, Random Forest se puede considerar como un sistema de **aprendizaje global** que se construye a partir de componentes de **aprendizaje local**. Cada árbol individual en el bosque es un sistema de aprendizaje local (como CART, que divide el problema en subproblemas más pequeños). Sin embargo, al combinar las predicciones de muchos de estos árboles, Random Forest logra una **aproximación de función global** muy robusta y flexible. La ventaja es que, si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresión (o clasificación) mediante una forma sofisticada de **regresión ponderada localmente**. La combinación de árboles diversos y descorrelacionados mitiga la desventaja de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo. Random Forest sobresale en capturar relaciones complejas y no lineales, manejar grandes conjuntos de datos con muchas características y es menos propenso al sobreajuste que un solo árbol de decisión grande.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica o Continua",
  "✅ Numéricas y categóricas (requiere codificación)",
  "✅ Captura relaciones no lineales e interacciones complejas",
  "❌ No requiere",
  "✅ Deseable pero no obligatorio",
  "❌ No se asume homoscedasticidad",
  "✅ Robusto a outliers (por agregación)",
  "✅ Robusto (selecciona subconjuntos aleatorios)",
  "⚠️ Moderada (difícil interpretar cientos de árboles)",
  "⚠️ Lento con muchos árboles o datos grandes",
  "✅ Recomendado usar k-fold",
  "❌ Puede sobreajustar si no se ajustan hiperparámetros (e.g. profundidad, número de árboles)"
)

detalles <- c(
  "Ensamble de árboles de decisión, cada uno entrenado en una muestra bootstrap y usando un subconjunto aleatorio de predictores.",
  "En clasificación predice la clase mayoritaria entre árboles; en regresión, el promedio de predicciones.",
  "Acepta muchas variables y selecciona automáticamente las más relevantes por importancia.",
  "Al generar múltiples árboles, capta interacciones no lineales sin necesidad de especificarlas.",
  "No hay supuestos sobre la distribución de los errores.",
  "Los árboles individuales pueden manejar correlación leve; el ensamble mitiga la dependencia.",
  "No necesita homogeneidad de varianza en los errores residuales.",
  "Cada árbol es poco sensible a outliers, y la agregación mejora robustez.",
  "Reduce el problema de colinealidad al seleccionar subconjuntos de variables por árbol.",
  "Es difícil de explicar, aunque se pueden usar métricas de importancia de variables.",
  "Puede volverse lento si se entrenan miles de árboles en datasets muy grandes.",
  "Cross-validation ayuda a evitar overfitting y evaluar generalización.",
  "No es ideal si el interpretabilidad es crítica o el tiempo computacional es limitado."
)

tabla_rf <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rf %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir random forest",
             subtitle = "Random Forest") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Stacked Generlization (Blending) {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Blending.png"))
```

**Stacked Generalization**, comúnmente conocido como **Stacking**, y su variante **Blending**, son técnicas avanzadas de **aprendizaje conjunto (ensemble learning)** que buscan combinar las predicciones de múltiples modelos de aprendizaje automático para obtener un rendimiento predictivo superior al de cualquier modelo individual. La idea fundamental es que, en lugar de simplemente promediar o votar las predicciones, se entrena un **modelo de segundo nivel (meta-modelo)** para aprender a combinar óptimamente las predicciones de los modelos de primer nivel (modelos base).

El proceso de Stacking generalmente implica dos o más "capas" de modelos:

1.  **Modelos Base (Nivel 0):** En la primera capa, se entrenan múltiples modelos de aprendizaje automático diversos (pueden ser de diferentes tipos, como árboles de decisión, máquinas de vectores de soporte, redes neuronales, etc.). Estos modelos base se entrenan sobre el conjunto de datos de entrenamiento original (o en particiones del mismo).

2.  **Generación de Meta-Características:** Las predicciones generadas por estos modelos base sobre un conjunto de datos "fuera de muestra" (que no se usó para entrenar los modelos base, típicamente a través de validación cruzada k-fold) se utilizan como **nuevas características** o "meta-características". Estas meta-características, junto con la variable objetivo original, forman un nuevo conjunto de datos de entrenamiento para el meta-modelo.

3.  **Meta-Modelo (Nivel 1):** En la segunda capa, se entrena un **meta-modelo** (a menudo un modelo más simple, como regresión lineal, regresión logística o un árbol de decisión poco profundo) utilizando estas meta-características como entrada y la variable objetivo original como salida. El meta-modelo aprende la relación entre las predicciones de los modelos base y la respuesta verdadera, y por lo tanto, cómo "pesar" o "combinar" esas predicciones de la mejor manera.

**Blending** es una variación más sencilla de Stacking. La principal diferencia es cómo se generan las meta-características para el meta-modelo. En Blending, se reserva una **subdivisión de validación (holdout set)** del conjunto de entrenamiento original. Los modelos base se entrenan en la parte restante del conjunto de entrenamiento, y luego sus predicciones sobre este conjunto de validación se utilizan directamente como meta-características para entrenar el meta-modelo. Esto simplifica el proceso de validación cruzada, pero el meta-modelo se entrena con menos datos.

En el contexto del **aprendizaje global vs. local**, Stacking/Blending es una estrategia de **aprendizaje global** que explota el poder de múltiples **aproximaciones de función local** (los modelos base) para construir un modelo final altamente sofisticado. Cada modelo base, dependiendo de su naturaleza, puede ser un sistema de aprendizaje local que descubre patrones en subregiones de datos. Sin embargo, el meta-modelo aprende una función de combinación global sobre las predicciones de estos modelos base. Si los datos no se distribuyen linealmente, Stacking/Blending aplica el concepto de regresión (o clasificación) de una manera muy flexible. Al permitir que un modelo de segundo nivel aprenda a combinar las predicciones de diversos modelos, supera la limitación de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo. Es particularmente eficaz en competiciones de machine learning donde se busca el máximo rendimiento, ya que aprovecha las fortalezas complementarias de diferentes algoritmos. Sin embargo, puede ser computacionalmente intensivo y más difícil de interpretar que los modelos individuales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica o Continua (depende de los modelos base)",
  "✅ Numéricas y/o categóricas (codificación según modelos base)",
  "✅ Captura relaciones complejas vía combinación de modelos base",
  "❌ No exige supuestos de normalidad en residuos",
  "✅ Deseable, pero no obligatorio (mejor si observaciones independientes)",
  "❌ No se asume homoscedasticidad",
  "⚠️ Moderadamente (outliers afectan modelos base individuales)",
  "⚠️ Puede verse afectado (depende de base learners y correlated features)",
  "⚠️ Baja (modelo meta difícil de interpretar directamente)",
  "⚠️ Lento en entrenamiento y predicción, según número de base learners",
  "✅ Esencial (usar CV anidada para entrenar meta-modelo)",
  "❌ Si datos muy escasos o muy ruidosos, riesgo de sobreajuste"
)

detalles <- c(
  "Ensamble supervisado que combina varias predicciones (base learners) mediante un modelo meta.",
  "El meta-modelo acepta la salida de modelos base; puede predecir clases o valores continuos.",
  "Usa predictores originales para los base learners; algunos requieren dummies, otros no.",
  "Aprende patrones no lineales e interacciones complejas a través de múltiples capas.",
  "No impone distribución normal: cada base learner tiene sus propios supuestos.",
  "Ideal si cada muestra es independiente; sensibles a dependencias en validaciones cruzadas.",
  "No requiere varianza constante, ya que se basa en agregación de predicciones.",
  "Modelos base (p. ej. ARBOTS, SVM) pueden verse influenciados por valores extremos;",
  "Modelos base diversificados reducen colinealidad, pero meta-modelo puede verse afectado.",
  "Difícil atribuir importancia directa; se pueden usar técnicas como SHAP para interpretación.",
  "Entrenamiento de múltiples base learners y meta-modelo incrementa tiempo; predicción también más lenta.",
  "Usar validación cruzada anidada: inner folds para entrenar base learners y stacking, outer folds para evaluar.",
  "No recomendable si hay muy pocos datos (stacking requiere dividir en folds) o si los base learners no aportan diversidad."
)

tabla_stacking <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_stacking %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir blending",
             subtitle = "Stacked Generlizaation (Blending)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```







<!--chapter:end:03-ensemble.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 🧠 4. Redes Neuronales {-}  

**Ejemplos:** MLP, CNN, RNN, Transformers.  
**Uso:** Perfectas para **imágenes** (CNN), **texto** (Transformers) y **series temporales** (RNN/LSTM), especialmente con **grandes volúmenes de datos no estructurados**.  
**Ventajas:** Muy poderosas para datos complejos.  
**Limitaciones:** Requieren **mucha data** y **computación**, y tienen **menor interpretabilidad**.  

---

## Autoenconder  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Autoenconder.png"))
```


Un **Autoencoder** es un tipo de **red neuronal artificial** diseñado para aprender una **representación (o codificación) eficiente y comprimida** de los datos de entrada, sin supervisión humana. Su objetivo principal es la **reducción de dimensionalidad** o el **aprendizaje de características**, lo que lo hace útil para tareas como la detección de anomalías, la denoising de imágenes, o la generación de datos.

La arquitectura básica de un Autoencoder se compone de dos partes principales:

1.  **Encoder (Codificador):** Esta parte de la red toma los datos de entrada y los transforma en una representación de menor dimensión, a menudo llamada **código, representación latente, o cuello de botella (bottleneck)**. Es decir, comprime la información esencial de la entrada.
2.  **Decoder (Decodificador):** Esta parte toma la representación comprimida (el código) del encoder y la reconstruye de nuevo a la dimensión original de los datos de entrada.

El Autoencoder se entrena para **minimizar la diferencia entre la entrada original y su reconstrucción** generada por el decoder. Esta diferencia se mide a través de una **función de pérdida de reconstrucción** (como el error cuadrático medio para datos continuos o la entropía cruzada para datos binarios). Al forzar a la red a reconstruir su propia entrada a partir de una representación comprimida, el Autoencoder aprende las características más salientes y útiles de los datos de forma no supervisada.

Existen varias variantes de Autoencoders, como los **Autoencoders Denoising** (que aprenden a reconstruir datos limpios a partir de datos con ruido), los **Autoencoders Variacionales (VAEs)** (que aprenden una distribución probabilística de la representación latente, útiles para la generación de datos), y los **Autoencoders Convolucionales** (que usan capas convolucionales, ideales para imágenes).


**Aprendizaje Global vs. Local:**

Un Autoencoder se considera principalmente un modelo de **aprendizaje global**, aunque con una perspectiva única debido a su naturaleza de compresión y reconstrucción.

* **Aspecto Global:** Un Autoencoder aprende una **transformación global** de los datos. El encoder aprende a mapear todo el espacio de entrada a un espacio de representación latente, y el decoder aprende a mapear ese espacio latente de vuelta al espacio de salida. Las ponderaciones y sesgos de la red se ajustan para encontrar esta transformación que funciona de manera óptima para todo el conjunto de datos de entrenamiento, permitiendo la reconstrucción más fiel posible en general. La **función de pérdida de reconstrucción** se minimiza a nivel de todo el conjunto de datos, no solo en vecindarios específicos.

* **Representación Local vs. Reconstrucción Global:** Aunque el objetivo final es una reconstrucción global de la entrada, la **representación latente (el código)** puede verse como una forma de capturar **características o patrones importantes** que, en cierto sentido, resumen la información "local" o particular de cada instancia de datos de una manera comprimida. Sin embargo, la forma en que estas características se aprenden y se utilizan para la reconstrucción se rige por un conjunto global de parámetros de la red. No se entrena un modelo separado para cada vecindario de datos, sino una única red que aprende una función de mapeo para todo el dominio.

En resumen, el Autoencoder aprende una representación eficiente y una capacidad de reconstrucción que se aplica de manera consistente a todos los datos, lo que lo clasifica como un modelo de aprendizaje global que busca una solución unificada para el problema de la codificación y decodificación de datos. 

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (aprendizaje no supervisado)",
  "❌ No aplica (no hay variable respuesta)",
  "✅ Numéricas (o categóricas codificadas)",
  "✅ Captura relaciones complejas y no lineales",
  "❌ No aplica (no es un modelo de regresión)",
  "❌ No aplica (no hay errores residuales)",
  "❌ No aplica",
  "✅ Sí, pueden afectar la reconstrucción",
  "✅ Puede ayudar a reducir efectos de multicolinealidad",
  "⚠️ Baja interpretabilidad (representaciones latentes)",
  "⚠️ Lento en entrenamiento, especialmente con muchas capas o datos",
  "⚠️ Se puede validar con reconstrucción y autoevaluación",
  "❌ Datos con mucha dispersión o sin estructura latente clara"
)

detalles <- c(
  "Red neuronal no supervisada que aprende a codificar y decodificar los datos para reducir dimensionalidad o detectar anomalías.",
  "No predice una variable externa, sino que reproduce la entrada como salida.",
  "Requiere variables numéricas (o una codificación previa en caso de categóricas).",
  "Es capaz de capturar estructuras complejas y no lineales al comprimir los datos.",
  "No tiene residuos como un modelo clásico, pero sí errores de reconstrucción.",
  "No modela errores independientes, ya que no es un modelo predictivo tradicional.",
  "Tampoco se evalúa homoscedasticidad, ya que no hay predicción como tal.",
  "Outliers distorsionan el entrenamiento, especialmente si no se normaliza.",
  "Ayuda a eliminar redundancias en los datos si están correlacionados.",
  "Las capas internas (representaciones) no son directamente interpretables.",
  "Requiere entrenamiento con varias iteraciones y puede tardar con arquitecturas grandes.",
  "Se evalúa con pérdida de reconstrucción o aplicando validación cruzada si se integra en modelos supervisados.",
  "Pierde eficacia si los datos no tienen una estructura latente útil para codificar."
)

tabla_autoencoder <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_autoencoder %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir autoencoder",
             subtitle = "Autoenconder")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Back - Propagation  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Back - Propagation.png"))
```


**Back-Propagation (Retropropagación)** es el algoritmo fundamental de entrenamiento utilizado para ajustar los pesos de las **redes neuronales artificiales multicapa (MLP)**. La idea central de Back-Propagation es calcular la **contribución de cada peso al error global de la red** y luego ajustar esos pesos para reducir dicho error, propagando la información del error "hacia atrás" desde la capa de salida hasta la capa de entrada.

A diferencia del Perceptron, que solo puede aprender patrones linealmente separables, Back-Propagation permite entrenar redes neuronales profundas con múltiples capas ocultas y funciones de activación no lineales, lo que les permite modelar relaciones complejas y no lineales en los datos.

El funcionamiento de Back-Propagation se divide en dos fases principales que se repiten iterativamente:

1.  **Fase de Propagación hacia Adelante (Forward Pass):**
    * Las entradas se pasan a través de la red, desde la capa de entrada, a través de las capas ocultas, hasta la capa de salida.
    * En cada neurona, se calcula la suma ponderada de sus entradas (incluido el sesgo) y se aplica la función de activación (ej. sigmoide, tanh, ReLU) para producir la salida de esa neurona.
    * La salida final de la red se compara con el valor objetivo real para calcular el **error global** (o "costo") de la red, utilizando una función de pérdida (ej. error cuadrático medio para regresión, entropía cruzada para clasificación).

2.  **Fase de Retropropagación (Backward Pass):**
    * El error global se propaga **hacia atrás** desde la capa de salida, a través de las capas ocultas, hasta la capa de entrada.
    * En cada capa, se calcula el **gradiente** del error con respecto a los pesos de las conexiones de esa capa. Esto implica el uso de la **regla de la cadena** del cálculo diferencial para determinar cuánto contribuye cada peso al error final.
    * Una vez calculados los gradientes, los pesos de la red se **actualizan** en la dirección opuesta al gradiente (es decir, en la dirección de mayor descenso) para reducir el error. Esta actualización se realiza con una **tasa de aprendizaje** que controla el tamaño del paso.
    $$w_{ij}^{\text{nuevo}} = w_{ij}^{\text{anterior}} - \alpha \cdot \frac{\partial E}{\partial w_{ij}}$$
    Donde $E$ es el error, $w_{ij}$ es el peso de la conexión entre la neurona $i$ y la neurona $j$, y $\alpha$ es la tasa de aprendizaje.

En el contexto del **aprendizaje global vs. local**, Back-Propagation es el corazón del entrenamiento de sistemas de **aprendizaje global** por excelencia (las redes neuronales multicapa). La red neuronal busca aprender una **aproximación de función global** que mapee las entradas a las salidas, minimizando el error en todo el conjunto de datos. Si los datos no se distribuyen linealmente, Back-Propagation permite que la red aprenda relaciones no lineales complejas a través de sus múltiples capas y funciones de activación no lineales. A diferencia de LOESS o los métodos de regresión ponderada localmente, Back-Propagation no divide explícitamente el problema en múltiples problemas locales independientes para minimizar funciones de costo locales. En cambio, busca minimizar una función de pérdida **global** para toda la red. Sin embargo, su capacidad para ajustar un gran número de parámetros (pesos) le permite construir representaciones internas de los datos que pueden ser increíblemente flexibles y adaptables, superando la limitación de que "a veces ningún valor de parámetro [en un modelo simple] puede proporcionar una aproximación suficientemente buena". La retropropagación es lo que permitió a las redes neuronales convertirse en poderosas herramientas de aprendizaje automático.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica (clasificación) o Continua (regresión)",
  "✅ Numéricas (requiere normalizar), Categóricas como dummies",
  "✅ Captura relaciones no lineales profundas",
  "❌ No requiere",
  "✅ Deseable, aunque no obligatorio",
  "❌ No se asume homoscedasticidad",
  "⚠️ Elevado (puede requerir robustez ante valores extremos)",
  "⚠️ Puede ralentizar convergencia si es muy alta",
  "⚠️ Baja (modelo de “caja negra”)",
  "⚠️ Depende de la arquitectura y tamaño del dataset",
  "✅ Recomendable para ajustar tasas de aprendizaje, capas y neuronas",
  "❌ No conviene con datos muy pequeños o alta dimensionalidad sin regularizar"
)

detalles <- c(
  "Algoritmo para entrenar redes neuronales multicapa ajustando pesos por retropropagación del error.",
  "En clasificación usa softmax o sigmoide en salida; en regresión, capa lineal para valor continuo.",
  "Debe escalarse cada característica; las categóricas transformarse a variables indicadoras antes de entrenar.",
  "Aprende funciones arbitrariamente complejas activando múltiples capas ocultas con funciones no lineales.",
  "No impone distribución específica en errores, se optimiza vía descenso de gradiente.",
  "Mejor si las observaciones son independientes; sensible a secuencias sin ajustes específicos.",
  "No requiere varianza constante, ya que los pesos se ajustan adaptativamente durante el entrenamiento.",
  "Valores extremos pueden causar activaciones saturadas (vanishing/exploding gradients) si no se manejan.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; Batch Normalization ayuda a mitigar.",
  "Difícil interpretar cada peso individual; se usan técnicas como LIME o SHAP para explicar decisiones.",
  "El tiempo crece con número de capas, neuronas y epochs; GPUs aceleran el proceso.",
  "Cross-validation (o k-fold) ayuda a elegir número de capas, neuronas por capa, tasa de aprendizaje y regularización.",
  "No funciona bien con datasets pequeños (overfitting fácil) o ruido elevado sin técnicas de regularización."
)

tabla_backprop <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_backprop %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir back-propagation",
             subtitle = "Back - Propagation")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Convolutional Neural Network (CNN)  {-}   


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/CNN.png"))
```

**Convolutional Neural Networks (CNNs)**, también conocidas como **ConvNets**, son una clase especializada de **redes neuronales profundas** que han demostrado ser excepcionalmente efectivas en tareas de **visión por computadora** (como clasificación de imágenes, detección de objetos, reconocimiento facial) y, más recientemente, en procesamiento de lenguaje natural. La idea fundamental de una CNN es imitar el funcionamiento del córtex visual en el cerebro humano, utilizando **capas de convolución** para detectar automáticamente patrones y características jerárquicas directamente de los datos de entrada sin necesidad de una extracción manual de características.

A diferencia de los Multilayer Perceptrons (MLPs) que conectan cada neurona de una capa con cada neurona de la siguiente capa (lo que resulta en una enorme cantidad de parámetros para datos de alta dimensión como imágenes), las CNNs aprovechan tres ideas arquitectónicas clave:

1.  **Capas de Convolución:** Estas capas aplican un pequeño conjunto de **filtros (kernels)** a la entrada (ej., una imagen). Cada filtro "se desliza" por la entrada (operación de convolución) y calcula un producto punto entre sus valores y los valores de la región de la entrada que está cubriendo. Esto genera un **mapa de características** que resalta la presencia de patrones específicos (bordes, texturas, formas) en diferentes ubicaciones de la entrada. La ventaja es que los mismos filtros se aplican en múltiples ubicaciones, lo que reduce drásticamente el número de parámetros y captura la **localidad** de los patrones y la **invarianza traslacional**.
2.  **Capas de Pooling (Submuestreo):** Estas capas se insertan periódicamente entre las capas convolucionales. Su función es reducir la dimensionalidad espacial de los mapas de características (ej., reduciendo el número de píxeles), lo que ayuda a hacer que el modelo sea más robusto a pequeñas variaciones o distorsiones en la posición de las características. Las operaciones comunes son el **max pooling** (tomar el valor máximo de una región) o el **average pooling** (tomar el promedio).
3.  **Capas Totalmente Conectadas (Dense):** Después de varias capas convolucionales y de pooling, los mapas de características finales se aplanan en un vector y se conectan a una o más capas totalmente conectadas (similares a las de un MLP). Estas capas finales realizan la clasificación o regresión basándose en las características de alto nivel extraídas por las capas anteriores.

El entrenamiento de una CNN se realiza utilizando el algoritmo de **Back-Propagation** y descenso de gradiente (con sus variantes como SGD, Adam, etc.), ajustando los pesos de los filtros y las conexiones de las capas densas para minimizar una función de pérdida.

En el contexto del **aprendizaje global vs. local**, las CNNs son un ejemplo sobresaliente de un sistema de **aprendizaje global** que, en sus capas iniciales, se beneficia de la detección de patrones **locales**. Cada filtro de convolución aprende a detectar un patrón local específico (un borde vertical, una esquina, etc.) que se repite en diferentes partes de la imagen (lo que es una forma de "regresión ponderada localmente" en el sentido de que el filtro "aplica" su conocimiento local a diferentes ventanas de entrada). Sin embargo, la combinación jerárquica de múltiples capas convolucionales y de pooling, seguida de capas totalmente conectadas, permite que la red construya representaciones cada vez más abstractas y globales del contenido de la imagen. Esto significa que si los datos no se distribuyen linealmente, las CNNs pueden aprender a modelar relaciones extremadamente complejas y no lineales al componer características locales en representaciones globales. La arquitectura de CNNs resuelve la limitación de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en modelos más simples al permitir que la red aprenda características relevantes de forma automática y jerárquica, adaptándose a las complejidades inherentes de datos como imágenes y videos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Continua (regresión) o Categórica (clasificación)",
  "✅ Imágenes (matrices de píxeles) y datos con estructura espacial",
  "✅ Captura relaciones locales y espaciales mediante filtros convolucionales",
  "❌ No requiere supuestos de normalidad en residuos",
  "✅ Deseable, aunque no obligatorio (mejor si instancias son independientes)",
  "❌ No se asume homoscedasticidad",
  "⚠️ Moderadamente (artefactos o ruido en imágenes puede afectar)",
  "⚠️ No se evalúa colinealidad de predictores, maneja correlaciones espaciales",
  "⚠️ Baja (modelo tipo ‘caja negra’, usar técnicas como Grad-CAM para interpretación)",
  "⚠️ Lento sin GPU, entrenamiento intensivo en cómputo",
  "✅ Robusto si se aplica k-fold o validación en conjunto de imágenes",
  "❌ No funciona bien con pocos datos o sin estructura espacial significativa"
)

detalles <- c(
  "Red neuronal profunda especializada en procesar datos con estructura de grilla (ej. imágenes).",
  "En clasificación utiliza softmax; en regresión, capa lineal para valores continuos.",
  "Requiere tensores de entrada (canales, altura, ancho); funciones de preprocesamiento para imágenes.",
  "Filtros convolucionales extraen características locales, max-pooling disminuye dimensionalidad manteniendo información relevante.",
  "No impone ninguna distribución en los errores, optimiza función de pérdida directamente.",
  "Ideal si las muestras son independientes; sensible a dependencias temporales o espaciales no modeladas.",
  "No requiere varianza constante pues se basa en convoluciones y pooling, no en un modelo paramétrico de error.",
  "Ruido o artefactos en píxeles pueden alterar el aprendizaje de filtros, es importante usar técnicas de regularización.",
  "La red aprende filtros que capturan patrones locales, por lo que no es necesario verificar colinealidad explícitamente.",
  "Difícil de interpretar cada filtro y capa; se utilizan mapas de activación o Grad-CAM para entender qué regiones influyen en la predicción.",
  "El entrenamiento con múltiples capas convolucionales y millones de parámetros es intensivo en GPU/TPU.",
  "Validación cruzada o separación de conjuntos (train/validation/test) ayuda a evitar overfitting.",
  "No es apropiado para datasets muy pequeños sin aumentar datos (data augmentation) o sin información espacial clara."
)

tabla_cnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles) 

tabla_cnn %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir CNN",
             subtitle = "Convolutional Neural Network (CNN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```


## Hopfield Network  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Hopfield Network.png"))
```

La **Red de Hopfield** es un tipo de **red neuronal recurrente** o **red neuronal con memoria asociativa**, propuesta por John Hopfield en 1982. A diferencia de las redes neuronales de propagación hacia adelante (como el Perceptrón o las MLP entrenadas con Back-Propagation) que se utilizan para el mapeo de entrada a salida, la idea fundamental de una Red de Hopfield es funcionar como un **sistema de memoria asociativa** y un **sistema dinámico que converge a estados estables**. Su objetivo principal es almacenar y recuperar patrones binarios, así como resolver problemas de optimización.

El funcionamiento de una Red de Hopfield se basa en los siguientes principios:

1.  **Neuronas Binarias:** La red consta de un conjunto de neuronas (nodos) que son **binarias**, lo que significa que solo pueden tomar dos estados posibles, generalmente $1$ o $-1$.
2.  **Conexiones Ponderadas:** Cada neurona está conectada a todas las demás neuronas (excepto a sí misma) mediante **conexiones simétricas y ponderadas**. Los pesos de estas conexiones se calculan de manera que los patrones que se quieren "memorizar" se conviertan en **estados de energía mínima** de la red. La regla de aprendizaje más común para establecer estos pesos es la **regla de Hebb**: si dos neuronas se activan juntas para un patrón, el peso entre ellas se incrementa.
3.  **Dinámica de Activación:** Cuando se presenta una entrada a la red (que puede ser un patrón ruidoso o incompleto), las neuronas se actualizan de forma asíncrona o síncrona. La activación de cada neurona se recalcula en función de la suma ponderada de las activaciones de las otras neuronas a las que está conectada.
    $$S_i = \text{sgn}\left(\sum_{j \neq i} W_{ij} S_j\right)$$
    Donde $S_i$ es el estado de la neurona $i$, $W_{ij}$ es el peso entre la neurona $i$ y $j$, y $\text{sgn}$ es la función signo.
4.  **Convergencia a Estados Estables:** Este proceso de actualización se repite hasta que la red alcanza un **estado estable** (un "atractor"), donde las activaciones de las neuronas ya no cambian. Si la red ha sido entrenada correctamente, este estado estable corresponderá al patrón memorizado más cercano a la entrada inicial (memoria asociativa).
5.  **Función de Energía:** La estabilidad de la red se puede describir mediante una **función de energía de Lyapunov**. Durante la dinámica de la red, la energía de la red siempre disminuye hasta que se alcanza un mínimo local (un patrón memorizado).

En el contexto del **aprendizaje global vs. local**, la Red de Hopfield es un sistema de **aprendizaje global** que exhibe un comportamiento de **optimización local**. La regla de aprendizaje (como la regla de Hebb) establece los pesos de todas las conexiones para que los patrones deseados se conviertan en mínimos de energía en todo el espacio de estados. Es decir, se busca una configuración global de pesos para memorizar un conjunto de patrones. Sin embargo, la **dinámica de recuperación** de la red es intrínsecamente un proceso de **convergencia local**: dada una entrada inicial, la red "cae" en el mínimo de energía más cercano, que corresponde al patrón memorizado.

Si los datos no se distribuyen linealmente, la Red de Hopfield no aplica el concepto de regresión (o clasificación) de la misma manera que LOESS o los árboles de decisión. En cambio, funciona como un sistema de **memoria y recuperación de patrones** no lineales. Puede almacenar y recuperar patrones complejos que no son linealmente separables. La red busca una solución global (un conjunto de pesos) para almacenar los patrones, y luego, en la recuperación, utiliza un proceso de "búsqueda" local en el espacio de energía para converger a un patrón memorizado. Esto aborda la idea de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un modelo de regresión lineal, ya que la Red de Hopfield no es un modelo de regresión en sí, sino un sistema dinámico que encuentra estados de equilibrio. Su capacidad para manejar patrones ruidosos o incompletos para recuperar el patrón completo es una de sus principales fortalezas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "⚠️ No es supervisado en el sentido clásico",
  "⚠️ No hay “target” continuo o categórico (almacenamiento de patrones)",
  "✅ Variables binarias o valores discretizados (patrones binarios)",
  "✅ Correlaciona patrones con pesos simétricos entre neuronas",
  "❌ No aplica (no hay residuos paramétricos)",
  "✅ Deseable, aunque no obligatorio (los estados deben actualizarse sin bucles no deseados)",
  "❌ No aplica (no modela varianza de errores)",
  "⚠️ Muy sensible (un solo nodo saturado puede distorsionar la red)",
  "⚠️ Puede verse afectado si los patrones de entrenamiento tienen redundancia fuerte",
  "⚠️ Media (la dinámica de atração es interpretable, pero las conexiones pueden ser complejas)",
  "⚠️ Moderada (dependiendo del número de neuronas y estados síncronos/asíncronos)",
  "❌ No se usa tradicionalmente, pero se puede validar estabilidad de memorias con pruebas de convergencia",
  "⚠️ No sirve si los patrones no son binarios o si hay alto ruido en entradas asociativas"
)

detalles <- c(
  "Red neuronal recurrente para recuperación asociativa de patrones, no requiere pares X→y.",
  "No predice una variable externa, recupera patrones completos a partir de entradas parciales o ruidosas.",
  "Requiere que cada elemento del patrón sea binario (±1) o esté discretizado; las variables continuas deben binarizarse.",
  "Los pesos simétricos se calculan por Hebb (p. ej. W = Σ pᵢ pᵢᵀ), sin umbral explícito para relaciones lineales.",
  "No hay un término de error paramétrico; la dinámica sigue la función de energía, no un residuo gaussiano.",
  "Es mejor si las actualizaciones de estado son independientes o síncronas; la dependencia temporal puede generar oscilaciones.",
  "No modela varianza de error, pues busca minimizar energía, no error cuadrático.",
  "Patrones fuera del rango binario pueden causar saturación o estados inestables.",
  "Patrones muy similares (colineales) pueden interferir en recuperaciones correctas (atractores vecinos).",
  "La dinámica de convergencia hacia un estado estable (atractor) se puede visualizar, pero la topología de pesos puede no ser transparente.",
  "Simulación de dinámicas es razonable para tamaños moderados (≤1000 neuronas); grandes redes requieren optimización paralela.",
  "No se usa CV tradicional; se analiza la robustez de memorias variando inicialización o agregando ruido.",
  "No apto si los datos no pueden discretizarse en patrones binarios, o si se requieren múltiples clases de salida simultáneas."
)

tabla_hopfield <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_hopfield %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir hopfield network",
             subtitle = "Hopfield Network")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  

```

## Gated Recurrent Unit (GRU) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/GRU.png"))
```

## Generative Adversarial Networks (GANs) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/GANs.png"))
```

## Long Short-Term Memory (LSTM) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/LSTM.png"))
```

## Multilayer Perceptron (MP)  {-}     

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/MP.png"))
```

El **Multilayer Perceptron (MLP)**, también conocido como **red neuronal de propagación hacia adelante clásica**, es un tipo fundamental de **red neuronal artificial** utilizada para una amplia gama de tareas de **aprendizaje supervisado**, incluyendo **clasificación** y **regresión**. La idea fundamental del MLP es extender el concepto del Perceptrón simple al incorporar una o más **capas ocultas** entre la capa de entrada y la capa de salida, y utilizando **funciones de activación no lineales** en estas capas. Esta arquitectura de múltiples capas es lo que le confiere a los MLP su capacidad para aprender y modelar relaciones complejas y no lineales en los datos.

La estructura de un MLP típicamente incluye:

1.  **Capa de Entrada:** Recibe las características de entrada del problema.
2.  **Capas Ocultas:** Son una o más capas intermedias donde se realizan cálculos complejos. Cada neurona en una capa oculta recibe entradas de la capa anterior, calcula una suma ponderada de estas entradas (más un sesgo), y luego aplica una **función de activación no lineal** (como la función sigmoide, tanh o ReLU) a esta suma. Es la no linealidad de estas funciones de activación la que permite al MLP aprender relaciones no lineales.
    $$a_j = f\left(\sum_{i=1}^{n} w_{ij} x_i + b_j\right)$$
    Donde $a_j$ es la activación de la neurona $j$, $x_i$ son las entradas de la capa anterior, $w_{ij}$ son los pesos, $b_j$ es el sesgo, y $f$ es la función de activación no lineal.
3.  **Capa de Salida:** Produce la predicción final de la red. La función de activación en esta capa depende del tipo de problema (ej., una función lineal para regresión, softmax para clasificación multiclase, o sigmoide para clasificación binaria).

El entrenamiento de un MLP se realiza típicamente utilizando el algoritmo de **Back-Propagation**, que ajusta los pesos de la red de manera iterativa para minimizar una función de pérdida (error) calculada en la capa de salida.

En el contexto del **aprendizaje global vs. local**, el Multilayer Perceptron es el paradigma de un sistema de **aprendizaje global**. La red aprende una **aproximación de función global** que mapea las entradas a las salidas, buscando minimizar la función de pérdida en todo el conjunto de datos de entrenamiento. A diferencia de los sistemas de aprendizaje local que dividen explícitamente el problema global en múltiples problemas más pequeños, el MLP ajusta todos sus pesos de forma interconectada para aprender una representación distribuida de los patrones en los datos. Si los datos no se distribuyen linealmente, el MLP es excepcionalmente capaz de modelar estas relaciones complejas gracias a sus capas ocultas y funciones de activación no lineales. Esto aborda directamente la desventaja de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en modelos lineales o más simples, ya que el MLP puede construir representaciones internas de gran complejidad para aproximar casi cualquier función continua. Hoy en día, los MLP son la base de muchas arquitecturas de "Deep Learning".


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica (clasificación) o Continua (regresión)",
  "✅ Numéricas (normalizar) y categóreas (dummies)",
  "✅ Captura relaciones no lineales profundas",
  "❌ No requiere supuestos de normalidad",
  "✅ Deseable, aunque no obligatorio",
  "❌ No se asume homoscedasticidad",
  "⚠️ Moderado (puede requerir robustez ante outliers)",
  "⚠️ Afecta la convergencia si es muy alta",
  "⚠️ Baja (modelo tipo 'caja negra')",
  "⚠️ Depende de arquitectura y tamaño del dataset",
  "✅ Recomendable (k-fold o repeated CV)",
  "❌ No conviene con pocos datos o ruido elevado"
)

detalles <- c(
  "Red neuronal con múltiples capas ocultas y función de activación no lineal.",
  "Clasificación con softmax/sigmoide; regresión con capa lineal en salida.",
  "Debe escalarse cada característica; las categóricas convierten a variables indicadoras.",
  "Aprende patrones complejos combinando múltiples capas y neuronas.",
  "No impone distribución específica de errores, se optimiza con optimizadores basados en gradiente.",
  "Funciona mejor si las muestras son independientes; sensibles a dependencia temporal sin ajustes.",
  "No requiere varianza constante, ajusta pesos en cada mini-batch o lote.",
  "Outliers pueden causar gradientes explosivos o desaparecidos sin mecanismos de robustez.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; batch normalization ayuda.",
  "Difícil de interpretar cada peso/neuronas; se usan técnicas como SHAP o LIME para explicación.",
  "El tiempo de entrenamiento aumenta con cada capa, neuronas y epochs; GPUs aceleran el proceso.",
  "Crucial para ajustar hiperparámetros: número de capas, neuronas por capa, tasa de aprendizaje, regularización.",
  "No útil para datasets muy pequeños (sobreajuste) o altamente ruidosos sin regularización."
)

tabla_mp <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mp %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir MP",
             subtitle = "Multilayer Perceptron (MP)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Perceptron  {-}     

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Perceptron.png"))
```

El **Perceptron** es el algoritmo de **aprendizaje supervisado** más simple y uno de los primeros modelos de **redes neuronales artificiales**, propuesto por Frank Rosenblatt en 1957. Está diseñado para tareas de **clasificación binaria**, es decir, para decidir si una entrada pertenece a una de dos clases posibles. Su idea fundamental es modelar cómo una neurona biológica podría tomar decisiones.

El funcionamiento de un Perceptron es bastante directo:

1.  **Entradas y Pesos:** Recibe múltiples **entradas** (características) y a cada entrada se le asigna un **peso**. Estos pesos representan la importancia de cada característica.
2.  **Suma Ponderada:** Las entradas se multiplican por sus respectivos pesos y se suman. A esta suma se le añade un **término de sesgo (bias)**.
    $$z = \sum_{i=1}^{n} w_i x_i + b$$
    Donde $x_i$ son las entradas, $w_i$ son los pesos, $b$ es el sesgo, y $n$ es el número de entradas.
3.  **Función de Activación:** El resultado de la suma ponderada ($z$) se pasa a través de una **función de activación** (generalmente una función escalón o *step function*). Esta función decide la salida final, que es 1 si la suma excede un umbral (o 0 si no lo excede). Para el Perceptron original, la salida es binaria.
    $$\text{salida} = \begin{cases} 1 & \text{si } z \geq \text{umbral} \\ 0 & \text{si } z < \text{umbral} \end{cases}$$
4.  **Aprendizaje (Regla de Perceptron):** El Perceptron aprende ajustando sus pesos de forma iterativa. Si la predicción es incorrecta, los pesos se actualizan para reducir el error en la siguiente iteración. La regla de actualización de pesos es:
    $$w_i^{\text{nuevo}} = w_i^{\text{anterior}} + \alpha \cdot (y - \hat{y}) \cdot x_i$$
    Donde $\alpha$ es la tasa de aprendizaje, $y$ es el valor real, y $\hat{y}$ es la predicción del Perceptron.

En el contexto del **aprendizaje global vs. local**, el Perceptron es un sistema de **aprendizaje global** por naturaleza. Busca encontrar un **hiperplano de separación lineal** único que divida el espacio de características en dos regiones. La idea es que, si los datos son **linealmente separables** (es decir, si existe una línea, plano o hiperplano que puede separar perfectamente las dos clases), el Perceptron está garantizado para converger y encontrar esa solución.

Sin embargo, precisamente porque busca una solución lineal global, si los datos no se distribuyen linealmente (es decir, no son linealmente separables), el Perceptron **no puede encontrar una solución convergente** y no puede aprender la relación. Esto ilustra la desventaja de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" cuando se busca una solución global rígida. El Perceptron original no puede aplicar el concepto de regresión ponderada localmente ni adaptarse a complejidades no lineales, a diferencia de modelos posteriores como las redes neuronales multicapa con funciones de activación no lineales o los algoritmos de árboles de decisión. A pesar de esta limitación, el Perceptron sentó las bases para el desarrollo posterior de redes neuronales más complejas.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Categórica binaria (0/1)",
  "✅ Numéricas (requiere normalizar), Categóricas como dummies",
  "⚠️ Aprendizaje lineal: separabilidad lineal requerida",
  "❌ No requiere",
  "✅ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "❌ No se asume homoscedasticidad",
  "⚠️ Moderado (outliers pueden cambiar el hiperplano)",
  "⚠️ Afecta la convergencia si está muy alta",
  "⚠️ Baja (modelo básico de una capa sin capas ocultas)",
  "✅ Muy rápido para datasets medianos",
  "✅ Útil para evaluar margen de separación",
  "❌ No sirve si las clases no son linealmente separables"
)

detalles <- c(
  "Red neuronal de una sola capa que ajusta un hiperplano separador.",
  "Diseñado para clasificación binaria; no predice valores continuos.",
  "Todas las features deben ser numéricas y escaladas; las categóricas deben convertirse en indicadores.",
  "Busca maximizar el margen de separación lineal entre dos clases; no captura no linealidades.",
  "No exige distribución normal de errores ya que optimiza con perceptrón simple.",
  "Funciona mejor si las instancias son independientes; sensible a dependencias temporales sin ajuste.",
  "No se basa en varianza de errores; el algoritmo actualiza pesos sin supuestos de varianza.",
  "Los valores extremos cercanos al margen pueden forzar ajustes bruscos de pesos.",
  "La colinealidad puede ralentizar la convergencia, aunque no impide la definición de hiperplano.",
  "Fácil de entender: el peso de cada característica indica dirección del hiperplano.",
  "Entrenamiento rápido usando regla de aprendizaje por error; escalable a datos medianos.",
  "Se usa CV para ajustar tasa de aprendizaje y número de épocas para evitar bajo/sobreajuste.",
  "Inútil si las clases no se pueden separar linealmente; requiere extensiones (por ejemplo, kernel) para no linealidad."
)

tabla_perceptron <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_perceptron %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir perceptron",
             subtitle = "Perceptron")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Radial Basis Function Network (RBFN)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/RBFN.png"))
```

**Radial Basis Function Network (RBFN)** es un tipo de **red neuronal artificial** que se utiliza tanto para tareas de **clasificación** como de **regresión**. A diferencia de las redes neuronales multicapa perceptrón tradicionales que utilizan funciones de activación sigmoide o ReLU, las RBFN emplean **funciones de base radial** como sus funciones de activación en la capa oculta. Su estructura es típicamente más simple que un perceptrón multicapa, consistiendo generalmente en tres capas: una capa de entrada, una capa oculta con neuronas de base radial, y una capa de salida.

La idea fundamental de una RBFN radica en su capacidad para modelar relaciones no lineales al mapear datos de entrada a un espacio de características de mayor dimensión donde pueden ser **linealmente separables** (para clasificación) o donde una **función lineal** puede aproximar la relación (para regresión). Esto se logra a través de las neuronas de la capa oculta, cada una de las cuales representa un "centro" en el espacio de características.

El funcionamiento de una RBFN implica:

1.  **Capa de Entrada:** Recibe las características de entrada.
2.  **Capa Oculta (Neuronas de Base Radial):** Cada neurona en esta capa tiene un **centro** ($c_i$) y un **radio (o desviación estándar, $\sigma_i$)**. La función de activación de estas neuronas (comúnmente una **función Gaussiana**) calcula la **distancia** entre el vector de entrada ($x$) y el centro de la neurona ($c_i$), y luego aplica la función de base radial. Cuanto más cerca esté la entrada del centro de la neurona, mayor será la activación de esa neurona.
    $$\phi_i(x) = \exp\left(-\frac{\|x - c_i\|^2}{2\sigma_i^2}\right)$$
    Donde $\phi_i(x)$ es la salida de la neurona $i$, $\|x - c_i\|$ es la distancia euclidiana entre la entrada $x$ y el centro $c_i$, y $\sigma_i$ es el radio (ancho) de la función Gaussiana.
3.  **Capa de Salida:** Las salidas de las neuronas de la capa oculta se combinan linealmente (ponderadas por unos coeficientes, $w_{ij}$) para producir la salida final de la red. Para regresión, es una suma ponderada; para clasificación, a menudo se usa una función de activación softmax.
    $$y_j = \sum_{i=1}^{M} w_{ij}\phi_i(x)$$
    Donde $y_j$ es la salida $j$, $M$ es el número de neuronas ocultas, y $w_{ij}$ son los pesos de la capa de salida.

En el contexto del **aprendizaje global vs. local**, las RBFN son intrínsecamente sistemas de **aprendizaje local**. Cada neurona de la capa oculta es sensible a una **región específica** del espacio de entrada, definida por su centro y su radio. La red como un todo es una combinación de estas respuestas locales. Si los datos no se distribuyen linealmente, el concepto de regresión (o clasificación) se aplica de forma muy eficaz mediante esta naturaleza de **regresión ponderada localmente**. Las RBFN pueden aproximar cualquier función continua con la suficiente cantidad de neuronas de base radial. Esto aborda directamente la desventaja de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en un solo modelo global, ya que la red puede adaptarse localmente a las características de diferentes regiones del espacio de datos. Son particularmente útiles para problemas de aproximación de funciones, series de tiempo y reconocimiento de patrones.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado",
  "✅ Continua (regresión) o Categórica (clasificación)",
  "✅ Numéricas (requiere normalización), Categóricas como dummies",
  "✅ Captura no linealidades mediante funciones base radiales",
  "❌ No requiere supuestos de normalidad",
  "✅ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "❌ No asume varianza constante",
  "⚠️ Moderadamente (centros pueden verse alterados por outliers)",
  "⚠️ Puede influir en la selección de centros, pero no tan crítico como en OLS",
  "⚠️ Baja (la capa oculta con RBF es difícil de interpretar)",
  "⚠️ Moderada (depende de número de centros y dimensiones)",
  "✅ Recomendable para ajustar número de bases y spread",
  "❌ Datos muy grandes o alta dimensionalidad sin reducción, mucho ruido"
)

detalles <- c(
  "Red neuronal de una capa oculta con funciones radial basis como activación.",
  "Para regresión predice un valor continuo; para clasificación usa votación o softmax sobre salidas.",
  "Requiere que las características numéricas estén escaladas; las categóricas deben convertirse a variables indicadoras.",
  "Cada neurona oculta calcula una función gaussiana (u otra RBF) centrada en un punto, captando curvas suaves.",
  "No impone distribución normal en los errores, pues optimiza en función de mínimos cuadrados o cross-entropy.",
  "Funciona mejor si las observaciones son independientes; sensible a estructuras de dependencia sin modelar.",
  "No requiere homocedasticidad ya que no se basa en un modelo paramétrico de error con varianza fija.",
  "Los valores extremos pueden desplazar los centros de las RBF, afectando la forma del modelo.",
  "La colinealidad puede dificultar la determinación de centros óptimos, pero no invalida el ajuste.",
  "Las neuronas ocultas representan combinaciones complejas de características, por lo que el modelo es tipo 'caja negra'.",
  "El entrenamiento implica fijar o aprender centros y spreads; para muchos centros o dimensiones altas, el costo crece rápido.",
  "Se usa CV para elegir el número de bases (centros) y el parámetro de ancho (`sigma` o `spread`) para evitar sobreajuste.",
  "No conviene cuando hay decenas de miles de características sin reducción previa o cuando el ruido es muy alto."
)

tabla_rbfn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rbfn %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir RBFN",
             subtitle = "Radial Basis Function Network (RBFN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Recurrent Neural Networks (RNNs) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/RNNs.png"))
```

**Recurrent Neural Networks (RNNs)** son un tipo de **red neuronal artificial** diseñado específicamente para manejar **datos secuenciales** o temporales, donde la información de pasos anteriores en la secuencia es relevante para la predicción actual. A diferencia de las redes de propagación hacia adelante (como MLP o CNN) que asumen que las entradas son independientes entre sí, las RNNs tienen "memoria" o **conexiones recurrentes** que les permiten mantener un **estado interno** que encapsula información de pasos de tiempo anteriores. Esta característica las hace ideales para tareas como el procesamiento de lenguaje natural (PLN), el reconocimiento de voz, la traducción automática y la predicción de series de tiempo.

La idea fundamental de una RNN es que una **unidad recurrente** aplica la misma función de transformación a cada elemento de una secuencia, con la particularidad de que la salida de la unidad en un paso de tiempo dado se realimenta como entrada para el mismo proceso en el siguiente paso de tiempo. Esto permite que la red "recuerde" y utilice información pasada al procesar la secuencia actual.

El funcionamiento básico de una RNN en un paso de tiempo ($t$) implica:

1.  **Entrada actual ($x_t$):** El elemento actual de la secuencia.
2.  **Estado oculto anterior ($h_{t-1}$):** La "memoria" o estado interno de la red del paso de tiempo anterior.
3.  **Cálculo del Estado Oculto Actual ($h_t$):** Se combina la entrada actual y el estado oculto anterior, y se aplica una función de activación (ej., tanh o ReLU).
    $$h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$
    Donde $W_{hh}$ son los pesos de la conexión recurrente, $W_{xh}$ son los pesos de la entrada, y $b_h$ es el sesgo.
4.  **Salida Actual ($y_t$):** Se genera una salida a partir del estado oculto actual.
    $$y_t = W_{hy} h_t + b_y$$
    Donde $W_{hy}$ son los pesos de la salida y $b_y$ es el sesgo.

Este proceso de actualización de estado y salida se repite para cada elemento de la secuencia. La "memoria" de la RNN está codificada en el estado oculto que se pasa de un paso de tiempo al siguiente.

El entrenamiento de las RNNs se realiza mediante una variante del algoritmo de Back-Propagation llamada **Back-Propagation Through Time (BPTT)**. BPTT desenrolla la red a lo largo del tiempo, tratando cada paso de tiempo como una capa separada, y luego aplica la retropropagación de manera similar a cómo se entrena un MLP, pero propagando los errores a través de las conexiones recurrentes. Sin embargo, las RNNs simples pueden sufrir de problemas como el **desvanecimiento del gradiente** (vanishing gradient) o el **explosión del gradiente** (exploding gradient) para secuencias largas, lo que llevó al desarrollo de arquitecturas más avanzadas como **LSTM (Long Short-Term Memory)** y **GRU (Gated Recurrent Unit)**.

En el contexto del **aprendizaje global vs. local**, las RNNs son sistemas de **aprendizaje global** que están diseñados para aprender y modelar **dependencias temporales y patrones secuenciales** en un dominio global. A diferencia de los métodos de regresión ponderada localmente como LOESS, que se enfocan en ajustar curvas en regiones específicas de datos, las RNNs intentan aprender una función de mapeo compleja que considera toda la secuencia histórica para producir una predicción. Si los datos (secuenciales) no se distribuyen linealmente, las RNNs son extremadamente efectivas para capturar estas relaciones no lineales y dependencias a largo plazo. Al tener un estado interno que recuerda información pasada, abordan directamente la limitación de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en modelos estáticos o lineales, ya que pueden adaptar sus predicciones dinámicamente en función del contexto secuencial, lo que las convierte en una herramienta fundamental para el análisis de series de tiempo y el procesamiento de lenguaje.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "⚠️ Supervisado secuencial",
  "✅ Continua (regresión) o Categórica (clasificación) en secuencias",
  "✅ Series temporales y datos secuenciales (texto, audio, series) convertidos a vectores",
  "✅ Captura dependencias temporales y de largo plazo entre pasos de la secuencia",
  "❌ No requiere supuestos de normalidad de residuos",
  "⚠️ Ideal si las secuencias son independientes entre sí; no modela dependencia exógena automáticamente",
  "❌ No asume homoscedasticidad",
  "⚠️ Moderado (outliers en la serie pueden sesgar el entrenamiento si no se detectan)",
  "⚠️ La colinealidad en características secuenciales puede afectar la convergencia (usar embeddings o reducción)",
  "⚠️ Baja (“caja negra” con muchas capas; usar técnicas de atención o visualización de activaciones)",
  "⚠️ Lento sin GPU/TPU; entrenamiento costoso para secuencias largas o redes profundas",
  "⚠️ Usar validación cronológica (time series split) es más apropiado que k-fold clásico",
  "❌ No conviene con muy pocas muestras temporales, secuencias extremadamente largas sin truncar, o datos muy ruidosos"
)

detalles <- c(
  "Red neuronal recurrente que procesa datos en pasos temporales manteniendo un estado interno.",
  "En clasificación, etiqueta cada elemento o secuencia; en regresión, predice valores continuos a lo largo del tiempo.",
  "Las entradas deben transformarse en vectores o embeddings; por ejemplo, texto a índices, series normalizadas.",
  "La arquitectura RNN (LSTM, GRU) retiene información de pasos anteriores para afectar salidas posteriores.",
  "No impone distribución en errores, ya que se optimiza vía descenso de gradiente sobre secuencias.",
  "Funciona mejor si cada secuencia (serie) es independiente; para datos con autocorrelación compleja, usar variantes especializadas.",
  "No requiere varianza constante, pues se basa en propagación de estado y no en un término de error paramétrico.",
  "Valores atípicos en la serie pueden provocar gradientes explosivos o desvanecidos sin mecanismos como clipping.",
  "La representación internal de patrones secuenciales puede verse afectada si hay características muy correlacionadas; usar regularización.",
  "Difícil interpretar pesos internos; se usan mecánicas como atención (attention) o visualización de celdas LSTM.",
  "El entrenamiento con backpropagation through time es intensivo; GPUs o TPUs aceleran enormemente el proceso.",
  "Para series temporales, se prefiere validación basada en ventanas de tiempo (rolling/expanding window) en lugar de random split.",
  "No apto si las secuencias son muy cortas o muy pocas, o hay mucho ruido sin filtrado; en esos casos, usar modelos estadísticos simples."
)

tabla_rnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rnn %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir RNN",
             subtitle = "Recurrent Neural Networks (RNNs)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Restricted Boltzmann Machine (RBM) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/RBM.png"))
```

## Transformers  {-}  


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Transformers.png"))
```

Los **Transformers** son una arquitectura de **red neuronal profunda** que ha revolucionado el campo del **Procesamiento de Lenguaje Natural (PLN)** y, más recientemente, se ha expandido a la visión por computadora y otras áreas. Introducidos en el artículo "Attention Is All You Need" (Vaswani et al., 2017), la idea fundamental de los Transformers es prescindir de la naturaleza recurrente de las RNNs y las convolucionales de las CNNs, basándose enteramente en un mecanismo llamado **auto-atención (self-attention)** para capturar dependencias de largo alcance en las secuencias de entrada.

Antes de los Transformers, las RNNs eran el modelo dominante para datos secuenciales. Sin embargo, las RNNs tenían limitaciones como la dificultad para capturar dependencias a muy largo plazo (problema del gradiente desvanecido) y la imposibilidad de paralelizar completamente el procesamiento de secuencias (debido a su naturaleza secuencial). Los Transformers resuelven estos problemas al permitir que cada elemento de la secuencia interactúe directamente con todos los demás elementos de la secuencia, sin importar su distancia.

Los componentes clave de un Transformer incluyen:

1.  **Mecanismo de Auto-Atención (Self-Attention):** Este es el corazón del Transformer. Para cada token (palabra) en una secuencia, el mecanismo de auto-atención calcula una puntuación de "relevancia" entre ese token y todos los demás tokens de la secuencia. Esto permite que el modelo "pese" la importancia de cada token al generar la representación de otro token. Este proceso se implementa a través de tres vectores para cada token: **Query (Q)**, **Key (K)** y **Value (V)**.
    $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
    Donde $d_k$ es la dimensión de los vectores Key.

2.  **Atención Multi-Cabeza (Multi-Head Attention):** Para mejorar la capacidad del modelo de enfocarse en diferentes aspectos de la secuencia, el mecanismo de auto-atención se aplica múltiples veces en paralelo con diferentes conjuntos de matrices de pesos (cabezas). Las salidas de estas cabezas se concatenan y se transforman linealmente.

3.  **Capas Feed-Forward (Posición por Posición):** Después del mecanismo de atención, hay una red neuronal de propagación hacia adelante (un MLP simple) que se aplica de forma independiente a cada posición en la secuencia.

4.  **Codificador-Decodificador (Encoder-Decoder Architecture):** El Transformer original consta de un **codificador** y un **decodificador**.
    * El **codificador** toma la secuencia de entrada y genera una representación. Consiste en múltiples capas idénticas, cada una con una capa de auto-atención multi-cabeza y una capa feed-forward.
    * El **decodificador** toma la representación del codificador y genera la secuencia de salida (por ejemplo, la traducción). También consiste en múltiples capas, cada una con auto-atención multi-cabeza, atención multi-cabeza (que atiende a la salida del codificador) y una capa feed-forward.

5.  **Codificación Posicional (Positional Encoding):** Dado que los Transformers procesan secuencias en paralelo y no tienen una noción inherente de la posición de los tokens (a diferencia de las RNNs), se añade información de la posición de cada token a sus incrustaciones de entrada.

En el contexto del **aprendizaje global vs. local**, los Transformers son un sistema de **aprendizaje global** que, gracias a su mecanismo de atención, pueden aprender **dependencias a largo alcance** y relaciones complejas que son inherentemente globales en la secuencia. Aunque los cálculos individuales de atención pueden verse como una forma de ponderación de la importancia local de los tokens, la red en su conjunto construye una representación global de la secuencia. Si los datos (secuenciales) no se distribuyen linealmente, los Transformers son excepcionalmente capaces de modelar estas relaciones no lineales y dependencias a través de su capacidad para "observar" toda la secuencia a la vez y ponderar la relevancia de cada parte. Esto resuelve de manera fundamental la limitación de que "a veces ningún valor de parámetro puede proporcionar una aproximación suficientemente buena" en modelos secuenciales anteriores, ya que la arquitectura de atención les permite aprender patrones complejos y no lineales en datos secuenciales sin las restricciones de memoria de las RNNs, lo que los convierte en la arquitectura dominante para tareas de PLN avanzadas.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "⚠️ Supervisado (frecuentemente secuencial o multitarea)",
  "✅ Categórica (clasificación) o Continua (regresión en secuencias)",
  "✅ Texto, secuencias, imágenes en vectores o embeddings",
  "✅ Captura dependencia secuencial y global mediante mecanismos de atención",
  "❌ No requiere supuestos de normalidad en residuos",
  "⚠️ Ideal si las muestras o secuencias son independientes; para datos correlacionados usar variantes específicas",
  "❌ No asume homoscedasticidad",
  "⚠️ Moderado (outliers en embeddings o entradas ruidosas pueden afectar atención)",
  "⚠️ La colinealidad en embedding space puede ralentizar aprendizaje; usar regularización",
  "⚠️ Baja (modelo de ’caja negra’, requieren métodos como attention visualization o interpretabilidad basada en pesos)",
  "⚠️ Lento sin hardware especializado (secuencialidad en atención puede ser costosa)",
  "⚠️ Validación temporal o k-fold anidada, según tarea; en NLP se prefiere holdout sobre texto sin mezclar",
  "❌ No es apropiado con muy pocos datos de entrenamiento o sin estructura secuencial clara"
)

detalles <- c(
  "Arquitectura basada en capas de atención para procesar secuencias completas en paralelo.",
  "Modelos como BERT, GPT, T5 pueden usarse para tareas de clasificación, traducción, regresión de valores continuos en secuencias.",
  "Entradas requieren tokenización y conversión a embeddings; pueden combinarse varias modalidades.",
  "La auto‐atención global permite capturar relaciones a largo y corto plazo sin sesgo posicional estricto.",
  "No impone distribución paramétrica de errores; se entrena con optimizadores basados en pérdidas cross‐entropy o MSE.",
  "Se prefiere que las secuencias en el batch no sean dependientes; para series de tiempo, usar variantes como Time‐Series Transformer.",
  "No se modela varianza del error; el entrenamiento se enfoca en minimizar función de pérdida directa.",
  "Ruido en texto (typos) o en datos numéricos de entrada puede inducir atención errática; usar limpieza de datos y regularización.",
  "Los embeddings pueden contener información redundante de características correlacionadas; ajustar tamaño de embedding y regularización.",
  "Interpretabilidad limitada; se usan técnicas como visualización de mapas de atención, LIME, SHAP para entender decisiones.",
  "El cómputo de atención es O(n²) en longitud de secuencia; GPUs/TPUs o variantes eficientes (Linformer, Performer) alivian costo.",
  "Para tareas de texto, a veces se usa train/validation/test sin CV clásica; para tareas generales, k-fold anidada ayuda a elegir hiperparámetros.",
  "No es adecuado con datasets muy pequeños, sin preentrenamiento o sin estructuras secuenciales definidas."
)

tabla_transformers <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_transformers %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir transformers",
             subtitle = "Transformers")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```





<!--chapter:end:04-neural-networks.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 🧩 5. Reducción de Dimensionalidad {-}  

**Ejemplos:** PCA (Análisis de Componentes Principales), t-SNE, UMAP.   
**Uso:** Fundamental para **visualizar datos de alta dimensión**, haciéndolos más comprensibles. También es un paso clave de **preprocesamiento** para eliminar ruido o multicolinealidad antes de aplicar otros modelos.  
**Ventajas:** Puede **mejorar significativamente el rendimiento y la velocidad** de otros algoritmos de *machine learning*.  
**Limitaciones:** A veces se **pierde la interpretabilidad** de los datos originales y no siempre garantiza una mejora en el desempeño de los modelos.   

---

## Flexible Discriminant Analysis (FDA)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/FDA.png"))
```

**Flexible Discriminant Analysis (FDA)** es un método de **clasificación** que generaliza el Análisis Discriminante Lineal (LDA) para manejar **relaciones no lineales** entre las variables predictoras y las clases. A diferencia de LDA, que asume límites de decisión lineales y distribuciones gaussianas con matrices de covarianza iguales, FDA es mucho más adaptable.

FDA logra esta flexibilidad al combinar dos conceptos:
1.  **Optimal Scoring:** Transforma las variables de respuesta categóricas en valores numéricos (scores óptimos) de manera que las clases sean más fácilmente separables linealmente.
2.  **Modelos de Regresión No Paramétricos:** En lugar de usar una regresión lineal simple (como en LDA), FDA utiliza métodos de regresión no paramétricos más flexibles, como las **Multivariate Adaptive Regression Splines (MARS)**. Esto permite que la relación entre las variables transformadas y los scores óptimos sea no lineal, lo que a su vez se traduce en fronteras de decisión no lineales en el espacio original de los datos.

Es decir, FDA toma los datos, los transforma de una manera inteligente para que sean más fáciles de separar, y luego aplica una discriminación lineal en ese espacio transformado, lo que resulta en una frontera de decisión compleja y flexible en el espacio original.

En el contexto del **aprendizaje global vs. local**, FDA se considera un modelo que **integra aspectos de ambos**.

* **Aspecto Global:** El objetivo final de FDA es encontrar una **función discriminante global** que separe las clases en el espacio transformado. Los scores óptimos y las funciones base del método de regresión (como MARS) se aprenden considerando la estructura general de los datos para lograr la mejor separación a nivel global. El modelo resultante es una función que se aplica de manera consistente a cualquier nueva observación.

* **Aspecto Local (debido al uso de modelos no paramétricos como MARS):** La flexibilidad de FDA proviene de su uso de métodos como MARS, que dividen el espacio de las características en **regiones locales** y ajustan relaciones simples dentro de cada una. Esto permite que el modelo se adapte a no linealidades y a cambios en la relación entre las variables en diferentes partes del espacio de datos. Así, si los datos no se distribuyen linealmente, FDA puede construir fronteras de decisión que capturan esas complejidades al "localizar" las relaciones importantes.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica (clases)",
  "✅ Numéricas (puede usar transformaciones)",
  "✅ No lineal (usa regresión flexible en el espacio transformado)",
  "❌ No aplica (no es regresión de residuos)",
  "❌ No aplica directamente",
  "⚠️ FDA suaviza este supuesto al modelar relaciones no lineales",
  "⚠️ Puede ser sensible a outliers, dependiendo del método de ajuste",
  "⚠️ Puede mitigar multicolinealidad si se usa penalización",
  "⚠️ Menos interpretable que LDA, pero permite mayor flexibilidad",
  "⚠️ Menor eficiencia que LDA por mayor complejidad computacional",
  "✅ Validación cruzada útil para seleccionar transformaciones o suavizados",
  "❌ En datos con pocos casos o ruido excesivo puede sobreajustarse"
)

detalles <- c(
  "Extensión de LDA que permite relaciones no lineales entre predictores y clases mediante técnicas como splines o regresión flexible.",
  "Clasifica observaciones en clases categóricas basándose en predictores transformados.",
  "Admite variables numéricas, las cuales pueden ser transformadas de forma no lineal.",
  "Usa regresión no lineal flexible (como splines) para modelar relaciones complejas en el espacio de discriminación.",
  "No genera residuos como regresión tradicional; es un modelo de clasificación.",
  "No se enfoca en errores secuenciales o dependientes.",
  "Relaja la homocedasticidad al no asumir distribución gaussiana estricta.",
  "Puede verse afectado por valores extremos, según el método de suavizado.",
  "La transformación flexible puede reducir colinealidad, pero no siempre la elimina.",
  "Los coeficientes y funciones discriminantes pueden ser difíciles de interpretar si se usan transformaciones complejas.",
  "Mayor costo computacional que LDA, pero más potente en patrones no lineales.",
  "Se recomienda CV para evaluar desempeño y evitar overfitting en el proceso de ajuste flexible.",
  "Si los datos no requieren flexibilidad o el tamaño muestral es bajo, FDA puede ser innecesariamente complejo."
)

tabla_fda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_fda %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir FDA",
             subtitle = "Flexible Discriminant Analysis (FDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Independent Component Analysis (ICA) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/ICA.png"))
```

## Kernel PCA (KPCA) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/KPCA.png"))
```
  
## Linear Discriminant Analysis (LDA)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/LDA.png"))
```

El **Análisis Discriminante Lineal (LDA)** es un método de **clasificación** y **reducción de dimensionalidad** utilizado para encontrar una combinación lineal de características que mejor separe dos o más clases de objetos o eventos. Su objetivo principal es modelar la diferencia entre las clases, lo que lo hace muy útil para tareas de clasificación supervisada.

LDA funciona proyectando los puntos de datos a un espacio de menor dimensión (generalmente una o pocas dimensiones) de tal manera que las clases estén lo más separadas posible. Para lograr esto, busca una dirección (un eje) que maximice la **separación entre las medias de las clases** (varianza entre clases) mientras minimiza la **varianza dentro de cada clase** (varianza intraclase). En un problema de clasificación binaria, esto significa encontrar la línea óptima para proyectar los datos de modo que las dos clases se superpongan lo menos posible.

A diferencia de modelos como la Regresión Logística, que buscan modelar la probabilidad de pertenencia a una clase, LDA modela directamente la distribución de los datos dentro de cada clase y luego utiliza el Teorema de Bayes para asignar una nueva observación a la clase más probable. LDA asume que las **varianzas (o matrices de covarianza) de las clases son iguales** y que los datos están distribuidos normalmente.

**Aprendizaje Global vs. Local:**

El Análisis Discriminante Lineal (LDA) es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** LDA busca una **única transformación lineal** o un conjunto de direcciones (ejes) que se aplican a **todos los datos** para lograr la máxima separación entre las clases en un espacio de menor dimensión. La frontera de decisión que resulta de LDA es siempre **lineal** y se define globalmente a partir de las medias y las varianzas combinadas (asumidas como iguales) de todas las clases. El modelo es "fijo" y se aplica uniformemente a cualquier nueva observación, sin importar su ubicación específica en el espacio de características. No se ajustan modelos diferentes para distintos vecindarios de datos, sino que se aprende una regla de separación que es válida para todo el dominio.

Por lo tanto, si los datos no se distribuyen linealmente o las fronteras de decisión entre las clases son inherentemente no lineales (por ejemplo, si una clase rodea a otra), LDA puede no ser el método más adecuado. En esos escenarios, modelos de aprendizaje local o más flexibles (como los árboles de decisión, SVM con kernels no lineales, o FDA que extiende LDA para no linealidades) suelen ofrecer un mejor rendimiento.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica (clases)",
  "✅ Numéricas (preferentemente)",
  "✅ Asume relaciones lineales entre variables y clases",
  "✅ Supone normalidad multivariante de los predictores dentro de cada clase",
  "✅ Supone independencia entre observaciones",
  "✅ Asume varianza-covarianza igual entre clases (homocedasticidad)",
  "⚠️ Sensible a valores atípicos",
  "⚠️ Puede verse afectado negativamente por alta colinealidad",
  "✅ Alta, coeficientes discriminantes son interpretables",
  "✅ Muy eficiente computacionalmente",
  "✅ Se recomienda para evaluar estabilidad y evitar sobreajuste",
  "❌ Mal desempeño si no se cumplen supuestos de normalidad y homocedasticidad"
)

detalles <- c(
  "Modelo supervisado clásico para clasificación que encuentra combinaciones lineales de predictores que separan clases.",
  "Requiere una variable categórica como objetivo, con dos o más clases.",
  "Mejor con predictores numéricos continuos; categóricos requieren codificación previa.",
  "Calcula funciones discriminantes lineales que maximizan la separación entre clases.",
  "Cada grupo debe seguir una distribución normal multivariante para resultados óptimos.",
  "Las observaciones deben ser independientes para validez de inferencia.",
  "Supone igual matriz de covarianzas entre grupos; si no se cumple, usar QDA.",
  "Outliers influyen en la media y la varianza estimada, distorsionando fronteras.",
  "Multicolinealidad puede hacer que los coeficientes discriminantes sean inestables.",
  "Las funciones discriminantes se interpretan como direcciones de máxima separación.",
  "Requiere bajo costo computacional y se entrena rápidamente.",
  "Se puede usar validación cruzada para elegir el número de componentes o verificar precisión.",
  "Cuando los datos no cumplen normalidad ni homocedasticidad, el modelo pierde precisión."
)

tabla_lda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lda %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir LDA",
             subtitle = "Linear Discriminant Analysis (LDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Locally Linear Embedding (LLE) {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/LLE.png"))
```
   
## Mixture Discriminant Analysis (MDA)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/MDA.png"))
```

El **Análisis Discriminante de Mezclas (MDA)** es una extensión del Análisis Discriminante Lineal (LDA) y del Análisis Discriminante Cuadrático (QDA) que aborda la limitación de que estas técnicas asumen que cada clase proviene de una única distribución normal (o gaussiana). MDA relaja esta suposición al permitir que **cada clase sea modelada como una mezcla de múltiples distribuciones gaussianas**. Esto le otorga una capacidad significativamente mayor para manejar clases con formas complejas o multimodales, que no pueden ser descritas adecuadamente por una sola distribución normal.  

MDA funciona de la siguiente manera:  

1.  **Modelado por Componentes de Mezcla:** Para cada clase, MDA estima los parámetros (media y matriz de covarianza) de varias distribuciones gaussianas ("componentes de mezcla") en lugar de solo una. Es similar al proceso de agrupamiento de mezclas gaussianas (Gaussian Mixture Models - GMM) aplicado dentro de cada clase.  
2.  **Asignación a la Clase:** Una vez que se han modelado las distribuciones de mezcla para cada clase, para una nueva observación, MDA calcula la probabilidad de que esa observación pertenezca a cada componente de mezcla en cada clase. Luego, asigna la observación a la clase que maximiza la probabilidad posterior, es decir, la clase que es más probable que haya generado esa observación.   
3.  **Fronteras de Decisión Flexibles:** Al modelar cada clase como una mezcla de gaussianas, MDA puede generar fronteras de decisión que son mucho más flexibles y no lineales que las de LDA (que son lineales) o QDA (que son cuadráticas). Esto le permite adaptarse a clases con estructuras complejas, que pueden tener "agrupaciones" internas o formas irregulares.  

Los parámetros del modelo (las medias, covarianzas y pesos de los componentes de mezcla para cada clase) se suelen estimar utilizando un algoritmo iterativo como la **Maximización de Expectativas (Expectation-Maximization - EM)**.   

**Aprendizaje Global vs. Local:**   

El Análisis Discriminante de Mezclas (MDA) se encuentra en un punto intermedio, inclinándose hacia un modelo que **combina aspectos de aprendizaje global y local**, con una mayor flexibilidad para capturar la estructura local de los datos en comparación con LDA o QDA.  

* **Aspecto Global:** Al igual que LDA, el objetivo final de MDA es crear un **clasificador global** que pueda asignar cualquier nueva observación a una de las clases. Las distribuciones de mezcla para cada clase se aprenden a partir de todo el conjunto de datos de entrenamiento para esas clases, y el clasificador resultante se aplica de manera consistente en todo el espacio de características. La regla de decisión final es una función que se deriva de las distribuciones aprendidas para todas las clases.  

* **Aspecto Local:** La "flexibilidad" de MDA y su capacidad para manejar no linealidades proviene de su suposición de que cada clase puede estar compuesta por **múltiples componentes gaussianos**. Esto significa que, dentro de una misma clase, puede haber sub-agrupaciones o densidades locales que son modeladas individualmente. Al permitir estas múltiples distribuciones gaussianas dentro de cada clase, MDA puede adaptarse mejor a las características y densidades de los datos en diferentes **vecindarios o subregiones** del espacio de características. Si los datos no se distribuyen linealmente y tienen formas complejas (como clusters separados dentro de una clase), MDA puede "localizar" y modelar estas estructuras, llevando a fronteras de decisión mucho más complejas y no lineales que se ajustan mejor a la forma real de las clases.   


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica (clases)",
  "✅ Numéricas",
  "✅ No lineal (usa mezclas de gaussianas para modelar clases)",
  "❌ No aplica como en regresión",
  "❌ No se evalúa como en regresión",
  "⚠️ Supone varianza homogénea dentro de componentes, pero puede variar entre clases",
  "⚠️ Puede ser sensible a outliers (afectan las medias y covarianzas)",
  "⚠️ Puede verse afectado, aunque usa reducción dimensional",
  "⚠️ Moderadamente interpretable (depende de componentes gaussianos)",
  "⚠️ Más lento que LDA/QDA, pero más flexible",
  "✅ Recomendable para elegir número de componentes y evitar sobreajuste",
  "❌ Mal desempeño si la distribución dentro de clases no es bien modelada por gaussianas"
)

detalles <- c(
  "Modelo supervisado de clasificación que combina regresión discriminante con mezclas gaussianas dentro de cada clase.",
  "Se usa para clasificar observaciones en grupos definidos por una variable categórica.",
  "Requiere predictores numéricos para ajustar distribuciones normales multivariadas.",
  "Modela cada clase como una combinación de distribuciones gaussianas, permitiendo formas no lineales.",
  "No hay residuos como en regresión, ya que se trata de una tarea de clasificación.",
  "No evalúa independencia clásica de errores; se enfoca en estimar la densidad condicional.",
  "Permite varianza distinta entre componentes, pero se puede ajustar homogeneidad según implementación.",
  "Outliers pueden afectar las medias y varianzas estimadas de las mezclas gaussianas.",
  "La multicolinealidad puede dificultar la estimación de matrices de covarianza.",
  "Interpretar los componentes internos (medias y pesos) puede ser complejo, pero ofrece buena visualización.",
  "Es más lento que LDA o QDA por su naturaleza iterativa y uso de EM (Expectation-Maximization).",
  "Se puede usar validación cruzada para seleccionar el número óptimo de mezclas por clase.",
  "Si las clases no se ajustan bien a combinaciones de gaussianas, el modelo pierde precisión."
)

tabla_mda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mda %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir MDA",
             subtitle = "Mixture Discriminant Analysis (MDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Multidimensional Scaling (MDS)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/MDS.png"))
```


El **Escalamiento Multidimensional (MDS)** es una técnica de **reducción de dimensionalidad** utilizada para visualizar y explorar las **similitudes o disimilitudes** entre un conjunto de objetos. Su objetivo principal es tomar datos de alta dimensión, donde las relaciones entre los puntos pueden ser difíciles de entender, y representarlos en un **espacio de menor dimensión** (típicamente 2D o 3D) de tal manera que las **distancias entre los puntos en el nuevo espacio reflejen lo más fielmente posible las distancias (o disimilitudes) originales** entre los objetos.   

Imagina que tienes una tabla de distancias de viaje entre varias ciudades. MDS intentaría dibujar un mapa de esas ciudades donde las distancias en el mapa se correspondieran lo más posible con las distancias de la tabla.   

El proceso general de MDS implica:   

1.  **Matriz de Disimilitud:** Se necesita una matriz que contenga las disimilitudes (distancias) entre cada par de objetos. Estas disimilitudes pueden ser distancias euclidianas, correlaciones, o cualquier otra medida de qué tan diferentes (o similares) son dos objetos.  
2.  **Optimización:** El algoritmo busca una configuración de puntos en el espacio de menor dimensión que minimice una **función de "estrés" o "ajuste"**. Esta función mide qué tan bien las distancias en el espacio reducido se corresponden con las disimilitudes originales. Una función de estrés baja indica un buen ajuste.  
3.  **Visualización:** Los puntos resultantes en el espacio de menor dimensión pueden ser graficados para revelar patrones, clusters o la estructura subyacente de los datos que no eran evidentes en las dimensiones originales.  

Existen varias variantes de MDS, como el **MDS Clásico (o Métrica)**, que asume que las disimilitudes son distancias euclidianas y busca una solución analítica, y el **MDS No-Métrico**, que solo busca preservar el **orden** de las disimilitudes (es decir, si A es más diferente de B que de C, esa relación se mantendrá en el espacio reducido, sin que las distancias exactas tengan que ser iguales).  


**Aprendizaje Global vs. Local:**  

El Escalamiento Multidimensional (MDS) se considera predominantemente una técnica de **aprendizaje global**.   

* **Aspecto Global:** MDS busca una **configuración única de puntos** en el espacio de baja dimensión que optimice el ajuste de **todas las disimilitudes** en el conjunto de datos de manera simultánea. La función de estrés que se minimiza considera las distancias entre *todos* los pares de puntos, buscando una solución que sea globalmente la mejor representación de esas relaciones. El objetivo es preservar la estructura general de las distancias en el conjunto de datos completo, no solo las relaciones en vecindarios específicos. La solución que se encuentra es una "vista aérea" o un "mapa" de las relaciones de todo el conjunto de datos.   

Aunque las disimilitudes originales son "locales" en el sentido de que son medidas entre pares de puntos, la forma en que MDS utiliza todas estas medidas para construir un mapa coherente y de baja dimensión es un proceso global de optimización. No se ajustan modelos separados para diferentes subconjuntos de datos; en su lugar, se busca una representación unificada que capture la estructura general de similaridad/disimilitud de todos los datos. Por lo tanto, si los datos tienen una estructura global bien definida basada en distancias, MDS es una herramienta efectiva para revelar esa estructura.   


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (reducción de dimensionalidad)",
  "❌ No aplica (no hay variable respuesta)",
  "✅ Numéricas (requiere matriz de distancias)",
  "✅ No lineal en MDS no clásico; lineal en MDS clásico",
  "❌ No aplica",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Sí, valores atípicos afectan distancias",
  "⚠️ No afecta directamente (no hay predictores)",
  "⚠️ Interpretación visual en 2D o 3D, no en ejes significativos",
  "❌ Lento si se usan distancias complejas o muchos puntos",
  "⚠️ Validación mediante 'stress' y visualización",
  "❌ Mal desempeño con datos sin estructura o ruido elevado"
)

detalles <- c(
  "Método no supervisado que proyecta datos de alta dimensión en espacios de 2D o 3D preservando distancias entre puntos.",
  "No busca predecir una variable, solo representar relaciones de cercanía entre observaciones.",
  "Se basa en distancias euclidianas u otras métricas aplicadas a datos numéricos.",
  "MDS clásico es lineal; el no clásico (por ejemplo metric o non-metric MDS) puede modelar relaciones no lineales.",
  "No se modelan residuos, por lo que no aplica la normalidad.",
  "No hay errores de predicción, por tanto no aplica este supuesto.",
  "No hay varianzas residuales, por lo que este supuesto tampoco aplica.",
  "Valores extremos modifican distancias y distorsionan la representación espacial.",
  "Al no haber regresores, la multicolinealidad no es un problema.",
  "El mapa generado se interpreta por proximidad relativa, no por pesos o coeficientes.",
  "Puede ser costoso computacionalmente si hay muchos puntos o si se optimiza la función de estrés.",
  "Se evalúa qué tan bien se preservan las distancias originales con la métrica de estrés o visualmente.",
  "No funciona bien si los datos no tienen estructura clara, están muy dispersos o contienen ruido irrelevante."
)

tabla_mds <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mds %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir MDS",
             subtitle = "Multidimensional Scaling (MDS)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Quadratic Discriminant Analysis (QDA) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/QDA.png"))
```

El **Análisis Discriminante Cuadrático (QDA)** es un método de **clasificación** que, al igual que el Análisis Discriminante Lineal (LDA), modela la distribución de cada clase para clasificar nuevas observaciones. Sin embargo, QDA es una extensión de LDA que relaja una de sus suposiciones clave: mientras que LDA asume que todas las clases comparten la misma matriz de covarianza (es decir, las distribuciones tienen la misma "forma" o "orientación"), **QDA permite que cada clase tenga su propia matriz de covarianza distinta**.  

Esta diferencia es fundamental:  
* **LDA:** Asume que la variación de los datos es la misma en todas las clases, lo que resulta en **fronteras de decisión lineales** entre las clases.  
* **QDA:** Permite que la variación de los datos sea diferente para cada clase, lo que resulta en **fronteras de decisión cuadráticas** entre las clases. Esto significa que las fronteras de decisión pueden ser curvas (elipsoides, parábolas, hipérbolas), lo que permite a QDA modelar relaciones más complejas y no lineales entre las variables y las clases.  

El funcionamiento de QDA implica:  
1.  **Modelado de Distribuciones:** Para cada clase, QDA estima la media y la matriz de covarianza específicas de esa clase, asumiendo una distribución normal multivariada.  
2.  **Clasificación:** Para una nueva observación, QDA calcula la probabilidad de que esa observación provenga de cada clase, utilizando las distribuciones normales modeladas para cada clase. Luego, asigna la observación a la clase con la probabilidad posterior más alta (aplicando el Teorema de Bayes).  

**Aprendizaje Global vs. Local:**  

El Análisis Discriminante Cuadrático (QDA) es, al igual que LDA, un modelo de **aprendizaje global**.  

* **Aspecto Global:** QDA construye un **clasificador global** basado en las distribuciones de probabilidad aprendidas para cada clase. Las medias y las matrices de covarianza se estiman a partir de todo el conjunto de datos de entrenamiento para cada clase, y estos parámetros definen una función discriminante que se aplica de manera uniforme a cualquier nueva observación en el espacio de características. La frontera de decisión, aunque cuadrática y no lineal, es una única función matemática definida a nivel global por los parámetros del modelo. No se ajustan modelos separados para diferentes vecindarios de datos.  

* **Mayor Flexibilidad Globalmente:** Aunque sigue siendo un modelo global, la capacidad de QDA para tener matrices de covarianza separadas para cada clase le otorga una **mayor flexibilidad para adaptarse a formas de clase más diversas** en comparación con LDA. Esto significa que QDA puede modelar situaciones donde las clases tienen diferentes orientaciones o dispersiones en el espacio de características, lo que resulta en fronteras de decisión que pueden capturar ciertas no linealidades de manera global. Sin embargo, sigue asumiendo distribuciones gaussianas para cada clase y una forma cuadrática para las fronteras, lo que puede ser una limitación si la verdadera complejidad de los datos es aún mayor o no se ajusta a estas suposiciones.  



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica (clases)",
  "✅ Numéricas",
  "✅ Modela separación cuadrática entre clases",
  "❌ No aplica (clasificación, no regresión)",
  "❌ No aplica (se asume independencia dentro de clases)",
  "❌ No se asume homoscedasticidad (cada clase tiene su propia matriz de covarianza)",
  "⚠️ Puede ser muy sensible a outliers (afectan las matrices de covarianza)",
  "⚠️ Puede verse afectado, especialmente si hay pocos datos",
  "✅ Relativamente interpretable (fronteras no lineales entre clases)",
  "⚠️ Más costoso que LDA; ineficiente con pocos datos o muchas variables",
  "✅ Recomendado para evitar overfitting, especialmente con pocos datos",
  "❌ Si hay pocos datos por clase, estimar matrices de covarianza es inestable"
)

detalles <- c(
  "Modelo supervisado de clasificación que permite que cada clase tenga su propia matriz de covarianza.",
  "Se utiliza para predecir a qué clase pertenece una observación con base en sus características.",
  "Requiere predictores numéricos continuos, ya que calcula medias y covarianzas.",
  "A diferencia de LDA, permite fronteras no lineales al no asumir varianzas iguales entre clases.",
  "No tiene residuos como en regresión, por lo que el supuesto de normalidad de errores no aplica.",
  "No aplica el supuesto de independencia de errores; se enfoca en la distribución conjunta por clase.",
  "Cada clase tiene su propia varianza y covarianza, lo que lo hace más flexible que LDA.",
  "Valores extremos pueden distorsionar la estimación de medias y covarianzas de cada clase.",
  "Multicolinealidad puede dificultar la inversión de la matriz de covarianza en clases pequeñas.",
  "Los coeficientes y decisiones son interpretables en términos de separaciones estadísticas entre clases.",
  "Más lento y costoso computacionalmente que LDA, especialmente con muchas variables.",
  "La validación cruzada ayuda a prevenir sobreajuste y a seleccionar características relevantes.",
  "Con clases poco representadas o muchas variables, las matrices de covarianza pueden volverse inestables."
)

tabla_qda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles) 

tabla_qda %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir QDA",
             subtitle = "Quadratic Discriminant Analysis (QDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

  
  
## Partial Least Squares Regression (PLSR)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/PLSR.png"))
```

**Partial Least Squares Regression (PLSR)** es una técnica de **regresión multivariada** que combina características de la **regresión por mínimos cuadrados ordinarios (OLS)** y el **análisis de componentes principales (PCA)**. Se utiliza para modelar la relación entre un conjunto de variables predictoras (X) y uno o más conjuntos de variables de respuesta (Y), siendo particularmente útil en situaciones donde hay un gran número de variables predictoras, **multicolinealidad** (altas correlaciones entre las variables predictoras), o cuando el número de predictoras excede el número de observaciones.  

La idea fundamental de PLSR es encontrar un conjunto de **componentes latentes** (también conocidos como "factores" o "variables latentes") tanto en el espacio de las variables X como en el de las variables Y. Estos componentes se construyen de tal manera que **maximizan la covarianza** entre las variables predictoras y las variables de respuesta. A diferencia de PCA, que solo busca componentes que expliquen la máxima varianza en X, PLSR busca componentes que sean relevantes para explicar la varianza en X *y* que también estén altamente correlacionados con Y. Una vez que se extraen estos componentes, se realiza una regresión de mínimos cuadrados ordinarios de Y sobre estos componentes latentes.  

El proceso general de PLSR implica:  

1.  **Extracción de Componentes Latentes:** PLSR construye iterativamente un conjunto de componentes latentes. En cada paso:
    * Identifica una combinación lineal de las variables X (un componente de X) y una combinación lineal de las variables Y (un componente de Y) que tienen la mayor covarianza entre sí.  
    * Estos componentes representan las direcciones en el espacio de datos que explican la mayor cantidad de la relación entre X y Y.
    * Una vez que se extrae un componente, la varianza explicada por ese componente se "deflacta" (se elimina) de las matrices X e Y, y el proceso se repite con los residuos para encontrar el siguiente componente ortogonal.  
2.  **Regresión:** Una vez que se ha determinado el número óptimo de componentes latentes (a menudo a través de validación cruzada), se realiza una regresión lineal estándar de las variables Y sobre estos componentes latentes de X.  

**Ventajas clave de PLSR:**  

* **Manejo de Multicolinealidad:** Es muy efectivo en la reducción de dimensionalidad y el manejo de predictoras altamente correlacionadas, donde la regresión OLS fallaría o produciría estimaciones inestables.  
* **Manejo de Datos de Alta Dimensionalidad:** Funciona bien cuando el número de variables predictoras es mayor que el número de observaciones.   
* **Enfoque Predictivo:** Se centra en desarrollar modelos con una fuerte capacidad predictiva.  

**Aprendizaje Global vs. Local:**  
  
La Regresión por Mínimos Cuadrados Parciales (PLSR) se considera un modelo de **aprendizaje global**.  

* **Aspecto Global:** PLSR construye un **modelo lineal global** que relaciona las variables predictoras con la variable de respuesta a través de sus componentes latentes. Los componentes PLS se derivan de la estructura de covarianza de **todas las variables** (tanto predictoras como de respuesta) en el conjunto de datos completo, y el modelo de regresión final se ajusta sobre estos componentes, generando una ecuación que se aplica de manera consistente a cualquier nueva observación. No se ajustan modelos separados para diferentes vecindarios de datos; en cambio, se busca una transformación global de los datos que facilite la predicción.  

Si bien PLSR no es un método de **regresión ponderada localmente** como LOESS (que ajusta modelos simples a subconjuntos locales de datos), comparte con ellos el objetivo de modelar relaciones complejas. Sin embargo, lo hace de una manera diferente. En lugar de dividir el espacio de características y aplicar modelos locales, PLSR transforma el espacio de características de forma global para encontrar una representación de menor dimensionalidad que sea óptima para la predicción. Cuando los datos no se distribuyen linealmente, PLSR puede no ser la herramienta más adecuada en su forma lineal básica, ya que sigue siendo una técnica lineal. Sin embargo, al encontrar las direcciones más relevantes en el espacio de los datos, puede capturar aspectos importantes de la estructura de los datos que son útiles incluso si la relación subyacente es no lineal. Para manejar la no linealidad explícitamente, existen extensiones como **Nonlinear Partial Least Squares (NPLS)** o **Kernel PLS (KPLS)**, que introducen funciones kernel para mapear los datos a un espacio de características de mayor dimensión donde la relación podría ser linealmente modelable por PLS.  


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "⚠️ Supervisado (regresión y clasificación con adaptación)",
  "✅ Continua (regresión) o Categórica (clasificación si se transforma)",
  "✅ Numéricas (requiere escalado), categóricas como dummies",
  "✅ Captura relaciones lineales y reduce dimensiones simultáneamente",
  "❌ No requiere estrictamente, pero mejora con residuos normales",
  "✅ Deseable, aunque no crítico",
  "✅ Deseable para homogeneizar varianza tras escalado",
  "⚠️ Moderado (outliers pueden influir en componentes latentes)",
  "✅ Diseñado para alta colinealidad entre predictores",
  "⚠️ Media (componentes latentes son interpretables, pero relaciones pueden ser complejas)",
  "⚠️ Moderada (depende de número de componentes y tamaño del dataset)",
  "✅ Usar k-fold para elegir número de componentes óptimos",
  "❌ No funciona bien si relaciones son muy no lineales o datos muy ruidosos sin preprocesar"
)

detalles <- c(
  "Modelo que proyecta predictores y respuesta a espacios latentes para maximizar covarianza.",
  "PLSR encuentra componentes que explican varianza en X y covarianza con Y.",
  "Todas las variables numéricas deben escalarse; convertir categóricas en indicadores.",
  "Combina reducción de dimensión (PCA-like) con regresión en componentes latentes.",
  "No impone supuestos estrictos, pero residuos normales facilitan inferencia estadística.",
  "Mejor si muestras son independientes; RLSR en datos correlacionados requiere cuidado.",
  "Escalar y homogeneizar predictores e incluso respuesta mejora la estabilidad.",
  "Outliers extremos pueden distorsionar cálculo de componentes; usar robust PLSR para mitigarlo.",
  "PLSR maneja colinealidad al construir pocas componentes que representan grupos de variables correlacionadas.",
  "Componentes latentes tienen pesos interpretables, pero interpretar combinaciones puede ser complejo.",
  "El método usa descomposición de matrices; eficiente con BLAS/LAPACK optimizado.",
  "Validación cruzada ayuda a determinar el número óptimo de componentes latentes a usar.",
  "No es adecuado para relaciones puramente no lineales; en ese caso usar Kernel PLSR o métodos no lineales."
)

tabla_plsr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_plsr %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir PLSR",
             subtitle = "Partial Least Squares Regression (PLSR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
``` 


## Partial Least Squares Discriminant Analysis (PLSDA)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/PLSDA.png"))
```

El **Análisis Discriminante de Mínimos Cuadrados Parciales (PLSDA)** es una extensión del algoritmo de **Regresión por Mínimos Cuadrados Parciales (PLSR)**, adaptada para problemas de **clasificación**. Al igual que PLSR, PLSDA es particularmente útil cuando se tienen muchas **variables predictoras (X)** y estas están **altamente correlacionadas (multicolinealidad)**, situaciones comunes en campos como la metabolómica, la proteómica o la espectroscopia.

En esencia, PLSDA transforma un problema de clasificación en un problema de regresión. Esto se logra de la siguiente manera:

1.  **Codificación de la Variable de Clase:** La variable de respuesta categórica (la clase a la que pertenece una observación) se transforma en una o más variables numéricas. Por ejemplo, en un problema de clasificación binaria, una clase puede codificarse como '0' y la otra como '1'. Para múltiples clases, se puede usar una codificación "one-hot encoding" (ej., [1,0,0] para Clase A, [0,1,0] para Clase B, etc.).
2.  **Extracción de Componentes Latentes:** Similar a PLSR, PLSDA construye componentes latentes (factores PLS) que son combinaciones lineales de las variables predictoras. Estos componentes se eligen para maximizar la covarianza entre las variables predictoras y las variables de respuesta codificadas. Esto asegura que los componentes capturen la varianza en X que es relevante para la separación de clases en Y.
3.  **Clasificación:** Una vez que se han obtenido los componentes PLS y se ha realizado la regresión sobre ellos para predecir los valores codificados de la clase, se aplica una regla de decisión (por ejemplo, un umbral o un clasificador lineal simple) a las predicciones para asignar cada observación a una clase. Si se usa codificación one-hot, la observación se asigna a la clase con el valor predicho más alto.

PLSDA es ventajoso porque puede manejar conjuntos de datos con muchas más variables que observaciones (problemas $p \gg n$), y es robusto a la multicolinealidad.


**Aprendizaje Global vs. Local:**

El Análisis Discriminante de Mínimos Cuadrados Parciales (PLSDA) es un modelo de **aprendizaje global**.

* **Aspecto Global:** PLSDA busca una **transformación lineal global** de las variables predictoras a componentes latentes, y luego una **relación lineal global** entre esos componentes y la variable de respuesta codificada (clase). Los componentes PLS se derivan de la estructura de covarianza de todo el conjunto de datos, y el modelo de regresión final (que se usa para la clasificación) se aplica de manera consistente a cualquier nueva observación. La frontera de decisión implícita en PLSDA es típicamente lineal en el espacio de los componentes PLS (y por lo tanto lineal o una combinación lineal de las variables originales), lo que resulta en un clasificador que opera globalmente en el espacio de características.

* **Enfoque en la Relevancia Global:** Aunque reduce la dimensionalidad y selecciona componentes que son relevantes para la respuesta, la solución final es un mapeo y una regla de decisión que son válidos para todo el dominio de los datos. No ajusta modelos locales para diferentes regiones del espacio de características. Por lo tanto, PLSDA es una técnica eficiente para encontrar patrones globales de separación de clases en presencia de alta dimensionalidad y multicolinealidad, pero si las relaciones entre las variables y las clases son inherentemente no lineales o tienen estructuras muy complejas que no pueden ser capturadas por una transformación lineal, su capacidad puede ser limitada.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica (binaria o multicategoría)",
  "✅ Numéricas (se proyectan a componentes)",
  "✅ Captura relaciones lineales y no lineales a través de proyecciones",
  "❌ No aplica directamente (modelo de clasificación)",
  "❌ No aplica como en regresión clásica",
  "❌ No se evalúa como en modelos de regresión",
  "⚠️ Algo sensible a outliers (pueden influir en componentes)",
  "✅ Muy útil si hay multicolinealidad",
  "⚠️ Menos interpretable que modelos clásicos; depende de componentes",
  "✅ Eficiente, especialmente con datos de alta dimensión",
  "✅ Se recomienda usar validación cruzada para elegir el número de componentes",
  "❌ Si las proyecciones no separan bien las clases o hay mucho ruido"
)

detalles <- c(
  "Modelo supervisado de clasificación basado en PLS (Partial Least Squares) que proyecta los datos para maximizar la separación entre clases.",
  "Se requiere que la variable dependiente sea categórica. PLS-DA funciona bien con 2 o más clases.",
  "Las variables predictoras deben ser numéricas para que el modelo pueda proyectarlas en componentes latentes.",
  "El modelo encuentra combinaciones de predictores que mejor separan las clases en el espacio proyectado.",
  "No se evalúa normalidad de residuos como en modelos de regresión; la salida es de clasificación.",
  "Tampoco aplica la independencia clásica de errores ya que se clasifican observaciones.",
  "El supuesto de homoscedasticidad no es relevante aquí.",
  "Outliers pueden afectar la construcción de componentes, distorsionando la separación de clases.",
  "PLS-DA es útil cuando los predictores están altamente correlacionados, ya que crea componentes ortogonales.",
  "Los componentes no son directamente interpretables como las variables originales, aunque se pueden analizar los pesos de carga.",
  "Es un algoritmo relativamente eficiente, especialmente para conjuntos con muchas variables.",
  "La validación cruzada es crítica para seleccionar el número óptimo de componentes y evitar overfitting.",
  "No funciona bien si las clases no están bien separadas en el espacio proyectado o si hay demasiado ruido en los datos."
)

tabla_plsda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_plsda %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir PLSDA",
             subtitle = "Partial Least Squares Discriminant Analysis (PLSDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Principal Component Analysis (PCA)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/PCA.png"))
```

El **Análisis de Componentes Principales (PCA)** es una técnica fundamental de **reducción de dimensionalidad** no supervisada. Su objetivo principal es simplificar conjuntos de datos complejos con muchas variables, transformándolos en un conjunto más pequeño de nuevas variables, llamadas **componentes principales**, sin perder demasiada información. Estos componentes principales son combinaciones lineales de las variables originales y son **ortogonales (no correlacionados)** entre sí.

PCA funciona identificando las direcciones en el espacio de datos donde la **varianza es máxima**. La primera componente principal (PC1) captura la mayor cantidad de varianza posible en los datos. La segunda componente principal (PC2) captura la mayor varianza restante, sujeta a ser ortogonal a la primera, y así sucesivamente. De esta manera, PCA organiza la varianza en los datos en un conjunto jerárquico de componentes.

Los usos comunes de PCA incluyen:
* **Reducción de dimensionalidad:** Disminuir el número de variables en un dataset, lo que puede acelerar los algoritmos de Machine Learning y reducir el riesgo de sobreajuste.
* **Visualización de datos:** Proyectar datos de alta dimensión en 2D o 3D para facilitar su visualización y la identificación de patrones, clusters o outliers.
* **Denoising:** Eliminar el ruido de los datos al retener solo los componentes principales que capturan la señal real.


**Aprendizaje Global vs. Local:**

El Análisis de Componentes Principales (PCA) es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** PCA busca una **transformación lineal global** del espacio de características. Los componentes principales se derivan de la matriz de covarianza (o correlación) de **todo el conjunto de datos**. Esto significa que las direcciones de máxima varianza se determinan considerando la estructura de dispersión general de todos los puntos de datos. El conjunto de componentes principales que se obtiene es un sistema de coordenadas global al que se proyecta cualquier punto de datos. No se ajustan diferentes transformaciones para distintas regiones o vecindarios de datos; en su lugar, se aprende una única proyección que se aplica uniformemente a todo el dominio.

Por lo tanto, si la estructura de los datos es consistentemente lineal o tiene relaciones de varianza que se extienden linealmente a lo largo del espacio, PCA funcionará muy bien. Sin embargo, si los datos tienen estructuras no lineales complejas (por ejemplo, datos que forman una espiral o una esfera), PCA puede tener limitaciones para capturar estas relaciones, ya que solo busca direcciones lineales de máxima varianza.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "⚠️ No supervisado (reducción de dimensiones)",
  "❌ No aplica (no hay target a predecir)",
  "✅ Numéricas (requiere escalado), categóricas procesadas como dummies",
  "✅ Captura correlaciones lineales entre predictores",
  "❌ No requiere supuestos de distribución en residuos",
  "⚠️ Ideal si las observaciones son independientes, aunque no crítico",
  "✅ Deseable (datos homogenizados tras escalado)",
  "⚠️ Moderado (outliers pueden distorsionar componentes principales)",
  "✅ Sensible a colinealidad (reduce variables correlacionadas a componentes)",
  "⚠️ Media (componentes lineales son interpretables, pero combinaciones pueden no serlo)",
  "✅ Rápido en datasets medianos; escalable con álgebra lineal optimizada",
  "⚠️ No se aplica CV clásico; se puede usar reconstrucción de error o validación por bloques",
  "❌ No funciona bien si las relaciones son no lineales o datos muy ruidosos sin preprocesar"
)

detalles <- c(
  "Método no supervisado para reducir la dimensión del espacio de predictores.",
  "No predice variables, se centra en variabilidad interna de los datos.",
  "Todas las variables numéricas deben escalarse; las categóricas convertir a variables indicadoras.",
  "Busca direcciones (componentes) que maximizan varianza lineal entre predictores.",
  "No impone supuestos sobre errores; se basa en descomposición de la matriz de covarianza.",
  "Mejor si las muestras no están correlacionadas en el tiempo o espacialmente.",
  "Escalar y homogeneizar mejora el cálculo de componentes principales.",
  "Outliers extremos pueden sesgar la dirección de los componentes principales.",
  "Reduce colinealidad al combinar variables correlacionadas en componentes ortogonales.",
  "Componentes iniciales pueden interpretarse mediante pesos, pero componentes posteriores son combinaciones lineales complejas.",
  "Computación depende de descomposición de matrices (SVD), es eficiente con optimización BLAS.",
  "Se puede evaluar número óptimo de componentes con validación de reconstrucción o bootstrap de SVD.",
  "No apto para relaciones no lineales complejas; en tal caso usar Kernel PCA o métodos no lineales."
)

tabla_pca <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pca %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir PCA",
             subtitle = "Principal Component Analysis (PCA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Principal Component Regression (PCR)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/PCR.png"))
```

La **Regresión de Componentes Principales (PCR)** es un método de **regresión** que combina el **Análisis de Componentes Principales (PCA)** con la **Regresión por Mínimos Cuadrados Ordinarios (OLS)**. Su principal utilidad radica en situaciones donde se tienen muchas **variables predictoras (X)** y existe una **alta multicolinealidad** (fuerte correlación entre ellas), lo que puede hacer que los modelos de regresión OLS sean inestables o ineficientes.

El proceso de PCR consta de dos pasos principales:

1.  **Reducción de Dimensionalidad con PCA:** Primero, se aplica PCA a las variables predictoras (X) para transformarlas en un conjunto más pequeño de **componentes principales**. Estos componentes son combinaciones lineales no correlacionadas de las variables originales y capturan la mayor parte de la varianza en las variables X. Se selecciona un subconjunto de estos componentes principales (aquellos que explican la mayor parte de la varianza total) para retener. Es importante destacar que, en este paso, PCA no tiene conocimiento de la variable de respuesta (Y); solo se enfoca en la estructura de las variables X.
2.  **Regresión OLS sobre Componentes:** Una vez que se han obtenido los componentes principales seleccionados, se realiza una regresión lineal estándar (OLS) de la variable de respuesta (Y) sobre estos componentes. Como los componentes principales son ortogonales, la multicolinealidad ya no es un problema en este paso de regresión.

El beneficio de PCR es que permite construir un modelo de regresión en escenarios con multicolinealidad severa, reduciendo el número de variables a un conjunto más manejable y estable, mientras se intenta preservar la mayor cantidad de información de las variables predictoras.

**Aprendizaje Global vs. Local:**

La Regresión de Componentes Principales (PCR) es un modelo de **aprendizaje global**.

* **Aspecto Global:** Ambos pasos de PCR son intrínsecamente globales.
    1.  **PCA (Paso Global):** Como se mencionó anteriormente, PCA es una técnica global que encuentra una transformación lineal de los datos que se aplica de manera uniforme a todo el espacio de características. Los componentes principales se derivan de la estructura de varianza global de las variables predictoras.
    2.  **OLS (Paso Global):** La regresión realizada sobre los componentes principales es un modelo OLS estándar, que también es una técnica global. Busca una única relación lineal que se aplica a todos los datos transformados.

En conjunto, PCR construye una **función de regresión global** que mapea el espacio de características original (transformado a componentes principales) a la variable de respuesta. La solución resultante es una ecuación que se aplica de manera consistente para todas las observaciones, sin ajustar modelos diferentes para subconjuntos locales de datos. Esto significa que si la relación entre las variables predictoras y la respuesta es no lineal o cambia drásticamente en diferentes regiones del espacio de características, PCR podría no ser la opción más flexible, ya que se basa en transformaciones y regresiones lineales globales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (combinación de PCA + regresión)",
  "✅ Variable continua (numérica)",
  "✅ Numéricas (se aplica PCA primero)",
  "✅ Puede capturar relaciones lineales (con reducción de dimensionalidad)",
  "⚠️ Requiere verificar residuos del modelo final",
  "⚠️ Se deben revisar los residuos como en regresión clásica",
  "⚠️ Requiere diagnóstico posterior a la regresión",
  "⚠️ PCA puede estar influenciada por outliers",
  "✅ Reduce multicolinealidad usando componentes ortogonales",
  "⚠️ Menos interpretable (usa componentes, no variables originales)",
  "✅ Eficiente, especialmente con datos de alta dimensión",
  "✅ Puede usar validación cruzada para elegir número de componentes",
  "❌ Si las primeras componentes no explican bien la variable respuesta"
)

detalles <- c(
  "Modelo supervisado que aplica PCA a los predictores y luego ajusta una regresión lineal sobre los componentes principales seleccionados.",
  "Se requiere que la variable dependiente sea numérica (continua).",
  "Se espera que los predictores sean numéricos para aplicar PCA adecuadamente.",
  "PCR puede detectar relaciones lineales al reducir la dimensionalidad primero y luego ajustar la regresión.",
  "Aunque el PCA es no supervisado, los residuos de la regresión deben ser normales para cumplir los supuestos de OLS.",
  "Es necesario revisar la independencia de errores como en cualquier regresión lineal.",
  "También deben analizarse posibles problemas de heterocedasticidad en los residuos.",
  "Outliers pueden influir en los componentes principales y, por lo tanto, en el modelo final.",
  "PCR es muy útil cuando los predictores están altamente correlacionados.",
  "Interpretar los resultados puede ser difícil porque las componentes no corresponden a variables originales.",
  "El proceso es rápido incluso con muchos predictores, ya que PCA reduce la dimensión.",
  "Usualmente se usa validación cruzada para determinar cuántas componentes usar.",
  "No es efectivo si los primeros componentes (con mayor varianza) no están relacionados con la variable dependiente."
)

tabla_pcr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pcr %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir PCR",
             subtitle = "Principal Component Regression (PCR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Projection Pursuit (PP)  {-}     

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/PP.png"))
```

**Projection Pursuit (PP)** es una técnica estadística de **reducción de dimensionalidad y análisis exploratorio de datos** utilizada para encontrar las proyecciones "más interesantes" de datos multivariados de alta dimensión en un espacio de menor dimensión (generalmente 1D o 2D). La clave de PP es que las proyecciones "interesantes" son aquellas que **se desvían más de una distribución normal (gaussiana)**, ya que las estructuras como agrupaciones, valores atípicos, o formas inusuales tienden a ser más evidentes en proyecciones no gaussianas.

El algoritmo de PP no busca simplemente la mayor varianza (como PCA), sino que intenta encontrar direcciones de proyección que revelen la estructura subyacente y las características no lineales de los datos. Lo hace **maximizando un "índice de proyección"** que mide la "interesante" o la "no-gaussianidad" de la proyección. Diferentes índices pueden enfocarse en diferentes aspectos, como la asimetría, la curtosis, o la presencia de múltiples modos (grupos).

Existen variantes de PP para diferentes propósitos, como:
* **Exploratory Projection Pursuit (EPP):** Para visualización y detección de estructuras.
* **Projection Pursuit Regression (PPR):** Para construir modelos de regresión no lineales.
* **Projection Pursuit Classification (PPC):** Para tareas de clasificación.

**Aprendizaje Global vs. Local:**

Projection Pursuit (PP) se puede considerar como un modelo que **combina aspectos de aprendizaje global y local**, con un fuerte énfasis en la detección de características locales en un contexto global.

* **Aspecto Global:** PP busca una **transformación lineal global** (la dirección de proyección) que se aplica a todo el conjunto de datos para encontrar las proyecciones "más interesantes". La optimización del índice de proyección se realiza sobre todo el espacio de características para identificar estas direcciones. Las funciones resultantes (como en PPR o PPC) son combinaciones de funciones no lineales aplicadas a estas proyecciones globales.

* **Aspecto Local (al revelar estructuras):** Donde PP exhibe un carácter "local" es en su capacidad para **resaltar estructuras que son intrínsecamente locales** (como clusters o valores atípicos) que podrían estar ocultas en las altas dimensiones o en proyecciones puramente globales (como PCA). Al buscar desviaciones de la normalidad, PP es capaz de "perseguir" (de ahí "pursuit") las direcciones que exponen agrupaciones densas o huecos en los datos, que son fenómenos locales. La idea es que si los datos no se distribuyen linealmente o tienen estructuras complejas, PP puede encontrar proyecciones donde la "densidad" o "forma" local de los datos es más informativa, permitiendo al usuario o a un algoritmo posterior identificar estas estructuras que son una forma de "regresión ponderada localmente" o un análisis local de patrones.

En resumen, PP es una técnica potente para explorar la estructura de datos de alta dimensión, especialmente cuando las relaciones son no lineales o complejas. Si bien el proceso de búsqueda de proyecciones es global, el "interés" de estas proyecciones a menudo radica en su capacidad para revelar características locales y no gaussianas que son cruciales para entender los datos.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (reducción de dimensionalidad)",
  "❌ No aplica (no hay variable respuesta)",
  "✅ Numéricas (requiere matriz de datos)",
  "✅ Detecta proyecciones no lineales con estructura interesante",
  "❌ No aplica",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Puede ser sensible a valores extremos",
  "⚠️ Puede verse afectado si hay alta redundancia",
  "⚠️ Interpretación más difícil que PCA; proyecciones no son ortogonales",
  "❌ Puede ser lento por búsqueda iterativa de proyecciones",
  "⚠️ Validación subjetiva o basada en heurísticas de interés",
  "❌ No útil si no hay estructuras no gaussianas en los datos"
)

detalles <- c(
  "Método no supervisado que busca proyecciones de los datos donde se maximice cierta 'interesantitud' (varianza no gaussiana, agrupamientos, etc.).",
  "No está diseñado para predicción, sino para exploración visual o estructural.",
  "Se aplica a datos numéricos, generalmente estandarizados, buscando direcciones relevantes.",
  "A diferencia del PCA (que busca máxima varianza), PP busca patrones como colas pesadas, clusters, o distribuciones no normales.",
  "No es un modelo predictivo, por tanto no se calculan residuos.",
  "No se modela el error; se enfoca en la estructura interna de los datos.",
  "No tiene varianzas residuales, por lo que no aplica homoscedasticidad.",
  "Proyecciones pueden verse distorsionadas por valores extremos.",
  "Variables muy correlacionadas pueden dominar las proyecciones si no se controlan.",
  "Proyecciones son difíciles de interpretar directamente; pueden requerir análisis posterior.",
  "Requiere métodos numéricos iterativos para encontrar direcciones de interés, lo que lo vuelve computacionalmente intensivo.",
  "Puede usarse validación visual (por ejemplo, si se detectan agrupamientos) o criterios como 'kurtosis'.",
  "Si los datos son gaussianos y no contienen patrones relevantes, PP no encuentra proyecciones útiles."
)

tabla_pp <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pp %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir PP",
             subtitle = "Projection Pursuit (PP) ")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Sammon Mapping  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/Sammon Mapping.png"))
```


**Sammon Mapping** es una técnica de **reducción de dimensionalidad no lineal** que se utiliza para visualizar datos de alta dimensión en un espacio de menor dimensión (generalmente 2D o 3D). Su principal objetivo es **preservar la estructura de distancia local** de los datos originales en la representación de menor dimensión.

A diferencia de técnicas como PCA que buscan preservar la varianza global (y por lo tanto las distancias euclidianas globales), Sammon Mapping se enfoca en que las **distancias pequeñas** (entre puntos cercanos) en el espacio original sean representadas con **mayor fidelidad** en el espacio reducido que las distancias grandes. Esto lo hace particularmente bueno para revelar agrupaciones o clusters que podrían estar ocultos en proyecciones lineales o en otras técnicas de reducción de dimensionalidad que no priorizan las distancias locales.

El algoritmo de Sammon Mapping funciona minimizando una **función de "error" o "estrés"** específica, conocida como el **"estrés de Sammon"**. Esta función penaliza más fuertemente las grandes discrepancias en las distancias pequeñas que las grandes discrepancias en las distancias grandes. La minimización de esta función se realiza mediante un proceso iterativo de descenso de gradiente.

**Aprendizaje Global vs. Local:**

Sammon Mapping es un modelo que exhibe un fuerte carácter de **aprendizaje local**, aunque la optimización se realiza sobre la totalidad de los datos.

* **Aspecto Local:** La característica distintiva de Sammon Mapping es su énfasis en la **preservación de las distancias locales**. Al penalizar más las distancias pequeñas que se deforman en la proyección, el algoritmo se esfuerza por mantener a los puntos que estaban cerca en el espacio original, cerca en el espacio de menor dimensión. Esto es crucial para revelar la **estructura local y las agrupaciones** dentro de los datos. Es como si el algoritmo estuviera haciendo una serie de "regresiones ponderadas localmente" para cada vecindario de puntos, ajustando las posiciones en el mapa de baja dimensión para que las relaciones cercanas se mantengan. Esta prioridad en las relaciones de vecindad es una marca del aprendizaje local.

* **Optimización Global:** A pesar de su enfoque local, la función de estrés de Sammon se calcula y se minimiza sobre **todos los pares de puntos** en el conjunto de datos. La solución final es una configuración global de puntos en el espacio de baja dimensión. Por lo tanto, el proceso de optimización es global, pero su criterio de "mejor ajuste" da una importancia desproporcionada a la preservación de las relaciones locales.

En resumen, Sammon Mapping es una técnica poderosa para visualizar datos de alta dimensión, especialmente cuando los clusters o las estructuras locales son importantes. Si los datos no se distribuyen linealmente y lo que se busca es entender cómo se agrupan los puntos en sus vecindarios, Sammon Mapping ofrece una representación donde las relaciones locales son el foco principal, lo que lo convierte en una excelente herramienta para la exploración de estructuras no lineales y la detección de agrupaciones.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (reducción de dimensionalidad)",
  "❌ No aplica (no hay variable respuesta)",
  "✅ Numéricas (distancias euclidianas)",
  "✅ No lineal, mantiene distancias entre puntos",
  "❌ No aplica",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Sí, es sensible a valores atípicos",
  "⚠️ No afecta directamente (no hay predictores)",
  "⚠️ Interpretación visual en 2D o 3D; no en componentes",
  "❌ Lento para conjuntos grandes (algoritmo iterativo)",
  "⚠️ Se puede validar visualmente o con estrés",
  "❌ Mal desempeño en datos ruidosos o de alta dimensión sin estructura"

)

detalles <- c(
  "Método no supervisado para proyectar datos de alta dimensión en espacios de menor dimensión preservando distancias.",
  "No busca predecir, sino representar relaciones de cercanía entre observaciones.",
  "Usa distancias entre puntos; solo variables numéricas tienen sentido.",
  "A diferencia de PCA, Sammon busca preservar distancias relativas entre puntos originales y proyectados.",
  "No genera residuos como un modelo predictivo, por lo tanto no se aplica la normalidad.",
  "No hay modelo de error porque no hay predicción.",
  "No aplica el supuesto de homoscedasticidad.",
  "Valores extremos alteran las distancias y distorsionan el mapa resultante.",
  "Como es una técnica de reducción, no le afecta multicolinealidad directamente.",
  "El mapa resultante puede interpretarse en términos de proximidad, no de pesos o coeficientes.",
  "Implementación clásica es iterativa y costosa computacionalmente en datasets grandes.",
  "Puede usarse estrés (error entre distancias originales y proyectadas) como métrica de calidad.",
  "Si las distancias no reflejan bien la estructura real (por ruido o dimensiones irrelevantes), el método falla en representar datos útiles."
)

tabla_sammon <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_sammon %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir sammon mapping",
             subtitle = "Sammon Mapping")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Regularized Discriminant Analysis (RDA)  {-}   


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/RDA.png"))
```

El **Análisis Discriminante Regularizado (RDA)** es un método de **clasificación** que actúa como un **intermedio flexible entre el Análisis Discriminante Lineal (LDA) y el Análisis Discriminante Cuadrático (QDA)**. Fue desarrollado por Jerome Friedman para abordar las limitaciones de LDA (que asume covarianzas iguales para todas las clases, lo que resulta en fronteras lineales) y QDA (que permite covarianzas separadas pero puede ser inestable con pocos datos o muchas variables).

RDA introduce dos **parámetros de regularización**, $\alpha$ y $\gamma$, que controlan la flexibilidad del modelo y su capacidad para adaptarse a los datos:

1.  **Parámetro $\alpha$ (alpha):** Controla el grado en que la matriz de covarianza de cada clase se **contrae hacia una matriz de covarianza común** (como en LDA).
    * Si $\alpha = 0$, RDA se comporta como **QDA** (cada clase tiene su propia matriz de covarianza).
    * Si $\alpha = 1$, RDA se comporta como **LDA** (todas las clases comparten una matriz de covarianza común).
    * Para valores entre 0 y 1, RDA utiliza un promedio ponderado de la matriz de covarianza específica de la clase y la matriz de covarianza común. Esto ayuda a estabilizar las estimaciones de covarianza en QDA, especialmente cuando los tamaños de muestra son pequeños o el número de variables es grande.

2.  **Parámetro $\gamma$ (gamma):** Controla el grado en que la matriz de covarianza (ya sea común o específica de la clase, dependiendo de $\alpha$) se **contrae hacia una matriz diagonal**.
    * Si $\gamma = 0$, no hay contracción diagonal adicional.
    * Si $\gamma = 1$, la matriz de covarianza se contrae completamente a una matriz diagonal (lo que implica independencia entre las variables).
    * Para valores entre 0 y 1, se aplica una contracción hacia la diagonal, lo que puede ser útil cuando hay multicolinealidad.

Al sintonizar estos dos parámetros (generalmente mediante validación cruzada), RDA puede encontrar un equilibrio óptimo entre la simplicidad de LDA y la flexibilidad de QDA, adaptándose mejor a la estructura de covarianza real de los datos y mejorando la estabilidad del modelo.

**Aprendizaje Global vs. Local:**

El Análisis Discriminante Regularizado (RDA) es un modelo de **aprendizaje global** que incorpora un grado de **adaptación local** a través de su regularización.

* **Aspecto Global:** Al igual que LDA y QDA, RDA construye un **clasificador global** basado en las distribuciones de probabilidad modeladas para cada clase. Las matrices de covarianza regularizadas y las medias de las clases se estiman a partir de todo el conjunto de datos de entrenamiento, y la regla de clasificación resultante se aplica de manera consistente en todo el espacio de características. La frontera de decisión que RDA define es una función global (que puede ser lineal o cuadrática, o una combinación de ambas, dependiendo de los parámetros de regularización).

* **Adaptación Local (a través de la regularización de covarianza):** La flexibilidad de RDA para ajustarse mejor a los datos que LDA o QDA proviene de su capacidad para modelar las **estructuras de covarianza de las clases de una manera más matizada**. Al permitir una contracción parcial de las matrices de covarianza hacia una común (parámetro $\alpha$) o hacia una diagonal (parámetro $\gamma$), RDA puede adaptar las formas de las distribuciones de las clases. Esto permite que el modelo capture mejor las características de dispersión de los datos en diferentes regiones, lo que en última instancia se traduce en fronteras de decisión más adaptables que pueden manejar cierto grado de no linealidad o formas complejas de clase. No es un ajuste local en el sentido de LOESS, sino una forma de adaptar la complejidad del modelo global a la estructura de covarianza percibida de cada clase.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica (clases)",
  "✅ Numéricas",
  "✅ No lineal (transición entre LDA y QDA)",
  "❌ No aplica (no es un modelo de regresión)",
  "❌ No aplica directamente",
  "⚠️ Controla la homoscedasticidad mediante regularización",
  "⚠️ Puede ser sensible, aunque la regularización reduce impacto",
  "✅ Reduce impacto mediante regularización de covarianzas",
  "⚠️ Menos interpretable que LDA/QDA puro, pero con mayor flexibilidad",
  "✅ Más eficiente que QDA en conjuntos pequeños o ruidosos",
  "✅ Muy útil para evitar overfitting, sobre todo con validación cruzada",
  "❌ Puede no mejorar sobre LDA/QDA si no hay problemas de varianza o sobreajuste"
)

detalles <- c(
  "Modelo supervisado de clasificación que combina LDA y QDA usando parámetros de regularización.",
  "Clasifica observaciones en clases discretas basándose en variables numéricas predictoras.",
  "Requiere variables numéricas para calcular medias y covarianzas por clase.",
  "Introduce parámetros de mezcla que ajustan la matriz de covarianza hacia la identidad (como ridge) y hacia la covarianza común.",
  "No genera residuos como un modelo de regresión, por lo tanto el supuesto no aplica.",
  "No se enfoca en errores independientes, sino en distribuciones de clase.",
  "La regularización suaviza las diferencias entre covarianzas, mitigando problemas de homoscedasticidad.",
  "Los valores atípicos pueden influir en la estimación, pero se reduce con regularización.",
  "Mejor manejo de multicolinealidad que QDA gracias a la matriz regularizada.",
  "La interpretación depende de los valores de regularización elegidos; más flexible pero menos directa.",
  "Reduce complejidad computacional respecto a QDA; útil con pocas observaciones por clase.",
  "Es común usar validación cruzada para seleccionar los parámetros de regularización óptimos.",
  "No aporta mejoras significativas si los supuestos de LDA o QDA se cumplen perfectamente sin sobreajuste."
)

tabla_rda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_rda %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir RDA",
             subtitle = "Regularized Discriminant Analysis (RDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## t-Distributed Stochastic Neighbor Embedding (t-SNE) {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/t-SNE.png"))
```

## Uniform Manifold Approximation and Projection (UMAP) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/UMAP.png"))
```

**Uniform Manifold Approximation and Projection (UMAP)** es una técnica de **reducción de dimensionalidad no lineal** de vanguardia, utilizada principalmente para la **visualización de datos** de alta dimensión y para el **aprendizaje de características (feature learning)**. Fue desarrollada por Leland McInnes, John Healy y James Melville. UMAP es una alternativa más reciente y a menudo más rápida y escalable a t-SNE (t-Distributed Stochastic Neighbor Embedding), manteniendo su capacidad para preservar la estructura local y global de los datos.

La idea central de UMAP se basa en la **teoría de los conjuntos difusos (fuzzy set theory)** y la **geometría riemanniana**. Intenta construir una representación de baja dimensión de los datos asumiendo que los datos de alta dimensión residen en una **variedad (manifold) subyacente de baja dimensión**. El algoritmo opera en dos fases:

1.  **Construcción del Grafo de Vecindad Difusa:**
    * Primero, UMAP construye un **grafo ponderado difuso** en el espacio de alta dimensión. Los nodos del grafo son los puntos de datos y los pesos de las aristas representan la probabilidad de que dos puntos estén conectados (es decir, qué tan similares o cercanos son).
    * Para ello, UMAP estima las distancias entre los puntos en el manifold subyacente y luego convierte estas distancias en probabilidades de conectividad. Esto es crucial porque le permite adaptarse a la densidad local de los datos (puntos en regiones densas pueden estar cerca incluso con distancias euclidianas grandes, y viceversa en regiones dispersas).

2.  **Optimización del Diseño en Baja Dimensión:**
    * Luego, UMAP optimiza el diseño de los puntos en un espacio de baja dimensión (ej., 2D) para que la **estructura del grafo construido en alta dimensión sea lo más similar posible** al grafo construido en baja dimensión.
    * Esto se logra minimizando una función de costo que intenta hacer que las probabilidades de conectividad en el espacio de baja dimensión coincidan con las probabilidades de conectividad del grafo de alta dimensión.

UMAP es valorado por su velocidad, escalabilidad a grandes conjuntos de datos, y su capacidad para preservar simultáneamente la **estructura local y global** de los datos, lo que lo hace ideal para visualizar agrupaciones y relaciones complejas.

**Aprendizaje Global vs. Local:**

UMAP es un excelente ejemplo de un modelo que logra un equilibrio sofisticado entre el **aprendizaje local y global**.

* **Aspecto Local:** UMAP pone un fuerte énfasis en la **preservación de la estructura local**. Al construir el grafo de vecindad difusa, se enfoca en las relaciones de los vecinos más cercanos de cada punto (controlado por el parámetro `n_neighbors`). La forma en que calcula las probabilidades de conectividad se adapta a la densidad local de los datos, asegurando que los clústeres y las agrupaciones cercanas se mantengan cohesivos en la representación de baja dimensión. Las relaciones "locales" son las que definen el "manifold" en primera instancia. Esto significa que si los datos no se distribuyen linealmente y tienen estructuras complejas con vecindarios distintos (como diferentes clusters o ramas en una estructura), UMAP es capaz de capturarlas con alta fidelidad, de forma similar a como una "regresión ponderada localmente" operaría en cada vecindario.

* **Aspecto Global:** A pesar de su énfasis local, UMAP también hace un esfuerzo consciente por preservar la **estructura global** de los datos. Al minimizar la función de costo para que la estructura del grafo se mantenga en el espacio de baja dimensión, UMAP no solo se asegura de que los puntos cercanos permanezcan cercanos, sino que también intenta que los grupos de puntos que estaban globalmente separados en la alta dimensión permanezcan separados en la baja dimensión. El parámetro `min_dist` ayuda a controlar cuán compactos deben ser los clústeres, lo que influye en la separación global. Esta capacidad de equilibrar ambos aspectos es una de las principales ventajas de UMAP sobre técnicas que a veces sacrifican la estructura global (como t-SNE, que puede "romper" grandes clústeres).


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (reducción de dimensionalidad)",
  "❌ No aplica (no hay variable respuesta)",
  "✅ Numéricas (o categóricas codificadas)",
  "✅ Captura relaciones no lineales y estructura local/global",
  "❌ No aplica",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Algo sensible a outliers (puede distorsionar estructuras)",
  "⚠️ No afecta directamente (no hay predictores)",
  "⚠️ Visual en 2D o 3D; difícil interpretación formal",
  "✅ Muy rápido incluso en grandes conjuntos de datos",
  "⚠️ No usa validación cruzada clásica, pero puede evaluarse la estabilidad",
  "❌ Datos con mucho ruido, escalas mal ajustadas o sin estructura latente clara"
)

detalles <- c(
  "Técnica no supervisada de reducción de dimensionalidad que preserva tanto estructura local como global de los datos.",
  "No busca predecir, sino proyectar observaciones a un espacio de menor dimensión.",
  "Funciona con datos numéricos; variables categóricas deben ser codificadas antes.",
  "A diferencia de PCA, puede descubrir relaciones no lineales más complejas.",
  "No genera residuos; no aplica el supuesto de normalidad.",
  "No hay modelo de error residual, por lo que no aplica la independencia.",
  "No es un modelo predictivo, así que no se evalúa homoscedasticidad.",
  "Outliers pueden influir en el mapa de manera desproporcionada.",
  "Como es una técnica de reducción, la multicolinealidad no le afecta directamente.",
  "La interpretación se limita a la distribución visual de puntos.",
  "UMAP es computacionalmente eficiente y escalable a grandes volúmenes de datos.",
  "No utiliza validación cruzada directa, pero puede evaluarse la estabilidad de la proyección.",
  "Cuando no existe una estructura clara en los datos, la proyección puede ser confusa o poco útil."
)

tabla_umap <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_umap %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir UMAP",
             subtitle = "Uniform Manifold Approximation and Projection (UMAP)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


<!--chapter:end:05-dimensionality_reduction.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 🧬 6. Modelos Bayesianos {-}  

**Ejemplos:** Naive Bayes, Redes Bayesianas.  
**Uso:** Ideales para **clasificación rápida**, especialmente en escenarios con **supuestos simples** sobre los datos. Son muy populares en tareas de **procesamiento de texto** y **detección de spam**.  
**Ventajas:** Son modelos **muy rápidos** de entrenar y predecir, y están sólidamente **fundamentados en la teoría de probabilidad**.  
**Limitaciones:** La principal es que **asumen independencia** entre las variables predictoras, lo cual no siempre se cumple en la realidad y puede afectar su precisión en ciertos problemas.  

---

## Averaged One - Dependence Estimators (AODE)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/AODE.png"))
```

**Averaged One-Dependence Estimators (AODE)** es un algoritmo de **clasificación supervisada** que pertenece a la familia de los clasificadores basados en **modelos bayesianos**. Es una mejora sobre el clásico **Naive Bayes (NB)**, diseñado para superar la limitación clave de NB: la **asunción de independencia condicional** estricta entre las variables predictoras (atributos) dado el valor de la clase. Esta suposición, aunque simplifica mucho el cálculo y permite a Naive Bayes ser muy eficiente, rara vez se cumple en la realidad y puede llevar a una pérdida de precisión.

AODE relaja parcialmente la suposición de independencia de Naive Bayes al considerar que **cada atributo es dependiente, como máximo, de un solo otro atributo** (además de la variable de clase). En lugar de construir un único modelo de árbol de dependencia (como en el Árbol de Dependencia de Atributos - ADTree), AODE construye una **colección de clasificadores "One-Dependence" (ODE)** y luego **promedia sus predicciones**.

El funcionamiento de AODE se puede resumir así:

1.  **Generación de Clasificadores ODE:** Para cada atributo predictivo $A_i$ en el conjunto de datos (que cumpla ciertos criterios, como tener suficientes instancias), AODE construye un clasificador ODE. Este clasificador asume que todos los demás atributos son condicionalmente independientes de $A_i$ dado la clase. En otras palabras, se estima la probabilidad condicional de cada atributo $A_j$ dado la clase $C$ y el atributo $A_i$: $P(A_j | C, A_i)$.
2.  **Ponderación y Promedio:** Cuando se hace una predicción para una nueva instancia, AODE calcula la probabilidad de cada clase para cada uno de los clasificadores ODE generados. Luego, estas probabilidades se combinan (típicamente promediando) para obtener una predicción final.

Al promediar las predicciones de múltiples modelos ODE, AODE logra mitigar el sesgo introducido por la suposición de independencia estricta de Naive Bayes, a menudo obteniendo un mejor rendimiento sin incurrir en una complejidad computacional excesiva.


**Aprendizaje Global vs. Local:**

Averaged One-Dependence Estimators (AODE) es un modelo que se clasifica como de **aprendizaje global**, aunque con una estructura que busca capturar dependencias que tienen una naturaleza más "local" en el contexto de las relaciones entre atributos.

* **Aspecto Global:** AODE construye un conjunto de modelos (los ODEs) que son entrenados sobre la **totalidad del conjunto de datos** para estimar las probabilidades condicionales. La combinación de estas probabilidades (el promedio) para llegar a una predicción final es una regla que se aplica de manera consistente a cualquier nueva observación. Los parámetros de cada clasificador ODE (las probabilidades condicionales) se estiman de manera global a partir de las frecuencias observadas en todo el conjunto de entrenamiento.

* **Matiz (Captura de Dependencias Locales):** Aunque el enfoque general es global, la razón por la que AODE es más potente que Naive Bayes radica en su capacidad para modelar **dependencias entre atributos**. Cada clasificador ODE considera que un atributo específico tiene una dependencia directa de otro atributo, lo que es una forma de capturar una relación "local" entre un par de atributos dado el contexto de la clase. Al promediar sobre estos múltiples modelos que capturan diferentes dependencias de "un solo par", AODE puede adaptarse mejor a las complejidades de los datos donde las relaciones no son puramente independientes y no se distribuyen linealmente, sin la necesidad de dividir el espacio de características en regiones discretas como los árboles de decisión. Sin embargo, la solución final de promediado es un clasificador global que se aplica a toda la instancia de entrada.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica",
  "✅ Categóricas principalmente",
  "⚠️ Modela dependencias limitadas entre atributos (mejora sobre NB)",
  "❌ No aplica (no es regresión)",
  "✅ Requiere independencia entre instancias",
  "❌ No aplica",
  "⚠️ Puede verse afectado por outliers si se usan variables numéricas sin tratamiento",
  "✅ Menos afectado que Naive Bayes por dependencias entre atributos",
  "⚠️ Moderadamente interpretable (combinación de varios modelos NB con 1 dependencia)",
  "⚠️ Más costoso que NB, pero aún eficiente",
  "✅ Puede validarse mediante k-fold cross-validation",
  "❌ Desempeña mal con muchos atributos irrelevantes o con pocos datos por combinación de atributos"
)

detalles <- c(
  "Clasificador bayesiano que promedia modelos con una única dependencia entre pares de atributos para mejorar sobre Naive Bayes.",
  "Diseñado para problemas de clasificación con clases categóricas.",
  "Se usa típicamente con variables categóricas, aunque puede adaptarse a discretizadas.",
  "Relaja el supuesto de independencia total de Naive Bayes permitiendo una única dependencia por atributo.",
  "No es un modelo de regresión, por lo que no aplica el supuesto de normalidad de residuos.",
  "Las observaciones deben ser independientes para que las estimaciones sean válidas.",
  "No supone homoscedasticidad debido a su naturaleza probabilística.",
  "Los valores atípicos pueden afectar la calidad de la estimación de probabilidades.",
  "Tolera mejor la multicolinealidad al permitir dependencias limitadas entre atributos.",
  "La interpretación es más compleja que NB, pero aún comprensible por su estructura promedio.",
  "Requiere más tiempo de cómputo que NB, pero sigue siendo razonablemente eficiente.",
  "La validación cruzada es útil para evaluar el desempeño y generalización del modelo.",
  "El rendimiento cae si hay muchos atributos irrelevantes o datos escasos por combinación de atributos."
)

tabla_aode <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_aode %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir AODE",
             subtitle = "Averaged One - Dependence Estimators (AODE)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Bayesian Network (BN)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/BN.png"))
```

Una **Red Bayesiana (BN)**, también conocida como **Red Bayesiana Causal** o **Modelo Gráfico Dirigido Acíclico (DAG)**, es un modelo probabilístico que representa un conjunto de **variables y sus relaciones de dependencia condicional** utilizando un **grafo dirigido acíclico**. En este grafo:

* **Nodos:** Representan las variables aleatorias (pueden ser discretas o continuas).
* **Arcos (flechas):** Representan las dependencias condicionales entre las variables. Una flecha de A a B significa que B depende directamente de A (A es "padre" de B). La ausencia de un arco entre dos nodos indica una independencia condicional.

La estructura del grafo de una Red Bayesiana permite visualizar y comprender las relaciones de causa y efecto (o asociación) entre las variables. Junto con la estructura del grafo, una BN también especifica las **distribuciones de probabilidad condicional (CPDs)** para cada nodo, dadas las combinaciones de estados de sus nodos padre. Por ejemplo, si un nodo tiene padres, se define la probabilidad de sus valores para cada combinación de valores de sus padres.

Las Redes Bayesianas son potentes para:
* **Modelado de Conocimiento:** Codificar el conocimiento experto o aprendido de los datos sobre cómo interactúan las variables.
* **Inferencia Probabilística:** Calcular la probabilidad de que una variable tome un valor específico, dadas las observaciones de otras variables (evidencia). Esto puede incluir diagnóstico (inferir causas a partir de efectos) o predicción (inferir efectos a partir de causas).
* **Aprendizaje de Estructura y Parámetros:** Aprender la estructura del grafo (las dependencias) y las CPDs a partir de datos.

**Aprendizaje Global vs. Local:**

Una Red Bayesiana (BN) es fundamentalmente un modelo de **aprendizaje global** en su estructura general, pero con una fuerte base en el **aprendizaje local** de las dependencias.

* **Aspecto Global:** La **estructura del grafo** y el conjunto de **tablas de probabilidad condicional (CPDs)** forman un **modelo probabilístico coherente y global** de la distribución de probabilidad conjunta de todas las variables. Este modelo global puede ser utilizado para realizar inferencias sobre cualquier combinación de variables en cualquier parte del espacio de datos. La red define cómo la información fluye y cómo las probabilidades se propagan a través de todas las variables, dando una visión holística de las interacciones del sistema.

* **Aspecto Local (Dependencias y Parametrización):** Donde la BN tiene un fuerte componente local es en la **definición de las dependencias y la parametrización de las CPDs**. Cada nodo solo necesita conocer las probabilidades condicionales dadas sus **padres directos**. Esto es un principio de **independencia condicional local**: una variable es independiente de sus no-descendientes dado sus padres. Esto descompone un problema complejo de modelado de la distribución conjunta en problemas más pequeños y manejables de modelar las dependencias locales. Por ejemplo, para estimar $P(X_i | Padres(X_i))$, solo se necesita información local relacionada con $X_i$ y sus padres, no con todas las demás variables en la red. Esta capacidad de modelar dependencias de forma localizada, y luego ensamblarlas en un modelo global, permite a las BNs manejar relaciones no lineales y complejas de una manera estructurada y probabilística. Si los datos no se distribuyen linealmente, la estructura de la BN puede adaptarse para reflejar las relaciones no lineales entre las variables a través de sus arcos y CPDs.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado o no supervisado",
  "✅ Categórica o continua (depende del tipo de red)",
  "✅ Categóricas y/o continuas",
  "✅ Modela relaciones condicionales entre variables (estructura dirigida)",
  "❌ No aplica (no hay residuos típicos)",
  "⚠️ Depende de la estructura de la red",
  "❌ No aplica como en regresión clásica",
  "⚠️ Puede ser sensible si afecta las probabilidades condicionales",
  "⚠️ Puede causar redundancia si no se ajusta bien la estructura",
  "✅ Muy interpretables si se visualiza la red y se conocen las dependencias",
  "⚠️ Aprendizaje de estructura puede ser computacionalmente costoso",
  "✅ Puede usarse validación cruzada para evaluar rendimiento predictivo",
  "❌ Cuando hay muchas variables y poca información para definir relaciones"
)

detalles <- c(
  "Modelo probabilístico que representa relaciones condicionales entre variables mediante un grafo dirigido acíclico (DAG).",
  "Puede predecir una variable (modo supervisado) o descubrir estructura entre variables (modo no supervisado).",
  "Acepta variables mixtas, aunque muchas implementaciones requieren discretización.",
  "Cada nodo representa una variable y las conexiones modelan dependencias condicionales.",
  "No genera residuos como los modelos de regresión, por lo que no aplica este supuesto.",
  "Algunas estructuras pueden implicar independencia condicional; otras no.",
  "No se evalúa homoscedasticidad, pues no hay predicción de error numérico.",
  "Valores atípicos pueden sesgar las probabilidades estimadas si no se controlan.",
  "Si hay variables redundantes o fuertemente correlacionadas, se debe ajustar la estructura de la red para evitar errores.",
  "La red permite interpretar cómo influyen unas variables sobre otras, ideal para razonamiento causal.",
  "El ajuste de parámetros es eficiente, pero aprender la estructura de la red puede ser lento.",
  "Puede evaluarse el desempeño con k-fold o validación simple en tareas supervisadas.",
  "Cuando no se tiene información previa o datos suficientes, la red puede no capturar relaciones reales."
)

tabla_bn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bn %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir BN",
             subtitle = "Bayesian Network (BN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Bayesian Belief Network (BBN)  {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/BBN.png"))
```

Una **Red de Creencia Bayesiana (BBN)** es simplemente **otro término para una Red Bayesiana (BN)**. No hay una diferencia fundamental entre ambos nombres; ambos se refieren al mismo tipo de modelo probabilístico. La terminología "Red de Creencia" a menudo enfatiza la capacidad del modelo para representar y actualizar "creencias" (probabilidades) sobre el estado de variables inciertas a medida que se introduce nueva evidencia.

Como ya se describió, una BBN (o BN) es un **modelo gráfico probabilístico dirigido acíclico (DAG)** que representa un conjunto de **variables aleatorias** como **nodos** y sus **relaciones de dependencia condicional** como **arcos (flechas)**. La ausencia de un arco entre dos nodos indica una independencia condicional. Cada nodo está asociado con una **distribución de probabilidad condicional (CPD)** que cuantifica la relación de ese nodo con sus padres.

Las BBNs son herramientas poderosas para:
* **Modelar el conocimiento incierto:** Permiten representar cómo diferentes factores interactúan bajo incertidumbre.
* **Inferencia probabilística:** Dada alguna evidencia (observaciones de algunas variables), la red puede calcular las probabilidades actualizadas de las otras variables. Esto es fundamental para el diagnóstico, la predicción y la toma de decisiones bajo incertidumbre.
* **Aprendizaje a partir de datos:** Las BBNs pueden ser aprendidas tanto en su estructura (cómo se conectan los nodos) como en sus parámetros (las CPDs) a partir de conjuntos de datos.


**Aprendizaje Global vs. Local:**

Al igual que una Red Bayesiana, una Red de Creencia Bayesiana es fundamentalmente un modelo de **aprendizaje global** en su formulación general, pero se basa en la **especificación local** de las dependencias probabilísticas.

* **Aspecto Global:** La BBN como un todo representa la **distribución de probabilidad conjunta global** de todas las variables en el sistema. Una vez que la estructura y las CPDs están definidas, la red puede usarse para calcular cualquier probabilidad marginal o condicional de interés, proporcionando una visión probabilística completa y coherente del dominio. Es una función que mapea el espacio de todas las posibles combinaciones de variables a sus probabilidades, y se aplica de manera consistente en todo el espacio.

* **Aspecto Local (Definición de Dependencias):** La fortaleza y eficiencia de las BBNs radica en el principio de **independencia condicional local**. Cada variable (nodo) solo necesita tener su distribución de probabilidad condicionada a sus **padres directos** en el grafo. No es necesario especificar las dependencias con todas las demás variables en la red. Esta factorización de la distribución conjunta en componentes locales (las CPDs) es lo que hace que las BBNs sean computacionalmente manejables y permite que el modelo capture **relaciones no lineales y complejas** entre las variables de una manera estructurada. Al modelar estas dependencias "locales" de forma explícita, la BBN puede representar con precisión cómo la probabilidad de un evento cambia en función de los eventos directamente relacionados, incluso si la relación no es lineal.

En resumen, las Redes de Creencia Bayesianas son modelos globales que permiten modelar relaciones probabilísticas complejas y no lineales al especificar dependencias de manera local entre las variables. Son herramientas poderosas para el razonamiento bajo incertidumbre y la toma de decisiones.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado y no supervisado (estructuras probabilísticas)",
  "✅ Categórica o continua (depende de implementación)",
  "✅ Categóricas o continuas discretizadas",
  "✅ Modela relaciones condicionales entre variables (gráficamente)",
  "❌ No aplica (no es modelo de regresión lineal)",
  "✅ Requiere independencia condicional entre nodos según la red",
  "❌ No aplica",
  "⚠️ Puede ser sensible a outliers si se estiman mal las distribuciones",
  "✅ Puede manejar correlación entre variables de forma explícita en la red",
  "✅ Alta interpretabilidad visual con grafos dirigidos acíclicos",
  "⚠️ Costoso computacionalmente en grandes redes o aprendizaje estructural",
  "✅ Validación cruzada puede aplicarse en tareas supervisadas (clasificación)",
  "❌ Mal rendimiento si hay muchas variables irrelevantes o dependencias no detectadas"
)

detalles <- c(
  "Modelo probabilístico gráfico que representa relaciones condicionales entre variables mediante una red bayesiana (DAG).",
  "Puede usarse para clasificación, predicción o inferencia probabilística.",
  "Se adapta a datos categóricos principalmente, pero también se puede usar con discretización de continuas.",
  "Captura relaciones condicionales entre variables explícitamente como conexiones dirigidas.",
  "No genera residuos como los modelos de regresión, por lo que la normalidad no aplica.",
  "Las dependencias condicionales deben estar bien modeladas en la estructura de la red.",
  "No hay un modelo de varianza/residuos tradicional como para aplicar homoscedasticidad.",
  "Distribuciones erróneas o mal estimadas pueden afectar resultados, especialmente con valores extremos.",
  "El modelo representa explícitamente la correlación entre variables mediante arcos.",
  "La estructura de la red permite ver cómo interactúan las variables entre sí.",
  "El aprendizaje estructural y de parámetros puede ser costoso en términos computacionales.",
  "Si se usa para clasificación, la validación cruzada es una forma estándar de evaluación.",
  "BBN requiere una buena estructura; datos mal preparados o muy ruidosos deterioran su capacidad explicativa."
)

tabla_bbn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bbn %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir BBN",
             subtitle = "Bayesian Belief Network (BBN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Bayesian Linear Regression (BLR) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/BLR.png"))
```

## Gaussian Naive Bayes (GNB) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/GNB.png"))
```

**Gaussian Naive Bayes (GNB)** es una variante del popular algoritmo **Naive Bayes (NB)**, utilizado para tareas de **clasificación supervisada**. Es particularmente adecuado cuando las **variables predictoras (atributos) son de tipo continuo**. Al igual que todos los clasificadores Naive Bayes, GNB se basa en el **Teorema de Bayes** y, fundamentalmente, en la **suposición de independencia condicional** entre las variables predictoras, dado el valor de la clase.

La diferencia clave entre GNB y otras variantes de Naive Bayes (como Multinomial Naive Bayes o Bernoulli Naive Bayes) es la forma en que modela la **probabilidad de los atributos continuos**. Específicamente:

1.  **Suposición de Distribución Gaussiana:** GNB asume que los valores de cada atributo continuo, *dada una clase específica*, siguen una **distribución normal (Gaussiana)**. Es decir, para cada clase y cada atributo, se estima la media ($\mu$) y la desviación estándar ($\sigma$) de los valores de ese atributo dentro de esa clase.
2.  **Cálculo de Probabilidades:** Cuando se necesita clasificar una nueva observación, GNB utiliza las funciones de densidad de probabilidad (PDF) de estas distribuciones Gaussianas para calcular la probabilidad de observar el valor del atributo para cada clase.
3.  **Aplicación del Teorema de Bayes:** Finalmente, utiliza el Teorema de Bayes para calcular la probabilidad posterior de cada clase, dadas las probabilidades de los atributos, y asigna la observación a la clase con la probabilidad posterior más alta.

A pesar de su suposición de independencia (que rara vez se cumple perfectamente en la práctica), GNB a menudo funciona sorprendentemente bien, especialmente en conjuntos de datos grandes o cuando las características son ruidosas. Su simplicidad y eficiencia computacional lo hacen un buen punto de partida para muchos problemas de clasificación.

**Aprendizaje Global vs. Local:**

Gaussian Naive Bayes (GNB) es un modelo de **aprendizaje global**.

* **Aspecto Global:** GNB construye un **modelo probabilístico global** de la relación entre las características y las clases. Las medias y desviaciones estándar de las distribuciones Gaussianas para cada atributo dentro de cada clase se estiman a partir de **todos los datos de entrenamiento**. La regla de clasificación final, que asigna una nueva instancia a la clase más probable, se basa en estas distribuciones paramétricas globales y en el Teorema de Bayes, aplicándose de manera uniforme en todo el espacio de características. No se ajustan modelos locales para diferentes vecindarios de datos.

* **Impacto de la Asunción de Independencia:** La suposición de independencia condicional (que los atributos son independientes entre sí dado la clase) significa que GNB no intenta capturar interacciones complejas o no lineales entre las variables predictoras. Si bien esto simplifica drásticamente el modelo y lo hace eficiente, también implica que su capacidad para modelar relaciones no lineales entre *predictores* es limitada. Si los datos no se distribuyen linealmente y las interacciones entre los predictores son cruciales para la clasificación, GNB podría no ser el modelo más flexible. Sin embargo, su robustez ante la violación de suposiciones y su velocidad lo mantienen como una opción valiosa en muchos escenarios.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica (clases)",
  "✅ Numéricas (asume distribución normal por clase)",
  "❌ No modela relaciones entre predictores (independencia asumida)",
  "❌ No aplica (no hay residuos como en regresión)",
  "✅ Asume independencia condicional entre predictores",
  "✅ Cada predictor se modela con varianza homogénea por clase",
  "⚠️ Sensible a outliers porque afectan media y varianza de la normal",
  "⚠️ Alta multicolinealidad viola el supuesto de independencia",
  "✅ Altamente interpretable: muestra probabilidades y distribución por clase",
  "✅ Muy rápido y eficiente, incluso con grandes datasets",
  "✅ Se puede validar fácilmente con k-fold o hold-out",
  "❌ Si los predictores no son aproximadamente normales por clase, el rendimiento baja"
)

detalles <- c(
  "Clasificador probabilístico que asume que cada predictor sigue una distribución normal dentro de cada clase.",
  "Se utiliza para predecir clases categóricas a partir de predictores continuos.",
  "Cada variable numérica se modela con una distribución Gaussiana separada por clase.",
  "No considera correlaciones entre predictores; cada uno contribuye de manera independiente.",
  "No genera residuos como un modelo de regresión, así que no aplica normalidad de errores.",
  "El supuesto clave es independencia condicional entre predictores dado la clase.",
  "Cada variable tiene su propia media y varianza por clase, sin heterocedasticidad.",
  "Los valores atípicos pueden distorsionar los parámetros estimados de la distribución normal.",
  "Altamente correlacionadas violan la independencia condicional asumida y afectan rendimiento.",
  "Fácil de explicar: se basa en la probabilidad de cada clase dado cada predictor.",
  "Uno de los algoritmos más rápidos para clasificación supervisada.",
  "Evaluación estándar con validación cruzada o conjunto de prueba.",
  "Si las variables no tienen forma aproximadamente normal dentro de clases, el modelo puede clasificarlas mal."
)

tabla_gnb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_gnb %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir GNB",
             subtitle = "Gaussian Naive Bayes (GNB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Hidden Markov Models (HMMs) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/HMMs.png"))
```

## Kalman Filter {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/KF.png"))
```

## Multinomial Naive Bayes (MNB) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/MNB.png"))
```

**Multinomial Naive Bayes (MNB)** es una variante del algoritmo **Naive Bayes** diseñada específicamente para la **clasificación de datos discretos**, y es particularmente popular en tareas de **procesamiento de lenguaje natural (NLP)**, como la clasificación de texto (ej., spam/no spam, clasificación de documentos por tema). Al igual que otras formas de Naive Bayes, se basa en el **Teorema de Bayes** y la **suposición clave de independencia condicional** entre las características, dado el valor de la clase.

La diferencia fundamental de MNB radica en que asume que las características (como el recuento de palabras en un documento de texto) provienen de una **distribución multinomial**. Esto significa que:

1.  **Características de Recuento:** MNB es ideal para características que representan **frecuencias o recuentos** (ej., el número de veces que aparece una palabra en un documento, el número de veces que ocurre un evento).
2.  **Modelado de Probabilidades:** Para cada clase, MNB calcula la probabilidad de observar cada característica (ej., cada palabra del vocabulario) dado que la instancia pertenece a esa clase. Estas probabilidades se estiman a menudo utilizando **suavizado Laplace (o aditivo)** para evitar probabilidades de cero para palabras no vistas durante el entrenamiento.
3.  **Aplicación del Teorema de Bayes:** Luego, para clasificar una nueva instancia, multiplica las probabilidades de las características (asumiendo independencia) por la probabilidad previa de cada clase, y elige la clase que tiene la probabilidad posterior más alta.

MNB es altamente eficiente, escalable para grandes conjuntos de datos y a menudo sorprendentemente efectivo a pesar de su ingenua suposición de independencia, lo que lo convierte en una línea base sólida para muchos problemas de clasificación de texto.

**Aprendizaje Global vs. Local:**

Multinomial Naive Bayes (MNB) es un modelo de **aprendizaje global**.

* **Aspecto Global:** MNB construye un **modelo probabilístico global** para la relación entre las características discretas (como recuentos de palabras) y las clases. Las probabilidades de las características dadas las clases (y las probabilidades previas de las clases) se estiman a partir de **todos los datos de entrenamiento**. La regla de clasificación final, basada en el Teorema de Bayes, se aplica de manera uniforme a cualquier nueva instancia en el espacio de características. No se ajustan modelos locales para diferentes vecindarios de datos; en su lugar, se utilizan las mismas probabilidades estimadas globalmente para todas las predicciones.

* **Impacto de la Asunción de Independencia:** La suposición de independencia condicional entre las características (ej., que la presencia de una palabra no influye en la probabilidad de otra palabra dada la categoría del documento) es una simplificación global. Si bien esta simplicidad permite que MNB sea muy eficiente y robusto a veces, también significa que no puede capturar interacciones complejas o dependencias no lineales entre las características en el mismo sentido que modelos más avanzados. Sin embargo, en muchas aplicaciones como la clasificación de texto, donde la frecuencia individual de las palabras es muy informativa, esta suposición es lo suficientemente robusta para un buen rendimiento. Es un modelo que asume una estructura de probabilidad global y la aplica consistentemente.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica (clases)",
  "✅ Discretas, conteos (ej. frecuencia de palabras)",
  "❌ No modela relaciones entre predictores (asume independencia condicional)",
  "❌ No aplica (no hay residuos)",
  "✅ Asume independencia condicional entre predictores",
  "❌ No aplica (no se modela varianza)",
  "⚠️ Menos sensible a outliers que Gaussian NB, pero aún puede verse afectado",
  "⚠️ Multicolinealidad viola el supuesto de independencia y puede degradar el rendimiento",
  "✅ Muy interpretable: probabilidades por clase y variable",
  "✅ Extremadamente eficiente en problemas de texto y alta dimensión",
  "✅ Puede usarse k-fold o validación simple",
  "❌ No es adecuado para variables continuas o datos que no representen conteos"
)

detalles <- c(
  "Clasificador basado en probabilidad que modela la distribución multinomial de conteos por clase.",
  "Usado típicamente para clasificación de texto, spam detection, y otros problemas con datos categóricos o de conteo.",
  "Funciona mejor con variables que representan frecuencia (número de veces que aparece un término).",
  "Asume que los predictores son independientes condicionalmente dados la clase, sin correlación entre ellos.",
  "No hay residuos como en modelos de regresión, por lo tanto no aplica este supuesto.",
  "La independencia condicional de los predictores es un supuesto fundamental del modelo.",
  "Como no se modela la varianza explícitamente, el supuesto de homoscedasticidad no aplica.",
  "Outliers tienen menor efecto porque se espera que los datos estén en formato de conteo (discretos).",
  "Predictores altamente correlacionados pueden afectar negativamente la precisión del modelo.",
  "La probabilidad condicional de cada clase y predictor es fácil de interpretar.",
  "Muy rápido incluso en datasets grandes con miles de características (como texto).",
  "La precisión puede evaluarse con validación cruzada como en otros clasificadores.",
  "Si las variables son continuas o no reflejan bien los conteos, el modelo puede fallar."
)

tabla_mnb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mnb %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir MNB",
             subtitle = "Multinomial Naive Bayes (MNB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Naive Bayes (NB) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/NB.png"))
```

**Naive Bayes (NB)** es un algoritmo de **clasificación supervisada** popular y computacionalmente eficiente, basado en el **Teorema de Bayes** y una **fuerte (o "ingenua") suposición de independencia condicional** entre las características (variables predictoras) dado el valor de la clase. Esta suposición significa que el modelo asume que la presencia o ausencia de una característica particular no afecta la presencia o ausencia de otra característica, una vez que se conoce la clase.

A pesar de que esta suposición rara vez se cumple perfectamente en problemas del mundo real, Naive Bayes a menudo ofrece un **rendimiento sorprendentemente bueno**, especialmente en tareas de clasificación de texto y con grandes conjuntos de datos. Su simplicidad y velocidad lo convierten en un excelente algoritmo de línea base.

El funcionamiento básico de Naive Bayes es el siguiente:

1.  **Cálculo de Probabilidades Previas:** Estima la probabilidad de cada clase en el conjunto de entrenamiento (ej., P(Clase A), P(Clase B)).
2.  **Cálculo de Probabilidades de Verosimilitud:** Para cada característica y cada clase, calcula la probabilidad de que la característica tome un valor específico, dado que la instancia pertenece a esa clase (ej., P(Característica X | Clase A)). Aquí es donde entran las diferentes variantes de Naive Bayes (Gaussian para características continuas, Multinomial para recuentos, Bernoulli para características binarias).
3.  **Aplicación del Teorema de Bayes:** Para clasificar una nueva instancia, utiliza el Teorema de Bayes para combinar estas probabilidades y calcular la probabilidad posterior de cada clase, dadas las características de la nueva instancia. Finalmente, asigna la instancia a la clase con la probabilidad posterior más alta.


**Aprendizaje Global vs. Local:**

Naive Bayes (NB) es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** Naive Bayes construye un **modelo probabilístico global** que describe la relación entre las características y las clases para todo el conjunto de datos. Las probabilidades previas de las clases y las probabilidades condicionales de las características dadas las clases se estiman a partir de **todos los datos de entrenamiento**. La regla de clasificación resultante se aplica de manera uniforme a cualquier nueva instancia en el espacio de características, sin ajustar modelos locales para diferentes vecindarios de datos. El modelo aprende una distribución de probabilidad que se asume válida para todo el dominio.

* **Impacto de la Suposición de Independencia:** La "ingenuidad" del modelo, es decir, la suposición de independencia entre las características, es una simplificación global. No intenta capturar interacciones complejas o no lineales entre las características en sí. Si bien esto puede ser una limitación cuando las relaciones entre las características son muy intrincadas y no lineales, es precisamente esta suposición la que le otorga su eficiencia y robustez en muchos escenarios prácticos. Es un modelo que asume una estructura de probabilidad global y la aplica de manera consistente.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica (clases)",
  "✅ Categóricas o numéricas (según variante del modelo)",
  "❌ Asume independencia condicional entre predictores",
  "❌ No aplica (no es regresión)",
  "✅ Observaciones deben ser independientes",
  "❌ No se requiere homocedasticidad",
  "⚠️ Puede ser sensible si se usa con predictores numéricos y hay valores extremos",
  "✅ No afectado por multicolinealidad debido a suponer independencia",
  "✅ Alta, se pueden interpretar probabilidades y efectos de cada variable",
  "✅ Muy rápido incluso con grandes conjuntos de datos",
  "✅ Puede usarse para afinar y evaluar desempeño del modelo",
  "❌ Bajo rendimiento si los predictores no son realmente independientes o las distribuciones asumidas no se cumplen"
)

detalles <- c(
  "Clasificador probabilístico basado en el teorema de Bayes con asunción de independencia condicional entre predictores.",
  "Funciona para clasificación en múltiples clases categóricas.",
  "Puede trabajar con variables categóricas (Multinomial NB) o numéricas (Gaussian NB).",
  "Supone que las variables predictoras son independientes entre sí dentro de cada clase.",
  "No genera residuos en el sentido clásico porque no es un modelo de regresión.",
  "Las observaciones deben ser independientes para que las probabilidades se combinen correctamente.",
  "No requiere igualdad de varianzas; Gaussian NB asume varianza igual por clase pero se puede ajustar.",
  "Los valores atípicos pueden distorsionar la estimación de probabilidades si hay predictores numéricos.",
  "La independencia entre predictores hace que la multicolinealidad no sea problema.",
  "Los resultados pueden interpretarse en términos de probabilidades a posteriori por clase.",
  "Muy eficiente para entrenamiento y predicción, incluso con muchos atributos.",
  "Puede validarse usando k-fold o leave-one-out para asegurar estabilidad del modelo.",
  "El supuesto fuerte de independencia condicional rara vez se cumple completamente, lo que puede afectar la precisión."
)

tabla_nb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_nb %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir NB",
             subtitle = "Naive Bayes (NB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Particle Filter {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/Particle Filter.png"))
```

<!--chapter:end:06-bayesian.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 🧮 7. Regularización {-}  

**Ejemplos:** L1 (Lasso), L2 (Ridge), Elastic Net.  
**Uso:** Esencial para **prevenir el sobreajuste** en modelos, especialmente los lineales y las redes neuronales. Muy útil cuando trabajas con **muchas variables** (alta dimensionalidad).  
**Ventajas:** Su principal beneficio es que **penaliza la complejidad del modelo**, forzándolo a ser más simple y generalizable.  
**Limitaciones:** Si se aplica en exceso, la regularización puede **eliminar variables útiles** y, por lo tanto, afectar el rendimiento del modelo.  

---

## Elastic Net  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/ENet.png"))
```

**Elastic Net** es un método de **regresión lineal regularizada** que combina las penalizaciones de **Ridge Regression (regresión L2)** y **Lasso Regression (regresión L1)**. Fue desarrollado para superar las limitaciones de Lasso, que puede tener problemas cuando hay un gran número de variables predictoras o cuando estas variables están altamente correlacionadas (multicolinealidad). Elastic Net es una herramienta muy versátil para la **selección de características**, la **reducción de sobreajuste** y el manejo de **datos de alta dimensión**.

La función de costo de Elastic Net añade dos términos de penalización a la suma de los errores cuadrados de los residuos (como en la regresión OLS):

1.  **Penalización L1 (Lasso):** La suma del valor absoluto de los coeficientes. Esta penalización tiende a **reducir los coeficientes de las variables menos importantes a cero**, realizando así una **selección automática de características**.
2.  **Penalización L2 (Ridge):** La suma del cuadrado de los coeficientes. Esta penalización **encoge los coeficientes** hacia cero, pero no los fuerza a ser exactamente cero. Es particularmente útil para manejar la **multicolinealidad**, ya que tiende a distribuir la influencia de las variables correlacionadas de manera más equitativa.

Elastic Net utiliza dos hiperparámetros de sintonización:

* **$\alpha$ (alpha):** Controla el **balance entre las penalizaciones L1 y L2**.
    * Si $\alpha = 0$, Elastic Net se convierte en **Ridge Regression**.
    * Si $\alpha = 1$, Elastic Net se convierte en **Lasso Regression**.
    * Para valores entre 0 y 1, es una mezcla de ambas.
* **$\lambda$ (lambda):** Controla la **fuerza general de la regularización**. Un $\lambda$ más grande implica una mayor penalización y, por lo tanto, coeficientes más pequeños.

Al combinar L1 y L2, Elastic Net logra lo mejor de ambos mundos: realiza selección de características como Lasso y maneja la multicolinealidad y la estabilidad de los coeficientes como Ridge. Esto lo hace muy robusto en escenarios donde hay muchas variables correlacionadas.


**Aprendizaje Global vs. Local:**

Elastic Net es un modelo de **aprendizaje global**.

* **Aspecto Global:** Elastic Net construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes de la regresión se estiman optimizando una función de costo que considera todos los puntos de datos simultáneamente. La penalización se aplica a todos los coeficientes de manera uniforme, lo que busca una solución que minimice el error de predicción y controle la complejidad del modelo a nivel global. La ecuación de regresión final es una función que se aplica de manera consistente a cualquier nueva observación, sin importar su ubicación en el espacio de características.

* **Influencia de la Regularización:** Aunque la regresión en sí es global, las penalizaciones de regularización pueden tener un efecto que podríamos considerar "adaptativo" en el sentido de que ajustan la influencia de las variables en función de su relación con otras variables y la respuesta. Por ejemplo, la penalización L1 puede "localizar" las variables más importantes al poner otras a cero, y la L2 puede distribuir la importancia entre variables correlacionadas. Sin embargo, estas son propiedades de la optimización global del modelo, no de ajustar modelos separados para diferentes subregiones del espacio de datos. La Elastic Net, al igual que OLS, Ridge y Lasso, busca una única relación lineal que describa la tendencia general de los datos.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (regresión)",
  "✅ Numérica continua",
  "✅ Numéricas (requiere estandarización)",
  "✅ Lineal (como OLS)",
  "⚠️ Deseable, pero no esencial",
  "✅ Supuesto importante",
  "✅ Requiere homoscedasticidad",
  "⚠️ Afectado por outliers (no tan robusto)",
  "✅ Ideal para multicolinealidad alta (mejor que LASSO)",
  "⚠️ Puede ser menos interpretable que LASSO si hay muchas variables seleccionadas",
  "✅ Rápido incluso con datos grandes",
  "✅ Requiere validar los hiperparámetros `lambda` y `alpha`",
  "❌ Relación no lineal, o pocos datos con muchas variables no relevantes"
)

detalles <- c(
  "Modelo de regresión penalizada que combina LASSO (L1) y Ridge (L2) en un solo modelo.",
  "Predice una variable continua a partir de variables independientes numéricas.",
  "Las variables deben estar estandarizadas para evitar que la penalización sesgue los coeficientes.",
  "Asume que la relación entre variables es lineal.",
  "La normalidad ayuda para inferencia, pero no es crítica para predicción.",
  "Errores deben ser independientes para que los coeficientes sean válidos.",
  "Varianza constante de los errores es un supuesto clave.",
  "Aunque regulariza, no es inmune a valores atípicos.",
  "Funciona bien cuando hay muchas variables correlacionadas entre sí.",
  "Mezcla selección de variables (L1) y regularización (L2), lo cual puede dificultar la interpretación directa.",
  "A pesar de usar dos penalizaciones, sigue siendo eficiente con librerías como `glmnet`.",
  "Validación cruzada se usa para seleccionar los mejores valores de `lambda` y `alpha`.",
  "Puede tener bajo rendimiento si no hay una relación lineal o si las variables relevantes no están presentes en el conjunto."
)

tabla_elastic_net <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_elastic_net %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir elastic net",
             subtitle = "Elastic Net")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Ridge Regression  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/RR.png"))
```

**Ridge Regression (Regresión Ridge)** es un método de **regresión lineal regularizada** que se utiliza para mejorar la estimación de los coeficientes en modelos lineales, especialmente cuando existe **multicolinealidad** (alta correlación entre las variables predictoras) o cuando el número de predictores es grande en relación con el número de observaciones. Ridge Regression fue una de las primeras técnicas de regularización y es fundamental para comprender métodos más avanzados como Lasso o Elastic Net.

La Regresión Ridge aborda los problemas de la regresión por mínimos cuadrados ordinarios (OLS) al añadir un **término de penalización L2** a la función de costo de los mínimos cuadrados. La función de costo que minimiza Ridge Regression es:

$$\text{RSS} + \lambda \sum_{j=1}^{p} \beta_j^2$$

Donde:
* $\text{RSS}$ es la suma de los errores cuadrados de los residuos (Residual Sum of Squares), que es lo que minimiza OLS.
* $\lambda$ (lambda) es un **parámetro de sintonización (hiperparámetro)** no negativo. Este parámetro controla la **fuerza de la penalización**.
* $\sum_{j=1}^{p} \beta_j^2$ es la **penalización L2**, que es la suma de los cuadrados de los coeficientes de regresión (excluyendo el intercepto).

**Efecto de la Penalización L2:**
* **Encogimiento de Coeficientes:** La penalización L2 **encoge los coeficientes** hacia cero. Cuanto mayor sea el valor de $\lambda$, mayor será el encogimiento y más pequeños serán los coeficientes.
* **Reducción de Varianza:** Este encogimiento reduce la varianza de las estimaciones de los coeficientes, haciéndolos más estables y menos sensibles a pequeñas variaciones en los datos de entrenamiento. Esto ayuda a **reducir el sobreajuste**.
* **Manejo de Multicolinealidad:** En presencia de multicolinealidad, OLS puede asignar grandes valores a los coeficientes de variables correlacionadas. Ridge Regression distribuye la influencia entre las variables correlacionadas de manera más uniforme y reduce la magnitud de estos coeficientes, lo que resulta en un modelo más robusto.
* **No realiza selección de características:** A diferencia de Lasso, Ridge Regression encoge los coeficientes, pero **rara vez los fuerza a ser exactamente cero**. Esto significa que todas las variables predictoras (o casi todas) seguirán en el modelo.

El valor óptimo de $\lambda$ se selecciona típicamente mediante técnicas de validación cruzada.

**Aprendizaje Global vs. Local:**

Ridge Regression es un modelo de **aprendizaje global**.

* **Aspecto Global:** Ridge Regression construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes se estiman optimizando una función de costo que considera todos los puntos de datos simultáneamente. La penalización L2 se aplica a todos los coeficientes para controlar la complejidad y la estabilidad del modelo a nivel global. La ecuación de regresión resultante es una función única que se aplica de manera consistente a cualquier nueva observación, sin importar su ubicación específica en el espacio de características.

* **Estabilización Global:** Aunque la regularización L2 mejora la estabilidad de las estimaciones de los coeficientes y ayuda a manejar la multicolinealidad, lo hace como parte de una optimización global. No implica la creación de múltiples modelos locales o la adaptación a subregiones específicas de los datos. La Regresión Ridge busca una relación lineal subyacente que sea la mejor aproximación para el conjunto de datos completo, penalizando la complejidad para mejorar la generalización global.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (regresión)",
  "✅ Numérica continua",
  "✅ Numéricas (requiere estandarización)",
  "✅ Lineal (como OLS)",
  "⚠️ Supuesto deseable pero no estricto",
  "✅ Supuesto necesario",
  "✅ Supuesto necesario",
  "⚠️ Puede verse afectado, pero menos que OLS",
  "✅ Diseñado para mitigarla mediante penalización",
  "⚠️ Menos interpretable que OLS (coeficientes sesgados)",
  "✅ Eficiente incluso con muchas variables",
  "✅ Requiere validación para ajustar parámetro lambda",
  "❌ Si la relación no es lineal o hay muchas variables irrelevantes"
)

detalles <- c(
  "Extensión de la regresión lineal que agrega penalización L2 para reducir sobreajuste y manejar multicolinealidad.",
  "Se utiliza cuando se desea predecir una variable numérica continua.",
  "Las variables deben ser numéricas y estar estandarizadas para que la penalización tenga sentido.",
  "Asume relación lineal entre predictores y variable respuesta, como la regresión lineal.",
  "La normalidad es deseable para inferencia, pero no indispensable para predicción.",
  "Se espera independencia entre observaciones para que el modelo sea válido.",
  "Es importante que los errores tengan varianza constante para predicciones fiables.",
  "Reduce varianza, pero valores extremos aún pueden afectar los resultados.",
  "La penalización reduce varianza al achicar coeficientes, útil con predictores correlacionados.",
  "Coeficientes penalizados dificultan la interpretación directa, pero mejoran estabilidad.",
  "Rápido y adecuado para problemas con muchas variables; incluso p > n.",
  "Se usa validación cruzada para elegir el mejor valor de lambda (parámetro de regularización).",
  "No se recomienda cuando la relación entre variables es no lineal o se requiere interpretación clara."
)

tabla_ridge <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_ridge %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir ridge",
             subtitle = "Ridge Regression")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Least Absolute Shrinkage and Selection Operator (LASSO)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/LASSO.png"))
```



**LASSO (Least Absolute Shrinkage and Selection Operator)** es un método de **regresión lineal regularizada** que, al igual que Ridge Regression, se utiliza para mejorar la estimación de los coeficientes en modelos lineales y para abordar el **sobreajuste**, especialmente en escenarios con un gran número de variables predictoras o cuando algunas de ellas son irrelevantes. LASSO es particularmente famoso por su capacidad para realizar **selección automática de características**.

LASSO logra esto añadiendo un **término de penalización L1** a la función de costo de los mínimos cuadrados. La función de costo que minimiza LASSO es:

$$\text{RSS} + \lambda \sum_{j=1}^{p} |\beta_j|$$

Donde:
* $\text{RSS}$ es la suma de los errores cuadrados de los residuos.
* $\lambda$ (lambda) es un **parámetro de sintonización (hiperparámetro)** no negativo que controla la **fuerza de la penalización**.
* $\sum_{j=1}^{p} |\beta_j|$ es la **penalización L1**, que es la suma del valor absoluto de los coeficientes de regresión (excluyendo el intercepto).

**Efecto de la Penalización L1:**
* **Encogimiento de Coeficientes:** Similar a Ridge, la penalización L1 encoge los coeficientes hacia cero.
* **Selección de Características:** La característica distintiva de LASSO es que, debido a la naturaleza de la penalización L1 (la suma de los valores absolutos), **puede forzar los coeficientes de las variables menos importantes a ser exactamente cero**. Esto significa que LASSO no solo encoge los coeficientes, sino que también **realiza una selección automática de características**, eliminando efectivamente las variables irrelevantes del modelo. Esto resulta en modelos más simples y fáciles de interpretar.
* **Manejo de Multicolinealidad (con cuidado):** Aunque LASSO puede manejar la multicolinealidad, tiende a seleccionar arbitrariamente una de las variables correlacionadas y poner a cero las demás, lo que puede ser una desventaja en comparación con Ridge (que distribuye la influencia). Elastic Net surgió para abordar esto.

El valor óptimo de $\lambda$ se selecciona típicamente mediante técnicas de validación cruzada.


**Aprendizaje Global vs. Local:**

LASSO es un modelo de **aprendizaje global**.

* **Aspecto Global:** LASSO construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes se estiman optimizando una función de costo que considera todos los puntos de datos simultáneamente. La penalización L1 se aplica a todos los coeficientes para controlar la complejidad y realizar la selección de características a nivel global. La ecuación de regresión final es una función única que se aplica de manera consistente a cualquier nueva observación, sin importar su ubicación en el espacio de características.

* **Selección Global de Características:** Aunque LASSO puede "localizar" qué variables son importantes al reducir sus coeficientes a cero, esto se hace como parte de un proceso de optimización global que evalúa la contribución de cada variable a la predicción general del modelo. No implica la creación de múltiples modelos locales o la adaptación a subregiones específicas de los datos. LASSO busca la relación lineal más parsimoniosa que mejor se ajuste al conjunto de datos completo.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (regresión)",
  "✅ Numérica continua",
  "✅ Numéricas (requiere estandarización)",
  "✅ Lineal (como OLS)",
  "⚠️ Deseable pero no estrictamente necesaria",
  "✅ Requiere independencia de errores",
  "✅ Requiere homoscedasticidad",
  "⚠️ Puede verse afectado por outliers extremos",
  "✅ Maneja multicolinealidad mediante regularización",
  "✅ Realiza selección de variables (coeficientes pueden ser 0)",
  "✅ Eficiente en alta dimensión; mejor que OLS",
  "✅ Validación cruzada necesaria para lambda",
  "❌ No es adecuado si la relación es no lineal o hay muchas variables correlacionadas con igual relevancia"
)

detalles <- c(
  "Modelo de regresión penalizada que agrega penalización L1, capaz de forzar coeficientes a cero (selección de variables).",
  "Se usa cuando se desea predecir una variable continua.",
  "Las variables predictoras deben estandarizarse para que la penalización sea justa entre coeficientes.",
  "Asume una relación lineal entre los predictores y la respuesta.",
  "La normalidad ayuda para inferencia, pero no es crítica para predicción.",
  "Los errores deben ser independientes para que las estimaciones sean válidas.",
  "Es deseable que la varianza de los errores sea constante a lo largo de los valores ajustados.",
  "Puede verse afectado por valores atípicos, aunque penaliza el modelo.",
  "Disminuye la varianza de los coeficientes y ayuda a estabilizar el modelo frente a multicolinealidad.",
  "Permite eliminar automáticamente variables irrelevantes, facilitando modelos más simples y explicables.",
  "Es computacionalmente eficiente, incluso cuando hay más variables que observaciones.",
  "Lambda (parámetro de penalización) se selecciona generalmente vía validación cruzada.",
  "Si hay muchas variables correlacionadas, LASSO tiende a seleccionar solo una de ellas, lo que puede ser inadecuado en algunos contextos."
)

tabla_lasso <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lasso %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir LASSO",
             subtitle = "Least Absolute Shrinkage and Selection Operator (LASSO)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```








<!--chapter:end:07-regularization.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 🔍 8. Modelos Basados en Instancias {-}  

**Ejemplos:** K-Nearest Neighbors (KNN).  
**Uso:** Son ideales cuando tienes una **cantidad limitada de datos** y esperas que los patrones relevantes se encuentren en la **similitud local** entre casos. Se utilizan mucho cuando la **similitud directa** entre las observaciones es un factor clave.   
**Ventajas:** Su implementación es **simple** y son bastante **eficaces** en problemas con pocas dimensiones.   
**Limitaciones:** **Escalan mal** con grandes volúmenes de datos debido a que necesitan almacenar y comparar cada instancia. Además, son **sensibles al ruido** en los datos.   

---

## Case-Based Reasoning (CBR) {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/CBR.png"))
```


## k - Nearest Neighbour (kNN)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/kNN.png"))
```


**k-Nearest Neighbour (kNN)** es un algoritmo de **Machine Learning no paramétrico** que se utiliza tanto para tareas de **clasificación** como de **regresión**. Es considerado uno de los algoritmos más simples y se basa en la idea de que los puntos de datos que están cerca entre sí en el espacio de características suelen tener propiedades similares. No es un algoritmo que "aprende" un modelo explícito durante la fase de entrenamiento, sino que es un **algoritmo perezoso (lazy learner)**.

**Funcionamiento de kNN:**

1.  **Entrenamiento:** En la fase de entrenamiento, kNN simplemente **almacena todo el conjunto de datos de entrenamiento**. No hay un proceso de "aprendizaje" de parámetros o construcción de un modelo, como en la regresión lineal o las redes neuronales.
2.  **Predicción (para una nueva instancia):**
    * **Identificar Vecinos:** Para clasificar o predecir el valor de una nueva instancia, kNN calcula la **distancia** entre esta nueva instancia y *todas* las instancias en el conjunto de entrenamiento. La métrica de distancia más común es la **distancia euclidiana**, pero se pueden usar otras (Manhattan, Minkowski, etc.).
    * **Seleccionar 'k' Vecinos Más Cercanos:** Se identifican los 'k' puntos de datos del entrenamiento que son más cercanos a la nueva instancia. El valor de 'k' es un **hiperparámetro** que debe ser seleccionado por el usuario.
    * **Clasificación:** Para tareas de clasificación, la nueva instancia se asigna a la clase que es la **mayoría entre sus 'k' vecinos más cercanos** (votación mayoritaria).
    * **Regresión:** Para tareas de regresión, el valor predicho para la nueva instancia es el **promedio (o mediana) de los valores de la variable de respuesta de sus 'k' vecinos más cercanos**.

La elección del valor de 'k' es crucial: un 'k' pequeño puede hacer el modelo sensible al ruido (sobreajuste), mientras que un 'k' grande puede suavizar demasiado la predicción (subajuste) y las fronteras de decisión.

**Aprendizaje Global vs. Local:**

k-Nearest Neighbour (kNN) es el ejemplo por excelencia de un modelo de **aprendizaje puramente local**.

* **Aspecto Local:** La predicción para una nueva instancia depende **exclusivamente de los 'k' puntos de datos más cercanos a ella en el espacio de características**. No se construye un modelo global que abarque todo el conjunto de datos. En cambio, para cada nueva consulta, el algoritmo "re-calcula" el vecindario relevante y realiza una predicción basada solo en la información de esa pequeña región local. Esto significa que la frontera de decisión (en clasificación) o la función de regresión (en regresión) se ajusta localmente a las características del vecindario del punto de consulta. Si los datos no se distribuyen linealmente y tienen estructuras complejas con patrones que varían en diferentes regiones, kNN es muy efectivo porque puede adaptarse a estas variaciones locales al funcionar como una **"regresión (o clasificación) ponderada localmente"**.

* **Sin Modelo Explícito Global:** Debido a su naturaleza de "aprendizaje perezoso", kNN no genera una función matemática explícita o un conjunto de coeficientes que describan la relación global entre las variables. Todo el conocimiento del modelo está implícito en la base de datos de entrenamiento.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación y regresión)",
  "✅ Numérica (regresión) o categórica (clasificación)",
  "✅ Numéricas (preferible), aunque puede adaptarse para categóricas",
  "❌ No asume ninguna forma funcional",
  "❌ No aplica",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Muy sensible a valores atípicos",
  "⚠️ Problemas si hay predictores muy correlacionados",
  "⚠️ Difícil de interpretar (modelo basado en instancias)",
  "❌ Lento con grandes volúmenes (requiere calcular distancias)",
  "✅ Crucial para elegir el mejor valor de *k*",
  "❌ No escala bien con datos grandes o con ruido"
)

detalles <- c(
  "Modelo no paramétrico que predice en función de la cercanía a ejemplos del conjunto de entrenamiento.",
  "Se usa tanto para clasificación como para regresión según el tipo de variable objetivo.",
  "Las variables deben estar en la misma escala; se recomienda estandarizar.",
  "No asume una relación específica entre variables; se basa en similitud.",
  "No se ajusta una función, por lo tanto no hay residuos como tal.",
  "No se estiman errores independientes, ya que no hay función de error explícita.",
  "No hay regresión residual, por lo tanto este supuesto no aplica.",
  "Outliers pueden alterar los vecinos más cercanos y afectar la predicción.",
  "No requiere modelo explícito, pero predictores correlacionados pueden afectar el peso relativo en la distancia.",
  "Predicción se basa en instancias cercanas, difícil de resumir en una fórmula.",
  "Requiere calcular distancia para cada predicción → lento con grandes bases.",
  "Se suele usar validación cruzada para encontrar el número óptimo de vecinos (*k*).",
  "Alta dimensión, ruido o escalas distintas entre variables afectan el rendimiento del modelo."
)

tabla_knn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_knn %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir kNN",
             subtitle = "k - Nearest Neighbour (kNN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Kernel Regression / Nadaraya-Watson Estimator   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/Kernel Regression.png"))
```

## Learning Vector Quantization (LVQ)  {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/LVQ.png"))
```

**Learning Vector Quantization (LVQ)** es un algoritmo de **clasificación supervisada basado en prototipos**, desarrollado por Teuvo Kohonen. Puede ser visto como un tipo de **red neuronal artificial** que utiliza un enfoque de "ganador se lleva todo" (winner-take-all) para aprender a clasificar datos. LVQ es una alternativa al algoritmo k-Nearest Neighbour (kNN) que busca reducir la cantidad de información necesaria para almacenar los datos de entrenamiento, aprendiendo un conjunto más pequeño de **prototipos** que representan las clases.

La idea central de LVQ es la siguiente:

1.  **Representación por Prototipos:** En lugar de memorizar todos los puntos de datos de entrenamiento (como kNN), LVQ aprende un conjunto de **vectores prototipo (o "codebook vectors")**. Cada prototipo está asociado a una clase específica y representa una "región" en el espacio de características que pertenece a esa clase.
2.  **Proceso de Aprendizaje (Entrenamiento Supervisado):**
    * Se inicializan los prototipos (a menudo aleatoriamente o con puntos de datos de entrenamiento).
    * Para cada instancia de entrenamiento:
        * Se encuentra el prototipo más cercano (el "ganador") a esa instancia utilizando una métrica de distancia (comúnmente la distancia euclidiana).
        * Se ajusta la posición de este prototipo ganador:
            * Si el prototipo ganador tiene la **misma clase** que la instancia de entrenamiento, el prototipo se **mueve ligeramente más cerca** de la instancia (recompensa).
            * Si el prototipo ganador tiene una **clase diferente** a la instancia de entrenamiento, el prototipo se **mueve ligeramente más lejos** de la instancia (penalización).
    * Este proceso iterativo continúa hasta que los prototipos convergen o se alcanza un número máximo de épocas. Las diferentes variantes de LVQ (LVQ1, LVQ2.1, LVQ3) tienen reglas de actualización ligeramente distintas.
3.  **Clasificación (Predicción):** Para clasificar una nueva instancia, simplemente se encuentra el prototipo más cercano a esa instancia en el espacio de características. La nueva instancia se asigna a la clase asociada con ese prototipo más cercano. Es similar a un clasificador 1-NN que opera sobre los prototipos aprendidos.

LVQ es valorado por la interpretabilidad de sus prototipos (ya que son puntos en el espacio de características que representan una clase) y por su eficiencia una vez que los prototipos han sido aprendidos, ya que la predicción es mucho más rápida que kNN en grandes conjuntos de datos.


**Aprendizaje Global vs. Local:**

Learning Vector Quantization (LVQ) es un modelo que exhibe características de **aprendizaje tanto global como local**.

* **Aspecto Local:** El corazón del aprendizaje en LVQ es la **adaptación local de los prototipos**. En cada paso de entrenamiento, solo el prototipo más cercano (o los dos prototipos más cercanos en algunas variantes como LVQ2.1 y LVQ3) a una instancia de entrenamiento se ajusta. Esto significa que las reglas de aprendizaje operan en un **vecindario localizado** alrededor de la instancia de entrada. Los prototipos se mueven en el espacio de características para delimitar mejor las fronteras de clase, lo que refleja la estructura local de los datos. De esta manera, LVQ puede modelar **relaciones no lineales** y estructuras de clase complejas al ajustar las posiciones de estos "representantes" locales de las clases.

* **Aspecto Global:** Aunque el ajuste es local, el conjunto de todos los prototipos de LVQ, una vez entrenados, forma una **representación global del espacio de características** que se utiliza para la clasificación. Estos prototipos definen un mapa de clasificación en todo el espacio de entrada, donde cada región (celda de Voronoi) se asocia con una clase. Por lo tanto, el modelo final, que es la colección de prototipos, se aplica de manera global para clasificar cualquier nueva observación. El proceso de optimización para encontrar las posiciones de los prototipos, aunque iterativo y basado en actualizaciones locales, busca una configuración global óptima que minimice el error de clasificación en todo el conjunto de entrenamiento.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Categórica",
  "✅ Numéricas (requiere cálculo de distancias)",
  "❌ No asume relación funcional directa",
  "❌ No aplica (modelo de clasificación, no regresión)",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Sensible a valores extremos (afectan los prototipos)",
  "⚠️ Variables correlacionadas pueden distorsionar distancias",
  "⚠️ Intermedio: prototipos ayudan pero no son tan interpretables como reglas",
  "✅ Rápido después del entrenamiento (dependiendo del número de prototipos)",
  "✅ Útil para ajustar número y posición de prototipos",
  "❌ Problemas si los datos no están bien escalados o si hay clases muy desbalanceadas"
)

detalles <- c(
  "Técnica supervisada basada en instancias que usa prototipos para representar clases.",
  "Se usa para tareas de clasificación en donde las clases están etiquetadas.",
  "Requiere variables numéricas porque se basa en distancias euclidianas para asignación de clases.",
  "No asume una relación funcional, simplemente asigna una clase basada en el prototipo más cercano.",
  "No se generan residuos, por tanto la normalidad no se evalúa.",
  "No hay error estructurado como en modelos de regresión, por lo tanto este supuesto no aplica.",
  "No se evalúa la varianza de errores ya que no es un modelo de regresión.",
  "Outliers pueden alterar la posición de los prototipos y generar errores de clasificación.",
  "Variables altamente correlacionadas pueden sesgar las distancias, lo que afecta la clasificación.",
  "Aunque los prototipos pueden ofrecer intuición sobre la clase, no son completamente transparentes.",
  "Después del ajuste de los prototipos, la clasificación es eficiente.",
  "Es común usar validación cruzada para seleccionar el número y la distribución de los prototipos.",
  "No es adecuado si las variables están en escalas distintas o si no hay separación clara entre clases."
)

tabla_lvq <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lvq %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir LVQ",
             subtitle = "Learning Vector Quantization (LVQ)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Locally Weighted Learning (LWL)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/LWL.png"))
```

**Locally Weighted Learning (LWL)** es una clase de algoritmos de **aprendizaje supervisado no paramétrico** que se distingue por su enfoque en la **construcción de modelos locales** para cada nueva instancia de consulta, en lugar de aprender un único modelo global para todo el conjunto de datos. Es un tipo de **"aprendizaje perezoso" (lazy learning)**, lo que significa que la mayor parte del "trabajo" (cálculos) se realiza en el momento de la predicción, no durante una fase de entrenamiento explícita.

La idea central de LWL es que, para predecir la salida de una nueva instancia de consulta, se construye un modelo simple (a menudo lineal o polinómico) utilizando solo las **instancias de entrenamiento que son "cercanas"** a la instancia de consulta. Además, a las instancias de entrenamiento más cercanas se les asigna un **peso mayor** en la construcción de este modelo local.

El proceso de LWL (especialmente para regresión, conocida como **Regresión Lineal Ponderada Localmente - LWLR** o LOESS/LOWESS) implica:

1.  **Sin Fase de Entrenamiento explícita:** El algoritmo simplemente almacena todo el conjunto de datos de entrenamiento.
2.  **Para cada Instancia de Consulta (Predicción):**
    * **Cálculo de Distancias:** Se calcula la distancia entre la instancia de consulta y todas las instancias de entrenamiento.
    * **Asignación de Pesos:** Se aplica una **función de kernel (función de ponderación)** a estas distancias para asignar un peso a cada instancia de entrenamiento. Las instancias más cercanas a la consulta reciben un peso mayor, y los pesos disminuyen a medida que la distancia aumenta. Un hiperparámetro llamado **ancho de banda (bandwidth)** controla qué tan rápido disminuyen los pesos con la distancia (determina el "tamaño del vecindario" influyente).
    * **Construcción del Modelo Local:** Se ajusta un modelo simple (ej., una regresión lineal) a las instancias de entrenamiento, pero esta vez, cada instancia se pondera según el peso calculado. Esto es, se minimiza una suma de errores cuadrados ponderada.
    * **Predicción:** El valor predicho para la instancia de consulta se obtiene utilizando este modelo local recién construido. El modelo local se descarta después de hacer la predicción para esa instancia.

LWL es muy efectivo para modelar relaciones **no lineales y complejas** en los datos porque puede adaptar la forma de la función de predicción a las variaciones locales. Es una generalización de k-Nearest Neighbors (kNN) donde en lugar de solo promediar o votar, se ajusta un modelo ponderado.


**Aprendizaje Global vs. Local:**

Locally Weighted Learning (LWL) es el epítome del **aprendizaje puramente local**.

* **Aspecto Local:** LWL es intrínsecamente local en su funcionamiento. Para **cada nueva predicción**, se construye un **modelo específico y único** que solo es válido en el **vecindario local** de la instancia de consulta. Los pesos asignados a las instancias de entrenamiento enfatizan las que están más cerca del punto de consulta, lo que significa que el modelo se "adapta" a la estructura de los datos en esa región particular del espacio de características. Esto le permite manejar eficientemente relaciones no lineales y heterogéneas, ya que la relación puede ser diferente en distintas partes del dominio de los datos.

* **Sin Modelo Explícito Global:** No hay un conjunto fijo de parámetros o una función matemática única que describa la relación entre las entradas y las salidas para todo el conjunto de datos. En cambio, el "modelo" se genera dinámicamente para cada punto de consulta, utilizando solo la información relevante de su vecindario. La complejidad computacional de LWL aumenta con el número de predicciones, ya que cada una requiere la construcción de un nuevo modelo local.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (regresión o clasificación)",
  "✅ Numérica o categórica, según tarea",
  "✅ Numéricas o categóricas (requiere distancias)",
  "✅ No lineal (ajustes locales en cada predicción)",
  "⚠️ Depende del modelo local usado (e.g., regresión lineal)",
  "⚠️ Puede no cumplir si hay dependencia local",
  "⚠️ Evaluada localmente, varía según vecindario",
  "✅ Sí, muy sensible a outliers en vecindarios locales",
  "⚠️ Puede causar inestabilidad en predicciones locales",
  "⚠️ Difícil de interpretar globalmente, clara localmente",
  "❌ Lento, necesita recalcular modelo para cada punto",
  "✅ Sí, especialmente leave-one-out o k-fold por zonas",
  "❌ Ineficiente con muchos datos o alta dimensión"
)

detalles <- c(
  "Modelo supervisado que ajusta un modelo distinto en cada punto de predicción usando los vecinos más cercanos.",
  "Puede ser regresión (respuesta numérica) o clasificación (respuesta categórica).",
  "Utiliza variables para calcular distancias a partir de un punto de consulta.",
  "Ajusta modelos simples en regiones locales, permitiendo capturar relaciones no lineales.",
  "En regresión local puede requerirse que los residuos sean normales si se desea inferencia.",
  "Los errores pueden no ser independientes si hay estructuras repetitivas locales.",
  "La varianza puede cambiar entre zonas del espacio, por lo que se revisa localmente.",
  "Los valores atípicos pueden sesgar el modelo local si caen cerca del punto de predicción.",
  "La multicolinealidad puede afectar si el modelo local es lineal, aunque su efecto se restringe localmente.",
  "La interpretación es clara en zonas locales, pero no se generaliza a toda la muestra.",
  "Cada predicción entrena un nuevo modelo, lo que es computacionalmente costoso.",
  "La validación cruzada ayuda a elegir parámetros como el ancho del vecindario (kernel).",
  "El rendimiento cae en grandes volúmenes de datos o si los datos no presentan estructura local clara."
)

tabla_lwl <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lwl %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir LWL",
             subtitle = "Locally Weighted Learning (LWL)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Self - Organizing Map (SOM)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/SOM.png"))
```

Una **Self-Organizing Map (SOM)**, también conocida como **Mapa Autoorganizado de Kohonen** o **Mapa de Características Autoorganizado (SOFM)**, es un tipo de **red neuronal artificial no supervisada** utilizada principalmente para **reducción de dimensionalidad y visualización de datos**. Su objetivo es producir una representación de baja dimensión (típicamente bidimensional) de un conjunto de datos de alta dimensión, mientras **preserva la estructura topológica** de los datos originales. Esto significa que los puntos de datos que son similares en el espacio de alta dimensión se mapean a neuronas cercanas en el mapa de baja dimensión.

A diferencia de otras redes neuronales que utilizan el aprendizaje por retropropagación y descenso de gradiente (aprendizaje basado en el error), las SOM utilizan un proceso de **aprendizaje competitivo**.

El funcionamiento de un SOM implica los siguientes pasos iterativos:

1.  **Inicialización:** Se crea una cuadrícula de "neuronas" (también llamadas unidades o nodos) en el espacio de baja dimensión (ej., una cuadrícula 2D). A cada neurona se le asigna un **vector de pesos** con la misma dimensionalidad que los datos de entrada. Estos vectores de pesos se inicializan aleatoriamente o de forma lineal.
2.  **Competencia:** Para cada vector de entrada (punto de datos) del conjunto de entrenamiento:
    * Se calcula la distancia (comúnmente euclidiana) entre el vector de entrada y el vector de pesos de cada neurona en la cuadrícula.
    * La neurona con el vector de pesos más cercano al vector de entrada se denomina **Unidad de Mejor Coincidencia (BMU - Best Matching Unit)**.
3.  **Cooperación (Vecindad):** La BMU y sus neuronas **vecinas** (dentro de un radio definido en la cuadrícula) son identificadas. El tamaño de este radio de vecindad disminuye con el tiempo a medida que avanza el entrenamiento. La influencia del ajuste de los pesos disminuye con la distancia de la BMU dentro de esta vecindad (definido por una **función de vecindad**, como una Gaussiana).
4.  **Adaptación:** Los vectores de pesos de la BMU y sus neuronas vecinas se **ajustan ligeramente** para que se acerquen al vector de entrada original. La magnitud del ajuste está determinada por una **tasa de aprendizaje**, que también disminuye con el tiempo. El ajuste es mayor para la BMU y menor para las neuronas más alejadas dentro del radio de vecindad.
5.  **Iteración:** Los pasos 2-4 se repiten para un gran número de épocas (iteraciones) y para todos los vectores de entrada, hasta que los pesos de las neuronas convergen y la red se "autoorganiza".

Al final del entrenamiento, las neuronas en el mapa se han organizado de tal manera que las neuronas cercanas representan datos de entrada similares, creando un "mapa" donde las regiones con densidades de datos similares forman grupos o clusters.

**Aprendizaje Global vs. Local:**

Una Self-Organizing Map (SOM) es un modelo que **combina aspectos de aprendizaje global y local** de una manera muy particular, que evoluciona a lo largo del proceso de entrenamiento.

* **Aspecto Global (Fases Iniciales del Entrenamiento):** Al principio del entrenamiento, el radio de vecindad y la tasa de aprendizaje son grandes. Esto significa que cuando una BMU se ajusta, un **gran número de neuronas circundantes en el mapa también se ajustan**, incluso aquellas que están relativamente lejos de la BMU. Este amplio ajuste permite que el mapa se "organice globalmente" para capturar la estructura general de los datos. La topología general de la proyección se establece en esta fase inicial. El mapa se estira y se contrae para abarcar la dispersión global de los datos, como si una "regresión ponderada localmente" de gran escala estuviera adaptando el mapa entero.

* **Aspecto Local (Fases Posteriores del Entrenamiento):** A medida que el entrenamiento avanza, el radio de vecindad y la tasa de aprendizaje **disminuyen gradualmente**. Esto hace que los ajustes a los pesos sean cada vez más localizados. En las etapas finales, solo la BMU y sus vecinos más cercanos (o incluso solo la BMU) se ajustan significativamente. Esta fase de "afinamiento" permite que el mapa capture los detalles más finos y las **estructuras locales** dentro de los datos, refinando las fronteras entre los grupos y asegurando que los puntos similares se agrupen con alta precisión.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (reducción de dimensionalidad y clustering)",
  "❌ No aplica (no hay variable respuesta)",
  "✅ Numéricas (basado en distancias)",
  "✅ No lineal (mapea datos de alta dimensión a una grilla)",
  "❌ No aplica",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Puede ser sensible a outliers (afectan la topología de la grilla)",
  "⚠️ No impacta directamente pero puede distorsionar distancias",
  "⚠️ Difícil de interpretar (requiere visualizaciones específicas)",
  "⚠️ Entrenamiento puede ser lento en datasets grandes, luego eficiente",
  "⚠️ No es tradicional, pero se puede evaluar topología y distorsión",
  "❌ Mal desempeño si los datos están mal escalados o no hay estructura"
)

detalles <- c(
  "Técnica no supervisada que proyecta datos de alta dimensión a una grilla 2D preservando la topología.",
  "No predice una variable, sino agrupa y organiza datos similares espacialmente.",
  "Basado en distancias euclidianas entre vectores de características; requiere variables numéricas.",
  "Preserva relaciones de vecindad: observaciones similares se ubican cerca en la grilla.",
  "No se generan residuos como en modelos de regresión o clasificación.",
  "No hay modelo de error; no aplica este supuesto.",
  "No hay varianza de errores al no haber predicción.",
  "Outliers pueden alterar las posiciones en la grilla y afectar la interpretación.",
  "La correlación entre variables puede afectar las distancias y la formación de grupos.",
  "Interpretación se basa en visualización de mapas de componentes y distancias.",
  "Entrenamiento iterativo, más lento que PCA pero útil para exploración visual.",
  "No se usa validación cruzada directa, pero se puede evaluar la topología y mapas de distancia.",
  "Datos ruidosos, mal escalados o con muchas variables irrelevantes dificultan resultados útiles."
)

tabla_som <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_som %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir SOM",
             subtitle = "Self - Organizing Map (SOM)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Prototype-Based Learning (General Concept) {-}     

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/PBL.png"))
```

<!--chapter:end:08-instance_based.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 📏 9. Clustering (Aprendizaje No Supervisado) {-}  

**Ejemplos:** K-Means, DBSCAN, Agrupamiento Jerárquico.   
**Uso:** Excelente para **agrupar datos sin etiquetas previas**, permitiéndote descubrir **estructuras ocultas** o identificar **segmentos de mercado** dentro de tus conjuntos de datos. Es una herramienta clave en la exploración de datos.   
**Ventajas:** Es increíblemente útil para la **exploración de datos** y para **reducir la complejidad** al encontrar patrones inherentes.  
**Limitaciones:** Generalmente, necesitas **elegir el número de grupos** de antemano (excepto en DBSCAN), lo cual puede ser un desafío. Además, algunos algoritmos pueden ser **sensibles a la escala** de las características de tus datos.

---

## Affinity Propagation {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/AF.png"))
```


## Agglomerative Clustering {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/Agglomerative Clustering.png"))
```

## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/DBSCAN.png"))
```

**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** es un algoritmo de **agrupamiento (clustering) no supervisado** que se distingue de los algoritmos basados en centroides (como k-Means) por su capacidad para encontrar **clusters de formas arbitrarias** y para identificar **puntos de ruido (outliers)**. Su idea central es que los clusters son regiones densas de puntos en el espacio de características, separadas por regiones de baja densidad.

DBSCAN define tres tipos de puntos:

1.  **Punto Núcleo (Core Point):** Un punto es un punto núcleo si, dentro de un radio especificado ($\epsilon$ o `eps`), contiene un número mínimo de otros puntos ( `MinPts`).
2.  **Punto Frontera (Border Point):** Un punto es un punto frontera si está dentro del radio $\epsilon$ de un punto núcleo, pero no es un punto núcleo en sí mismo (no tiene `MinPts` vecinos dentro de su propio radio $\epsilon$).
3.  **Punto de Ruido (Noise Point):** Cualquier punto que no es un punto núcleo ni un punto frontera. Estos puntos son considerados outliers.

El algoritmo de DBSCAN opera de la siguiente manera:

1.  **Inicialización:** Selecciona un punto arbitrario del conjunto de datos que aún no ha sido visitado.
2.  **Expansión de Cluster:**
    * Si el punto seleccionado es un **punto núcleo**, se inicia un nuevo cluster. Todos sus vecinos dentro del radio $\epsilon$ se añaden al cluster.
    * Recursivamente, se visitan y añaden los vecinos de esos nuevos puntos. Si un vecino es también un punto núcleo, sus propios vecinos también se añaden al cluster. Este proceso continúa hasta que no se puedan añadir más puntos al cluster (es decir, todos los puntos alcanzables por densidad han sido encontrados).
    * Si el punto seleccionado **no es un punto núcleo**, se marca como ruido (o se deja para ser procesado más tarde si es un punto frontera de otro cluster ya formado).
3.  **Iteración:** El proceso se repite con otro punto no visitado hasta que todos los puntos han sido procesados.

DBSCAN es particularmente útil para encontrar clusters complejos en conjuntos de datos ruidosos y no requiere que el usuario especifique el número de clusters de antemano. Sus dos hiperparámetros clave son `eps` (el radio de búsqueda de vecindad) y `MinPts` (el número mínimo de puntos para formar un núcleo).

**Aprendizaje Global vs. Local:**

DBSCAN es un algoritmo de **agrupamiento inherentemente local**, aunque el resultado final es una partición global de los datos en clusters y ruido.

* **Aspecto Local:** El corazón de DBSCAN reside en la definición de densidad local y la conectividad. Las decisiones sobre si un punto es un núcleo, un frontera o ruido, y si dos puntos pertenecen al mismo clúster, se basan **exclusivamente en la densidad de puntos en un vecindario muy localizado** definido por el radio $\epsilon$ y el `MinPts`. El algoritmo "expande" los clústeres al moverse de un punto núcleo a sus vecinos, y de estos a sus vecinos, y así sucesivamente. Esta capacidad de crecer y formar clústeres orgánicamente a partir de las densidades locales es lo que permite a DBSCAN descubrir formas arbitrarias y adaptarse a la estructura local de los datos. No hay una función global o centroides predefinidos que guíen la agrupación; todo se deriva de las propiedades de densidad local.

* **Resultado Global (Partición):** Aunque el proceso es local, el resultado final es una **partición global del conjunto de datos** en varios clústeres y un conjunto de puntos de ruido. Una vez que todos los puntos han sido procesados y los clústeres expandidos, se obtiene una vista global de la estructura de agrupamiento.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (clustering)",
  "❌ No aplica (no hay variable respuesta)",
  "✅ Numéricas (distancias euclidianas u otras métricas)",
  "✅ Detecta clusters basados en densidad, no forma lineal",
  "❌ No aplica",
  "❌ No aplica",
  "❌ No aplica",
  "✅ Robusto a outliers (los detecta como ruido)",
  "⚠️ No afecta directamente (no hay predictores)",
  "⚠️ Clusters pueden ser arbitrarios, pero es intuitivo identificar ruido",
  "✅ Razonablemente rápido para conjuntos medianos",
  "❌ No usa validación cruzada clásica; se evalúa con métricas de clustering",
  "❌ No funciona bien con clusters de densidades muy diferentes o alta dimensionalidad"
)

detalles <- c(
  "Algoritmo de clustering basado en densidad que agrupa puntos cercanos y marca puntos aislados como ruido.",
  "No busca predecir, sino agrupar observaciones.",
  "Se basa en distancias; variables numéricas adecuadas; variables categóricas necesitan transformación.",
  "No asume formas de clusters lineales ni convexas; puede detectar clusters arbitrarios.",
  "No genera residuos; no aplica normalidad.",
  "No hay modelo de error residual, no aplica independencia.",
  "No es un modelo predictivo, no aplica homoscedasticidad.",
  "Detecta outliers etiquetándolos como ruido, siendo robusto frente a ellos.",
  "No hay predictores en sentido tradicional, por lo que multicolinealidad no afecta.",
  "Interpretación basada en grupos densos y puntos aislados (ruido).",
  "Es eficiente, aunque su rendimiento puede disminuir en alta dimensionalidad.",
  "No utiliza validación cruzada estándar; evaluación se basa en índices de clustering como Silhouette.",
  "Dificultades con clusters con diferentes densidades y cuando la dimensionalidad es muy alta."
)

tabla_dbscan <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_dbscan %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir DBSCAN",
             subtitle = "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Expectation Maximization (EM) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/EM.png"))
```

El algoritmo **Expectation-Maximization (EM)** es un método iterativo utilizado en estadística para encontrar las **estimaciones de máxima verosimilitud (MLE)** o las estimaciones de máxima a posteriori (MAP) de los parámetros en modelos estadísticos, especialmente cuando el modelo depende de **variables latentes (no observadas o "ocultas")** o cuando los datos están "incompletos".

EM es particularmente útil para modelos de mezcla, donde se asume que los datos observados son una mezcla de varias distribuciones subyacentes, y la pertenencia de cada punto de datos a una distribución específica es la variable latente. El algoritmo consta de dos pasos principales que se alternan hasta la convergencia:

1.  **Paso E (Expectation Step - Paso de Expectativa):**
    * En este paso, dadas las estimaciones actuales de los parámetros del modelo, se calculan las **probabilidades esperadas (o "responsabilidades")** de que cada punto de datos observado pertenezca a cada una de las componentes latentes (o de que las variables latentes tomen ciertos valores).
    * Esencialmente, se está haciendo una "suposición" sobre los valores de las variables latentes basándose en los parámetros actuales del modelo y los datos observados.

2.  **Paso M (Maximization Step - Paso de Maximización):**
    * En este paso, utilizando las "responsabilidades" calculadas en el Paso E (tratándolas como si fueran observaciones completas), se **re-estiman los parámetros del modelo** para maximizar la verosimilitud esperada.
    * Esto es típicamente un problema de optimización más simple que el problema original de máxima verosimilitud con datos incompletos. Se ajustan los parámetros (ej., medias, varianzas, pesos de mezcla) para que el modelo se ajuste mejor a los datos, considerando las asignaciones "blandas" a las variables latentes.

Los Pasos E y M se repiten iterativamente. La verosimilitud del modelo está garantizada para no disminuir en cada iteración, y el algoritmo converge a un **máximo local** de la función de verosimilitud.

**Aplicaciones comunes:**
* **Modelos de Mezcla Gaussiana (GMMs):** Un uso prototípico del EM para el clustering no supervisado.
* **Modelos Ocultos de Markov (HMMs):** Para problemas de reconocimiento de voz y bioinformática.
* **Imputación de datos faltantes:** Para estimar valores faltantes en un conjunto de datos.
* **Análisis de componentes latentes.**


**Aprendizaje Global vs. Local:**

El algoritmo Expectation-Maximization (EM) es un método de **aprendizaje global**, pero es importante entender un matiz sobre su convergencia.

* **Aspecto Global:** EM tiene como objetivo encontrar los **parámetros de un modelo probabilístico global** (como un GMM completo que describe la distribución de todo el conjunto de datos) que maximicen la verosimilitud de los datos observados. Los parámetros que se estiman (medias, covarianzas, pesos de mezcla en un GMM) son válidos para todo el espacio de características. El algoritmo itera sobre todo el conjunto de datos en cada paso E y M para actualizar estos parámetros globales. La solución que busca EM es una representación unificada y global de las distribuciones subyacentes de los datos.

* **Convergencia a Máximos Locales:** Aunque EM busca una solución global, una limitación crítica es que **solo está garantizado para converger a un máximo local** de la función de verosimilitud, no necesariamente al máximo global. Esto significa que el resultado final puede depender de la **inicialización** de los parámetros del modelo. Si la función de verosimilitud tiene múltiples "picos" (máximos locales), EM puede quedar "atrapado" en uno de ellos. Para mitigar esto, es una práctica común ejecutar EM varias veces con diferentes inicializaciones aleatorias y seleccionar el resultado con la verosimilitud más alta.

Por lo tanto, mientras que el objetivo de EM es aprender un modelo global que abarque todo el espacio de datos, su método iterativo de optimización lo hace susceptible a encontrar óptimos locales en la función de verosimilitud. La forma en que un modelo probabilístico como un GMM puede modelar **relaciones no lineales** en los datos es que, al combinar múltiples distribuciones gaussianas (lineales), el modelo resultante puede capturar formas y densidades complejas y no lineales en el espacio de características. EM es el algoritmo que permite aprender estos componentes subyacentes.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (estimación de parámetros en modelos con datos incompletos o mixtos)",
  "❌ No aplica directamente (modelo probabilístico)",
  "✅ Variables numéricas o categóricas según modelo",
  "✅ Estima parámetros máximos de verosimilitud, puede manejar modelos complejos",
  "⚠️ Depende del modelo específico usado con EM",
  "⚠️ Depende del modelo; errores independientes si asume modelo estadístico clásico",
  "⚠️ Depende del modelo estadístico subyacente",
  "⚠️ Puede ser sensible a outliers dependiendo del modelo y datos",
  "⚠️ Depende del modelo y las variables involucradas",
  "⚠️ La interpretación depende del modelo y parámetros estimados",
  "❌ Puede ser lento si el modelo es complejo o datos muy grandes",
  "❌ Validación cruzada depende del modelo, no es intrínseco a EM",
  "❌ Puede converger a máximos locales; requiere buen punto inicial y modelo adecuado"
)

detalles <- c(
  "Algoritmo iterativo para estimar parámetros de modelos estadísticos con datos faltantes o variables latentes.",
  "No genera predicciones directas, sino estima parámetros para modelos probabilísticos.",
  "Aplicable a datos numéricos o categóricos dependiendo del modelo (mezcla de Gaussianas, por ejemplo).",
  "Maximiza la función de verosimilitud de manera iterativa, estimando variables latentes y parámetros.",
  "La normalidad depende del modelo (por ejemplo, mezcla de Gaussianas asume normalidad).",
  "Si el modelo asume errores independientes, entonces sí; depende del modelo estadístico usado.",
  "Homoscedasticidad depende del modelo estadístico subyacente.",
  "Sensibilidad a outliers varía según la robustez del modelo y datos.",
  "Multicolinealidad afecta según la estructura del modelo y variables involucradas.",
  "Interpretación es sobre parámetros estimados y variables latentes, no sobre coeficientes directos.",
  "Puede requerir muchas iteraciones, afectando velocidad en modelos complejos.",
  "La validación cruzada depende del modelo aplicado tras la estimación por EM.",
  "Puede quedarse atrapado en soluciones subóptimas; se recomienda múltiples inicios."
)

tabla_em <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_em %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir EM",
             subtitle = "Expectation Maximization (EM)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Gaussian Mixture Models (GMMs) {-}   


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/GMMs.png"))
```

## Hierarchical Clustering (hclust) {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/hclust.png"))
```

El **Agrupamiento Jerárquico (Hierarchical Clustering)**, a menudo abreviado como **hclust**, es un método de **agrupamiento (clustering) no supervisado** que construye una **jerarquía de clusters** en lugar de una partición plana de los datos (como k-Means). El resultado de un agrupamiento jerárquico se visualiza comúnmente como un **dendrograma**, un diagrama en forma de árbol que muestra la secuencia de fusiones o divisiones de los clusters.

Existen dos tipos principales de agrupamiento jerárquico:

1.  **Agrupamiento Aglomerativo ("Bottom-Up"):** Es el tipo más común.
    * Comienza tratando **cada punto de datos como un cluster individual**.
    * En cada paso, **fusiona los dos clusters más cercanos** en un nuevo cluster.
    * Este proceso continúa hasta que todos los puntos de datos pertenecen a un único cluster grande.
    * La "cercanía" entre clusters se define por una **métrica de enlace (linkage)**. Las métricas de enlace comunes incluyen:
        * **Enlace Único (Single Linkage):** Distancia mínima entre dos puntos en diferentes clusters. Tiende a formar clusters "largos" y "delgados".
        * **Enlace Completo (Complete Linkage):** Distancia máxima entre dos puntos en diferentes clusters. Tiende a formar clusters compactos.
        * **Enlace Promedio (Average Linkage):** Distancia promedio entre todos los pares de puntos en diferentes clusters.
        * **Método de Ward:** Minimiza la varianza total dentro de los clusters después de la fusión. Tiende a formar clusters compactos de tamaño similar.

2.  **Agrupamiento Divisivo ("Top-Down"):**
    * Comienza con **todos los puntos en un solo cluster grande**.
    * En cada paso, **divide el cluster actual en dos sub-clusters** más pequeños.
    * Este proceso continúa hasta que cada punto de datos está en su propio cluster individual.
    * Es menos común en la práctica debido a su mayor complejidad computacional.

La principal ventaja de hclust es que no requiere especificar el número de clusters de antemano; en cambio, el número de clusters se puede determinar inspeccionando el dendrograma y "cortándolo" a una altura apropiada. También es muy bueno para revelar la estructura anidada de los datos.


**Aprendizaje Global vs. Local:**

El Agrupamiento Jerárquico (hclust) es un algoritmo que se puede clasificar como de **aprendizaje local** en su construcción incremental, pero que al final revela una **estructura global** de los datos.

* **Aspecto Local (Proceso de Fusión/División):** En cada paso del agrupamiento aglomerativo, la decisión de qué clusters fusionar se basa **exclusivamente en la distancia (o similitud) entre los clusters más cercanos en ese momento**. Esta es una decisión puramente local, ya que solo se consideran los pares de clusters más próximos. El algoritmo construye la jerarquía fusionando iterativamente los vecinos más cercanos, lo que le permite adaptarse a la forma y densidad local de los datos. Las fronteras de los clústeres no están predefinidas por un modelo global; en cambio, emergen de las relaciones de proximidad locales. Esto permite a hclust descubrir clusters de **formas arbitrarias** y **relaciones no lineales** que podrían no ser detectadas por métodos que asumen formas específicas de clusters (como k-Means con suposiciones esféricas).

* **Aspecto Global (Dendrograma):** Aunque las decisiones de fusión son locales, el resultado final (el dendrograma) es una **representación jerárquica global** de las relaciones de todos los puntos de datos. Proporciona una visión completa de cómo todos los puntos se agrupan en diferentes niveles de granularidad, desde clusters individuales hasta un solo cluster grande. Esta estructura global revela patrones de anidamiento y relaciones a diferentes escalas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (clustering jerárquico)",
  "❌ No aplica (no hay variable respuesta)",
  "✅ Variables numéricas o categóricas (según medida de distancia)",
  "✅ Agrupa observaciones en base a similitud o distancia",
  "❌ No aplica",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Sensible a valores atípicos que pueden distorsionar distancias",
  "⚠️ No afecta directamente (no hay predictores ni multicolinealidad)",
  "✅ Dendrograma facilita interpretación visual de grupos",
  "⚠️ Puede ser lento en datasets muy grandes",
  "❌ No se suele usar validación cruzada, pero sí métodos de evaluación interna",
  "❌ Resultados muy sensibles a elección de distancia y método de enlace"
)

detalles <- c(
  "Método no supervisado para agrupar observaciones en una jerarquía basada en distancias.",
  "No busca predecir, sino identificar grupos o clusters.",
  "Puede trabajar con variables numéricas y categóricas si se define distancia adecuada.",
  "Construye dendrograma que muestra agrupamientos sucesivos desde observaciones individuales hasta un solo cluster.",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia de errores.",
  "No requiere homoscedasticidad.",
  "Valores atípicos pueden alterar significativamente la estructura del dendrograma.",
  "Como es una técnica de agrupamiento, no existe multicolinealidad entre variables predictoras.",
  "Dendrograma permite interpretar las relaciones y agrupamientos entre observaciones.",
  "La complejidad aumenta rápido con el número de observaciones (O(n^3)).",
  "Se evalúan índices de validación de clusters (silhouette, Dunn, etc.) en lugar de CV.",
  "La elección de métrica de distancia (Euclidiana, Manhattan) y método de enlace (completo, promedio, single) afecta mucho los resultados."
)

tabla_hclust <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_hclust %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir HClust",
             subtitle = "Hierarchical Clustering (hclust)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## k-Means  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/k-means.png"))
```

**k-Means** es uno de los algoritmos de **agrupamiento (clustering) no supervisado** más populares y ampliamente utilizados. Su objetivo es particionar un conjunto de $n$ observaciones en $k$ grupos o "clusters", donde cada observación pertenece al cluster cuyo centroide (media) es el más cercano.

El algoritmo k-Means opera de la siguiente manera:

1.  **Inicialización:**
    * Se elige un número predefinido de clusters, $k$. Este es un **hiperparámetro** que debe ser especificado por el usuario.
    * Se inicializan $k$ **centroides** (puntos centrales de los clusters). Esto se puede hacer de forma aleatoria seleccionando $k$ puntos de datos al azar como centroides iniciales, o utilizando métodos más sofisticados como k-Means++.

2.  **Paso de Asignación (Expectation / E-step):**
    * Para cada punto de datos en el conjunto, se calcula su distancia (comúnmente euclidiana) a cada uno de los $k$ centroides.
    * Cada punto de datos se **asigna al cluster cuyo centroide es el más cercano**.

3.  **Paso de Actualización (Maximization / M-step):**
    * Para cada uno de los $k$ clusters, se **recalcula la posición del centroide** como la media (promedio) de todos los puntos de datos que han sido asignados a ese cluster.

4.  **Iteración:**
    * Los pasos de Asignación y Actualización se repiten iterativamente.
    * El algoritmo converge cuando las asignaciones de los puntos a los clusters ya no cambian, o cuando las posiciones de los centroides no cambian significativamente entre iteraciones.

El objetivo del algoritmo es minimizar la **suma de los cuadrados de las distancias** de cada punto a su centroide asignado (también conocida como la inercia del cluster o la suma de cuadrados dentro del cluster - WCSS).

**Ventajas:** Es simple de implementar, computacionalmente eficiente y escalable para grandes conjuntos de datos.
**Limitaciones:** Requiere que el número de clusters $k$ sea especificado de antemano, es sensible a la inicialización de los centroides, y tiende a formar clusters esféricos de tamaño similar, lo que puede ser una desventaja si los clusters tienen formas arbitrarias o densidades muy diferentes. También es sensible a los outliers.


**Aprendizaje Global vs. Local:**

k-Means es un modelo de **aprendizaje global**.

* **Aspecto Global:** k-Means busca una **partición global de todo el conjunto de datos** en $k$ clusters. El objetivo de la optimización (minimizar la suma de los cuadrados de las distancias a los centroides) se calcula sobre **todos los puntos de datos** y todos los clusters simultáneamente. Los centroides, una vez convergidos, representan los "centros" de los clusters en el espacio de características, y estos se utilizan para asignar cualquier nuevo punto a su cluster correspondiente. La solución final es una asignación de cada punto a un cluster que se aplica a nivel global.

* **Asignaciones Locales dentro de una Optimización Global:** Aunque en cada iteración los puntos se asignan a su centroide "local" más cercano, esta asignación es parte de un proceso iterativo que busca optimizar un criterio global (la inercia total del cluster). Los centroides mismos son influenciados por todos los puntos asignados a su cluster, y la reubicación de los centroides afecta las asignaciones de todos los puntos en la siguiente iteración. El resultado son **fronteras de decisión lineales (hiperplanos)** entre los clusters (cuyas combinaciones pueden formar polígonos de Voronoi), que son una característica de un modelo global que divide el espacio. Si los datos no se distribuyen linealmente y los clusters tienen formas no esféricas o densidades muy diferentes, k-Means puede tener dificultades para descubrirlos, precisamente por su naturaleza global de optimización de la distancia euclidiana a un centroide.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (clustering por partición)",
  "❌ No aplica (no hay variable respuesta)",
  "✅ Variables numéricas (recomendado estandarizar)",
  "✅ Agrupa observaciones según distancia a centroides",
  "❌ No aplica",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Sensible a valores atípicos y centroides iniciales",
  "⚠️ No afecta directamente (no hay predictores ni multicolinealidad)",
  "✅ Fácil interpretación de clusters y centroides",
  "✅ Rápido y eficiente para datasets grandes",
  "❌ No se usa validación cruzada, pero sí índices de cluster (silhouette, etc.)",
  "❌ No funciona bien con clusters no esféricos o tamaños muy dispares"
)

detalles <- c(
  "Método no supervisado que particiona datos en k clusters minimizando suma de cuadrados dentro de clusters.",
  "No busca predecir, sino encontrar grupos o clusters.",
  "Requiere variables numéricas; es común estandarizarlas para evitar sesgos por escala.",
  "Cada observación se asigna al cluster con el centroide más cercano (distancia Euclidiana).",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia.",
  "No requiere homoscedasticidad.",
  "Los outliers pueden mover centroides y distorsionar clusters.",
  "Como técnica de agrupamiento, no hay multicolinealidad entre variables.",
  "Centroides y clusters son fáciles de interpretar y visualizar.",
  "Algoritmo rápido, converge rápido en general.",
  "Se usan índices externos e internos para evaluar calidad del clustering, no validación cruzada.",
  "No funciona bien si los clusters tienen formas complejas, tamaños muy distintos o solapamientos."
)

tabla_kmeans <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_kmeans %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir k-means",
             subtitle = "K - Means")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## k-Medians  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/k-medians.png"))
```

**k-Medians** es un algoritmo de **agrupamiento (clustering) no supervisado** que es una variante de **k-Means**. Al igual que k-Means, su objetivo es particionar un conjunto de $n$ observaciones en $k$ grupos o "clusters". La principal diferencia radica en cómo se calcula el "centro" de cada cluster y la métrica de distancia utilizada en su función de costo.

Mientras que k-Means utiliza la **media (mean)** de los puntos de un cluster como su centroide y minimiza la suma de los **cuadrados de las distancias euclidianas** (norma L2), k-Medians utiliza la **mediana (median)** de los puntos de un cluster como su "centro" y minimiza la **suma de las distancias absolutas** (norma L1 o distancia de Manhattan).

El algoritmo k-Medians opera de manera muy similar a k-Means:

1.  **Inicialización:**
    * Se elige un número predefinido de clusters, $k$.
    * Se inicializan $k$ **medianas** (puntos centrales de los clusters), a menudo de forma aleatoria.

2.  **Paso de Asignación:**
    * Para cada punto de datos en el conjunto, se calcula su **distancia de Manhattan (L1)** a cada una de las $k$ medianas.
    * Cada punto de datos se **asigna al cluster cuya mediana es la más cercana**.

3.  **Paso de Actualización:**
    * Para cada uno de los $k$ clusters, se **recalcula la posición de la mediana** como la mediana multivariada (componente por componente) de todos los puntos de datos que han sido asignados a ese cluster.

4.  **Iteración:**
    * Los pasos de Asignación y Actualización se repiten iterativamente.
    * El algoritmo converge cuando las asignaciones de los puntos a los clusters ya no cambian, o cuando las posiciones de las medianas no cambian significativamente.

**Ventajas clave de k-Medians sobre k-Means:**

* **Robustez a Outliers:** Al usar la mediana en lugar de la media, k-Medians es significativamente **más robusto a los valores atípicos (outliers)**. Los outliers influyen fuertemente en la media (tirando de ella), pero tienen un impacto mucho menor en la mediana.
* **Métrica de Distancia:** La distancia L1 es a veces más apropiada que la L2 cuando las diferencias entre las características son más importantes que sus valores al cuadrado, o cuando los datos no son necesariamente continuos o gaussianos.

**Limitaciones:**
* Requiere que el número de clusters $k$ sea especificado de antemano.
* La mediana multivariada puede ser más compleja de calcular que la media.
* Puede ser más lento que k-Means en algunos escenarios.

**Aprendizaje Global vs. Local:**

Al igual que k-Means, **k-Medians es un modelo de aprendizaje global**.

* **Aspecto Global:** k-Medians busca una **partición global de todo el conjunto de datos** en $k$ clusters. El objetivo de la optimización (minimizar la suma de las distancias L1 a las medianas) se calcula sobre **todos los puntos de datos** y todos los clusters simultáneamente. Las medianas, una vez convergidas, representan los "centros" robustos de los clusters en el espacio de características, y estos se utilizan para asignar cualquier nuevo punto a su cluster correspondiente. La solución final es una asignación de cada punto a un cluster que se aplica a nivel global.

* **Asignaciones Locales dentro de una Optimización Global:** Si bien en cada iteración los puntos se asignan a su mediana "local" más cercana, esta asignación es parte de un proceso iterativo que busca optimizar un criterio global (la suma total de distancias L1). Las medianas mismas son influenciadas por todos los puntos asignados a su cluster, y la reubicación de las medianas afecta las asignaciones de todos los puntos en la siguiente iteración. El resultado son **fronteras de decisión lineales** (debido al uso de la distancia L1, similar a las fronteras de Voronoi), que son una característica de un modelo global que divide el espacio. Aunque es más robusto a outliers, k-Medians todavía tiende a encontrar clusters que son más o menos "esféricos" o convexos en la métrica L1, y puede tener dificultades con clusters de formas muy arbitrarias o no lineales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "❌ No supervisado (clustering por partición)",
  "❌ No aplica (no hay variable respuesta)",
  "✅ Variables numéricas (recomendado estandarizar)",
  "✅ Agrupa observaciones según distancia a la mediana (Manhattan)",
  "❌ No aplica",
  "❌ No aplica",
  "❌ No aplica",
  "✅ Más robusto a valores atípicos que k-Means",
  "⚠️ No afecta directamente (no hay predictores ni multicolinealidad)",
  "✅ Interpretación clara de clusters y medianas",
  "⚠️ Algo más lento que k-Means por cálculo de medianas",
  "❌ No se usa validación cruzada, pero sí índices de cluster (silhouette, etc.)",
  "❌ No funciona bien con clusters no esféricos o tamaños muy dispares"
)

detalles <- c(
  "Método no supervisado que particiona datos en k clusters minimizando suma de distancias absolutas dentro de clusters.",
  "No busca predecir, sino encontrar grupos o clusters.",
  "Requiere variables numéricas; se recomienda estandarización para evitar sesgo por escala.",
  "Cada observación se asigna al cluster con la mediana más cercana usando distancia Manhattan.",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia.",
  "No requiere homoscedasticidad.",
  "Más robusto frente a outliers porque usa medianas en lugar de medias.",
  "Como técnica de agrupamiento, no hay multicolinealidad entre variables.",
  "Medianas y clusters son fáciles de interpretar y visualizar.",
  "Computacionalmente puede ser un poco más lento que k-Means.",
  "Se usan índices externos e internos para evaluar calidad del clustering, no validación cruzada.",
  "No funciona bien si los clusters tienen formas complejas, tamaños muy distintos o solapamientos."
)

tabla_kmedians <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_kmedians %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir k-medians",
             subtitle = "K - Medians")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Mean-Shift {-}   


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/Mean-Shift.png"))
```

## Ordering Points To Identify the Clustering Structure (OPTICS) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/OPTICS.png"))
```

## Spectral Clustering {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/Spectral Clustering.png"))
```


<!--chapter:end:09-clustering.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 📐 10. Sistemas Basados en Reglas {-}  

**Ejemplos:** RuleFit, Árboles de Decisión con reglas, lógica difusa.  
**Uso:** Estos sistemas son ideales cuando la **interpretabilidad es absolutamente crucial**, como en decisiones legales o médicas donde entender el "porqué" es tan importante como el "qué". También son perfectos para **incorporar conocimiento experto** humano directamente en el modelo.  
**Ventajas:** Son notablemente **fáciles de entender y auditar**, lo que los hace transparentes y confiables en entornos regulados.  
**Limitaciones:** Su principal desventaja es que no suelen ser tan **precisos** como otros métodos más complejos cuando se enfrentan a datos muy intrincados o patrones no lineales.  

---


## Associative Classification (e.g., CBA, CMAR, FP-Growth based methods) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Associative Classification.png"))
```

## CN2 Algorithm {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/CN2.png"))
```

## Decision List/Decision Tree to Rules {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Decision Tree to Rules.png"))
```

## Decision Rules  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Decision Rules.png"))
```

Las **Reglas de Decisión** no son un algoritmo de Machine Learning en sí mismas, sino una **forma de representar un modelo predictivo** que es altamente **interpretable y fácil de entender**. Son una de las formas más intuitivas de expresar el conocimiento extraído de los datos. Una regla de decisión típicamente toma la forma de una declaración **"SI-ENTONCES" (IF-THEN)**, donde la parte "SI" (antecedente) describe las condiciones que deben cumplirse y la parte "ENTONCES" (consecuente) especifica la predicción o la acción a tomar.

**Estructura de una Regla de Decisión:**

Una regla de decisión básica tiene la siguiente forma:

**SI** *condición 1* **Y** *condición 2* **Y** ... **Y** *condición N* **ENTONCES** *predicción / clase / valor*

**Ejemplos:**

* **Para Clasificación:**
    * **SI** `Edad` > 30 **Y** `Ingreso` < 50,000 **ENTONCES** `Clase: Riesgo Bajo`
    * **SI** `Clima` = Soleado **Y** `Humedad` > 70% **ENTONCES** `Acción: No Jugar Tenis`

* **Para Regresión:**
    * **SI** `Tamaño_Casa` > 150 m² **Y** `Num_Habitaciones` = 3 **ENTONCES** `Precio_Estimado: $300,000`

**Características Clave:**   

* **Interpretabilidad:** Son inherentemente fáciles de entender por los humanos, incluso por aquellos sin conocimientos técnicos profundos.
* **Modularidad:** Un modelo completo puede estar compuesto por un conjunto de reglas. Cada regla es independiente y fácil de examinar.
* **No Linealidad:** Aunque cada regla es una declaración lógica simple, un conjunto de reglas puede modelar relaciones no lineales complejas en los datos, ya que cada regla cubre una región diferente del espacio de características.
* **Selección de Características Implícita:** Al construir reglas, solo las características relevantes para la condición se incluyen, lo que realiza una selección implícita de características.
* **Robustez:** A menudo son bastante robustas a los outliers y a los datos ruidosos si las reglas se derivan y podan correctamente.

**Algoritmos que Generan Reglas de Decisión:** 

Si bien las reglas de decisión son la forma de representación, varios algoritmos de Machine Learning se especializan en aprender estas reglas a partir de los datos:

* **Árboles de Decisión:** Cada ruta desde la raíz hasta una hoja en un árbol de decisión se puede traducir directamente en una regla de decisión.
* **Algoritmos Basados en Reglas:**
    * **OneR:** Genera una única regla basada en el atributo más predictivo.
    * **RIPPER:** Produce conjuntos de reglas optimizados para la clasificación, con énfasis en la poda y la simplicidad.
    * **RuleFit:** Combina reglas extraídas de ensamblajes de árboles con características originales en un modelo lineal regularizado.
* **Sistemas de Lógica Difusa:** Utilizan reglas difusas para manejar la incertidumbre y la imprecisión.


**Aprendizaje Global vs. Local:**

Los modelos basados en **Reglas de Decisión** combinan de manera muy efectiva **aspectos de aprendizaje global y local**.

* **Aspecto Local (Cada Regla Individual):**
    * Cada regla de decisión es intrínsecamente **local**. Define una **región específica** (un subespacio, a menudo un hiperrectángulo) en el espacio de características. Dentro de esta región, la regla hace una predicción particular. Por ejemplo, la regla "SI `Edad` > 30 Y `Ingreso` < 50,000" solo se aplica a un subconjunto específico de instancias.
    * La naturaleza "local" de cada regla permite al modelo adaptarse a **relaciones no lineales y complejas**. Diferentes reglas pueden activarse en distintas partes del espacio de datos, capturando patrones que varían significativamente de una región a otra, similar a una **"regresión (o clasificación) ponderada localmente"** donde cada regla representa un modelo simple para su región.

* **Aspecto Global (El Conjunto de Reglas):**
    * Aunque las reglas individuales son locales, el **conjunto completo de reglas** que conforma el modelo se aplica para cubrir **todo el espacio de características relevante**. Este conjunto de reglas forma un **modelo predictivo global** que puede clasificar o predecir un valor para cualquier nueva instancia.
    * Los algoritmos que generan estos conjuntos de reglas (como RIPPER) a menudo optimizan la colección de reglas para un rendimiento global, buscando un equilibrio entre la precisión y la complejidad del modelo en su conjunto.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación o regresión)",
  "✅ Numérica o categórica",
  "✅ Numéricas y categóricas",
  "✅ Basado en reglas (if-then) que segmentan el espacio predictor",
  "❌ No aplica directamente (no es un modelo paramétrico)",
  "⚠️ Asume independencia de observaciones",
  "❌ No aplica",
  "⚠️ Moderadamente sensible, outliers pueden crear reglas poco útiles",
  "⚠️ No afecta directamente, pero reglas redundantes pueden complicar modelo",
  "✅ Muy interpretable, reglas claras y simples",
  "✅ Rápido para datasets pequeños y medianos",
  "✅ Compatible con validación cruzada",
  "❌ Puede sobreajustar si hay mucho ruido o datos muy complejos"
)

detalles <- c(
  "Modelos que usan reglas lógicas para realizar predicciones, fácilmente entendibles.",
  "Permite predecir tanto clases (clasificación) como valores numéricos (regresión).",
  "Acepta tanto variables categóricas como numéricas como predictores.",
  "Segmenta el espacio en regiones mediante reglas que dividen las variables predictoras.",
  "No genera residuos ni supone distribución de error como modelos paramétricos.",
  "Las reglas asumen que las observaciones son independientes.",
  "No aplica homoscedasticidad porque no es un modelo estadístico clásico.",
  "Outliers pueden influir en la generación de reglas, creando reglas poco generalizables.",
  "La multicolinealidad no afecta directamente, pero puede generar reglas redundantes.",
  "La fortaleza es la interpretabilidad clara y sencilla de las reglas obtenidas.",
  "Generalmente eficiente, pero depende del número de reglas y complejidad del dataset.",
  "Se recomienda validar con métodos como k-fold para evitar sobreajuste.",
  "No es ideal para datasets con mucho ruido o cuando la relación es muy compleja o sutil."
)

tabla_rules <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rules %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir decision rules",
             subtitle = "Decision Rules")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Lógica Difusa (Fuzzy Logic) {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Fuzzy Logic.png"))
```

La **Lógica Difusa (Fuzzy Logic)** no es un algoritmo de Machine Learning en sí mismo, sino un **paradigma de computación basado en la noción de "grado de verdad"** en lugar de los valores binarios "verdadero o falso" (1 o 0) de la lógica booleana clásica. Permite modelar el razonamiento humano, que a menudo involucra información imprecisa, ambigua o vaga. Es un marco para representar y manipular el conocimiento que es inherentemente incierto o subjetivo.

La idea central de la lógica difusa es que un elemento puede **pertenecer a un conjunto en un cierto grado** (entre 0 y 1), en lugar de pertenecer completamente o no pertenecer en absoluto. Por ejemplo, una persona puede ser "alta" en un grado de 0.8 y "mediana" en un grado de 0.2, en lugar de ser estrictamente alta o estrictamente mediana.

Los componentes clave de un sistema de lógica difusa suelen incluir:

1.  **Conjuntos Difusos (Fuzzy Sets):** Definen el grado de pertenencia de un elemento a una categoría. Por ejemplo, un conjunto difuso para "temperatura alta" podría tener una función de pertenencia que asigne un valor de 0 a 10 grados, 0.5 a 20 grados, y 1 a 30 grados.
2.  **Variables Lingüísticas:** Son variables cuyos valores son palabras o oraciones del lenguaje natural (ej., "temperatura", cuyos valores pueden ser "frío", "tibio", "caliente").
3.  **Funciones de Pertenencia (Membership Functions):** Gráficos que definen matemáticamente el grado de pertenencia de un elemento a un conjunto difuso.
4.  **Reglas Difusas (Fuzzy Rules):** Reglas de tipo "SI-ENTONCES" que utilizan variables lingüísticas y conjuntos difusos. Por ejemplo: "SI la temperatura es *caliente* Y la humedad es *alta* ENTONCES la velocidad del ventilador es *rápida*".
5.  **Fuzificación:** Proceso de convertir valores de entrada nítidos (crisp inputs) en grados de pertenencia a conjuntos difusos.
6.  **Motor de Inferencia:** Aplica las reglas difusas para producir una salida difusa.
7.  **Defuzificación:** Proceso de convertir la salida difusa en un valor de salida nítido (crisp output) que pueda ser utilizado en el mundo real.

La lógica difusa es ampliamente utilizada en sistemas de control (ej., lavadoras, sistemas de frenos ABS, cámaras de video), sistemas expertos y en el procesamiento de información imprecisa.

**Aprendizaje Global vs. Local:**

La Lógica Difusa, como paradigma, tiene la capacidad de integrar aspectos de **modelado global y local**, dependiendo de cómo se implemente y se "entrene" (o sintonice).

* **Aspecto Local (Granularidad de las Reglas y Conjuntos Difusos):**
    * Las **reglas difusas** operan sobre condiciones locales (ej., "SI la temperatura es *caliente*"). Cada regla cubre una porción específica del espacio de entrada/salida. Los **conjuntos difusos** y sus funciones de pertenencia particionan el espacio de características en regiones "borrosas" o traslapadas, lo que permite que el modelo se adapte a las características de los datos en vecindarios específicos. Es decir, la respuesta del sistema se construye a partir de la activación ponderada de varias reglas locales, cada una representando un comportamiento en una región del espacio de entrada. Esto es muy similar a una **"regresión ponderada localmente"**, donde la contribución de cada regla (o modelo implícito) se pondera por el grado en que la entrada actual pertenece a la región de esa regla. Esta granularidad y superposición le permiten manejar **relaciones no lineales y complejas** al aproximarlas con una combinación de estas contribuciones locales.

* **Aspecto Global (Coherencia del Sistema y Cobertura):**
    * Aunque las reglas son locales, un sistema de lógica difusa bien diseñado cubre todo el espacio de entrada relevante y proporciona una **respuesta global coherente**. El conjunto de todas las reglas y funciones de pertenencia, junto con el motor de inferencia, forma un **sistema global** que puede mapear cualquier entrada a una salida. La defuzificación final produce un valor nítido que es el resultado de la combinación de todas las activaciones de las reglas.

* **Aprendizaje y Sintonización:** Cuando se combinan con técnicas de Machine Learning (como redes neuronales o algoritmos genéticos), los sistemas difusos pueden "aprender" o "sintonizar" sus funciones de pertenencia y reglas. Este proceso de aprendizaje puede optimizar el rendimiento del sistema a nivel global (minimizando un error general), pero los ajustes siguen afectando las propiedades locales de las reglas y conjuntos difusos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado o no supervisado (depende del sistema)",
  "✅ Variables de salida pueden ser continuas o categóricas (fuzzy sets)",
  "✅ Variables numéricas o categóricas, modeladas como grados de pertenencia",
  "✅ Modela relaciones imprecisas, no binarias, usando lógica difusa",
  "❌ No aplica (no modelo estadístico tradicional)",
  "⚠️ Puede asumir independencia pero depende del diseño del sistema",
  "❌ No aplica",
  "⚠️ Moderadamente sensible, depende de la función de membresía",
  "⚠️ No afecta directamente pero variables correlacionadas pueden complicar reglas",
  "✅ Interpretación basada en reglas lingüísticas y grados de verdad",
  "⚠️ Puede ser lento con muchos conjuntos difusos y reglas complejas",
  "⚠️ Validación cruzada posible, pero no estándar en lógica difusa",
  "❌ No es adecuado si los datos son muy exactos y no presentan incertidumbre"
)

detalles <- c(
  "Sistemas que usan lógica difusa para manejar incertidumbre y aproximación en los datos.",
  "Las variables respuesta pueden ser valores continuos o categorías definidas mediante conjuntos difusos.",
  "Los predictores se transforman en grados de pertenencia a conjuntos difusos para evaluar reglas.",
  "Permite modelar relaciones vagamente definidas, imprecisas o con fronteras difusas.",
  "No se basa en supuestos estadísticos clásicos, por eso no aplica normalidad.",
  "La independencia depende de cómo se diseñen las reglas y sistemas difusos.",
  "No es un modelo paramétrico clásico, por eso homoscedasticidad no aplica.",
  "Outliers pueden afectar las funciones de pertenencia y la lógica aplicada.",
  "Variables altamente correlacionadas pueden hacer que las reglas sean redundantes o complejas.",
  "Los resultados se interpretan mediante reglas tipo 'Si... entonces...' con grados de verdad.",
  "La complejidad crece con número de variables y reglas, afectando velocidad.",
  "Validar es menos estándar, se usan métodos específicos según la aplicación.",
  "No es útil para datos precisos donde no existe incertidumbre o imprecisión."
)

tabla_fuzzy <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_fuzzy %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir decision fuzzy rules",
             subtitle = "Fuzzy Rules")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Minsky's Perceptron {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Minsky's Perceptron.png"))
```

## One Rule (OneR)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/OneR.png"))
```


**One Rule (OneR)** es un algoritmo de **clasificación supervisada** notable por su **simplicidad y alta interpretabilidad**. Fue propuesto por Robert Holte en 1993 y, a pesar de su sencillez, a menudo logra una precisión sorprendentemente buena en comparación con algoritmos mucho más complejos, sirviendo como una excelente línea base (benchmark) para el rendimiento del modelo.

La idea central de OneR es construir un clasificador que se base en **una única regla** para tomar decisiones. Esta regla se deriva de **un solo atributo (característica)** del conjunto de datos que es el más predictivo de la clase de salida.

El funcionamiento del algoritmo OneR es el siguiente:

1.  **Iterar a Través de Cada Atributo:** Para cada atributo en el conjunto de datos de entrenamiento (se asume que los atributos son categóricos; si son continuos, primero deben discretizarse):
    * **Crear una Regla para Cada Valor:** Para cada valor único que puede tomar ese atributo, se construye una regla.
    * **Encontrar la Clase Más Frecuente:** Para cada una de estas reglas, se cuenta cuántas veces aparece cada clase de destino cuando el atributo toma ese valor. La clase que ocurre con mayor frecuencia se convierte en la predicción para esa regla.
    * **Calcular el Error de la Regla:** Se calcula el número de errores que comete esta regla (es decir, el número de instancias para las cuales la clase predicha no coincide con la clase real).
2.  **Seleccionar el Mejor Atributo:** Una vez que se han generado reglas y calculado los errores para *todos* los atributos, OneR selecciona el atributo (y su conjunto de reglas asociadas) que tiene el **menor error total**. Si hay un empate entre varios atributos, se puede elegir el primero o usar un criterio secundario (como el test de chi-cuadrado).
3.  **El Modelo Final:** El conjunto de reglas derivado de este atributo seleccionado se convierte en el modelo final de clasificación.

Por ejemplo, si tenemos un atributo "Clima" con valores "Soleado", "Nublado", "Lluvioso" y una clase "Jugar al Golf" (Sí/No):
* Si Clima = Soleado: La mayoría juega golf (Sí). Error: 2 (de 5)
* Si Clima = Nublado: La mayoría juega golf (Sí). Error: 0 (de 4)
* Si Clima = Lluvioso: La mayoría NO juega golf (No). Error: 1 (de 5)
* Error total para "Clima" = 2 + 0 + 1 = 3.

Este proceso se repetiría para otros atributos como "Temperatura", "Humedad", etc., y el atributo con el menor error total sería el elegido.

**Aprendizaje Global vs. Local:**

One Rule (OneR) es un modelo de **aprendizaje fundamentalmente global**, aunque la regla que aprende implica una partición del espacio de características.

* **Aspecto Global:** OneR evalúa **todos los atributos en su totalidad** y selecciona el **único atributo que es globalmente el más predictivo** para la tarea de clasificación sobre todo el conjunto de datos. La regla elegida y sus condiciones se aplican uniformemente a cualquier nueva instancia en el espacio de características. No se construyen modelos separados o locales para diferentes regiones del espacio de características. El algoritmo busca la mejor regla única que resuma el patrón más fuerte en todos los datos.

* **Partición del Espacio (Reglas):** Aunque el modelo es global, la "regla" que genera sí que particiona el espacio de características. Por ejemplo, si el atributo seleccionado es "Color" y tiene valores "Rojo", "Azul", "Verde", el modelo crea una regla para cada uno de estos valores. Esto crea "regiones" en el espacio de datos (instancias donde Color=Rojo, donde Color=Azul, etc.). Sin embargo, la predicción dentro de cada una de estas regiones es simplemente la clase mayoritaria observada en esa región, y el modelo en su conjunto es una única estructura de decisión global basada en ese único atributo. No es un ajuste dinámico o ponderado localmente de parámetros como en otros modelos de aprendizaje local.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Variable respuesta categórica (clases)",
  "✅ Variables predictoras numéricas o categóricas",
  "✅ Relación simple basada en una sola regla (una variable predictora)",
  "❌ No aplica (no es modelo estadístico paramétrico)",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Puede ser sensible a outliers si afectan la regla",
  "⚠️ No afecta directamente (usa solo una variable para la regla)",
  "✅ Muy interpretable: una regla sencilla basada en un solo predictor",
  "✅ Muy rápido y eficiente, especialmente en datasets pequeños o medianos",
  "✅ Se puede usar validación cruzada para evaluar rendimiento",
  "❌ No funciona bien si las relaciones son complejas y requieren múltiples variables"
)

detalles <- c(
  "Modelo supervisado para clasificación basado en la regla más simple y efectiva de un solo predictor.",
  "Predice la clase de salida basándose en el valor de una única variable predictora.",
  "Acepta variables categóricas o numéricas (estas se discretizan para generar reglas).",
  "Genera una regla simple: 'Si predictor X tiene valor Y, entonces clase Z'.",
  "No es un modelo paramétrico ni estadístico tradicional, no evalúa residuos.",
  "No considera errores ni supuestos de independencia.",
  "No aplica el supuesto de homoscedasticidad.",
  "Outliers pueden influir si cambian la regla seleccionada.",
  "Como usa solo un predictor, la multicolinealidad no es un problema.",
  "Fácil de entender y explicar, ideal para explicar decisiones simples.",
  "Muy rápido de entrenar y evaluar, útil para benchmarks o como baseline.",
  "Se recomienda evaluar mediante validación cruzada para evitar sobreajuste.",
  "No es útil para problemas que requieren modelar interacciones complejas entre variables."
)

tabla_oner <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_oner %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir decision OneR",
             subtitle = "One Rule (OneR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Repeated Incremental Pruning to Produce Error Reduction (RIPPER)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/RIPPER.png"))
```

**RIPPER (Repeated Incremental Pruning to Produce Error Reduction)** es un algoritmo de **clasificación supervisada** muy conocido, desarrollado por William W. Cohen. Es una extensión del algoritmo de reglas `IREP` y es especialmente valorado por su capacidad para generar **conjuntos de reglas de clasificación precisos y de alta calidad** que son a menudo más simples e interpretables que los modelos de árbol de decisión complejos, al tiempo que es muy eficiente en términos computacionales, incluso con grandes conjuntos de datos.

RIPPER construye un conjunto de reglas `IF-THEN` para cada clase de forma secuencial. Opera con un enfoque de **"divide y vencerás"**, pero con un fuerte énfasis en la **poda (pruning)** para evitar el sobreajuste.

El proceso general de RIPPER para construir reglas para una clase específica es el siguiente:

1.  **Generación de Reglas (Growing):**
    * Comienza con una regla vacía.
    * Añade términos (condiciones) a la regla que maximicen alguna métrica de calidad (por ejemplo, el ratio de ganancia de información) hasta que la regla cubre un cierto número de ejemplos positivos (ejemplos de la clase actual) y pocos ejemplos negativos.
2.  **Poda de Reglas (Pruning):**
    * Una vez que una regla ha sido generada, se somete a un proceso de poda incremental. Se eliminan términos de la regla que no mejoran significativamente el error de la regla en un conjunto de validación separado (el "conjunto de poda"). Esto ayuda a generalizar la regla y evitar el sobreajuste.
3.  **Adición de Reglas:**
    * Después de podar una regla, se añade al conjunto de reglas para la clase actual.
    * Los ejemplos cubiertos por esta nueva regla se eliminan del conjunto de entrenamiento.
    * Se repiten los pasos 1-3 para construir nuevas reglas para la misma clase hasta que no queden suficientes ejemplos de la clase o no se puedan generar más reglas que cumplan ciertos criterios.
4.  **Optimización Global y Re-poda:**
    * Una vez que se ha generado un conjunto inicial de reglas para una clase, RIPPER realiza varias pasadas de optimización.
    * En cada pasada, se intenta reemplazar o modificar reglas para reducir aún más el error total del conjunto de reglas. Esto incluye estrategias como la sustitución de reglas (reemplazar una regla existente por una mejor), la reversión de reglas (eliminar una regla), y la combinación de reglas.
    * Se utiliza una métrica como la **descripción mínima de la longitud (MDL - Minimum Description Length)** para penalizar la complejidad del modelo.

El proceso se repite para todas las clases, y las reglas se organizan en un orden de prioridad.


**Aprendizaje Global vs. Local:**

RIPPER es un algoritmo que combina de manera muy efectiva aspectos de **aprendizaje global y local**, pero con un fuerte énfasis en la generación de **reglas locales** que se combinan en un modelo global.

* **Aspecto Local (Generación de Reglas Individuales):** Cada regla que RIPPER aprende es esencialmente un **modelo local** que cubre una región específica del espacio de características. Las condiciones de la regla (`IF A AND B THEN Class X`) definen un hiperrectángulo (o una región más compleja) en el espacio de características. La regla se aprende para predecir correctamente los puntos dentro de esa región. La poda de reglas, en particular, se enfoca en optimizar el rendimiento de la regla en su "vecindario" de datos cubiertos, evitando el sobreajuste a puntos de entrenamiento individuales. Las reglas se ajustan a patrones y relaciones **locales** dentro de los datos.

* **Aspecto Global (Conjunto de Reglas y Priorización):** Aunque las reglas individuales son locales, el **conjunto final de reglas** para todas las clases, y su orden de prioridad, forman un **modelo de clasificación global** que cubre todo el espacio de características. Cuando una nueva instancia necesita ser clasificada, se evalúa contra todas las reglas en orden hasta que una se activa, y esa regla dicta la predicción. La optimización global del conjunto de reglas, mediante la repoda y las fases de reemplazo de reglas, asegura que el modelo final sea coherente y preciso a nivel de todo el conjunto de datos.    


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación)",
  "✅ Variable respuesta categórica",
  "✅ Variables predictoras numéricas o categóricas",
  "✅ Relación no lineal basada en reglas de decisión",
  "❌ No aplica (no es modelo paramétrico)",
  "❌ No aplica",
  "❌ No aplica",
  "⚠️ Moderadamente sensible a outliers, depende de la calidad de datos",
  "⚠️ No afecta directamente, pero multicolinealidad puede confundir reglas",
  "⚠️ Interpretación mediante reglas, más complejas que OneR pero aún legibles",
  "⚠️ Moderada, puede ser lento en datasets muy grandes",
  "✅ Se puede validar con métodos de validación cruzada",
  "❌ No funciona bien si las reglas no separan claramente las clases o en datos muy ruidosos"
)

detalles <- c(
  "Algoritmo supervisado para clasificación que genera reglas de decisión con poda incremental para reducir error.",
  "Predice clases usando conjuntos de reglas de decisión extraídas y podadas iterativamente.",
  "Soporta variables numéricas y categóricas, con discretización interna si es necesario.",
  "Captura relaciones no lineales y complejas mediante reglas combinadas.",
  "No utiliza modelos estadísticos clásicos, por lo que no aplica análisis de residuos.",
  "No asume independencia entre observaciones.",
  "No requiere homoscedasticidad ni otros supuestos de regresión.",
  "Outliers pueden afectar la calidad y la complejidad de las reglas generadas.",
  "Multicolinealidad no afecta directamente, pero puede inducir reglas redundantes.",
  "Las reglas generadas son más interpretables que árboles complejos, pero menos simples que OneR.",
  "El proceso iterativo y poda pueden ser computacionalmente costosos en grandes datasets.",
  "Validación cruzada es recomendada para evaluar generalización y evitar sobreajuste.",
  "No es adecuado para datos con ruido elevado o donde no existen reglas claras para separar clases."
)

tabla_ripper <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_ripper %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir decision RIPPER",
             subtitle = "Repeated Incremental Pruning to Produce Error Reduction (RIPPER)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Rule Fit  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Rule Fit.png"))
```

**RuleFit** es un potente y, al mismo tiempo, **interpretable algoritmo de aprendizaje automático** desarrollado por Jerome H. Friedman y Bogdan Popescu. Está diseñado para combinar la **precisión de los métodos de ensamblaje (como el *gradient boosting* o los *random forests*)** con la **interpretabilidad de los modelos lineales y las reglas de decisión**. RuleFit es particularmente útil cuando necesita un modelo que funcione bien *y* que le permita comprender el "porqué" detrás de sus predicciones.

La idea central de RuleFit es aprender un **modelo lineal disperso** que utiliza tanto las **características de entrada originales** como un conjunto de **"características de regla" recién generadas** como predictores. Estas características de regla se derivan de un ensamblaje de árboles de decisión.

Así es como funciona RuleFit:

1.  **Generación del Ensamblaje de Árboles:**
    * Primero, RuleFit entrena un **ensamblaje de árboles de decisión poco profundos** en el conjunto de datos. Esto se puede hacer utilizando algoritmos como *Gradient Boosting Machines* (GBM) o *Random Forests*. Los árboles suelen mantenerse poco profundos (por ejemplo, una profundidad máxima de 3-5) para producir reglas más simples e interpretables.
    * Estos árboles se entrenan para predecir la variable objetivo, lo que significa que sus divisiones son significativas para la tarea.

2.  **Extracción de Reglas:**
    * Cada **ruta desde la raíz hasta un nodo hoja** en cualquiera de los árboles de decisión generados se extrae y se convierte en una **regla de decisión binaria**.
    * Por ejemplo, si una ruta en un árbol es "si (Característica1 > 10) Y (Característica2 < 5)", esto se convierte en una regla.
    * Cada regla se trata entonces como una **nueva característica binaria** para cada instancia de datos: toma un valor de 1 si la instancia satisface todas las condiciones de la regla, y 0 en caso contrario.

3.  **Ajuste del Modelo Lineal con Regularización:**
    * Las características originales del conjunto de datos se combinan con estas características de regla binarias recién creadas.
    * Luego, se ajusta un **modelo lineal disperso** (normalmente una **regresión Lasso**, que utiliza regularización L1) a este conjunto de características expandido.
    * La regularización Lasso realiza automáticamente la **selección de características**, estableciendo los coeficientes de las características originales y de las características de regla menos importantes en cero. Esto da como resultado un modelo final más simple e interpretable que solo incluye los términos más relevantes.

El modelo RuleFit final es una ecuación lineal:
$\text{predicción} = \beta_0 + \sum_{j=1}^{p} \beta_j X_j + \sum_{k=1}^{R} \alpha_k r_k(X)$

Donde:
* $\beta_0$ es el intercepto.
* $\beta_j X_j$ son los términos para las características lineales originales $X_j$.
* $\alpha_k r_k(X)$ son los términos para las características de regla binarias $r_k(X)$.

Los coeficientes ($\beta_j$ y $\alpha_k$) indican la importancia y la dirección del efecto de cada característica original y de cada regla en la predicción.

**Aprendizaje Global vs. Local:**

RuleFit es un excelente ejemplo de un algoritmo que combina perfectamente las características de **aprendizaje global y local**.

* **Aspecto Global (Modelo Lineal Final y Estructura General):**
    * La etapa final de RuleFit consiste en ajustar un **único modelo lineal global** (con regularización Lasso). Este modelo lineal es un predictor global que combina las características originales y todas las características de regla generadas. Los coeficientes en este modelo lineal global definen la relación general entre las características de entrada (originales y basadas en reglas) y la variable objetivo en todo el conjunto de datos.
    * Este modelo lineal proporciona una **comprensión global** de la importancia de las características y de cómo contribuyen colectivamente a la predicción.

* **Aspecto Local (Características de Regla y Efectos de Interacción):**
    * El poder de RuleFit para manejar relaciones no lineales e interacciones proviene de su **aspecto local**: las **reglas de decisión**. Cada regla, extraída de una ruta en un árbol de decisión, define una **subregión específica** o "vecindario" del espacio de características. Por ejemplo, una regla "SI la edad es > 30 Y el ingreso es < 50k" captura una condición local muy específica.
    * Al incluir estas características de regla binarias, el modelo lineal puede aprender efectivamente **diferentes relaciones lineales dentro de diferentes regiones locales** definidas por las reglas. Estas reglas capturan explícitamente los **efectos de interacción** entre características que un modelo lineal estándar pasaría por alto.
    * Esto es similar a tener un **modelo local (implícitamente, una constante o un modelo lineal simple dentro de la región de la regla)** que se activa cuando se cumplen sus condiciones, lo que permite que el modelo lineal global adapte sus predicciones basándose en estos patrones locales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación y regresión)",
  "✅ Numérica o categórica según el tipo de problema",
  "✅ Variables numéricas y categóricas",
  "✅ Modela relaciones no lineales a través de reglas y combinaciones lineales",
  "❌ No es modelo paramétrico clásico, no se evalúa normalidad",
  "❌ No aplica directamente",
  "❌ No aplica",
  "⚠️ Puede ser sensible a outliers, pero menos que modelos lineales puros",
  "⚠️ Puede verse afectado, se recomienda análisis previo",
  "⚠️ Interpretabilidad moderada: combina reglas fáciles con coeficientes lineales",
  "⚠️ Moderado, depende del número de reglas generadas",
  "✅ Se puede validar con validación cruzada",
  "❌ No funciona bien en datasets muy pequeños o con relaciones altamente complejas sin reglas claras"
)

detalles <- c(
  "Combina árboles de decisión para generar reglas y luego aplica regresión lineal para asignar pesos a estas reglas.",
  "Puede usarse para problemas de regresión o clasificación.",
  "Admite variables categóricas y numéricas sin grandes restricciones.",
  "Captura tanto efectos lineales como interacciones no lineales vía reglas extraídas.",
  "No asume distribución normal de errores.",
  "No requiere independencia estricta entre observaciones.",
  "No requiere homoscedasticidad.",
  "Puede mitigar el impacto de outliers mediante regularización.",
  "Multicolinealidad puede afectar coeficientes pero el modelo regulariza para evitarlo.",
  "Las reglas extraídas facilitan la interpretación frente a otros modelos complejos.",
  "La eficiencia depende del número de reglas y tamaño de datos.",
  "Validación cruzada ayuda a evitar sobreajuste y seleccionar hiperparámetros.",
  "Puede fallar si las reglas generadas no capturan bien la relación o en datos ruidosos."
)

tabla_rulefit <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rulefit %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir decision rule fit",
             subtitle = "Rule Fit")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Zero Rule (ZeroR)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Rule Fit.png"))
```

**Zero Rule (ZeroR)** es el algoritmo de **clasificación supervisada** más simple imaginable. No utiliza ninguna de las características predictoras en el conjunto de datos para hacer sus predicciones. En su lugar, simplemente **predice la clase más frecuente (mayoritaria)** que se observa en el conjunto de datos de entrenamiento.

**Cómo funciona ZeroR:**   

1.  **Conteo de Frecuencias:** El algoritmo cuenta las ocurrencias de cada clase en el conjunto de datos de entrenamiento.
2.  **Identificación de la Clase Mayoritaria:** Identifica la clase que tiene la mayor frecuencia (es decir, la clase que aparece más veces).
3.  **Predicción Universal:** Para cualquier nueva instancia, ZeroR simplemente predice esta clase mayoritaria.

**Ejemplo:** Si en un conjunto de datos para predecir si un cliente "comprará" o "no comprará" un producto, el 70% de los clientes en el conjunto de entrenamiento "no compraron" y el 30% "compraron", ZeroR predecirá "no comprará" para *todos* los nuevos clientes. Su precisión en el conjunto de entrenamiento sería del 70%.

**¿Por qué es importante ZeroR?**  

Aunque ZeroR no tiene ningún poder predictivo real en el sentido de aprender patrones complejos de los datos, es **fundamentalmente importante en Machine Learning como una línea base (benchmark)**.

* **Punto de Referencia:** Cualquier algoritmo de clasificación más sofisticado debe superar la precisión de ZeroR para ser considerado útil. Si un modelo complejo tiene una precisión inferior a la de ZeroR, significa que el modelo no está aprendiendo nada significativo de los datos, o incluso está aprendiendo patrones incorrectos.
* **Detección de Sesgos de Clase:** En conjuntos de datos desequilibrados (donde una clase es mucho más frecuente que otras), ZeroR puede lograr una precisión aparentemente alta. Esto resalta la importancia de usar métricas de evaluación más allá de la simple precisión en esos casos (como precisión, recall, F1-score, o AUC-ROC), ya que una alta precisión de ZeroR podría ser engañosa.
* **Simplicidad Extrema:** Sirve como el punto de partida más básico para entender cómo los algoritmos de clasificación intentan mejorar sobre una suposición trivial.

**Aprendizaje Global vs. Local:**

ZeroR es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** ZeroR calcula una única estadística (la clase mayoritaria) a partir de todo el conjunto de datos de entrenamiento y aplica esta predicción **uniformemente a todas las instancias**, sin importar sus características individuales o su ubicación en el espacio de datos. No hay ninguna adaptación local o consideración de subregiones del espacio de características. El modelo es una única regla global e inmutable.

* **Sin Modelado de Relaciones:** Al ignorar todas las características de entrada, ZeroR no modela ninguna relación, lineal o no lineal, entre los predictores y la variable objetivo. Su conocimiento se limita a la distribución marginal de la clase de salida.

En resumen, ZeroR es el clasificador más simple y sirve como un punto de referencia crucial para evaluar el rendimiento de modelos de Machine Learning más avanzados. Su naturaleza es inherentemente global, ya que aplica una única decisión derivada del patrón más frecuente en todo el conjunto de datos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Supervisado (clasificación y regresión)",
  "✅ Categórica (modo) o numérica (media)",
  "❌ No utiliza predictores",
  "❌ No considera relaciones, predice constante",
  "❌ No aplica (no hay residuos de regresión)",
  "❌ No aplica (no hay modelo estructurado)",
  "❌ No aplica",
  "❌ No aplica (no hay influencia de valores atípicos)",
  "❌ No aplica (no hay predictores)",
  "✅ Alta, ya que solo predice un valor fijo",
  "✅ Extremadamente rápido",
  "⚠️ Se puede usar como baseline en validación",
  "❌ No debe usarse como modelo final salvo como referencia base"
)

detalles <- c(
  "Modelo supervisado trivial que predice el valor más frecuente (clasificación) o promedio (regresión) de la variable respuesta.",
  "Funciona para variable respuesta categórica (modo) o continua (media).",
  "No utiliza ninguna variable predictora; solo la respuesta.",
  "No aprende relaciones, útil solo como línea base de comparación.",
  "No genera residuos, pues predice una constante.",
  "No hay estructura de error o modelo que se evalúe.",
  "No hay dispersión de errores porque no hay ajuste.",
  "Outliers no afectan porque el modelo predice una constante global.",
  "No existe relación entre predictores, así que no aplica multicolinealidad.",
  "Fácil de interpretar: siempre predice lo mismo.",
  "Modelo extremadamente simple y rápido de calcular.",
  "Sirve como línea base para comparar modelos más complejos.",
  "Inútil para hacer predicciones significativas; solo sirve como referencia comparativa."
)

tabla_zeror <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_zeror %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir decision ZeroR",
             subtitle = "Zero Rule (ZeroR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```








<!--chapter:end:10-rule_based_systems.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# 🤖 4. Deep Learning {-}  

**Ejemplos:** CNN, RNN, Transformers.  
**Uso:** Ideal para **imágenes**, **texto** y **series temporales**, especialmente con **grandes datos no estructurados**.  
**Ventajas:** Poderoso para datos complejos.   
**Limitaciones:** Exige **mucha data** y **computación**; **poca interpretabilidad**.   

---



## Deep Boltzman Machine (DBM) {-}  


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/DBM.png"))
```

El modelo de **Deep Boltzman Machine (DBM)** es un tipo de **red neuronal profunda generativa** que pertenece a la familia de los **modelos gráficos probabilísticos**. Se construye apilando múltiples **Máquinas de Boltzmann Restringidas (RBMs)**, lo que le permite aprender representaciones jerárquicas y abstractas de los datos de entrada. Su principal objetivo es modelar la **distribución de probabilidad conjunta** entre un conjunto de variables observables y múltiples capas de variables latentes (ocultas).

Las DBMs son modelos **no dirigidos** (las conexiones entre las neuronas son simétricas y no tienen una dirección específica) y están compuestas por capas de unidades visibles (los datos de entrada) y varias capas de unidades ocultas. A diferencia de las RBMs simples que tienen una sola capa oculta, las DBMs tienen **múltiples capas ocultas**, lo que les permite capturar dependencias más complejas y características de alto nivel en los datos. El proceso de aprendizaje en una DBM busca ajustar los pesos de las conexiones de manera que la red asigne una alta probabilidad a los datos de entrenamiento y una baja probabilidad a los datos que no son de entrenamiento.

Las características clave de las DBMs incluyen:

1.  **Representación Jerárquica:** Cada capa oculta aprende representaciones progresivamente más abstractas de los datos. Las primeras capas pueden capturar características de bajo nivel (ej., bordes en imágenes), mientras que las capas superiores combinan estas para formar representaciones de alto nivel (ej., partes de objetos o conceptos).
2.  **Aprendizaje No Supervisado:** Las DBMs se entrenan típicamente de forma **no supervisada**, lo que significa que no requieren etiquetas para el entrenamiento. Esto las hace valiosas para el pre-entrenamiento de modelos profundos en conjuntos de datos grandes y sin etiquetar, donde pueden aprender características útiles que luego pueden ser utilizadas en tareas de aprendizaje supervisado (como la clasificación).
3.  **Inferencia y Generación:** Una vez entrenadas, las DBMs pueden ser utilizadas tanto para **inferencia** (estimar las representaciones ocultas dadas las entradas visibles) como para **generación** (muestrear nuevas instancias de datos a partir de la distribución aprendida del modelo).

Debido a su complejidad computacional en el entrenamiento exacto, las DBMs a menudo se entrenan utilizando un enfoque de **aprendizaje codicioso por capas** (entrenando RBMs individuales y apilándolas) seguido de un ajuste fino de todo el modelo utilizando algoritmos como el **Contraste Divergente Aproximado (ACD)**.


**Aprendizaje Global vs. Local:**

El modelo de **Máquina de Boltzmann Profunda (DBM)** es un modelo de **aprendizaje global**.

* **Aspecto Global:** Las DBMs construyen un **modelo probabilístico unificado y global** de la distribución de los datos. Los pesos de conexión en todas las capas de la red se ajustan para representar las dependencias y correlaciones en todo el espacio de entrada. No se crean modelos específicos para subconjuntos locales de datos; en cambio, el modelo aprende una representación coherente y jerárquica que se aplica a todos los puntos de datos. La función de energía (o función de coste) de la DBM se define sobre el espacio completo de variables visibles y ocultas, y el entrenamiento busca minimizar esta energía globalmente para que los datos de entrenamiento tengan una energía baja.

* **Impacto de la Estructura Jerárquica:** Aunque la DBM aprende representaciones en diferentes niveles de abstracción (jerarquías), estas representaciones contribuyen a un entendimiento cohesivo y global de los datos. La interacción entre las capas y las unidades es parte de una estructura probabilística interconectada que busca modelar la distribución general de los datos. El proceso de inferencia y generación, aunque implica pasar información a través de las capas, se basa en los parámetros globales de la red para producir resultados consistentes y representativos de la distribución aprendida. Esto contrasta con modelos locales que podrían segmentar el espacio de entrada y construir modelos independientes para cada segmento.  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Generativo / Discriminativo (apilado, no supervisado/supervisado)",
  "✅ Numérica continua o categórica (binaria, multiclase)",
  "✅ Numéricas y/o Categóricas (requiere preprocesamiento)",
  "✅ No lineal y Compleja",
  "❌ No aplica directamente",
  "⚠️ Asume independencia condicional",
  "❌ No aplica directamente",
  "⚠️ Robusto hasta cierto punto, pero datos muy ruidosos afectan",
  "✅ Maneja bien multicolinealidad",
  "⚠️ Baja (tipo caja negra)",
  "⚠️ Lento en entrenamiento, rápido en inferencia",
  "✅ Útil para ajuste de hiperparámetros",
  "❌ Datos insuficientes; ❌ alta dimensionalidad sin suficiente regularización; ❌ problemas de escalabilidad en conjuntos muy grandes."
)

detalles <- c(
  "Modelo generativo y discriminativo de aprendizaje profundo. Se entrena capa por capa de forma no supervisada (como RBMs) y luego se ajusta con una capa supervisada.",
  "Puede usarse para regresión (respuesta continua) o clasificación (respuesta categórica).",
  "Acepta diversos tipos de variables predictoras, pero requieren normalización/estandarización y codificación (ej., one-hot encoding para categóricas).",
  "Capaz de modelar relaciones altamente no lineales y complejas entre las variables.",
  "Los DBMs no asumen ni requieren la normalidad de los residuos, ya que no son modelos estadísticos lineales tradicionales.",
  "A nivel de las capas, los DBMs asumen independencia condicional entre las variables visibles e隐variables latentes dada la otra capa.",
  "Al igual que la normalidad de residuos, la homoscedasticidad no es un supuesto directo para DBMs.",
  "Puede manejar cierto nivel de ruido, pero el rendimiento disminuye con outliers extremos que distorsionan el espacio latente.",
  "Debido a su naturaleza de aprendizaje de representaciones, los DBMs son inherentemente capaces de manejar la multicolinealidad entre predictores.",
  "Generalmente, los DBMs son modelos de 'caja negra' con baja interpretabilidad de las relaciones directas entre las entradas y salidas.",
  "El entrenamiento de DBMs puede ser computacionalmente costoso y lento, especialmente con grandes datasets y muchas capas. La inferencia es más rápida.",
  "La validación cruzada es esencial para seleccionar hiperparámetros como el número de capas, unidades por capa, tasas de aprendizaje y regularización.",
  "Requiere una cantidad significativa de datos para entrenar de forma efectiva. Puede tener dificultades con conjuntos de datos pequeños. La alta dimensionalidad sin una regularización adecuada puede llevar a sobreajuste o problemas de escalabilidad en conjuntos de datos muy grandes."
)

tabla_dbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_dbm %>%
  gt() %>%
  tab_header(title = "Guía rápida para elegir Deep Boltzmann Machine (DBM)",
             subtitle = "Características y Consideraciones") %>%
  tab_footnote(footnote = "Fuente: Elaboración propia y principios de aprendizaje profundo") %>%
  tab_options(heading.title.font.size = 14,
              heading.subtitle.font.size = 12,
              table.font.names = "Century Gothic",
              table.font.size = 10,
              data_row.padding = px(1)) %>%
  tab_style(style = list(cell_text(align = "left",
                                   weight = 'bold')),
            locations = list(cells_title(groups = c("title")))) %>%
  tab_style(style = list(cell_text(align = "left")),
            locations = list(cells_title(groups = c("subtitle")))) %>%
  cols_width(starts_with("Detalles") ~ px(500),
             everything() ~ px(200)) %>%
  as_raw_html()
```


## Deep Belief Networks (DBNet) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/DBNet.png"))
```

El **Deep Belief Network (DBN)** es un modelo de **red neuronal profunda generativa** que se construye apilando múltiples **Máquinas de Boltzmann Restringidas (RBMs)**. Fue un avance significativo en el campo del aprendizaje profundo, particularmente en la superación de los desafíos de entrenamiento de redes neuronales con muchas capas ocultas. Las DBNs son modelos probabilísticos que buscan aprender una **distribución de probabilidad conjunta** sobre los datos de entrada y sus representaciones latentes (ocultas).   

La arquitectura de una DBN es una jerarquía de capas, donde cada capa es una RBM. La capa inferior es la capa visible (o de entrada), que recibe los datos. Las capas subsiguientes son capas ocultas, y la característica clave es que el resultado de la capa oculta de una RBM se convierte en la capa visible para la siguiente RBM en la pila. La conexión entre la capa superior más alta (que es una RBM) es no dirigida y bidireccional, mientras que las conexiones entre las capas inferiores suelen ser dirigidas (de arriba hacia abajo) en la fase generativa después del entrenamiento.  

Las DBNs se caracterizan por:  
  
1.  **Construcción por Capas:** Se construyen apilando RBMs, donde cada RBM se entrena de forma independiente y no supervisada para aprender una representación de su entrada.   
2.  **Pre-entrenamiento Codicioso por Capas:** La innovación clave de las DBNs fue el algoritmo de pre-entrenamiento codicioso por capas. En lugar de intentar entrenar toda la red a la vez (lo que era difícil debido a problemas como los gradientes desvanecientes/explosivos y los mínimos locales), cada RBM se entrena individualmente para aprender características útiles de la entrada que recibe. La salida de la capa oculta de una RBM entrenada se utiliza como entrada para la capa visible de la siguiente RBM. Este proceso continúa hasta que se entrenan todas las capas.  
3.  **Aprendizaje No Supervisado para Extracción de Características:** La fase de pre-entrenamiento es completamente no supervisada. Las RBMs aprenden a reconstruir sus entradas, lo que les permite extraer características relevantes y de alto nivel de los datos sin necesidad de etiquetas. Esto es especialmente valioso para conjuntos de datos grandes y no etiquetados.  
4.  **Ajuste Fino (Fine-tuning) Supervisado:** Después del pre-entrenamiento no supervisado, la DBN puede ser "desenrollada" y tratada como una red neuronal feed-forward para tareas supervisadas como la clasificación. Se añade una capa de salida (ej., softmax) en la parte superior, y toda la red se ajusta utilizando algoritmos de aprendizaje supervisado como la retropropagación. El pre-entrenamiento actúa como una buena inicialización de los pesos, ayudando a que el entrenamiento supervisado converja más rápido y alcance mejores mínimos.  
5.  **Generación de Datos:** Dado que son modelos generativos, las DBNs pueden aprender la distribución subyacente de los datos y, por lo tanto, pueden generar nuevas muestras de datos similares a las de entrenamiento.   

Las DBNs fueron fundamentales para demostrar la viabilidad del entrenamiento de redes profundas y abrieron el camino para el resurgimiento del aprendizaje profundo.   


  
**Aprendizaje Global vs. Local:**
  
El modelo **Deep Belief Network (DBN)** emplea un enfoque híbrido, pero en su fase de aprendizaje de características, se inclina hacia un **aprendizaje global** a través de una estrategia local y progresiva.

* **Aspecto Global (Objetivo Final y Representación):** El objetivo general de una DBN es construir un **modelo probabilístico jerárquico global** de los datos. Aunque el entrenamiento se realiza capa por capa, la intención es que cada capa capture características que contribuyan a una comprensión más abstracta y completa de la distribución de los datos de entrada en su conjunto. Las características de bajo nivel aprendidas por las primeras RBMs se combinan en las capas superiores para formar representaciones más complejas y de alto nivel, que son intrínsecas a la estructura global de los datos. El modelo final, una vez que todas las RBMs están entrenadas y se aplica el ajuste fino, opera como una red unificada que mapea entradas a salidas basadas en un entendimiento global de las relaciones en los datos.

* **Aspecto Local (Estrategia de Entrenamiento):** La fase de **pre-entrenamiento codicioso por capas** de las DBNs tiene un fuerte componente local. Cada **RBM individual** se entrena de manera local, optimizando sus propios pesos para modelar la relación entre su capa visible y su capa oculta, **sin considerar explícitamente las capas más allá de sí misma en ese momento**. La entrada para cada RBM superior proviene de la activación de la capa oculta de la RBM inferior ya entrenada. Este entrenamiento local y secuencial es lo que permite que las DBNs escalen a redes profundas y eviten problemas de optimización de modelos globales complejos desde cero. Sin embargo, esta "localidad" es solo en la etapa de entrenamiento por partes; el efecto acumulativo de estas optimizaciones locales es la construcción de una representación jerárquica que eventualmente se une en un modelo global cuando se realiza el ajuste fino de toda la red.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Generativo / Discriminativo (apilado, no supervisado/supervisado)",
  "✅ Numérica continua o categórica (binaria, multiclase)",
  "✅ Numéricas y/o Categóricas (requiere preprocesamiento)",
  "✅ No lineal y Compleja",
  "❌ No aplica directamente",
  "⚠️ Asume independencia condicional entre capas",
  "❌ No aplica directamente",
  "⚠️ Robusto hasta cierto punto, pero datos muy ruidosos afectan",
  "✅ Maneja bien multicolinealidad",
  "⚠️ Baja (tipo caja negra)",
  "⚠️ Lento en entrenamiento, rápido en inferencia",
  "✅ Útil para ajuste de hiperparámetros",
  "❌ Datos insuficientes; ❌ alta dimensionalidad sin suficiente regularización; ❌ problemas de escalabilidad."
)

detalles <- c(
  "Modelo generativo y discriminativo que consiste en apilar varias Restricted Boltzmann Machines (RBMs) o componentes similares. Cada RBM se entrena de forma no supervisada, y luego la red completa puede ser ajustada de forma supervisada.",
  "Puede ser utilizado tanto para tareas de regresión (variables continuas) como de clasificación (variables categóricas, incluyendo binarias y multiclase), especialmente después de un ajuste supervisado (fine-tuning).",
  "Acepta una variedad de tipos de variables de entrada. Las variables numéricas generalmente requieren normalización o estandarización, y las categóricas necesitan ser codificadas (ej. one-hot encoding).",
  "Las DBNs son extremadamente capaces de aprender y modelar relaciones complejas y no lineales entre las variables de entrada y salida, gracias a su arquitectura profunda y sus capas ocultas.",
  "Al igual que con los DBMs, las DBNs no se basan en supuestos de normalidad de los residuos, ya que no son modelos estadísticos lineales tradicionales.",
  "La independencia condicional es un supuesto clave en la forma en que cada RBM dentro de la DBN procesa la información entre sus capas visible y oculta.",
  "La homoscedasticidad no es un supuesto inherente o un requisito directo para el entrenamiento o la aplicación de las DBNs.",
  "Aunque pueden ser algo robustas al ruido en los datos, los valores atípicos extremos pueden afectar negativamente el proceso de aprendizaje de las representaciones en las capas ocultas.",
  "Su capacidad para aprender representaciones jerárquicas y de bajo nivel de los datos ayuda a mitigar los problemas causados por la multicolinealidad entre las variables predictoras.",
  "Las DBNs, como muchos modelos de aprendizaje profundo, son consideradas 'cajas negras'. Es difícil interpretar directamente cómo las características de entrada se mapean a las decisiones de salida.",
  "El entrenamiento de una DBN puede ser muy lento y costoso computacionalmente, especialmente en conjuntos de datos grandes o con muchas capas. Sin embargo, una vez entrenadas, la fase de inferencia es generalmente rápida.",
  "La validación cruzada es una técnica crucial para la selección y optimización de hiperparámetros importantes, como el número de RBMs, el número de unidades en cada capa, las tasas de aprendizaje y los coeficientes de regularización.",
  "Requieren grandes volúmenes de datos etiquetados (para el fine-tuning supervisado) o no etiquetados (para el pre-entrenamiento no supervisado) para aprender representaciones significativas. Pueden tener problemas de escalabilidad con conjuntos de datos masivos o arquitecturas muy profundas."
)

tabla_dbn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_dbn %>%
  gt() %>%
  tab_header(title = "Guía rápida para elegir Deep Belief Network (DBN)",
             subtitle = "Características y Consideraciones") %>%
  tab_footnote(footnote = "Fuente: Elaboración propia y principios de aprendizaje profundo") %>%
  tab_options(heading.title.font.size = 14,
              heading.subtitle.font.size = 12,
              table.font.names = "Century Gothic",
              table.font.size = 10,
              data_row.padding = px(1)) %>%
  tab_style(style = list(cell_text(align = "left",
                                   weight = 'bold')),
            locations = list(cells_title(groups = c("title")))) %>%
  tab_style(style = list(cell_text(align = "left")),
            locations = list(cells_title(groups = c("subtitle")))) %>%
  cols_width(starts_with("Detalles") ~ px(500),
             everything() ~ px(200)) %>%
  as_raw_html()
```

## Reinforcement Learning (DL-based RL) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/Reinforcement Learning.png"))
```


## Stacked Auto-Enconders {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/SAE.png"))
```

Un **Autoencoder Apilado (Stacked Autoencoder - SAE)** es un tipo de **red neuronal profunda** que se construye apilando múltiples **autoencoders (AE)** simples. Al igual que los Autoencoders individuales, su propósito principal es el **aprendizaje de características no supervisado** y la **reducción de dimensionalidad**. La idea central es aprender representaciones compactas y de baja dimensionalidad (codificaciones) de los datos de entrada, que capturen las características más importantes.

La arquitectura de un SAE se organiza en capas, donde cada capa es un autoencoder. Un autoencoder básico consta de dos partes: un **codificador (encoder)** que mapea la entrada a una representación de menor dimensión (el "código" o "bottleneck"), y un **decodificador (decoder)** que reconstruye la entrada original a partir de esta representación. En un autoencoder apilado:

1.  **Codificador y Decodificador:** Cada autoencoder en la pila tiene su propio codificador y decodificador.
2.  **Formación de Capas:** La salida de la capa codificadora de un autoencoder se convierte en la entrada para la siguiente capa (el autoencoder superior). De esta manera, las capas progresivas aprenden representaciones de características cada vez más abstractas y de alto nivel.

Las características clave de los Stacked Autoencoders incluyen:

1.  **Pre-entrenamiento Codicioso por Capas:** Similar a las DBNs, los SAEs se entrenan utilizando un enfoque de **pre-entrenamiento codicioso por capas**.
    * Primero, se entrena un autoencoder para aprender una representación de la capa de entrada original.
    * Una vez entrenado, la capa del codificador de este AE se "congela" y sus salidas (las características aprendidas) se utilizan como entrada para el entrenamiento del siguiente autoencoder en la pila.
    * Este proceso se repite, entrenando un nuevo autoencoder sobre las representaciones aprendidas por el autoencoder anterior, construyendo así una jerarquía de características.
2.  **Aprendizaje No Supervisado:** Toda la fase de pre-entrenamiento es **no supervisada**, lo que significa que los SAEs pueden aprender representaciones poderosas de datos sin necesidad de etiquetas. Esto los hace muy útiles en escenarios donde los datos etiquetados son escasos.
3.  **Reducción de Dimensionalidad y Extracción de Características:** El objetivo de cada autoencoder es encontrar una representación de baja dimensionalidad que permita una buena reconstrucción de la entrada. Al apilar estos, el SAE aprende una jerarquía de características donde las capas más profundas capturan abstracciones más complejas y significativas de los datos.
4.  **Ajuste Fino (Fine-tuning) Supervisado:** Después del pre-entrenamiento no supervisado, el decodificador de cada autoencoder suele descartarse. Se toma la pila de codificadores como una red de extracción de características. A esta red se le añade una capa de salida (ej., una capa softmax para clasificación) y todo el modelo se ajusta finamente utilizando un algoritmo de aprendizaje supervisado (como retropropagación con gradiente descendente) en una tarea específica. El pre-entrenamiento actúa como una excelente inicialización de los pesos, lo que ayuda a evitar mínimos locales pobres y a acelerar la convergencia.

Los Stacked Autoencoders fueron un modelo popular antes del auge de las Redes Convolucionales y Recurrentes más especializadas, y demostraron la efectividad del pre-entrenamiento no supervisado para inicializar redes profundas.



**Aprendizaje Global vs. Local:**

El modelo de **Autoencoder Apilado (Stacked Autoencoder - SAE)** es un modelo de **aprendizaje global** que se construye a través de una estrategia de entrenamiento local y secuencial.

* **Aspecto Global (Objetivo Final y Representación):** El objetivo final de un SAE es aprender una **representación global y jerárquica** de los datos. Cada capa codificadora en la pila extrae características de un nivel de abstracción creciente, contribuyendo a una comprensión más profunda y compacta de toda la distribución de los datos de entrada. La codificación final producida por el SAE es una representación de baja dimensionalidad que intenta encapsular la información más relevante de los datos en su conjunto, permitiendo su reconstrucción. Cuando se utiliza para tareas posteriores (como clasificación) después del ajuste fino, la red opera como un modelo unificado que aplica las características globales aprendidas a nuevas entradas.

* **Aspecto Local (Estrategia de Entrenamiento por Capas):** La fase de **pre-entrenamiento codicioso por capas** de los SAEs tiene un componente fuertemente local. Cada **autoencoder individual** en la pila se entrena de forma independiente para aprender una codificación óptima y una reconstrucción de su propia entrada. Esto significa que los pesos de cada autoencoder se optimizan localmente, en un momento dado, sin considerar directamente la optimización simultánea de toda la red. La entrada a cada autoencoder subsiguiente es la representación codificada aprendida por el autoencoder anterior. Esta estrategia de entrenamiento "por partes" permite que las redes profundas sean entrenadas de manera más eficiente y eficaz, ya que descompone un problema de optimización complejo en subproblemas más manejables. Sin embargo, el resultado acumulado de estas optimizaciones locales es la construcción de una jerarquía de características que, en última instancia, forma parte de un modelo global y unificado de los datos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "✅ Aprendizaje de representación (no supervisado) / Discriminativo (supervisado)",
  "✅ Numérica continua o categórica (binaria, multiclase)",
  "✅ Numéricas y/o Categóricas (requiere preprocesamiento)",
  "✅ No lineal y Compleja",
  "❌ No aplica directamente",
  "⚠️ No es un supuesto directo, pero la compresión busca regularidad",
  "❌ No aplica directamente",
  "⚠️ Sensible, pueden afectar la calidad de las representaciones aprendidas",
  "✅ Maneja bien multicolinealidad",
  "⚠️ Baja (tipo caja negra)",
  "⚠️ Lento en entrenamiento, rápido en inferencia",
  "✅ Útil para ajuste de hiperparámetros",
  "❌ Datos insuficientes; ❌ arquitectura inadecuada (cuello de botella); ❌ si las representaciones no son significativas para la tarea final."
)

detalles <- c(
  "Compuesto por múltiples autoencoders apilados. Cada autoencoder se entrena para aprender una representación (codificación) de la entrada de la capa anterior. Se usa para pre-entrenamiento no supervisado y luego fine-tuning supervisado para tareas específicas.",
  "La capa de salida final puede adaptarse para tareas de regresión (variables continuas) o clasificación (variables categóricas).",
  "Requiere preprocesamiento: variables numéricas estandarizadas/normalizadas y variables categóricas codificadas (ej. one-hot encoding).",
  "Excelente para capturar relaciones complejas, no lineales y de alta dimensionalidad en los datos a través de representaciones aprendidas.",
  "Los Stacked Autoencoders son modelos de aprendizaje automático no paramétricos y no tienen supuestos sobre la normalidad de los residuos.",
  "No es un supuesto explícito, pero el objetivo de los autoencoders de reconstruir la entrada fomenta la captura de dependencias y regularidades en los datos, no necesariamente errores independientes.",
  "La homoscedasticidad no es una consideración directa ni un requisito para el entrenamiento de Stacked Autoencoders.",
  "Pueden ser sensibles a valores atípicos, ya que estos pueden influir fuertemente en la forma en que se aprenden las representaciones latentes, potencialmente llevando a una reconstrucción deficiente o a características sesgadas.",
  "Al aprender representaciones de características de menor dimensionalidad, los SAEs son robustos frente a la multicolinealidad en las variables predictoras originales.",
  "Similar a otras redes neuronales profundas, los SAEs operan como 'cajas negras', haciendo difícil la interpretación directa de las características latentes que aprenden.",
  "El entrenamiento capa por capa puede ser intensivo en tiempo y recursos computacionales, especialmente con grandes datasets y arquitecturas complejas. La fase de inferencia es rápida.",
  "La validación cruzada es crucial para la selección de hiperparámetros, como el número de capas, el número de unidades en cada capa latente, las tasas de aprendizaje y los términos de regularización.",
  "Requieren una cantidad sustancial de datos para aprender representaciones significativas y evitar el sobreajuste. Una arquitectura de cuello de botella mal diseñada puede limitar la capacidad de representación. Si las características aprendidas por los autoencoders no son relevantes para la tarea final supervisada, el rendimiento puede ser pobre."
)

tabla_sae <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_sae %>%
  gt() %>%
  tab_header(title = "Guía rápida para elegir Stacked Autoencoders (SAE)",
             subtitle = "Características y Consideraciones") %>%
  tab_footnote(footnote = "Fuente: Elaboración propia y principios de aprendizaje profundo") %>%
  tab_options(heading.title.font.size = 14,
              heading.subtitle.font.size = 12,
              table.font.names = "Century Gothic",
              table.font.size = 10,
              data_row.padding = px(1)) %>%
  tab_style(style = list(cell_text(align = "left",
                                   weight = 'bold')),
            locations = list(cells_title(groups = c("title")))) %>%
  tab_style(style = list(cell_text(align = "left")),
            locations = list(cells_title(groups = c("subtitle")))) %>%
  cols_width(starts_with("Detalles") ~ px(500),
             everything() ~ px(200)) %>%
  as_raw_html()
```


## Variational Autoencoders (VAEs) {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/VAEs.png"))
```

<!--chapter:end:11-deep-learning.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
`r if (knitr::is_html_output()) '
# References {-}
'`

Sagi, S. (2019). ML Algorithms: One SD (σ). The obvious questions to ask when… | by Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6 

Kuhn, M. (2019). The caret Package. https://topepo.github.io/caret/index.html

<!--chapter:end:12-references.Rmd-->

