--- 
title: "Machine Learning (Apuntes) "
author: "Diana Villasana Ocampo"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::bs4_book,
  set in the _output.yml file.
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---
```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```

# Machine Learning {-}


## üìå Cuadro {-}

```{r, echo = FALSE}
algoritmos_ml <- data.frame(
                            Tipo = c(
                              "Regressi√≥n",
                              "√Årboles / Decision Tree",
                              "Ensambles",
                              "Deep Learning",
                              "Reducci√≥n de Dim.",
                              "Bayesianos",
                              "Regularizaci√≥n",
                              "Instance-Based",
                              "Clustering",
                              "Rule-Based"
                            ),
                            Problema_tipico = c(
                              "Valores num√©ricos",
                              "Clasificaci√≥n, regresi√≥n",
                              "Clasificaci√≥n, regresi√≥n",
                              "Im√°genes, texto, audio",
                              "Visualizaci√≥n, preprocesamiento",
                              "Clasificaci√≥n r√°pida",
                              "Evitar overfitting",
                              "Clasificaci√≥n",
                              "Agrupamiento no supervisado",
                              "Interpretabilidad"
                            ),
                            Ventajas = c(
                              "Simplicidad",
                              "Interpretabilidad",
                              "Precisi√≥n",
                              "Modelos complejos",
                              "Mejora eficiencia",
                              "Velocidad",
                              "Generalizaci√≥n",
                              "Simple, no requiere entrenamiento",
                              "Descubrir estructuras ocultas",
                              "L√≥gica clara"
                            ),
                            Cuando_usarlo = c(
                              "Relaciones lineales",
                              "Datos tabulares",
                              "Alto rendimiento, Kaggle",
                              "Datos grandes y no estructurados",
                              "Datos con muchas variables",
                              "Texto, spam detection",
                              "Modelos lineales con muchas variables",
                              "Pocos datos y relaciones claras",
                              "Segmentaci√≥n sin etiquetas",
                              "Reglas conocidas, decisiones explicables"
                            ),
                            stringsAsFactors = FALSE
                          )

require(gt)

algoritmos_ml %>% 
 gt() %>%
  tab_header(title = "Modelos y cuando usarlos") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     fmt_integer(columns = names(data)[4:22], 
                sep_mark = " ") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Cuando_") ~ px(300),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```



```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üîç **1. Regressi√≥n** {-}

**Ejemplos:** Linear Regression, Ridge, Lasso   
**Cu√°ndo usarlo:**

* Predicci√≥n de valores num√©ricos continuos (e.g. precios, temperaturas).  
* Relaciones lineales entre variables. 

**Ventajas:** Simple, interpretable.   
**Limitaciones:** Mal desempe√±o con relaciones no lineales complejas.

## Ordinary Least Squares Regression (`OLSR`) {-} 

**Regresi√≥n de M√≠nimos Cuadrados Ordinarios (OLSR)**: un m√©todo de regresi√≥n lineal para estimar los par√°metros desconocidos mediante la creaci√≥n de un modelo que minimizar√° la suma de los errores cuadrados entre los datos observados y los predichos (valores observados y valores estimados).   

```{r, echo = FALSE}
# Datos de la tabla
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas y/o categ√≥ricas",
  "‚úÖ Lineal (supuesto clave)",
  "‚òëÔ∏è Deseable",
  "‚úÖ Necesaria",
  "‚úÖ Necesaria",
  "‚ö†Ô∏è S√≠",
  "‚ö†Ô∏è Problema com√∫n",
  "‚úÖ Alta",
  "‚úÖ Muy alta",
  "‚úÖ Compatible",
  "‚ùå Relaciones no lineales, outliers severos, colinealidad"
)

detalles <- c(
  "Se entrena con datos X ‚Üí y",
  "Ej. mpg, precio, ingresos",
  "Categor√≠as convertidas a dummies",
  "Se asume una relaci√≥n lineal entre X e Y",
  "Importante para intervalos de confianza v√°lidos",
  "Errores deben ser independientes",
  "Varianza de errores debe ser constante",
  "Outliers pueden influir mucho en el modelo",
  "Usar VIF para detectar problemas",
  "Modelo f√°cil de explicar",
  "R√°pido incluso con datos grandes",
  "Ayuda a prevenir overfitting",
  "Evitar si no hay linealidad o hay muchos outliers"
)

# Crear y mostrar tabla
tabla_olsr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

require(gt)

tabla_olsr %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir OLSR",
             subtitle = "Regresi√≥n de M√≠nimos Cuadrados Ordinarios (OLSR)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Linear Regression {-} 

**Regresi√≥n lineal** : se utiliza para estimar valores reales (costo de las casas, n√∫mero de visitas, ventas totales, etc.) basados en variables continuas.  


## Logistic Regression {-} 

**Regresi√≥n log√≠stica** : se utiliza para estimar valores discretos (valores binarios como 0/1, s√≠/no, verdadero/falso) basados en un conjunto dado de variables independientes. 

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Lineal entre log-odds y predictores",
  "‚ùå No es requisito",
  "‚úÖ Necesaria",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠",
  "‚ö†Ô∏è Puede afectar",
  "‚úÖ Alta (coeficientes interpretables)",
  "‚úÖ Alta",
  "‚úÖ Compatible",
  "‚ùå Respuesta no binaria o multiclase sin ajuste"
)

detalles <- c(
  "Clasificaci√≥n binaria",
  "Ej. √©xito/fracaso, s√≠/no",
  "Convertir categ√≥ricas a dummies",
  "Relaci√≥n entre log(p/(1-p)) y X debe ser lineal",
  "No se exige normalidad en errores",
  "Independencia entre observaciones",
  "Idealmente varianza constante",
  "Outliers pueden alterar los coeficientes",
  "Usar VIF y regularizaci√≥n si hay problema",
  "Coeficientes en t√©rminos de odds/log-odds",
  "R√°pido y estable para datasets medianos",
  "K-fold funciona muy bien",
  "Evitar si hay multiclase sin ajuste"
)

tabla_logit <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

require(gt)

tabla_logit %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir logit",
             subtitle = "Regresi√≥n log√≠stica") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Stepwise Regression {-} 

**Regresi√≥n por pasos** : a√±ade caracter√≠sticas al modelo una a una hasta encontrar la puntuaci√≥n √≥ptima para tu conjunto de caracter√≠sticas. La selecci√≥n por pasos alterna entre el avance y el retroceso, incorporando y eliminando variables que cumplen los criterios de entrada o eliminaci√≥n, hasta alcanzar un conjunto estable de variables.  

## Multivariate Adaptive Regression Splines (`MARS`) {-} 

**Splines de Regresi√≥n Adaptativa Multivariante (`MARS`)**: un m√©todo de regresi√≥n flexible que busca interacciones y relaciones no lineales que ayudan a maximizar la precisi√≥n predictiva. Este algoritmo es inherentemente no lineal (lo que significa que no es necesario adaptar el modelo a patrones no lineales en el datos agregando manualmente t√©rminos del modelo (squared terms, interaction effects).   

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua o categ√≥rica (binaria con extensi√≥n)",
  "‚úÖ Num√©ricas (categ√≥ricas con dummies)",
  "‚úÖ No lineal (autom√°tico)",
  "‚ùå No requerida",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠ (aunque algo robusto)",
  "‚ö†Ô∏è Puede afectar",
  "‚ö†Ô∏è Media (modelo tipo caja negra)",
  "‚úÖ Razonable para tama√±os medianos",
  "‚úÖ Recomendado (ej. repeated k-fold)",
  "‚ùå Relaci√≥n puramente lineal o muchos factores irrelevantes"
)

detalles <- c(
  "Regresi√≥n flexible no lineal",
  "Ideal para regresi√≥n continua (tambi√©n clasificaci√≥n con `earth`)",
  "Crea autom√°ticamente 'splines' por variable",
  "Crea funciones por tramos con 'nudos'",
  "No exige distribuci√≥n espec√≠fica de errores",
  "Mejor si los datos no est√°n correlacionados temporalmente",
  "Idealmente errores con varianza constante",
  "Puede ser sensible a valores extremos",
  "Detecta interacciones, pero VIF sigue siendo √∫til",
  "Coeficientes no tan interpretables como OLS",
  "M√°s lento que OLS pero m√°s flexible",
  "CV ayuda a elegir n√∫mero √≥ptimo de t√©rminos",
  "Tiene riesgo de sobreajuste si no se controla bien"
)

tabla_mars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

require(gt)

tabla_mars %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MARS",
             subtitle = "Splines de Regresi√≥n Adaptativa Multivariante (MARS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Locally Estimated Scatterplot Smoothing (`LOESS`) {-} 

**Locally Estimated Scatterplot Smoothing (`LOESS`)**: un m√©todo para ajustar una curva suave entre dos variables o una superficie  entre un resultado y hasta cuatro variables predictoras. La idea es que, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n. Se puede aplicar regresi√≥n, lo que se denomina regresi√≥n ponderada localmente. Se puede aplicar LOESS cuando la relaci√≥n entre las variables independientes y dependientes no es lineal. Hoy en d√≠a, la mayor√≠a de los algoritmos (como las redes neuronales de propagaci√≥n hacia adelante cl√°sicas, las m√°quinas de vectores de soporte, los algoritmos del vecino m√°s cercano, etc.) son sistemas de aprendizaje global que se utilizan para minimizar las funciones de p√©rdida globales (por ejemplo, el error cuadr√°tico medio). Por el contrario, los sistemas de aprendizaje local dividen el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Esto generalmente se logra dividiendo la funci√≥n de costo en m√∫ltiples funciones de costo locales independientes. Una de las desventajas de los m√©todos globales es que, a veces, ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena. Pero entonces surge LOESS, una alternativa a la aproximaci√≥n de funciones globales.   


```{r, echo = FALSE} 
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua",
  "‚úÖ Num√©ricas (usualmente 1 o 2 predictores)",
  "‚úÖ No lineal y suave",
  "‚ùå No necesaria",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è Muy sensible",
  "‚ùå No aplica (pocos predictores)",
  "‚úÖ Muy interpretable gr√°ficamente",
  "‚ö†Ô∏è Lento en grandes vol√∫menes de datos",
  "‚úÖ Puede usarse para elegir 'span'",
  "‚ùå Datos grandes o con ruido fuerte"
)

detalles <- c(
  "Modelo no param√©trico local",
  "Regresi√≥n para valores continuos",
  "Generalmente 1 o 2 variables num√©ricas",
  "Ajuste por vecindad, suaviza la curva",
  "No asume distribuci√≥n espec√≠fica",
  "Supuesto deseable si hay dependencias temporales",
  "Ideal si la varianza no cambia mucho localmente",
  "Altamente afectado por outliers locales",
  "No es una t√©cnica multivariable compleja",
  "La curva ajustada se interpreta visualmente",
  "Computacionalmente costoso con datos grandes",
  "Ayuda a seleccionar el mejor 'span'",
  "Poco eficaz en alta dimensi√≥n o datos muy dispersos"
)

tabla_loess <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_loess %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LOESS",
             subtitle = "Locally Estimated Scatterplot Smoothing (LOESS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



<!--chapter:end:01-regression.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üå≤ **2. √Årboles de Decisi√≥n y Derivados** {-}  

**Ejemplos:** Decision Tree, Random Forest, Gradient Boosting  
**Cu√°ndo usarlo:**  

* Problemas tabulares con relaciones no lineales y variables categ√≥ricas o num√©ricas.
* Cuando interpretabilidad es importante.

**Ventajas:** Manejan datos heterog√©neos, f√°ciles de interpretar (√°rboles simples).   
**Limitaciones:** Sobreajuste en √°rboles simples; menor desempe√±o en datos muy ruidosos sin ensambles.

---


## Classification and Regression Tree (CART)  {-} 

**Classification and Regression Tree (CART)** es un m√©todo no param√©trico que se utiliza para construir **√°rboles de decisi√≥n** tanto para problemas de **clasificaci√≥n** como de **regresi√≥n**. La idea central es dividir recursivamente el espacio de las caracter√≠sticas en regiones m√°s peque√±as y manejables, creando as√≠ un modelo con forma de √°rbol que es f√°cil de interpretar.

A diferencia de los modelos lineales o algunos algoritmos de aprendizaje global, CART no asume una relaci√≥n lineal entre las variables. En su lugar, el algoritmo identifica los mejores **puntos de divisi√≥n** en las variables predictoras para maximizar la **homogeneidad** de las respuestas dentro de cada regi√≥n resultante. Para problemas de clasificaci√≥n, esto se mide com√∫nmente con m√©tricas como la **impureza Gini** o la **ganancia de informaci√≥n**, mientras que para la regresi√≥n, se busca minimizar la **suma de los cuadrados de los residuos**.

Mientras que muchos algoritmos (como las redes neuronales cl√°sicas o las m√°quinas de vectores de soporte) son sistemas de **aprendizaje global** que buscan minimizar una funci√≥n de p√©rdida √∫nica para todo el conjunto de datos, CART se puede considerar m√°s como un sistema de **aprendizaje local**. Construye el modelo tomando decisiones de divisi√≥n locales en cada nodo del √°rbol, lo que le permite capturar relaciones complejas y no lineales en los datos. Esto es particularmente √∫til cuando una aproximaci√≥n de funci√≥n global √∫nica podr√≠a no ser suficiente para modelar la relaci√≥n entre las variables. Una de las ventajas de CART es su capacidad para manejar diferentes tipos de datos (num√©ricos y categ√≥ricos) y su interpretabilidad, ya que la ruta desde la ra√≠z hasta una hoja del √°rbol representa un conjunto de reglas de decisi√≥n.



```{r, echo =FALSE}

criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y Categ√≥ricas",
  "‚úÖ No lineal y con interacciones",
  "‚ùå No requiere",
  "‚ö†Ô∏è Puede verse afectado",
  "‚ö†Ô∏è No necesario pero deseable",
  "‚ö†Ô∏è S√≠, en puntos de corte",
  "‚úÖ No se ve afectado",
  "‚úÖ Alta (gr√°fico del √°rbol)",
  "‚úÖ R√°pido en datasets medianos",
  "‚úÖ Muy usado para poda y ajuste",
  "‚ùå Muy profundo (overfitting), datos muy ruidosos"
)

detalles <- c(
  "Algoritmo basado en divisiones binarias",
  "Puede predecir clases o valores continuos",
  "Acepta todo tipo de variables predictoras",
  "Captura relaciones complejas y no lineales",
  "No requiere distribuci√≥n normal",
  "Idealmente los errores deben ser independientes",
  "La varianza constante mejora resultados",
  "Puede generar divisiones extremas por valores at√≠picos",
  "No necesita preocuparse por colinealidad",
  "F√°cil de entender, especialmente con √°rboles peque√±os",
  "Escalable pero no √≥ptimo en grandes vol√∫menes sin poda",
  "Usa poda y validaci√≥n cruzada para evitar sobreajuste",
  "Tiende al sobreajuste si no se poda o se regulariza"
)

tabla_cart <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

require(gt) 

tabla_cart %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CART",
             subtitle = "Classification and Regression Tree (CART)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Support Vector Machine (SVM) {-}  

**Support Vector Machine (SVM)** es un potente m√©todo de **aprendizaje supervisado** utilizado para tareas de **clasificaci√≥n** y **regresi√≥n**, aunque es m√°s conocido y aplicado en problemas de clasificaci√≥n. La idea fundamental detr√°s de SVM es encontrar el **hiperplano √≥ptimo** que mejor separe las clases de datos en un espacio de caracter√≠sticas.

A diferencia de otros algoritmos que podr√≠an intentar ajustar una curva o superficie a todos los datos, SVM se enfoca en los **vectores de soporte**: los puntos de datos que est√°n m√°s cerca del hiperplano de decisi√≥n y que, por lo tanto, son los m√°s dif√≠ciles de clasificar. Estos vectores de soporte son los que definen el margen de separaci√≥n entre las clases. El objetivo de SVM es maximizar este **margen** entre el hiperplano y los vectores de soporte m√°s cercanos, lo que generalmente conduce a una mejor **generalizaci√≥n** y menor riesgo de sobreajuste.

Mientras que algunos algoritmos buscan una aproximaci√≥n de funci√≥n global √∫nica que minimice una funci√≥n de p√©rdida (como el error cuadr√°tico medio), SVM puede ser visto como un sistema de **aprendizaje global** que busca optimizar un criterio global: el margen. Sin embargo, su capacidad para trabajar con **kernels** (como el kernel lineal, polinomial, de base radial (RBF) o sigmoide) le permite mapear los datos a un espacio de dimensiones superiores donde pueden ser linealmente separables, incluso si no lo son en el espacio original. Esta "transformaci√≥n" a trav√©s de la funci√≥n kernel permite a SVM manejar relaciones complejas y no lineales entre las variables independientes y dependientes, lo que lo convierte en una alternativa robusta a la aproximaci√≥n de funciones globales simples. Si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se puede aplicar eficazmente utilizando esta capacidad de mapeo para encontrar una separaci√≥n √≥ptima.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas (categor√≠as deben codificarse)",
  "‚úÖ Capta relaciones no lineales (kernel)",
  "‚ùå No requiere",
  "‚úÖ Idealmente s√≠",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠, especialmente sin margen amplio",
  "‚úÖ Puede manejarla bien",
  "‚ùå Baja (modelo es una caja negra)",
  "‚ö†Ô∏è Lento con muchos datos o predictores",
  "‚úÖ Esencial para elegir kernel y par√°metros",
  "‚ùå Datos con mucho ruido o solapamiento entre clases"
)

detalles <- c(
  "Modelo supervisado que maximiza el margen entre clases",
  "Clasificaci√≥n binaria, multiclase o regresi√≥n (SVR)",
  "Requiere escalar o estandarizar las variables num√©ricas",
  "Puede usar kernel para resolver problemas no lineales",
  "No requiere supuestos cl√°sicos como normalidad",
  "Mejor si los datos son independientes",
  "Puede usarse aunque haya heterocedasticidad",
  "Los outliers cercanos al margen afectan el modelo",
  "Los kernels pueden reducir el efecto de multicolinealidad",
  "Dif√≠cil de explicar; es un modelo de tipo caja negra",
  "Puede ser costoso computacionalmente con datos grandes",
  "Par√°metros como C y gamma se ajustan v√≠a validaci√≥n cruzada",
  "No es ideal si hay ruido o datos mal etiquetados"
)

tabla_svm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

require(gt) 

tabla_svm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir SVM",
             subtitle = "Support Vector Machine (SVM) ") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Iterative Dichotomiser 3 (ID3)  {-}    
**Iterative Dichotomiser 3 (ID3)** es un algoritmo cl√°sico para construir **√°rboles de decisi√≥n**, dise√±ado principalmente para tareas de **clasificaci√≥n**. Fue uno de los primeros algoritmos de √°rboles de decisi√≥n desarrollados por Ross Quinlan. La idea central de ID3 es construir un √°rbol de clasificaci√≥n seleccionando en cada nodo del √°rbol el atributo que mejor divide el conjunto de datos en subconjuntos m√°s puros y homog√©neos.

ID3 opera de forma **iterativa** y **dicot√≥mica** (aunque puede manejar atributos con m√°s de dos categor√≠as), dividiendo el conjunto de datos en cada paso bas√°ndose en el atributo m√°s informativo. La selecci√≥n del "mejor" atributo se basa en m√©tricas de **teor√≠a de la informaci√≥n**, principalmente la **ganancia de informaci√≥n** (Information Gain). La ganancia de informaci√≥n mide la reducci√≥n en la **entrop√≠a** (una medida de la impureza o desorden de un conjunto de datos) que se logra al dividir los datos seg√∫n un atributo particular. El atributo con la mayor ganancia de informaci√≥n es elegido como el nodo de decisi√≥n en cada nivel del √°rbol.

A diferencia de los sistemas de aprendizaje global que buscan minimizar funciones de p√©rdida globales (como el error cuadr√°tico medio), ID3 es un algoritmo de **aprendizaje local** en el sentido de que toma decisiones de divisi√≥n √≥ptimas en cada nodo bas√°ndose en la informaci√≥n disponible en ese subconjunto de datos. Aunque la construcci√≥n del √°rbol es un proceso global, cada paso de la divisi√≥n se optimiza localmente para maximizar la pureza de los subconjuntos resultantes. Esto le permite a ID3 capturar relaciones no lineales entre las variables, ya que no asume una distribuci√≥n lineal de los datos. En esencia, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n (o clasificaci√≥n, en este caso) de manera ponderada localmente al dividir el espacio de caracter√≠sticas en regiones m√°s manejables. Sin embargo, una desventaja de ID3 es que tiende a favorecer atributos con muchos valores y puede ser propenso al sobreajuste.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Principalmente categ√≥ricas (num√©ricas requieren discretizaci√≥n)",
  "‚úÖ No lineal (basado en ganancia de informaci√≥n)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente (valores at√≠picos pueden generar ramas poco representativas)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol simple de interpretar)",
  "‚úÖ R√°pido con datos moderados y discretizados",
  "‚úÖ Recomendable para equilibrar datos y evitar overfitting",
  "‚ùå Respuesta continua, muchos valores faltantes o ruido elevado"
)

detalles <- c(
  "Construye un √°rbol de decisi√≥n dividiendo por ganancia de informaci√≥n (entrop√≠a).",
  "Clasifica muestras en categor√≠as discretas, ej. S√≠/No, A/B/C.",
  "Mejor con variables categ√≥ricas nativas; las num√©ricas deben transformarse en rangos.",
  "No asume ninguna relaci√≥n funcional: usa particiones basadas en criterios de informaci√≥n.",
  "No hay residuos en el sentido param√©trico; no exige distribuci√≥n normal.",
  "Las instancias deben ser independientes; no orientado a series temporales.",
  "No requiere varianzas constantes porque no hay t√©rmino de error param√©trico.",
  "Los outliers categ√≥ricos pueden crear nodos muy peque√±os no representativos.",
  "ID3 ignora correlaciones altas, pero demasiadas variables correlacionadas pueden ralentizar la b√∫squeda de mejores divisiones.",
  "Cada nodo muestra la regla de divisi√≥n; el √°rbol global es f√°cil de visualizar para pocos niveles.",
  "La construcci√≥n recursiva es eficiente para datos discretizados; se vuelve lento si hay muchas categor√≠as o atributos.",
  "Se usa para podar y seleccionar profundidad √≥ptima del √°rbol, equilibrando sesgo y varianza.",
  "No es recomendable si la variable objetivo es continua o si hay mucho ruido sin transformar."
)

tabla_id3 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_id3 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir IDE3",
             subtitle = "Iterative Dichotomiser 3 (ID3)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## C4.5  {-}   

**C4.5** es una extensi√≥n del algoritmo **ID3**, tambi√©n desarrollado por Ross Quinlan, y es uno de los algoritmos de **√°rboles de decisi√≥n** m√°s influyentes y ampliamente utilizados para tareas de **clasificaci√≥n**. Fue dise√±ado para abordar algunas de las limitaciones de su predecesor, ID3, y se ha convertido en un est√°ndar de facto en el aprendizaje autom√°tico para construir modelos predictivos interpretables.

Al igual que ID3, C4.5 construye un √°rbol de clasificaci√≥n seleccionando en cada nodo el atributo que mejor divide el conjunto de datos. Sin embargo, en lugar de usar solo la **ganancia de informaci√≥n**, C4.5 utiliza la **relaci√≥n de ganancia** (Gain Ratio). La relaci√≥n de ganancia normaliza la ganancia de informaci√≥n por la entrop√≠a intr√≠nseca del atributo, lo que ayuda a mitigar el sesgo de ID3 hacia atributos con muchos valores. Adem√°s, C4.5 introduce varias mejoras significativas:

* **Manejo de atributos continuos:** Puede discretizar atributos num√©ricos continuos dividiendo el rango en intervalos.
* **Manejo de valores faltantes:** Puede manejar datos con valores ausentes asignando una probabilidad fraccionada a cada rama posible.
* **Poda del √°rbol:** Implementa una t√©cnica de poda para reducir el sobreajuste, lo que implica eliminar ramas del √°rbol que no aportan significativamente a la clasificaci√≥n o que representan ruido en los datos.

En el contexto del **aprendizaje global vs. local**, C4.5, al igual que ID3 y CART, opera como un sistema de **aprendizaje local**. La construcci√≥n del √°rbol se logra a trav√©s de decisiones de divisi√≥n que se optimizan localmente en cada nodo, buscando la m√°xima homogeneidad o pureza en los subconjuntos resultantes. Esto le permite a C4.5 manejar eficazmente relaciones no lineales entre las variables independientes y dependientes. La idea es que, si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se puede aplicar de forma efectiva mediante esta **regresi√≥n ponderada localmente**, donde el algoritmo divide el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Al centrarse en divisiones √≥ptimas a nivel de subconjuntos, C4.5 ofrece una alternativa robusta a los m√©todos de aproximaci√≥n de funciones globales, que a veces pueden fallar en proporcionar una buena aproximaci√≥n cuando la relaci√≥n entre las variables no es lineal.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Categ√≥ricas y num√©ricas",
  "‚úÖ Captura relaciones no lineales",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es relevante",
  "‚ö†Ô∏è Moderadamente (puede hacer overfitting con ruido)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol interpretable)",
  "‚úÖ Relativamente r√°pido",
  "‚úÖ Recomendable para evitar sobreajuste",
  "‚ùå Demasiadas categor√≠as o ruido en datos"
)

detalles <- c(
  "Modelo supervisado tipo √°rbol de decisi√≥n",
  "Clasifica variables categ√≥ricas en ramas l√≥gicas",
  "Divide por puntos de corte para variables num√©ricas",
  "No asume forma funcional entre predictores y respuesta",
  "No necesita normalidad de errores",
  "Mejor si las observaciones son independientes",
  "No requiere varianzas constantes",
  "Datos ruidosos pueden afectar las ramas",
  "No se ve afectado por correlaciones entre predictores",
  "Salida f√°cil de visualizar y explicar",
  "Escala bien para tama√±os de muestra medianos",
  "Evita sobreajuste con poda y validaci√≥n",
  "Muchas clases con pocos datos pueden sobreajustar"
)

tabla_c45 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_c45 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir C4.5") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## C5.0  {-}  

**C5.0** es la versi√≥n m√°s reciente y avanzada de los algoritmos de √°rboles de decisi√≥n desarrollados por Ross Quinlan, sucediendo a ID3 y C4.5. Es un algoritmo propietario (aunque se ofrece una versi√≥n de c√≥digo abierto bajo ciertas licencias) y es ampliamente reconocido por su **rapidez**, **precisi√≥n** y **eficiencia** en la construcci√≥n de **√°rboles de decisi√≥n** y **reglas de clasificaci√≥n** para tareas de **clasificaci√≥n**.

Al igual que sus predecesores, C5.0 construye un √°rbol de clasificaci√≥n mediante la divisi√≥n recursiva de los datos en subconjuntos m√°s homog√©neos. Sin embargo, C5.0 incorpora mejoras significativas que lo hacen superior en muchos aspectos:

* **Velocidad y eficiencia:** Es notablemente m√°s r√°pido y m√°s eficiente en el uso de memoria que C4.5, lo que le permite manejar conjuntos de datos mucho m√°s grandes.
* **Impulso (Boosting):** C5.0 puede usar la t√©cnica de **boosting** (espec√≠ficamente, una variante de AdaBoost) para crear m√∫ltiples √°rboles de decisi√≥n y combinarlos para producir una predicci√≥n m√°s robusta y precisa. Esto reduce significativamente los errores de clasificaci√≥n y mejora la generalizaci√≥n.
* **Poda mejorada:** Ofrece t√©cnicas de poda m√°s sofisticadas para evitar el sobreajuste y producir √°rboles m√°s peque√±os y comprensibles.
* **Manejo de valores faltantes y atributos continuos:** Al igual que C4.5, maneja de manera efectiva valores faltantes y atributos num√©ricos continuos.
* **Generaci√≥n de reglas:** Adem√°s de √°rboles de decisi√≥n, C5.0 puede generar conjuntos de **reglas de clasificaci√≥n** concisas, que a menudo son m√°s f√°ciles de interpretar que un √°rbol completo.

En el contexto de la **regresi√≥n localmente ponderada**, C5.0, como los dem√°s algoritmos de √°rboles de decisi√≥n, opera bajo la premisa de un **aprendizaje local**. La construcci√≥n del √°rbol implica tomar decisiones de divisi√≥n √≥ptimas en cada nodo, bas√°ndose en la informaci√≥n local de ese subconjunto de datos. Si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n, que es su enfoque principal) se puede aplicar eficazmente al dividir el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Cada divisi√≥n en el √°rbol se puede ver como una forma de **regresi√≥n ponderada localmente**, donde el algoritmo se enfoca en aproximar la relaci√≥n dentro de un subespacio espec√≠fico del conjunto de datos. Esto convierte a C5.0 en una potente alternativa a los m√©todos de aproximaci√≥n de funciones globales, especialmente cuando la relaci√≥n entre las variables independientes y dependientes no es lineal y se busca un modelo interpretable y robusto.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Categ√≥ricas y num√©ricas",
  "‚úÖ Captura relaciones no lineales",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es relevante",
  "‚ö†Ô∏è Moderadamente (puede generar ramas excesivas)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol f√°cil de visualizar)",
  "‚úÖ Relativamente r√°pido en training",
  "‚úÖ Recomendable para evitar sobreajuste",
  "‚ùå Clases muy desbalanceadas sin ajuste"
)

detalles <- c(
  "Algoritmo de √°rbol de decisi√≥n avanzado basado en C4.5",
  "Clasifica en m√∫ltiples categor√≠as (tambi√©n multiclase)",
  "Divide autom√°ticamente variables num√©ricas con puntos de corte",
  "No asume funci√≥n lineal: usa ganancia de informaci√≥n y boosting",
  "No exige normalidad de errores",
  "Mejor si las instancias son independientes",
  "No requiere varianzas constantes",
  "Los valores extremos pueden influir en ramas profundas",
  "No se ve afectado por correlaci√≥n alta entre predictores",
  "Salida clara con reglas y pesos de boosting",
  "M√°s r√°pido que C4.5 y con opciones de boosting",
  "Usar k-fold o repeated CV para determinar par√°metros √≥ptimos",
  "Muchos atributos irrelevantes pueden generar sobreajuste"
)

tabla_c50 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_c50 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir C5.0") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```

## Chi-squared Automatic Interaction Detection (CHAID)  {-}    

**Chi-squared Automatic Interaction Detection (CHAID)** es un algoritmo de **√°rboles de decisi√≥n** utilizado principalmente para tareas de **clasificaci√≥n** y, en menor medida, para la **regresi√≥n** (aunque se aplica m√°s com√∫nmente a variables dependientes categ√≥ricas). La idea fundamental de CHAID es construir un √°rbol de decisi√≥n al encontrar las mejores divisiones en las variables predictoras que maximicen la significancia estad√≠stica de la relaci√≥n con la variable dependiente.

A diferencia de ID3, C4.5 o CART, que utilizan medidas de impureza como la entrop√≠a o el √≠ndice Gini, CHAID se basa en pruebas estad√≠sticas de **chi-cuadrado ($\chi^2$)** para identificar las divisiones √≥ptimas. Cuando la variable dependiente es nominal o ordinal, CHAID eval√∫a cada variable predictora para encontrar la combinaci√≥n de categor√≠as que sea m√°s significativamente diferente de otras combinaciones en t√©rminos de la distribuci√≥n de la variable dependiente. El algoritmo fusiona las categor√≠as de una variable predictora si no son significativamente diferentes, y luego selecciona la variable predictora y la divisi√≥n que resultan en el valor m√°s bajo de $p$ (es decir, la mayor significancia estad√≠stica) de la prueba $\chi^2$. Para variables dependientes continuas, se utiliza una prueba F.

En el contexto del **aprendizaje global vs. local**, CHAID opera como un sistema de **aprendizaje local**. La construcci√≥n del √°rbol es un proceso iterativo y recursivo donde las decisiones de divisi√≥n se toman en cada nodo bas√°ndose en la significancia estad√≠stica local de la interacci√≥n entre las variables predictoras y la variable dependiente. Esto le permite a CHAID descubrir relaciones complejas y no lineales en los datos. La idea es que, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n (o clasificaci√≥n) de manera efectiva mediante lo que se denomina **regresi√≥n ponderada localmente**. Esto se logra al dividir el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples, donde cada rama del √°rbol representa una regi√≥n del espacio de caracter√≠sticas donde las interacciones son evaluadas y modeladas localmente. Esto hace de CHAID una alternativa robusta a los m√©todos de aproximaci√≥n de funciones globales, especialmente cuando se busca un modelo interpretable y se quieren identificar las interacciones entre las variables de una manera estad√≠sticamente rigurosa.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n multinivel)",
  "‚úÖ Categ√≥ricas nativas (num√©ricas requieren binarizaci√≥n o discretizaci√≥n)",
  "‚úÖ No lineal (explora interacciones autom√°ticas con œá¬≤)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente (outliers categ√≥ricos pueden crear nodos muy peque√±os)",
  "‚úÖ Robusto a multicolinealidad (usa œá¬≤, no varianzas)",
  "‚úÖ Media (√°rboles con muchos nodos pueden resultar complejos)",
  "‚ö†Ô∏è Razonablemente r√°pido en datasets moderados, lento si hay muy altas cardinalidades",
  "‚úÖ Recomendable para determinar profundidad y grado de interacci√≥n",
  "‚ùå Variable objetivo continua o muchos niveles con pocas observaciones"
)

detalles <- c(
  "Construye un √°rbol de decisi√≥n usando pruebas œá¬≤ para detectar interacciones entre predictores y variable objetivo.",
  "Dise√±ado para clasificar en m√∫ltiples categor√≠as sin orden; puede manejar targets con m√°s de dos niveles.",
  "Funciona mejor con predictores categ√≥ricos; las variables num√©ricas deben transformarse en categor√≠as mediante binning.",
  "No asume ninguna forma funcional; detecta autom√°ticamente relaciones complejas basadas en œá¬≤.",
  "No depende de supuestos de normalidad de errores ni de forma de distribuci√≥n de residuos.",
  "Las instancias deben ser independientes; no es ideal para datos con fuerte dependencia temporal sin procesar.",
  "Homoscedasticidad no se eval√∫a, ya que no se basa en un t√©rmino de error param√©trico como OLS.",
  "Los valores extremos en variables categ√≥ricas con pocas observaciones pueden crear ramas muy espec√≠ficas, pero CHAID maneja cardinalidades moderadas.",
  "Al basarse en œá¬≤, CHAID no se ve afectado directamente por colinealidad, aunque variables muy correlacionadas pueden crear redundancia en las divisiones.",
  "Cada divisi√≥n se basa en pruebas de œá¬≤; el √°rbol resultante puede interpretarse visualmente, pero muchos niveles pueden reducir claridad.",
  "La creaci√≥n recursiva de nodos por agrupaci√≥n de categor√≠as es eficiente para conjuntos de datos moderados; puede volverse lento si hay muchas categor√≠as de predictores.",
  "Se usa validaci√≥n cruzada para podar el √°rbol y elegir el nivel √≥ptimo de interacci√≥n, equilibrando sesgo y varianza.",
  "No es adecuado si la variable objetivo es continua (sin discretizar) o si hay demasiados niveles con muy pocos casos en cada uno."
)

tabla_chaid <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_chaid %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CHAID",
             subtitle = "Chi-squared Automatic Interaction Detection (CHAID)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Decision Stump  {-}  

Un **Decision Stump** es el tipo de **√°rbol de decisi√≥n** m√°s simple y fundamental, compuesto por un **√∫nico nodo de decisi√≥n (la ra√≠z)** que se conecta directamente a los **nodos hoja**. La idea es que un *decision stump* toma una decisi√≥n de clasificaci√≥n o regresi√≥n bas√°ndose en una sola caracter√≠stica o atributo de entrada.

Aunque parece demasiado simple, la l√≥gica es que, a pesar de su simplicidad, un *decision stump* identifica el mejor umbral o categor√≠a dentro de una √∫nica variable para separar los datos de la manera m√°s efectiva posible. Para problemas de clasificaci√≥n, esto significa encontrar la caracter√≠stica que, por s√≠ sola, maximice alguna medida de **pureza** (como la ganancia de informaci√≥n, la impureza Gini, o la significancia chi-cuadrado) o minimice el error de clasificaci√≥n. Para regresi√≥n, buscar√° el punto de divisi√≥n en una sola caracter√≠stica que minimice la suma de los cuadrados de los errores.

En el contexto del **aprendizaje local vs. global**, un *decision stump* es inherentemente un sistema de **aprendizaje local**. Su "aprendizaje" se limita a encontrar la mejor divisi√≥n dentro de una √∫nica variable, lo que es una forma extrema de **regresi√≥n ponderada localmente**. Si los datos no se distribuyen linealmente, un *decision stump* no puede por s√≠ mismo modelar relaciones complejas. Sin embargo, su valor no reside en ser un modelo predictivo robusto por s√≠ mismo, sino en ser un **"clasificador d√©bil"** o **"regresor d√©bil"** que puede ser combinado en **conjuntos de modelos (ensembles)** m√°s potentes. Por ejemplo, los *decision stumps* son los bloques de construcci√≥n m√°s comunes para algoritmos de **boosting** como **AdaBoost**. En estos casos, m√∫ltiples *decision stumps* se entrenan secuencialmente, cada uno enfoc√°ndose en los errores que cometieron los *stumps* anteriores, sumando sus "aprendizajes locales" para formar un modelo global m√°s preciso. Esto contrarresta la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n simplificada)",
  "‚úÖ Num√©ricas y/o categ√≥ricas",
  "‚ö†Ô∏è Captura solo una divisi√≥n (muy simple, un solo nodo interno)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No relevante",
  "‚úÖ Relativamente robusto (poca complejidad)",
  "‚úÖ Ignora colinealidad (usa solo una variable)",
  "‚úÖ Muy alta (un solo umbral para dividir)",
  "‚úÖ Extremadamente r√°pido",
  "‚úÖ Se puede usar k-fold para evaluar estabilidad",
  "‚ùå No funciona bien si la relaci√≥n es compleja o no hay un buen umbral √∫nico"
)

detalles <- c(
  "Modelo de √°rbol con un solo nivel de decisi√≥n (un umbral en una sola variable).",
  "En clasificaci√≥n predice una clase binaria; en regresi√≥n, un valor medio para cada divisi√≥n.",
  "Selecciona la mejor variable con el punto de corte que maximiza ganancia (clasificaci√≥n) o reduce varianza (regresi√≥n).",
  "Solo ajusta un umbral, por lo que no modela interacciones ni no linealidades complejas.",
  "No hay supuestos param√©tricos de distribuci√≥n de errores.",
  "Mejor si las instancias no est√°n correlacionadas (por ejemplo, no aplica a series de tiempo sin agrupar).",
  "La varianza constante no se eval√∫a, pues el modelo es no param√©trico y muy simple.",
  "Un solo punto de corte es menos sensible a outliers en comparaci√≥n con √°rboles profundos, pero a√∫n puede verse afectado si un outlier define el umbral.",
  "Como solo usa una variable, no se ve afectado por correlaciones altas entre predictores.",
  "El modelo entero es resumido en un √∫nico umbral; f√°cil de explicar.",
  "Muy r√°pido de entrenar y predecir, pues solo se eval√∫a un umbral en un predictor.",
  "Es √∫til para comprobar si hay una √∫nica variable con gran poder predictivo; k-fold ayuda a validar que el umbral se mantenga estable.",
  "No sirve si el problema requiere varias divisiones, interacciones o relaciones no lineales profundas."
)

tabla_stump <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_stump %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Decision Stump",
             subtitle = "Decision Stump") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Conditional Decision Trees (Conditional Inference Trees - CITs)  {-}   

**Conditional Decision Trees**, often referred to as **Conditional Inference Trees (CITs)**, represent a class of **√°rboles de decisi√≥n** que abordan una limitaci√≥n importante de los algoritmos de √°rboles de decisi√≥n tradicionales como CART, ID3, y C4.5: el **sesgo en la selecci√≥n de variables**. Mientras que los algoritmos tradicionales pueden favorecer variables predictoras con muchas categor√≠as o valores continuos (debido a que estas variables tienen m√°s "oportunidades" de generar una divisi√≥n que parezca √≥ptima), los CITs emplean un enfoque basado en **pruebas estad√≠sticas** para seleccionar la mejor divisi√≥n.

La idea fundamental de los Conditional Decision Trees es que cada divisi√≥n en el √°rbol se basa en la **significancia estad√≠stica** de la asociaci√≥n entre las variables predictoras y la variable de respuesta. En lugar de seleccionar el atributo que maximiza una medida de impureza (como la ganancia de informaci√≥n o la impureza Gini), los CITs realizan una serie de **pruebas de inferencia condicional** (t√≠picamente **pruebas de permutaci√≥n**).

El algoritmo opera de la siguiente manera:
1.  En cada nodo, se eval√∫a una **hip√≥tesis nula** de independencia entre cada variable predictora y la variable de respuesta.
2.  Se calcula el valor de $p$ para cada variable predictora.
3.  La variable predictora con el valor de $p$ m√°s peque√±o (es decir, la asociaci√≥n m√°s estad√≠sticamente significativa) es seleccionada para la divisi√≥n, siempre y cuando este valor de $p$ sea menor que un umbral de significancia predefinido.
4.  Una vez seleccionada la variable, se encuentra la mejor divisi√≥n binaria (generalmente) dentro de esa variable para ese nodo.
5.  Este proceso se repite recursivamente hasta que no haya m√°s variables significativas para dividir o se alcance un criterio de parada.

En el contexto del **aprendizaje global vs. local**, los Conditional Decision Trees se pueden considerar como un enfoque de **aprendizaje local** con un fuerte respaldo estad√≠stico. Aunque el √°rbol resultante es un modelo global, cada decisi√≥n de divisi√≥n se toma localmente bas√°ndose en la inferencia estad√≠stica sobre la relaci√≥n entre las variables en ese subconjunto de datos. Esto significa que si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se aplica de forma efectiva mediante lo que se denomina **regresi√≥n ponderada localmente**. Al utilizar pruebas de significancia para las divisiones, los CITs evitan el problema de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en una √∫nica aproximaci√≥n global, ya que las divisiones se determinan por la evidencia estad√≠stica local. Esto los convierte en una alternativa robusta que ofrece una selecci√≥n de variables menos sesgada y modelos con una mayor interpretabilidad estad√≠stica.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ No lineal, usa tests condicionales para particionar",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No relevante",
  "‚ö†Ô∏è Moderadamente (consume tests basados en permutaciones)",
  "‚úÖ Robusto a colinealidad",
  "‚úÖ Alta (cada divisi√≥n est√° basada en criterios estad√≠sticos claros)",
  "‚ö†Ô∏è M√°s lento que CART en datasets grandes",
  "‚úÖ Recomendable para podar y evitar sobreajuste",
  "‚ùå Datos muy peque√±os por nodo o variables irrelevantes"
)

detalles <- c(
  "Construye √°rboles basados en test de independencia condicional (ctree).",
  "Permite tanto regresi√≥n (valor continuo) como clasificaci√≥n multinivel.",
  "Acepta variables num√©ricas y categ√≥ricas sin necesidad de dummies.",
  "Detecta relaciones complejas y no lineales usando tests basados en permutaciones.",
  "No exige que los residuos sigan una distribuci√≥n espec√≠fica.",
  "Ideal si las observaciones no est√°n correlacionadas en el tiempo.",
  "No requiere homoscedasticidad porque no se basa en un modelo param√©trico de error.",
  "Los outliers pueden afectar el c√°lculo de los tests, aunque es m√°s robusto que CART.",
  "El algoritmo ctree no se ve afectado por predictores altamente correlacionados.",
  "Los √°rboles generados son f√°ciles de visualizar y explicar.",
  "Para cada divisi√≥n realiza m√∫ltiples tests, por lo que es m√°s lento en datos muy grandes.",
  "Usar k-fold o repeated CV para elegir la profundidad y evitar sobreajuste.",
  "No es apto si tienes muy pocas observaciones en cada parto o muchas variables irrelevantes."
)

tabla_ctree <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_ctree %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir ctree",
             subtitle = "Conditional Decision Trees") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## M5 (Model Tree) {-}  

**M5**, a menudo referida como **M5'** o **M5P** (su implementaci√≥n en el software Weka), es un algoritmo de **√°rboles de decisi√≥n** espec√≠ficamente dise√±ado para **tareas de regresi√≥n**, es decir, para predecir valores num√©ricos continuos. Desarrollado por Ross Quinlan en 1992 y luego mejorado por Wang y Witten en 1997, M5 se destaca de los √°rboles de regresi√≥n tradicionales (como los de CART que solo tienen valores constantes en las hojas) al incorporar **modelos de regresi√≥n lineal** en sus nodos hoja.

La idea fundamental de M5 es combinar la interpretabilidad de un √°rbol de decisi√≥n con la capacidad predictiva de los modelos de regresi√≥n lineal. Funciona en dos etapas principales:

1.  **Construcci√≥n del √Årbol:** M5 construye un √°rbol de decisi√≥n de forma recursiva, similar a otros algoritmos de √°rboles. Sin embargo, en lugar de usar medidas de impureza para clasificaci√≥n, utiliza la **reducci√≥n de la desviaci√≥n est√°ndar (SDR)** como criterio de divisi√≥n. El algoritmo selecciona el atributo y el punto de divisi√≥n que maximizan la reducci√≥n de la desviaci√≥n est√°ndar del valor objetivo en los subconjuntos resultantes. Este proceso contin√∫a hasta que el n√∫mero de instancias en un nodo es muy peque√±o o la desviaci√≥n est√°ndar es muy baja.

2.  **Poda y Suavizado:** Una vez construido el √°rbol inicial, M5 lo **poda** para evitar el sobreajuste. En lugar de reemplazar los nodos con un valor constante, los nodos hoja (y a veces nodos internos) son reemplazados por **modelos de regresi√≥n lineal multivariados**. Estos modelos lineales se construyen utilizando los atributos relevantes para esa rama del √°rbol. Adem√°s, M5 aplica un proceso de **suavizado** para compensar las discontinuidades bruscas que podr√≠an surgir entre las predicciones de modelos lineales adyacentes. Este suavizado ajusta el valor predicho en una hoja bas√°ndose en las predicciones de los modelos en los nodos a lo largo de la ruta desde la ra√≠z hasta esa hoja.

En el contexto del **aprendizaje global vs. local**, M5 es un h√≠brido interesante. Por un lado, la construcci√≥n del √°rbol se basa en decisiones de divisi√≥n **locales**, buscando la mejor reducci√≥n de la desviaci√≥n est√°ndar en cada nodo. Esto permite a M5 modelar relaciones no lineales, ya que "si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n de manera ponderada localmente". El √°rbol divide el problema de regresi√≥n global en m√∫ltiples subproblemas m√°s peque√±os. Por otro lado, al tener **modelos de regresi√≥n lineal** en las hojas, M5 incorpora un componente de **aproximaci√≥n de funci√≥n local** m√°s sofisticado que un simple valor constante. Estos modelos lineales son "locales" para la regi√≥n de datos que representa esa hoja, pero internamente son modelos globales para esa subregi√≥n. Esto permite a M5 ofrecer una alternativa potente a las aproximaciones de funciones puramente globales, especialmente cuando las relaciones entre las variables son complejas y se benefician de una combinaci√≥n de particionamiento del espacio y modelado lineal dentro de esas particiones.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua",
  "‚úÖ Num√©ricas (categ√≥ricas procesar como dummies)",
  "‚úÖ Lineal por segmentos (√°rbol + regresi√≥n en hojas)",
  "‚ùå No requiere estrictamente",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es requisito",
  "‚ö†Ô∏è Moderadamente (outliers pueden distorsionar regresiones locales)",
  "‚úÖ Relativamente robusto (regresi√≥n en hojas mitiga algo la colinealidad)",
  "‚ö†Ô∏è Media (√°rbol complejo, hojas lineales m√°s interpretables)",
  "‚ö†Ô∏è Moderado (depende de n√∫mero de nodos y atributos)",
  "‚úÖ Recomendable para optimizar n√∫mero de nodos y hojas",
  "‚ùå Muchos nodos con pocos casos o ruido elevado"
)

detalles <- c(
  "Modelo de √°rbol de regresi√≥n con ajustes lineales en cada hoja.",
  "Predice valores continuos, p. ej., precio, consumo, etc.",
  "Requiere que variables categ√≥ricas se conviertan a indicadores antes de ajuste.",
  "Combina particiones basadas en atributos con regresiones m√∫ltiples en hojas.",
  "No exige que los residuos en cada hoja sean normales, aunque mejora inferencia.",
  "Ideal si las observaciones son independientes; en series de tiempo hay que agrupar.",
  "La varianza constante no es cr√≠tica, cada hoja ajusta localmente.",
  "Los extremos pueden afectar las regresiones locales; poda puede mitigar esto.",
  "El m√©todo divide el espacio antes de ajustar, reduciendo efectos de colinealidad.",
  "El √°rbol completo puede ser grande, pero cada hoja contiene una funci√≥n lineal clara.",
  "Construcci√≥n y poda del √°rbol m√°s costosas que OLS, pero razonables para tama√±os medianos.",
  "Ayuda a determinar n√∫mero √≥ptimo de hojas y complejidad del √°rbol.",
  "Si hay muy pocas observaciones por hoja o ruido demasiado alto, las regresiones locales fallan."
)

tabla_m5 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_m5 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir M5",
             subtitle = "M5 model tree algorithm") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


<!--chapter:end:02-decision_tree.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üåü **3. Ensambles (Ensemble Methods)** {-}

**Ejemplos:** Random Forest, AdaBoost, XGBoost, LightGBM   
**Cu√°ndo usarlo:**   

* Cuando buscas alto rendimiento en clasificaci√≥n o regresi√≥n tabular.
* Competencias de datos (como Kaggle).

**Ventajas:** Alta precisi√≥n, robustez.   
**Limitaciones:** Dif√≠cil de interpretar; m√°s costosos computacionalmente.

---

## Random Forest  {-}   

**Random Forest** es un algoritmo de **aprendizaje conjunto (ensemble learning)** altamente popular y potente, utilizado tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**. Fue desarrollado por Leo Breiman en 2001 y se basa en la idea de combinar las predicciones de m√∫ltiples **√°rboles de decisi√≥n** para lograr una mayor precisi√≥n y robustez que un solo √°rbol. La fuerza de Random Forest reside en dos conceptos clave: **bagging (bootstrap aggregation)** y la **aleatoriedad en la selecci√≥n de caracter√≠sticas**.

La idea fundamental detr√°s de Random Forest es construir un "bosque" de √°rboles de decisi√≥n de una manera espec√≠fica:

1.  **Bagging (Bootstrap Aggregation):** En lugar de entrenar un solo √°rbol en todo el conjunto de datos, Random Forest entrena cada √°rbol en una **muestra de arranque (bootstrap sample)** diferente. Una muestra de arranque es un subconjunto del conjunto de datos original, muestreado con reemplazo. Esto significa que algunos puntos de datos pueden aparecer varias veces en una muestra, mientras que otros pueden no aparecer en absoluto. Este muestreo genera diversidad entre los √°rboles.

2.  **Aleatoriedad en la Selecci√≥n de Caracter√≠sticas:** Cuando cada √°rbol se construye, en cada paso de divisi√≥n (nodo), Random Forest no considera todas las caracter√≠sticas disponibles. En cambio, solo considera un **subconjunto aleatorio de caracter√≠sticas** para encontrar la mejor divisi√≥n. Esta aleatoriedad adicional (adem√°s del muestreo de arranque) descorrelaciona a√∫n m√°s los √°rboles, lo que es crucial para el rendimiento del algoritmo. Si los √°rboles estuvieran altamente correlacionados, el error de un √°rbol promedio no se reducir√≠a al promediar.

Una vez que se han construido numerosos √°rboles (t√≠picamente cientos o miles), las predicciones se combinan: para **clasificaci√≥n**, se utiliza la **votaci√≥n por mayor√≠a** (la clase m√°s votada por los √°rboles individuales); para **regresi√≥n**, se calcula el **promedio** de las predicciones de todos los √°rboles.

En el contexto del **aprendizaje global vs. local**, Random Forest se puede considerar como un sistema de **aprendizaje global** que se construye a partir de componentes de **aprendizaje local**. Cada √°rbol individual en el bosque es un sistema de aprendizaje local (como CART, que divide el problema en subproblemas m√°s peque√±os). Sin embargo, al combinar las predicciones de muchos de estos √°rboles, Random Forest logra una **aproximaci√≥n de funci√≥n global** muy robusta y flexible. La ventaja es que, si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) mediante una forma sofisticada de **regresi√≥n ponderada localmente**. La combinaci√≥n de √°rboles diversos y descorrelacionados mitiga la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Random Forest sobresale en capturar relaciones complejas y no lineales, manejar grandes conjuntos de datos con muchas caracter√≠sticas y es menos propenso al sobreajuste que un solo √°rbol de decisi√≥n grande.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n)",
  "‚úÖ Captura relaciones no lineales e interacciones complejas",
  "‚ùå No requiere",
  "‚úÖ Deseable pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚úÖ Robusto a outliers (por agregaci√≥n)",
  "‚úÖ Robusto (selecciona subconjuntos aleatorios)",
  "‚ö†Ô∏è Moderada (dif√≠cil interpretar cientos de √°rboles)",
  "‚ö†Ô∏è Lento con muchos √°rboles o datos grandes",
  "‚úÖ Recomendado usar k-fold",
  "‚ùå Puede sobreajustar si no se ajustan hiperpar√°metros (e.g. profundidad, n√∫mero de √°rboles)"
)

detalles <- c(
  "Ensamble de √°rboles de decisi√≥n, cada uno entrenado en una muestra bootstrap y usando un subconjunto aleatorio de predictores.",
  "En clasificaci√≥n predice la clase mayoritaria entre √°rboles; en regresi√≥n, el promedio de predicciones.",
  "Acepta muchas variables y selecciona autom√°ticamente las m√°s relevantes por importancia.",
  "Al generar m√∫ltiples √°rboles, capta interacciones no lineales sin necesidad de especificarlas.",
  "No hay supuestos sobre la distribuci√≥n de los errores.",
  "Los √°rboles individuales pueden manejar correlaci√≥n leve; el ensamble mitiga la dependencia.",
  "No necesita homogeneidad de varianza en los errores residuales.",
  "Cada √°rbol es poco sensible a outliers, y la agregaci√≥n mejora robustez.",
  "Reduce el problema de colinealidad al seleccionar subconjuntos de variables por √°rbol.",
  "Es dif√≠cil de explicar, aunque se pueden usar m√©tricas de importancia de variables.",
  "Puede volverse lento si se entrenan miles de √°rboles en datasets muy grandes.",
  "Cross-validation ayuda a evitar overfitting y evaluar generalizaci√≥n.",
  "No es ideal si el interpretabilidad es cr√≠tica o el tiempo computacional es limitado."
)

tabla_rf <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rf %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir random forest",
             subtitle = "Random Forest") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Gradient Boosting Machines (GBM)  {-}   

**Gradient Boosting Machines (GBM)** es un algoritmo de **aprendizaje conjunto (ensemble learning)** extremadamente potente y vers√°til, utilizado para **clasificaci√≥n**, **regresi√≥n** y otras tareas predictivas. A diferencia de Random Forest que construye √°rboles de forma independiente en paralelo (bagging), GBM construye los √°rboles de forma **secuencial** y aditiva. La idea central es que cada nuevo √°rbol en el conjunto intenta corregir los errores residuales (residuos) del conjunto de √°rboles construidos previamente.

El concepto fundamental detr√°s de GBM es el **impulso (boosting)**, donde los modelos "d√©biles" (generalmente √°rboles de decisi√≥n, a menudo √°rboles poco profundos o "stumps") se combinan para formar un modelo "fuerte". GBM logra esto de una manera espec√≠fica:

1.  **Modelo Inicial:** Comienza con una predicci√≥n inicial para todos los datos (por ejemplo, el valor promedio para regresi√≥n o la probabilidad logar√≠tmica para clasificaci√≥n).
2.  **C√°lculo de Residuos (Pseudo-residuos):** En cada iteraci√≥n, el algoritmo calcula los **residuos** (o m√°s precisamente, los "pseudo-residuos" o gradientes negativos de la funci√≥n de p√©rdida) entre los valores reales y las predicciones actuales del modelo. Estos residuos representan los "errores" que el modelo actual no ha podido capturar.
3.  **Entrenamiento de un Nuevo √Årbol:** Se entrena un nuevo √°rbol de decisi√≥n para **predecir estos residuos**. Este √°rbol es t√≠picamente peque√±o y d√©bil, dise√±ado para centrarse en las √°reas donde el modelo actual tiene los mayores errores.
4.  **Actualizaci√≥n del Modelo:** La predicci√≥n de este nuevo √°rbol se a√±ade a la predicci√≥n acumulada del modelo existente, multiplicada por una **tasa de aprendizaje (learning rate)**. Esta tasa de aprendizaje controla el tama√±o del paso de cada √°rbol, evitando que el modelo se sobreajuste r√°pidamente.
5.  **Iteraci√≥n:** Este proceso se repite para un n√∫mero predefinido de iteraciones, o hasta que una m√©trica de rendimiento deje de mejorar. Cada nuevo √°rbol contribuye a reducir los errores restantes.

En el contexto del **aprendizaje global vs. local**, GBM es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol individual en el proceso de boosting es un sistema de aprendizaje local (como los *decision stumps* o √°rboles poco profundos) que se enfoca en una parte espec√≠fica del error. Sin embargo, la combinaci√≥n aditiva y secuencial de estos modelos "d√©biles" produce un modelo predictivo global altamente sofisticado y preciso. Si los datos no se distribuyen linealmente, GBM aplica el concepto de regresi√≥n (o clasificaci√≥n) mediante una forma incremental y adaptativa de **regresi√≥n ponderada localmente**. Al centrarse en los errores residuales, GBM aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su capacidad para minimizar la funci√≥n de p√©rdida de forma gradual y dirigida lo hace excepcionalmente eficaz para modelar relaciones complejas y no lineales, a menudo logrando un rendimiento superior en muchos problemas del mundo real.


```{r, echo = FALSE}

criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n)",
  "‚úÖ Captura no linealidades e interacciones complejas v√≠a boosting",
  "‚ùå No requiere",
  "‚úÖ Deseable pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (los outliers pueden influir en √°rboles individuales)",
  "‚úÖ Robusto (los arboles manejan colinealidad localmente)",
  "‚ö†Ô∏è Baja (modelo de tipo caja negra con varios √°rboles secuenciales)",
  "‚ö†Ô∏è Lento con muchos √°rboles, datos grandes o par√°metros altos",
  "‚úÖ Recomendable con k-fold o repeated CV para ajuste de hiperpar√°metros",
  "‚ùå Si se tienen pocos datos, muchas categor√≠as o ruido alto"
)

detalles <- c(
  "Ensamble de √°rboles secuenciales donde cada √°rbol corrige errores del anterior.",
  "En clasificaci√≥n se combinan probabilidades; en regresi√≥n se promedian predicciones.",
  "Funciona con muchas variables y aprende la importancia autom√°ticamente.",
  "Construye √°rboles d√©biles que se enfocan en los errores residuales previos.",
  "No impone supuestos en la distribuci√≥n de los errores.",
  "Los datos deben ser observaciones independientes; sensible a dependencias temporales.",
  "No requiere varianza constante puesto que se basa en √°rboles.",
  "Los outliers pueden exagerar los gradientes y forzar ajustes extremos en √°rboles individuales.",
  "Los √°rboles reducen impacto de colinealidad, pero m√∫ltiples √°rboles pueden still complicarla.",
  "Dif√≠cil de interpretar el conjunto; se pueden usar importance plots o SHAP para explicaci√≥n.",
  "La construcci√≥n secuencial de cientos de √°rboles puede ser costosa en tiempo y memoria.",
  "La validaci√≥n cruzada ayuda a determinar tasa de aprendizaje, n√∫mero de √°rboles y profundidad.",
  "No es ideal cuando se tienen muy pocos datos o categor√≠as con pocos ejemplos."
)

tabla_gbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_gbm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GBM",
             subtitle = "Gradient Boosting Machines (GBM)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```

## Boosting  {-}   

**Boosting** es una t√©cnica de **aprendizaje conjunto (ensemble learning)** que busca transformar un conjunto de **modelos "d√©biles" o "base"** en un **modelo "fuerte" o "preciso"**. La idea fundamental es construir modelos de forma **secuencial** e **iterativa**, donde cada nuevo modelo se centra en corregir los errores o deficiencias de los modelos construidos en las iteraciones anteriores. A diferencia del *bagging* (como en Random Forest), donde los modelos se entrenan de forma independiente, el *boosting* es intr√≠nsecamente secuencial y adaptativo.

El concepto clave de Boosting radica en la asignaci√≥n de **pesos** o en el enfoque en los **errores residuales**:

1.  **Iteraciones Secuenciales:** El proceso comienza con un modelo base inicial (a menudo simple, como un *decision stump*).
2.  **Enfoque en los Errores:** En cada iteraci√≥n subsiguiente, el algoritmo presta m√°s atenci√≥n a los ejemplos que fueron clasificados (o predichos) incorrectamente por los modelos anteriores, o a los errores residuales no explicados. Esto se logra ya sea **re-ponderando** los datos (dando m√°s peso a los ejemplos mal clasificados) o **ajustando** el nuevo modelo para que prediga los residuos de los modelos anteriores.
3.  **Combinaci√≥n Ponderada:** Las predicciones de todos los modelos d√©biles se combinan, generalmente a trav√©s de una suma ponderada, donde los modelos m√°s precisos reciben un mayor peso en la predicci√≥n final.

La fuerza del boosting radica en su capacidad para reducir el **sesgo** y la **varianza** del modelo final, al construir un modelo complejo a partir de componentes simples que se complementan entre s√≠.

En el contexto del **aprendizaje global vs. local**, Boosting es una estrategia de **aprendizaje global** que opera construyendo una serie de aproximaciones **locales**. Cada modelo "d√©bil" que se entrena en una iteraci√≥n puede verse como una forma de **regresi√≥n ponderada localmente** (o clasificaci√≥n ponderada localmente), ya que se enfoca en una parte espec√≠fica del espacio de las caracter√≠sticas o en los datos con mayor error. El proceso iterativo de Boosting busca corregir estos errores localizados. Si los datos no se distribuyen linealmente, el boosting permite que el concepto de regresi√≥n (o clasificaci√≥n) se aplique de manera muy flexible y potente. La capacidad de concentrarse en los "errores" residuales aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Al ensamblar muchos modelos d√©biles que se adaptan a los errores de los anteriores, Boosting construye un modelo final que es una aproximaci√≥n de funci√≥n global altamente adaptable y precisa. Algoritmos como AdaBoost y Gradient Boosting Machines (GBM) son ejemplos prominentes de esta t√©cnica.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para algunas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones complejas mediante aprendizaje secuencial",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio para muchos algoritmos",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (puede ajustar demasiado a outliers si no se controla)",
  "‚úÖ Robusto (cada iteraci√≥n utiliza un subconjunto ponderado de datos)",
  "‚ö†Ô∏è Baja (modelo en su conjunto es tipo caja negra)",
  "‚ö†Ô∏è Lento con muchos √°rboles o altas iteraciones",
  "‚úÖ Recomendable con k-fold o repeated CV para ajustar tasa de aprendizaje y n√∫mero de iteraciones",
  "‚ùå Si se tienen pocos datos, alto ruido o target muy desbalanceado sin ajuste"
)

detalles <- c(
  "Ensamble supervisado que combina varios modelos d√©biles (ej. √°rboles peque√±os) de forma secuencial",
  "En clasificaci√≥n se usan votaciones ponderadas; en regresi√≥n se suman predicciones graduadas",
  "Acepta variables mixtas; algunas bibliotecas requieren convertir categ√≥ricas en dummies",
  "Construye modelos d√©biles en cada iteraci√≥n, enfoc√°ndose en muestras mal clasificadas o con alto residuo",
  "No exige distribuci√≥n de errores, ya que se basa en funci√≥n de p√©rdida sin supuestos param√©tricos",
  "Mejor si los ejemplos son independientes; puede usar t√©cnicas especiales para datos correlacionados",
  "No asume varianza constante, usa funci√≥n de p√©rdida directa para optimizar",
  "Los outliers pueden recibir peso excesivo en iteraciones posteriores, por lo que es necesario regularizar o usar robust loss",
  "La selecci√≥n de variables se hace impl√≠citamente, reduciendo el impacto de colinealidad",
  "Dif√≠cil de interpretar directamente; se pueden usar m√©tricas de importancia, SHAP o partial dependence para explicaci√≥n",
  "Cada iteraci√≥n entrena un modelo d√©bil, por lo que puede ser costoso si el n√∫mero de iteraciones es alto",
  "CV ayuda a determinar la tasa de aprendizaje (learning rate), n√∫mero de iteraciones y complejidad de base learners",
  "No es adecuado si hay muy pocos ejemplos, alta dimensionalidad con poco se√±al o target extremadamente desequilibrado"
)

tabla_boosting <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_boosting %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Boosting",
             subtitle = "Boosting") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Bootstrapped Aggregation (Bagging)  {-}    

**Bootstrapped Aggregation (Bagging)** es una t√©cnica de **aprendizaje conjunto (ensemble learning)** dise√±ada para mejorar la estabilidad y precisi√≥n de los algoritmos de aprendizaje autom√°tico, particularmente para reducir la **varianza** en los modelos. Fue introducida por Leo Breiman en 1996 y es la base de algoritmos muy populares como **Random Forest**. La idea fundamental de Bagging es entrenar m√∫ltiples versiones de un mismo modelo base en diferentes subconjuntos del conjunto de datos original y luego combinar sus predicciones.

El proceso central de Bagging implica dos pasos clave:

1.  **Muestreo Bootstrap:** En lugar de entrenar un √∫nico modelo en todo el conjunto de datos de entrenamiento, Bagging crea **m√∫ltiples conjuntos de datos de arranque (bootstrap samples)**. Cada muestra de arranque se crea seleccionando aleatoriamente, **con reemplazo**, un n√∫mero de observaciones igual al tama√±o del conjunto de datos original. Esto significa que algunos puntos de datos pueden aparecer varias veces en una muestra de arranque, mientras que otros pueden no aparecer en absoluto. Este muestreo aleatorio introduce diversidad entre los conjuntos de entrenamiento para cada modelo.

2.  **Agregaci√≥n (Aggregation):** Una vez que se han entrenado **m√∫ltiples modelos base independientes** (por ejemplo, √°rboles de decisi√≥n) en cada una de estas muestras de arranque, sus predicciones se combinan. Para tareas de **clasificaci√≥n**, la combinaci√≥n se realiza mediante **votaci√≥n por mayor√≠a** (la clase m√°s votada). Para tareas de **regresi√≥n**, las predicciones se promedian. Esta agregaci√≥n de predicciones de modelos diversos reduce la varianza y, por lo tanto, hace que el modelo final sea m√°s robusto y menos propenso al sobreajuste que un solo modelo entrenado en todo el conjunto de datos.

En el contexto del **aprendizaje global vs. local**, Bagging es una estrategia que combina las ventajas de los modelos de **aprendizaje local** para construir una **aproximaci√≥n de funci√≥n global** m√°s estable. Cada modelo base (ej. un √°rbol de decisi√≥n) que se entrena en una muestra de arranque puede considerarse un sistema de aprendizaje local, ya que toma decisiones basadas en el subconjunto de datos que le ha sido asignado. Sin embargo, al entrenar estos m√∫ltiples modelos en paralelo y luego agregarlos, Bagging construye un modelo final que es una aproximaci√≥n de funci√≥n global altamente adaptable. La ventaja principal es que, si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se puede aplicar eficazmente mediante esta forma de **regresi√≥n ponderada localmente** (donde los "pesos" son impl√≠citos a trav√©s de la agregaci√≥n de predicciones de modelos entrenados en subconjuntos aleatorios de datos). Bagging aborda el problema de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo al promediar o votar las predicciones de m√∫ltiples modelos, lo que reduce la varianza y mejora la generalizaci√≥n.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Captura relaciones no lineales al promediar m√∫ltiples modelos",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚úÖ Robusto (cada bootstrap reduce el impacto de outliers)",
  "‚úÖ Robusto (la agregaci√≥n mitiga colinealidad)",
  "‚ö†Ô∏è Moderada (dif√≠cil interpretar conjunto de modelos)",
  "‚ö†Ô∏è Moderado (depende del n√∫mero de √°rboles y tama√±o del dataset)",
  "‚úÖ Recomendable usar k-fold",
  "‚ùå No es ideal con muy pocos datos o si los base learners son demasiado simples"
)

detalles <- c(
  "Ensamble supervisado que ajusta varios modelos (usualmente √°rboles) sobre muestras bootstrap y promedia predicciones.",
  "En clasificaci√≥n predice la clase m√°s votada; en regresi√≥n, promedia los valores predichos.",
  "Acepta todo tipo de variables; las categ√≥ricas deben codificarse adecuadamente.",
  "Al promediar m√∫ltiples modelos, reduce varianza y captura no linealidades impl√≠citamente.",
  "No impone supuestos sobre la distribuci√≥n de errores.",
  "Los datos deben ser independientes; funciona peor en datos con fuerte autocorrelaci√≥n sin ajuste.",
  "No requiere varianza constante puesto que se basa en agregaci√≥n de m√∫ltiples predicciones.",
  "Cada muestra bootstrap y √°rbol es menos sensible a valores extremos; la agregaci√≥n aumenta robustez.",
  "La selecci√≥n aleatoria de subconjuntos y bootstrap reduce el efecto de predictores correlacionados.",
  "El modelo final es un conjunto de muchos √°rboles, lo que dificulta su explicaci√≥n directa.",
  "Entrenar cientos de √°rboles toma tiempo, pero es paralelizable; la predicci√≥n es relativamente r√°pida.",
  "CV ayuda a ajustar par√°metros como n√∫mero de √°rboles y profundidad m√°xima de cada √°rbol.",
  "Con pocos ejemplos, los bootstrap no aportan diversidad suficiente; si los base learners son muy simples, no capturan bien patrones complejos."
)

tabla_bagging <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bagging %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir bagging",
             subtitle = "Bootstrapped Aggregation (Bagging) ") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Adaptive Boosting (AdaBoost)  {-}  

**AdaBoost (Adaptive Boosting)** es uno de los algoritmos de **boosting** m√°s influyentes y el primero en ser propuesto con √©xito, desarrollado por Yoav Freund y Robert Schapire en 1995. Es una t√©cnica de **aprendizaje conjunto (ensemble learning)** utilizada principalmente para **clasificaci√≥n**, aunque sus principios pueden extenderse a la regresi√≥n. La idea fundamental de AdaBoost es construir un modelo fuerte combinando secuencialmente las predicciones de m√∫ltiples **clasificadores "d√©biles" o "base"**, y lo hace prestando m√°s atenci√≥n a los ejemplos que los modelos anteriores clasificaron incorrectamente.

El funcionamiento de AdaBoost se basa en un sistema de **re-ponderaci√≥n de datos** en cada iteraci√≥n:

1.  **Inicializaci√≥n de Pesos:** Se asigna un peso inicial igual a cada ejemplo de entrenamiento.
2.  **Entrenamiento del Clasificador D√©bil:** En cada iteraci√≥n, se entrena un clasificador d√©bil (a menudo un **Decision Stump**, que es un √°rbol de decisi√≥n de un solo nivel) en el conjunto de datos actual. Este clasificador se enfoca en minimizar el error ponderado.
3.  **C√°lculo del Error Ponderado:** Se calcula el error del clasificador d√©bil, teniendo en cuenta los pesos de los ejemplos. Los ejemplos mal clasificados tienen un mayor impacto en este error.
4.  **Actualizaci√≥n de Pesos de Datos:** Los pesos de los ejemplos mal clasificados por el clasificador actual son **aumentados**, mientras que los pesos de los ejemplos correctamente clasificados son **disminuidos**. Esto asegura que el siguiente clasificador d√©bil se enfoque m√°s en los ejemplos que son dif√≠ciles de clasificar.
5.  **C√°lculo del Peso del Clasificador:** Se asigna un peso (o "contribuci√≥n") al clasificador d√©bil actual en funci√≥n de su precisi√≥n. Los clasificadores m√°s precisos reciben un peso mayor en la predicci√≥n final del conjunto.
6.  **Combinaci√≥n de Predicciones:** Las predicciones finales del modelo AdaBoost se obtienen mediante una **suma ponderada** de las predicciones de todos los clasificadores d√©biles.

En el contexto del **aprendizaje global vs. local**, AdaBoost es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada clasificador d√©bil que se entrena en una iteraci√≥n puede verse como una forma de **regresi√≥n ponderada localmente** (o, m√°s precisamente, clasificaci√≥n ponderada localmente), ya que ajusta su enfoque bas√°ndose en los ejemplos que el modelo combinado anterior no pudo clasificar bien. Al iterar y ajustar los pesos de los datos, AdaBoost se enfoca progresivamente en las regiones del espacio de caracter√≠sticas donde el modelo actual tiene un rendimiento deficiente. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de clasificaci√≥n (y por extensi√≥n, las ideas de regresi√≥n) de manera altamente adaptativa. La capacidad de AdaBoost para concentrarse en los "errores" m√°s dif√≠ciles aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. El resultado es un clasificador global muy preciso y robusto, capaz de modelar relaciones complejas y no lineales, que es una combinaci√≥n ponderada de muchas decisiones locales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n adaptada)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para algunas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones mediante reponderaci√≥n iterativa",
  "‚ùå No requiere supuestos de normalidad en los residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers pueden obtener demasiado peso durante iteraciones)",
  "‚úÖ Robusto (reduce colinealidad al iterar sobre subconjuntos ponderados)",
  "‚ö†Ô∏è Baja (modelo resultante es especie de ‚Äôcaja negra‚Äô)",
  "‚ö†Ô∏è Lento con muchas iteraciones o datos grandes",
  "‚úÖ Recomendable para ajustar tasa de aprendizaje y n√∫mero de iteraciones",
  "‚ùå No es ideal con datos muy ruidosos o clases extremadamente desbalanceadas sin t√©cnicas adicionales"
)

detalles <- c(
  "Ensamble supervisado que combina varios modelos d√©biles (ej. √°rboles simples) ajustando pesos seg√∫n errores anteriores.",
  "En clasificaci√≥n ajusta pesos para mal clasificados; en regresi√≥n, adapta predicci√≥n por minimizaci√≥n de p√©rdida.",
  "Puede trabajar con datos mixtos; para variables categ√≥ricas suele usar codificaci√≥n de dummies.",
  "Cada iteraci√≥n repondera observaciones dif√≠ciles, enfoc√°ndose en patrones que previos modelos no capturaron.",
  "No impone distribuci√≥n de errores; se basa en funci√≥n de p√©rdida, no en supuestos param√©tricos.",
  "Funciona mejor si cada observaci√≥n es independiente; sensible a dependencias temporales si no se corrige.",
  "No requiere varianza constante, ya que funciona sobre el error iterativo en lugar de residuos tradicionales.",
  "Outliers dif√≠ciles de clasificar tienden a recibir mayor peso, lo que puede sesgar el ensamble si no se controla el learning rate.",
  "La reponderaci√≥n de muestras aten√∫a el efecto de predictores correlacionados, pues cada iteraci√≥n puede focalizarse en subconjuntos distintos.",
  "Es complejo desentra√±ar la contribuci√≥n de cada modelo d√©bil; se pueden usar m√©tricas de importancia o SHAP para interpretaci√≥n.",
  "Cada iteraci√≥n entrena un modelo d√©bil; muchas iteraciones o modelos complejos pueden ralentizar el entrenamiento.",
  "K-fold o repeated CV ayudan a elegir tasa de aprendizaje (learning rate) y n√∫mero de iteraciones (trials).",
  "No conviene con instancias altamente ruidosas: se puede sobreajustar r√°pidamente si la tasa de aprendizaje es alta o no se regula iteraciones."
)

tabla_adaboost <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_adaboost %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir adaboost",
             subtitle = "AdaBoost") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Stacked Generlization (Blending) {-}   

**Stacked Generalization**, com√∫nmente conocido como **Stacking**, y su variante **Blending**, son t√©cnicas avanzadas de **aprendizaje conjunto (ensemble learning)** que buscan combinar las predicciones de m√∫ltiples modelos de aprendizaje autom√°tico para obtener un rendimiento predictivo superior al de cualquier modelo individual. La idea fundamental es que, en lugar de simplemente promediar o votar las predicciones, se entrena un **modelo de segundo nivel (meta-modelo)** para aprender a combinar √≥ptimamente las predicciones de los modelos de primer nivel (modelos base).

El proceso de Stacking generalmente implica dos o m√°s "capas" de modelos:

1.  **Modelos Base (Nivel 0):** En la primera capa, se entrenan m√∫ltiples modelos de aprendizaje autom√°tico diversos (pueden ser de diferentes tipos, como √°rboles de decisi√≥n, m√°quinas de vectores de soporte, redes neuronales, etc.). Estos modelos base se entrenan sobre el conjunto de datos de entrenamiento original (o en particiones del mismo).

2.  **Generaci√≥n de Meta-Caracter√≠sticas:** Las predicciones generadas por estos modelos base sobre un conjunto de datos "fuera de muestra" (que no se us√≥ para entrenar los modelos base, t√≠picamente a trav√©s de validaci√≥n cruzada k-fold) se utilizan como **nuevas caracter√≠sticas** o "meta-caracter√≠sticas". Estas meta-caracter√≠sticas, junto con la variable objetivo original, forman un nuevo conjunto de datos de entrenamiento para el meta-modelo.

3.  **Meta-Modelo (Nivel 1):** En la segunda capa, se entrena un **meta-modelo** (a menudo un modelo m√°s simple, como regresi√≥n lineal, regresi√≥n log√≠stica o un √°rbol de decisi√≥n poco profundo) utilizando estas meta-caracter√≠sticas como entrada y la variable objetivo original como salida. El meta-modelo aprende la relaci√≥n entre las predicciones de los modelos base y la respuesta verdadera, y por lo tanto, c√≥mo "pesar" o "combinar" esas predicciones de la mejor manera.

**Blending** es una variaci√≥n m√°s sencilla de Stacking. La principal diferencia es c√≥mo se generan las meta-caracter√≠sticas para el meta-modelo. En Blending, se reserva una **subdivisi√≥n de validaci√≥n (holdout set)** del conjunto de entrenamiento original. Los modelos base se entrenan en la parte restante del conjunto de entrenamiento, y luego sus predicciones sobre este conjunto de validaci√≥n se utilizan directamente como meta-caracter√≠sticas para entrenar el meta-modelo. Esto simplifica el proceso de validaci√≥n cruzada, pero el meta-modelo se entrena con menos datos.

En el contexto del **aprendizaje global vs. local**, Stacking/Blending es una estrategia de **aprendizaje global** que explota el poder de m√∫ltiples **aproximaciones de funci√≥n local** (los modelos base) para construir un modelo final altamente sofisticado. Cada modelo base, dependiendo de su naturaleza, puede ser un sistema de aprendizaje local que descubre patrones en subregiones de datos. Sin embargo, el meta-modelo aprende una funci√≥n de combinaci√≥n global sobre las predicciones de estos modelos base. Si los datos no se distribuyen linealmente, Stacking/Blending aplica el concepto de regresi√≥n (o clasificaci√≥n) de una manera muy flexible. Al permitir que un modelo de segundo nivel aprenda a combinar las predicciones de diversos modelos, supera la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Es particularmente eficaz en competiciones de machine learning donde se busca el m√°ximo rendimiento, ya que aprovecha las fortalezas complementarias de diferentes algoritmos. Sin embargo, puede ser computacionalmente intensivo y m√°s dif√≠cil de interpretar que los modelos individuales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua (depende de los modelos base)",
  "‚úÖ Num√©ricas y/o categ√≥ricas (codificaci√≥n seg√∫n modelos base)",
  "‚úÖ Captura relaciones complejas v√≠a combinaci√≥n de modelos base",
  "‚ùå No exige supuestos de normalidad en residuos",
  "‚úÖ Deseable, pero no obligatorio (mejor si observaciones independientes)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers afectan modelos base individuales)",
  "‚ö†Ô∏è Puede verse afectado (depende de base learners y correlated features)",
  "‚ö†Ô∏è Baja (modelo meta dif√≠cil de interpretar directamente)",
  "‚ö†Ô∏è Lento en entrenamiento y predicci√≥n, seg√∫n n√∫mero de base learners",
  "‚úÖ Esencial (usar CV anidada para entrenar meta-modelo)",
  "‚ùå Si datos muy escasos o muy ruidosos, riesgo de sobreajuste"
)

detalles <- c(
  "Ensamble supervisado que combina varias predicciones (base learners) mediante un modelo meta.",
  "El meta-modelo acepta la salida de modelos base; puede predecir clases o valores continuos.",
  "Usa predictores originales para los base learners; algunos requieren dummies, otros no.",
  "Aprende patrones no lineales e interacciones complejas a trav√©s de m√∫ltiples capas.",
  "No impone distribuci√≥n normal: cada base learner tiene sus propios supuestos.",
  "Ideal si cada muestra es independiente; sensibles a dependencias en validaciones cruzadas.",
  "No requiere varianza constante, ya que se basa en agregaci√≥n de predicciones.",
  "Modelos base (p. ej. ARBOTS, SVM) pueden verse influenciados por valores extremos;",
  "Modelos base diversificados reducen colinealidad, pero meta-modelo puede verse afectado.",
  "Dif√≠cil atribuir importancia directa; se pueden usar t√©cnicas como SHAP para interpretaci√≥n.",
  "Entrenamiento de m√∫ltiples base learners y meta-modelo incrementa tiempo; predicci√≥n tambi√©n m√°s lenta.",
  "Usar validaci√≥n cruzada anidada: inner folds para entrenar base learners y stacking, outer folds para evaluar.",
  "No recomendable si hay muy pocos datos (stacking requiere dividir en folds) o si los base learners no aportan diversidad."
)

tabla_stacking <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_stacking %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir blending",
             subtitle = "Stacked Generlizaation (Blending)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Gradient Boosted Regression Trees (GBRT)  {-}   

**Gradient Boosted Regression Trees (GBRT)**, a menudo conocida como **Gradient Boosting Machines (GBM)** cuando los modelos base son √°rboles de decisi√≥n de regresi√≥n, es una t√©cnica de **aprendizaje conjunto (ensemble learning)** extremadamente potente y ampliamente utilizada para tareas de **regresi√≥n** (predicci√≥n de valores num√©ricos continuos) y tambi√©n puede adaptarse para **clasificaci√≥n**. Su fortaleza radica en su capacidad para construir un modelo predictivo robusto y preciso mediante la combinaci√≥n secuencial de m√∫ltiples √°rboles de decisi√≥n "d√©biles".

La idea central de GBRT se basa en el principio de **boosting**, donde cada nuevo √°rbol en el conjunto se entrena para **corregir los errores residuales** (la diferencia entre los valores reales y las predicciones acumuladas del modelo hasta ese momento) de los √°rboles construidos en las iteraciones anteriores. Este proceso es iterativo y aditivo:

1.  **Modelo Inicial:** El proceso comienza con una predicci√≥n inicial simple para todos los datos, a menudo el valor promedio de la variable objetivo.
2.  **C√°lculo de Pseudo-Residuos:** En cada iteraci√≥n, GBRT calcula los "pseudo-residuos", que son los **gradientes negativos de la funci√≥n de p√©rdida** con respecto a la predicci√≥n actual. Para la p√©rdida cuadr√°tica media (com√∫n en regresi√≥n), estos pseudo-residuos son simplemente los errores tradicionales (valor real - predicci√≥n).
3.  **Entrenamiento de un √Årbol de Regresi√≥n:** Se entrena un nuevo **√°rbol de decisi√≥n de regresi√≥n** (que es un "aprendiz d√©bil", a menudo un √°rbol poco profundo o un *decision stump*) para **predecir estos pseudo-residuos**. El √°rbol busca los mejores puntos de divisi√≥n para reducir estos errores.
4.  **Actualizaci√≥n del Modelo:** La predicci√≥n de este nuevo √°rbol de regresi√≥n se a√±ade a la predicci√≥n acumulada del modelo existente, pero se escala por una **tasa de aprendizaje (learning rate)**. Esta tasa de aprendizaje es un hiperpar√°metro crucial que controla la "contribuci√≥n" de cada nuevo √°rbol y ayuda a prevenir el sobreajuste.
5.  **Iteraci√≥n:** Los pasos 2 a 4 se repiten para un n√∫mero predefinido de iteraciones. Cada nuevo √°rbol se enfoca en las deficiencias del modelo combinado anterior, refinando gradualmente la predicci√≥n.

En el contexto del **aprendizaje global vs. local**, GBRT es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol de regresi√≥n individual es un sistema de aprendizaje local que divide el espacio de caracter√≠sticas y aprende patrones en subregiones. Sin embargo, el proceso de boosting, al combinar estos √°rboles secuencialmente para reducir los errores residuales globales, construye una **aproximaci√≥n de funci√≥n global** altamente flexible y precisa. La clave es que, si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n de manera muy efectiva a trav√©s de esta **regresi√≥n ponderada localmente**. Al centrarse en los errores que el modelo actual no puede explicar, GBRT aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Es excepcionalmente potente para capturar relaciones complejas y no lineales, y es ampliamente utilizado en diversas aplicaciones, desde la predicci√≥n de precios hasta la optimizaci√≥n de rutas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para ciertas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones complejas mediante boosting de √°rboles",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers pueden influir en √°rboles individuales)",
  "‚úÖ Robusto (los √°rboles reducen el impacto de colinealidad localmente)",
  "‚ö†Ô∏è Baja (modelo en su conjunto es ‚Äúcaja negra‚Äù)",
  "‚ö†Ô∏è Lento con muchos √°rboles o datos extensos",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajuste de hiperpar√°metros",
  "‚ùå No es ideal si hay muy pocos datos o ruido excesivo"
)

detalles <- c(
  "Ensamble de √°rboles de regresi√≥n secuenciales donde cada √°rbol corrige errores del anterior mediante gradiente.",
  "Predice valores continuos sumando las predicciones ponderadas de m√∫ltiples √°rboles d√©biles.",
  "Funciona con variables mixtas; las categ√≥ricas suelen transformarse en dummies.",
  "Cada nuevo √°rbol se enfoca en los residuos del modelo anterior, capturando patrones complejos.",
  "No impone distribuci√≥n normal porque optimiza una funci√≥n de p√©rdida (por ejemplo, MSE) directamente.",
  "Mejor si las observaciones no est√°n correlacionadas en el tiempo; ajustar para series si es necesario.",
  "No requiere varianza constante puesto que se basa en √°rboles, no en un modelo param√©trico de errores.",
  "Los valores extremos pueden provocar ajustes excesivos en √°rboles individuales; usar tasa de aprendizaje baja ayuda a mitigar.",
  "Los √°rboles reducen el impacto de variables correlacionadas, aunque m√∫ltiples iteraciones pueden complicar interpretaciones.",
  "Dif√≠cil de interpretar directamente; se puede usar importancia de variables o herramientas como SHAP para explicaci√≥n.",
  "Cada iteraci√≥n entrena un √°rbol nuevo; muchos √°rboles o gran profundidad de √°rbol incrementan el tiempo de entrenamiento.",
  "CV ayuda a determinar par√°metros como tasa de aprendizaje (`learning_rate`), n√∫mero de √°rboles (`n.trees`) y profundidad m√°xima (`max_depth`).",
  "No es adecuado cuando el dataset es muy peque√±o o extremadamente ruidoso, ya que puede sobreajustar f√°cilmente."
)

tabla_gbrt <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_gbrt %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GBRT",
             subtitle = "Gradient Boosted Regression Trees (GBRT)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Extreme Gradient Boosting (XGBoost)  {-}    

**XGBoost (Extreme Gradient Boosting)** es una implementaci√≥n optimizada y altamente eficiente del algoritmo de **Gradient Boosting Machines (GBM)**, ampliamente reconocida por su **velocidad**, **rendimiento** y **escalabilidad** en problemas de **clasificaci√≥n** y **regresi√≥n**. Gan√≥ una inmensa popularidad debido a su √©xito en numerosas competiciones de *machine learning* (como Kaggle). Aunque se basa en los principios de GBM, XGBoost introduce varias mejoras clave que lo hacen superior en muchos escenarios.

La idea fundamental de XGBoost, al igual que GBM, es construir un modelo aditivo de forma **secuencial**, donde cada nuevo √°rbol intenta corregir los errores residuales del conjunto de √°rboles previos. Sin embargo, XGBoost optimiza este proceso con las siguientes caracter√≠sticas:

1.  **Paralelizaci√≥n:** Aunque el *boosting* es inherentemente secuencial, XGBoost permite la paralelizaci√≥n de la construcci√≥n de los √°rboles individuales. Por ejemplo, en el paso de b√∫squeda de la mejor divisi√≥n, puede evaluar las posibles divisiones en paralelo a trav√©s de m√∫ltiples n√∫cleos de CPU.
2.  **Regularizaci√≥n:** Incorpora t√©rminos de **regularizaci√≥n L1 (Lasso)** y **L2 (Ridge)** en la funci√≥n de costo para controlar la complejidad del modelo y evitar el sobreajuste. Esto es crucial para la generalizaci√≥n.
3.  **Manejo de Valores Faltantes:** Tiene una capacidad incorporada para manejar valores faltantes en los datos, permitiendo al algoritmo aprender la mejor direcci√≥n para los valores ausentes.
4.  **Poda por Profundidad (Depth-First Search):** A diferencia de muchos algoritmos de √°rboles que crecen nivel por nivel, XGBoost puede usar un enfoque de poda por profundidad, lo que a menudo resulta en √°rboles m√°s eficientes.
5.  **Cach√©-Aware Computing:** Optimiza el acceso a la memoria para manejar grandes conjuntos de datos de manera eficiente.
6.  **Flexibilidad de Funci√≥n de P√©rdida:** Permite el uso de funciones de p√©rdida personalizadas, lo que lo hace adaptable a una amplia gama de problemas.

En el contexto del **aprendizaje global vs. local**, XGBoost es una poderosa estrategia de **aprendizaje global** que se construye iterativamente a partir de componentes de **aprendizaje local**. Cada √°rbol de regresi√≥n (o clasificaci√≥n) individual es un "aprendiz d√©bil" que se enfoca en las deficiencias del modelo acumulado. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) de manera altamente sofisticada mediante esta **regresi√≥n ponderada localmente**. Al centrarse en los errores residuales y optimizar el proceso de manera rigurosa, XGBoost aborda de manera excepcional la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su combinaci√≥n de precisi√≥n, velocidad y capacidad para manejar grandes conjuntos de datos lo ha convertido en uno de los algoritmos m√°s populares y efectivos en la pr√°ctica del *machine learning*.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para dummies o label encoding)",
  "‚úÖ Captura no linealidades e interacciones complejas v√≠a √°rboles en boosting",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (puede sobreajustar a outliers si no regula)",
  "‚úÖ Robusto (reduce efecto de colinealidad al usar √°rboles secuenciales)",
  "‚ö†Ô∏è Baja (modelo complejo y tipo ‚Äôcaja negra‚Äô)",
  "‚úÖ Muy r√°pido y escalable (implementaci√≥n optimizada, paralelizable)",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajustar hiperpar√°metros",
  "‚ùå No es ideal con datos muy peque√±os, ruido alto o target extremadamente desbalanceado sin ajuste"
)

detalles <- c(
  "Ensamble supervisado que combina m√∫ltiples √°rboles d√©biles optimizados con gradiente descendente acelerado.",
  "En regresi√≥n predice valores continuos; en clasificaci√≥n combina probabilidades o clases mediante log-loss o multiclass objectives.",
  "Acepta variables mixtas; las categ√≥ricas deben convertirse a formatos compatibles (p. ej. factor numerico, one-hot encoding).",
  "Cada iteraci√≥n ajusta un nuevo √°rbol enfoc√°ndose en los residuos del modelo anterior, capturando patrones complejos.",
  "No impone distribuci√≥n param√©trica de errores, ya que optimiza funciones de p√©rdida directamente.",
  "Funciona mejor si cada muestra es independiente; sensible a series de tiempo sin preparaci√≥n apropiada.",
  "No requiere varianza constante, porque basa la optimizaci√≥n en gradientes del loss, no en supuestos de error.",
  "Los outliers dif√≠ciles de predecir pueden recibir demasiado peso en iteraciones sucesivas; usar _learning_rate_ bajo y _max_depth_ peque√±o para regular.",
  "Los √°rboles reducen el impacto de variables altamente correlacionadas, aunque m√∫ltiples iteraciones pueden a√∫n privilegiar caracter√≠sticas correlacionadas.",
  "Dif√≠cil interpretar directamente cada √°rbol; se utilizan m√©tricas de importancia, SHAP values o partial dependence plots para explicaci√≥n.",
  "Implementaci√≥n en C++ altamente optimizada (CPU/GPU), permite entrenamiento muy r√°pido incluso con millones de filas.",
  "Validaci√≥n cruzada anidada o simple ayuda a elegir hiperpar√°metros como `eta` (learning_rate), `nrounds` (n√∫mero de √°rboles), `max_depth`, `subsample`, `colsample_bytree`.",
  "No conviene con datasets muy peque√±os, ya que puede sobreajustar; tampoco si el ruido es muy alto y no se regula bien la complejidad."
)

tabla_xgboost <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_xgboost %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir xgboost",
             subtitle = "XGBoost")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Light Gradient Boosting Machine (LightGBM)  {-}   

**LightGBM (Light Gradient Boosting Machine)** es otro algoritmo de **Gradient Boosting Machines (GBM)** de alto rendimiento, desarrollado por Microsoft. Est√° dise√±ado para ser **extremadamente r√°pido** y **eficiente** en el uso de memoria, especialmente con grandes conjuntos de datos, sin sacrificar una precisi√≥n significativa. Al igual que XGBoost, ha ganado popularidad en competiciones de *machine learning* por su velocidad y capacidad para manejar grandes vol√∫menes de datos.

La idea fundamental de LightGBM es la misma que la de otros algoritmos de boosting: construir un modelo aditivo de forma **secuencial**, donde cada nuevo √°rbol intenta corregir los errores residuales del modelo combinado anterior. Sin embargo, LightGBM introduce varias optimizaciones clave para lograr su notable eficiencia:

1.  **Gradient-based One-Side Sampling (GOSS):** A diferencia de XGBoost que usa todas las instancias para cada iteraci√≥n, GOSS se enfoca en las instancias que tienen un **mayor gradiente** (es decir, las que contribuyen m√°s al error). Descarta las instancias con gradientes peque√±os o las muestrea con menos frecuencia, lo que acelera el entrenamiento sin perder demasiada precisi√≥n.
2.  **Exclusive Feature Bundling (EFB):** EFB agrupa caracter√≠sticas mutuamente exclusivas (es decir, caracter√≠sticas que rara vez toman valores distintos de cero al mismo tiempo) en un solo "bundle". Esto reduce el n√∫mero de caracter√≠sticas y acelera el c√°lculo del histograma sin afectar la precisi√≥n.
3.  **Histogram-based Algorithm:** En lugar de construir √°rboles en una forma de pre-orden que es com√∫n en muchos algoritmos (lo que puede ser lento al enumerar todos los puntos de divisi√≥n), LightGBM utiliza un **algoritmo basado en histogramas**. Convierte los valores de las caracter√≠sticas continuas en *bins* discretos. Esto acelera significativamente el proceso de b√∫squeda del mejor punto de divisi√≥n.
4.  **Leaf-wise (Best-first) Tree Growth:** A diferencia de la mayor√≠a de los √°rboles de decisi√≥n que crecen nivel por nivel (como en XGBoost), LightGBM crece el √°rbol **"hoja por hoja" (leaf-wise)**. Esto significa que en cada paso, selecciona la hoja con la mayor reducci√≥n de p√©rdida y la divide. Este enfoque puede llevar a √°rboles m√°s profundos y asim√©tricos que pueden ser m√°s precisos para el mismo n√∫mero de nodos, aunque puede ser m√°s propenso al sobreajuste (lo cual se mitiga con la regularizaci√≥n).

En el contexto del **aprendizaje global vs. local**, LightGBM, al igual que otros algoritmos de boosting, es una estrategia de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol que se entrena es un "aprendiz d√©bil" que se enfoca en las deficiencias residuales del modelo acumulado. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) de manera muy eficiente mediante esta **regresi√≥n ponderada localmente**. Al centrarse en los errores y optimizar los c√°lculos, LightGBM aborda de manera sobresaliente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su √©nfasis en la velocidad y la eficiencia lo hace ideal para conjuntos de datos muy grandes o escenarios donde el tiempo de entrenamiento es una preocupaci√≥n cr√≠tica.  



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n apropiada)",
  "‚úÖ Captura no linealidades e interacciones mediante √°rboles en boosting",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (los outliers pueden influir en pesos de hojas)",
  "‚úÖ Robusto (usa histogram-based split que aten√∫a colinealidad)",
  "‚ö†Ô∏è Baja (modelo complejo tipo ‚Äòcaja negra‚Äô)",
  "‚úÖ Muy r√°pido y escalable (optimized gradient-based)",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajustar hiperpar√°metros",
  "‚ùå No conviene con datos muy peque√±os o muy ruidosos sin regularizaci√≥n"
)

detalles <- c(
  "Ensamble supervisado que entrenan √°rboles de decisi√≥n usando histogram-based gradient boosting.",
  "En regresi√≥n predice valores continuos; en clasificaci√≥n maximiza log-loss u otras funciones objetivo.",
  "Acepta variables mixtas; las categ√≥ricas deben convertirse a formato num√©rico o usar encoding interno.",
  "Cada iteraci√≥n ajusta un √°rbol enfoc√°ndose en los residuos del anterior, capturando patrones complejos.",
  "No impone distribuci√≥n param√©trica de errores; optimiza la funci√≥n de p√©rdida directamente.",
  "Funciona mejor si las muestras son independientes; sensible a series de tiempo sin preparaci√≥n adecuada.",
  "No requiere varianza constante, dado que es un m√©todo basado en √°rbol, no en supuestos de error.",
  "Los valores extremos pueden afectar el c√°lculo de gradientes y splits; usar regularizaci√≥n y par√°metros de manejo de outliers.",
  "La divisi√≥n basada en histogramas reduce el impacto de predictores altamente correlacionados.",
  "Dif√≠cil interpretar cada √°rbol; se utilizan m√©tricas de importancia y herramientas como SHAP para explicaci√≥n.",
  "Implementaci√≥n en C++ altamente optimizada que permite entrenamiento muy r√°pido incluso con grandes vol√∫menes de datos.",
  "CV ayuda a elegir par√°metros como `learning_rate`, `num_leaves`, `max_depth`, `feature_fraction`, `bagging_fraction`.",
  "No es ideal si el dataset es muy peque√±o, pues el boosting puede sobreajustar; tampoco con mucho ruido sin regularizaci√≥n adecuada."
)

tabla_lightgbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lightgbm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LightGBM",
             subtitle = "LightGBM")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


<!--chapter:end:03-ensemble.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üß† **4. Redes Neuronales y Deep Learning** {-}  

**Ejemplos:** MLP, CNN, RNN, Transformers   
**Cu√°ndo usarlo:**   

* Im√°genes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).
* Grandes vol√∫menes de datos no estructurados.

**Ventajas:** Muy poderosos para datos complejos y no estructurados.   
**Limitaciones:** Requieren mucha data y poder computacional. Menor interpretabilidad.

---


## Radial Basis Function Network (RBFN)  {-}      

**Radial Basis Function Network (RBFN)** es un tipo de **red neuronal artificial** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**. A diferencia de las redes neuronales multicapa perceptr√≥n tradicionales que utilizan funciones de activaci√≥n sigmoide o ReLU, las RBFN emplean **funciones de base radial** como sus funciones de activaci√≥n en la capa oculta. Su estructura es t√≠picamente m√°s simple que un perceptr√≥n multicapa, consistiendo generalmente en tres capas: una capa de entrada, una capa oculta con neuronas de base radial, y una capa de salida.

La idea fundamental de una RBFN radica en su capacidad para modelar relaciones no lineales al mapear datos de entrada a un espacio de caracter√≠sticas de mayor dimensi√≥n donde pueden ser **linealmente separables** (para clasificaci√≥n) o donde una **funci√≥n lineal** puede aproximar la relaci√≥n (para regresi√≥n). Esto se logra a trav√©s de las neuronas de la capa oculta, cada una de las cuales representa un "centro" en el espacio de caracter√≠sticas.

El funcionamiento de una RBFN implica:

1.  **Capa de Entrada:** Recibe las caracter√≠sticas de entrada.
2.  **Capa Oculta (Neuronas de Base Radial):** Cada neurona en esta capa tiene un **centro** ($c_i$) y un **radio (o desviaci√≥n est√°ndar, $\sigma_i$)**. La funci√≥n de activaci√≥n de estas neuronas (com√∫nmente una **funci√≥n Gaussiana**) calcula la **distancia** entre el vector de entrada ($x$) y el centro de la neurona ($c_i$), y luego aplica la funci√≥n de base radial. Cuanto m√°s cerca est√© la entrada del centro de la neurona, mayor ser√° la activaci√≥n de esa neurona.
    $$\phi_i(x) = \exp\left(-\frac{\|x - c_i\|^2}{2\sigma_i^2}\right)$$
    Donde $\phi_i(x)$ es la salida de la neurona $i$, $\|x - c_i\|$ es la distancia euclidiana entre la entrada $x$ y el centro $c_i$, y $\sigma_i$ es el radio (ancho) de la funci√≥n Gaussiana.
3.  **Capa de Salida:** Las salidas de las neuronas de la capa oculta se combinan linealmente (ponderadas por unos coeficientes, $w_{ij}$) para producir la salida final de la red. Para regresi√≥n, es una suma ponderada; para clasificaci√≥n, a menudo se usa una funci√≥n de activaci√≥n softmax.
    $$y_j = \sum_{i=1}^{M} w_{ij}\phi_i(x)$$
    Donde $y_j$ es la salida $j$, $M$ es el n√∫mero de neuronas ocultas, y $w_{ij}$ son los pesos de la capa de salida.

En el contexto del **aprendizaje global vs. local**, las RBFN son intr√≠nsecamente sistemas de **aprendizaje local**. Cada neurona de la capa oculta es sensible a una **regi√≥n espec√≠fica** del espacio de entrada, definida por su centro y su radio. La red como un todo es una combinaci√≥n de estas respuestas locales. Si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se aplica de forma muy eficaz mediante esta naturaleza de **regresi√≥n ponderada localmente**. Las RBFN pueden aproximar cualquier funci√≥n continua con la suficiente cantidad de neuronas de base radial. Esto aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo global, ya que la red puede adaptarse localmente a las caracter√≠sticas de diferentes regiones del espacio de datos. Son particularmente √∫tiles para problemas de aproximaci√≥n de funciones, series de tiempo y reconocimiento de patrones.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas (requiere normalizaci√≥n), Categ√≥ricas como dummies",
  "‚úÖ Captura no linealidades mediante funciones base radiales",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "‚ùå No asume varianza constante",
  "‚ö†Ô∏è Moderadamente (centros pueden verse alterados por outliers)",
  "‚ö†Ô∏è Puede influir en la selecci√≥n de centros, pero no tan cr√≠tico como en OLS",
  "‚ö†Ô∏è Baja (la capa oculta con RBF es dif√≠cil de interpretar)",
  "‚ö†Ô∏è Moderada (depende de n√∫mero de centros y dimensiones)",
  "‚úÖ Recomendable para ajustar n√∫mero de bases y spread",
  "‚ùå Datos muy grandes o alta dimensionalidad sin reducci√≥n, mucho ruido"
)

detalles <- c(
  "Red neuronal de una capa oculta con funciones radial basis como activaci√≥n.",
  "Para regresi√≥n predice un valor continuo; para clasificaci√≥n usa votaci√≥n o softmax sobre salidas.",
  "Requiere que las caracter√≠sticas num√©ricas est√©n escaladas; las categ√≥ricas deben convertirse a variables indicadoras.",
  "Cada neurona oculta calcula una funci√≥n gaussiana (u otra RBF) centrada en un punto, captando curvas suaves.",
  "No impone distribuci√≥n normal en los errores, pues optimiza en funci√≥n de m√≠nimos cuadrados o cross-entropy.",
  "Funciona mejor si las observaciones son independientes; sensible a estructuras de dependencia sin modelar.",
  "No requiere homocedasticidad ya que no se basa en un modelo param√©trico de error con varianza fija.",
  "Los valores extremos pueden desplazar los centros de las RBF, afectando la forma del modelo.",
  "La colinealidad puede dificultar la determinaci√≥n de centros √≥ptimos, pero no invalida el ajuste.",
  "Las neuronas ocultas representan combinaciones complejas de caracter√≠sticas, por lo que el modelo es tipo 'caja negra'.",
  "El entrenamiento implica fijar o aprender centros y spreads; para muchos centros o dimensiones altas, el costo crece r√°pido.",
  "Se usa CV para elegir el n√∫mero de bases (centros) y el par√°metro de ancho (`sigma` o `spread`) para evitar sobreajuste.",
  "No conviene cuando hay decenas de miles de caracter√≠sticas sin reducci√≥n previa o cuando el ruido es muy alto."
)

tabla_rbfn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rbfn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RBFN",
             subtitle = "Radial Basis Function Network (RBFN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Perceptron  {-}     

El **Perceptron** es el algoritmo de **aprendizaje supervisado** m√°s simple y uno de los primeros modelos de **redes neuronales artificiales**, propuesto por Frank Rosenblatt en 1957. Est√° dise√±ado para tareas de **clasificaci√≥n binaria**, es decir, para decidir si una entrada pertenece a una de dos clases posibles. Su idea fundamental es modelar c√≥mo una neurona biol√≥gica podr√≠a tomar decisiones.

El funcionamiento de un Perceptron es bastante directo:

1.  **Entradas y Pesos:** Recibe m√∫ltiples **entradas** (caracter√≠sticas) y a cada entrada se le asigna un **peso**. Estos pesos representan la importancia de cada caracter√≠stica.
2.  **Suma Ponderada:** Las entradas se multiplican por sus respectivos pesos y se suman. A esta suma se le a√±ade un **t√©rmino de sesgo (bias)**.
    $$z = \sum_{i=1}^{n} w_i x_i + b$$
    Donde $x_i$ son las entradas, $w_i$ son los pesos, $b$ es el sesgo, y $n$ es el n√∫mero de entradas.
3.  **Funci√≥n de Activaci√≥n:** El resultado de la suma ponderada ($z$) se pasa a trav√©s de una **funci√≥n de activaci√≥n** (generalmente una funci√≥n escal√≥n o *step function*). Esta funci√≥n decide la salida final, que es 1 si la suma excede un umbral (o 0 si no lo excede). Para el Perceptron original, la salida es binaria.
    $$\text{salida} = \begin{cases} 1 & \text{si } z \geq \text{umbral} \\ 0 & \text{si } z < \text{umbral} \end{cases}$$
4.  **Aprendizaje (Regla de Perceptron):** El Perceptron aprende ajustando sus pesos de forma iterativa. Si la predicci√≥n es incorrecta, los pesos se actualizan para reducir el error en la siguiente iteraci√≥n. La regla de actualizaci√≥n de pesos es:
    $$w_i^{\text{nuevo}} = w_i^{\text{anterior}} + \alpha \cdot (y - \hat{y}) \cdot x_i$$
    Donde $\alpha$ es la tasa de aprendizaje, $y$ es el valor real, y $\hat{y}$ es la predicci√≥n del Perceptron.

En el contexto del **aprendizaje global vs. local**, el Perceptron es un sistema de **aprendizaje global** por naturaleza. Busca encontrar un **hiperplano de separaci√≥n lineal** √∫nico que divida el espacio de caracter√≠sticas en dos regiones. La idea es que, si los datos son **linealmente separables** (es decir, si existe una l√≠nea, plano o hiperplano que puede separar perfectamente las dos clases), el Perceptron est√° garantizado para converger y encontrar esa soluci√≥n.

Sin embargo, precisamente porque busca una soluci√≥n lineal global, si los datos no se distribuyen linealmente (es decir, no son linealmente separables), el Perceptron **no puede encontrar una soluci√≥n convergente** y no puede aprender la relaci√≥n. Esto ilustra la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" cuando se busca una soluci√≥n global r√≠gida. El Perceptron original no puede aplicar el concepto de regresi√≥n ponderada localmente ni adaptarse a complejidades no lineales, a diferencia de modelos posteriores como las redes neuronales multicapa con funciones de activaci√≥n no lineales o los algoritmos de √°rboles de decisi√≥n. A pesar de esta limitaci√≥n, el Perceptron sent√≥ las bases para el desarrollo posterior de redes neuronales m√°s complejas.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas (requiere normalizar), Categ√≥ricas como dummies",
  "‚ö†Ô∏è Aprendizaje lineal: separabilidad lineal requerida",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers pueden cambiar el hiperplano)",
  "‚ö†Ô∏è Afecta la convergencia si est√° muy alta",
  "‚ö†Ô∏è Baja (modelo b√°sico de una capa sin capas ocultas)",
  "‚úÖ Muy r√°pido para datasets medianos",
  "‚úÖ √ötil para evaluar margen de separaci√≥n",
  "‚ùå No sirve si las clases no son linealmente separables"
)

detalles <- c(
  "Red neuronal de una sola capa que ajusta un hiperplano separador.",
  "Dise√±ado para clasificaci√≥n binaria; no predice valores continuos.",
  "Todas las features deben ser num√©ricas y escaladas; las categ√≥ricas deben convertirse en indicadores.",
  "Busca maximizar el margen de separaci√≥n lineal entre dos clases; no captura no linealidades.",
  "No exige distribuci√≥n normal de errores ya que optimiza con perceptr√≥n simple.",
  "Funciona mejor si las instancias son independientes; sensible a dependencias temporales sin ajuste.",
  "No se basa en varianza de errores; el algoritmo actualiza pesos sin supuestos de varianza.",
  "Los valores extremos cercanos al margen pueden forzar ajustes bruscos de pesos.",
  "La colinealidad puede ralentizar la convergencia, aunque no impide la definici√≥n de hiperplano.",
  "F√°cil de entender: el peso de cada caracter√≠stica indica direcci√≥n del hiperplano.",
  "Entrenamiento r√°pido usando regla de aprendizaje por error; escalable a datos medianos.",
  "Se usa CV para ajustar tasa de aprendizaje y n√∫mero de √©pocas para evitar bajo/sobreajuste.",
  "In√∫til si las clases no se pueden separar linealmente; requiere extensiones (por ejemplo, kernel) para no linealidad."
)

tabla_perceptron <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_perceptron %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir perceptron",
             subtitle = "Perceptron")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Back - Propagation  {-}  

**Back-Propagation (Retropropagaci√≥n)** es el algoritmo fundamental de entrenamiento utilizado para ajustar los pesos de las **redes neuronales artificiales multicapa (MLP)**. La idea central de Back-Propagation es calcular la **contribuci√≥n de cada peso al error global de la red** y luego ajustar esos pesos para reducir dicho error, propagando la informaci√≥n del error "hacia atr√°s" desde la capa de salida hasta la capa de entrada.

A diferencia del Perceptron, que solo puede aprender patrones linealmente separables, Back-Propagation permite entrenar redes neuronales profundas con m√∫ltiples capas ocultas y funciones de activaci√≥n no lineales, lo que les permite modelar relaciones complejas y no lineales en los datos.

El funcionamiento de Back-Propagation se divide en dos fases principales que se repiten iterativamente:

1.  **Fase de Propagaci√≥n hacia Adelante (Forward Pass):**
    * Las entradas se pasan a trav√©s de la red, desde la capa de entrada, a trav√©s de las capas ocultas, hasta la capa de salida.
    * En cada neurona, se calcula la suma ponderada de sus entradas (incluido el sesgo) y se aplica la funci√≥n de activaci√≥n (ej. sigmoide, tanh, ReLU) para producir la salida de esa neurona.
    * La salida final de la red se compara con el valor objetivo real para calcular el **error global** (o "costo") de la red, utilizando una funci√≥n de p√©rdida (ej. error cuadr√°tico medio para regresi√≥n, entrop√≠a cruzada para clasificaci√≥n).

2.  **Fase de Retropropagaci√≥n (Backward Pass):**
    * El error global se propaga **hacia atr√°s** desde la capa de salida, a trav√©s de las capas ocultas, hasta la capa de entrada.
    * En cada capa, se calcula el **gradiente** del error con respecto a los pesos de las conexiones de esa capa. Esto implica el uso de la **regla de la cadena** del c√°lculo diferencial para determinar cu√°nto contribuye cada peso al error final.
    * Una vez calculados los gradientes, los pesos de la red se **actualizan** en la direcci√≥n opuesta al gradiente (es decir, en la direcci√≥n de mayor descenso) para reducir el error. Esta actualizaci√≥n se realiza con una **tasa de aprendizaje** que controla el tama√±o del paso.
    $$w_{ij}^{\text{nuevo}} = w_{ij}^{\text{anterior}} - \alpha \cdot \frac{\partial E}{\partial w_{ij}}$$
    Donde $E$ es el error, $w_{ij}$ es el peso de la conexi√≥n entre la neurona $i$ y la neurona $j$, y $\alpha$ es la tasa de aprendizaje.

En el contexto del **aprendizaje global vs. local**, Back-Propagation es el coraz√≥n del entrenamiento de sistemas de **aprendizaje global** por excelencia (las redes neuronales multicapa). La red neuronal busca aprender una **aproximaci√≥n de funci√≥n global** que mapee las entradas a las salidas, minimizando el error en todo el conjunto de datos. Si los datos no se distribuyen linealmente, Back-Propagation permite que la red aprenda relaciones no lineales complejas a trav√©s de sus m√∫ltiples capas y funciones de activaci√≥n no lineales. A diferencia de LOESS o los m√©todos de regresi√≥n ponderada localmente, Back-Propagation no divide expl√≠citamente el problema en m√∫ltiples problemas locales independientes para minimizar funciones de costo locales. En cambio, busca minimizar una funci√≥n de p√©rdida **global** para toda la red. Sin embargo, su capacidad para ajustar un gran n√∫mero de par√°metros (pesos) le permite construir representaciones internas de los datos que pueden ser incre√≠blemente flexibles y adaptables, superando la limitaci√≥n de que "a veces ning√∫n valor de par√°metro [en un modelo simple] puede proporcionar una aproximaci√≥n suficientemente buena". La retropropagaci√≥n es lo que permiti√≥ a las redes neuronales convertirse en poderosas herramientas de aprendizaje autom√°tico.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas (requiere normalizar), Categ√≥ricas como dummies",
  "‚úÖ Captura relaciones no lineales profundas",
  "‚ùå No requiere",
  "‚úÖ Deseable, aunque no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Elevado (puede requerir robustez ante valores extremos)",
  "‚ö†Ô∏è Puede ralentizar convergencia si es muy alta",
  "‚ö†Ô∏è Baja (modelo de ‚Äúcaja negra‚Äù)",
  "‚ö†Ô∏è Depende de la arquitectura y tama√±o del dataset",
  "‚úÖ Recomendable para ajustar tasas de aprendizaje, capas y neuronas",
  "‚ùå No conviene con datos muy peque√±os o alta dimensionalidad sin regularizar"
)

detalles <- c(
  "Algoritmo para entrenar redes neuronales multicapa ajustando pesos por retropropagaci√≥n del error.",
  "En clasificaci√≥n usa softmax o sigmoide en salida; en regresi√≥n, capa lineal para valor continuo.",
  "Debe escalarse cada caracter√≠stica; las categ√≥ricas transformarse a variables indicadoras antes de entrenar.",
  "Aprende funciones arbitrariamente complejas activando m√∫ltiples capas ocultas con funciones no lineales.",
  "No impone distribuci√≥n espec√≠fica en errores, se optimiza v√≠a descenso de gradiente.",
  "Mejor si las observaciones son independientes; sensible a secuencias sin ajustes espec√≠ficos.",
  "No requiere varianza constante, ya que los pesos se ajustan adaptativamente durante el entrenamiento.",
  "Valores extremos pueden causar activaciones saturadas (vanishing/exploding gradients) si no se manejan.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; Batch Normalization ayuda a mitigar.",
  "Dif√≠cil interpretar cada peso individual; se usan t√©cnicas como LIME o SHAP para explicar decisiones.",
  "El tiempo crece con n√∫mero de capas, neuronas y epochs; GPUs aceleran el proceso.",
  "Cross-validation (o k-fold) ayuda a elegir n√∫mero de capas, neuronas por capa, tasa de aprendizaje y regularizaci√≥n.",
  "No funciona bien con datasets peque√±os (overfitting f√°cil) o ruido elevado sin t√©cnicas de regularizaci√≥n."
)

tabla_backprop <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_backprop %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir back-propagation",
             subtitle = "Back - Propagation")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Hopfield Network  {-}   

La **Red de Hopfield** es un tipo de **red neuronal recurrente** o **red neuronal con memoria asociativa**, propuesta por John Hopfield en 1982. A diferencia de las redes neuronales de propagaci√≥n hacia adelante (como el Perceptr√≥n o las MLP entrenadas con Back-Propagation) que se utilizan para el mapeo de entrada a salida, la idea fundamental de una Red de Hopfield es funcionar como un **sistema de memoria asociativa** y un **sistema din√°mico que converge a estados estables**. Su objetivo principal es almacenar y recuperar patrones binarios, as√≠ como resolver problemas de optimizaci√≥n.

El funcionamiento de una Red de Hopfield se basa en los siguientes principios:

1.  **Neuronas Binarias:** La red consta de un conjunto de neuronas (nodos) que son **binarias**, lo que significa que solo pueden tomar dos estados posibles, generalmente $1$ o $-1$.
2.  **Conexiones Ponderadas:** Cada neurona est√° conectada a todas las dem√°s neuronas (excepto a s√≠ misma) mediante **conexiones sim√©tricas y ponderadas**. Los pesos de estas conexiones se calculan de manera que los patrones que se quieren "memorizar" se conviertan en **estados de energ√≠a m√≠nima** de la red. La regla de aprendizaje m√°s com√∫n para establecer estos pesos es la **regla de Hebb**: si dos neuronas se activan juntas para un patr√≥n, el peso entre ellas se incrementa.
3.  **Din√°mica de Activaci√≥n:** Cuando se presenta una entrada a la red (que puede ser un patr√≥n ruidoso o incompleto), las neuronas se actualizan de forma as√≠ncrona o s√≠ncrona. La activaci√≥n de cada neurona se recalcula en funci√≥n de la suma ponderada de las activaciones de las otras neuronas a las que est√° conectada.
    $$S_i = \text{sgn}\left(\sum_{j \neq i} W_{ij} S_j\right)$$
    Donde $S_i$ es el estado de la neurona $i$, $W_{ij}$ es el peso entre la neurona $i$ y $j$, y $\text{sgn}$ es la funci√≥n signo.
4.  **Convergencia a Estados Estables:** Este proceso de actualizaci√≥n se repite hasta que la red alcanza un **estado estable** (un "atractor"), donde las activaciones de las neuronas ya no cambian. Si la red ha sido entrenada correctamente, este estado estable corresponder√° al patr√≥n memorizado m√°s cercano a la entrada inicial (memoria asociativa).
5.  **Funci√≥n de Energ√≠a:** La estabilidad de la red se puede describir mediante una **funci√≥n de energ√≠a de Lyapunov**. Durante la din√°mica de la red, la energ√≠a de la red siempre disminuye hasta que se alcanza un m√≠nimo local (un patr√≥n memorizado).

En el contexto del **aprendizaje global vs. local**, la Red de Hopfield es un sistema de **aprendizaje global** que exhibe un comportamiento de **optimizaci√≥n local**. La regla de aprendizaje (como la regla de Hebb) establece los pesos de todas las conexiones para que los patrones deseados se conviertan en m√≠nimos de energ√≠a en todo el espacio de estados. Es decir, se busca una configuraci√≥n global de pesos para memorizar un conjunto de patrones. Sin embargo, la **din√°mica de recuperaci√≥n** de la red es intr√≠nsecamente un proceso de **convergencia local**: dada una entrada inicial, la red "cae" en el m√≠nimo de energ√≠a m√°s cercano, que corresponde al patr√≥n memorizado.

Si los datos no se distribuyen linealmente, la Red de Hopfield no aplica el concepto de regresi√≥n (o clasificaci√≥n) de la misma manera que LOESS o los √°rboles de decisi√≥n. En cambio, funciona como un sistema de **memoria y recuperaci√≥n de patrones** no lineales. Puede almacenar y recuperar patrones complejos que no son linealmente separables. La red busca una soluci√≥n global (un conjunto de pesos) para almacenar los patrones, y luego, en la recuperaci√≥n, utiliza un proceso de "b√∫squeda" local en el espacio de energ√≠a para converger a un patr√≥n memorizado. Esto aborda la idea de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un modelo de regresi√≥n lineal, ya que la Red de Hopfield no es un modelo de regresi√≥n en s√≠, sino un sistema din√°mico que encuentra estados de equilibrio. Su capacidad para manejar patrones ruidosos o incompletos para recuperar el patr√≥n completo es una de sus principales fortalezas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è No es supervisado en el sentido cl√°sico",
  "‚ö†Ô∏è No hay ‚Äútarget‚Äù continuo o categ√≥rico (almacenamiento de patrones)",
  "‚úÖ Variables binarias o valores discretizados (patrones binarios)",
  "‚úÖ Correlaciona patrones con pesos sim√©tricos entre neuronas",
  "‚ùå No aplica (no hay residuos param√©tricos)",
  "‚úÖ Deseable, aunque no obligatorio (los estados deben actualizarse sin bucles no deseados)",
  "‚ùå No aplica (no modela varianza de errores)",
  "‚ö†Ô∏è Muy sensible (un solo nodo saturado puede distorsionar la red)",
  "‚ö†Ô∏è Puede verse afectado si los patrones de entrenamiento tienen redundancia fuerte",
  "‚ö†Ô∏è Media (la din√°mica de atra√ß√£o es interpretable, pero las conexiones pueden ser complejas)",
  "‚ö†Ô∏è Moderada (dependiendo del n√∫mero de neuronas y estados s√≠ncronos/as√≠ncronos)",
  "‚ùå No se usa tradicionalmente, pero se puede validar estabilidad de memorias con pruebas de convergencia",
  "‚ö†Ô∏è No sirve si los patrones no son binarios o si hay alto ruido en entradas asociativas"
)

detalles <- c(
  "Red neuronal recurrente para recuperaci√≥n asociativa de patrones, no requiere pares X‚Üíy.",
  "No predice una variable externa, recupera patrones completos a partir de entradas parciales o ruidosas.",
  "Requiere que cada elemento del patr√≥n sea binario (¬±1) o est√© discretizado; las variables continuas deben binarizarse.",
  "Los pesos sim√©tricos se calculan por Hebb (p. ej. W = Œ£ p·µ¢ p·µ¢·µÄ), sin umbral expl√≠cito para relaciones lineales.",
  "No hay un t√©rmino de error param√©trico; la din√°mica sigue la funci√≥n de energ√≠a, no un residuo gaussiano.",
  "Es mejor si las actualizaciones de estado son independientes o s√≠ncronas; la dependencia temporal puede generar oscilaciones.",
  "No modela varianza de error, pues busca minimizar energ√≠a, no error cuadr√°tico.",
  "Patrones fuera del rango binario pueden causar saturaci√≥n o estados inestables.",
  "Patrones muy similares (colineales) pueden interferir en recuperaciones correctas (atractores vecinos).",
  "La din√°mica de convergencia hacia un estado estable (atractor) se puede visualizar, pero la topolog√≠a de pesos puede no ser transparente.",
  "Simulaci√≥n de din√°micas es razonable para tama√±os moderados (‚â§1000 neuronas); grandes redes requieren optimizaci√≥n paralela.",
  "No se usa CV tradicional; se analiza la robustez de memorias variando inicializaci√≥n o agregando ruido.",
  "No apto si los datos no pueden discretizarse en patrones binarios, o si se requieren m√∫ltiples clases de salida simult√°neas."
)

tabla_hopfield <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_hopfield %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir hopfield network",
             subtitle = "Hopfield Network")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  

```

## Multilayer Perceptron (MP)  {-}     

El **Multilayer Perceptron (MLP)**, tambi√©n conocido como **red neuronal de propagaci√≥n hacia adelante cl√°sica**, es un tipo fundamental de **red neuronal artificial** utilizada para una amplia gama de tareas de **aprendizaje supervisado**, incluyendo **clasificaci√≥n** y **regresi√≥n**. La idea fundamental del MLP es extender el concepto del Perceptr√≥n simple al incorporar una o m√°s **capas ocultas** entre la capa de entrada y la capa de salida, y utilizando **funciones de activaci√≥n no lineales** en estas capas. Esta arquitectura de m√∫ltiples capas es lo que le confiere a los MLP su capacidad para aprender y modelar relaciones complejas y no lineales en los datos.

La estructura de un MLP t√≠picamente incluye:

1.  **Capa de Entrada:** Recibe las caracter√≠sticas de entrada del problema.
2.  **Capas Ocultas:** Son una o m√°s capas intermedias donde se realizan c√°lculos complejos. Cada neurona en una capa oculta recibe entradas de la capa anterior, calcula una suma ponderada de estas entradas (m√°s un sesgo), y luego aplica una **funci√≥n de activaci√≥n no lineal** (como la funci√≥n sigmoide, tanh o ReLU) a esta suma. Es la no linealidad de estas funciones de activaci√≥n la que permite al MLP aprender relaciones no lineales.
    $$a_j = f\left(\sum_{i=1}^{n} w_{ij} x_i + b_j\right)$$
    Donde $a_j$ es la activaci√≥n de la neurona $j$, $x_i$ son las entradas de la capa anterior, $w_{ij}$ son los pesos, $b_j$ es el sesgo, y $f$ es la funci√≥n de activaci√≥n no lineal.
3.  **Capa de Salida:** Produce la predicci√≥n final de la red. La funci√≥n de activaci√≥n en esta capa depende del tipo de problema (ej., una funci√≥n lineal para regresi√≥n, softmax para clasificaci√≥n multiclase, o sigmoide para clasificaci√≥n binaria).

El entrenamiento de un MLP se realiza t√≠picamente utilizando el algoritmo de **Back-Propagation**, que ajusta los pesos de la red de manera iterativa para minimizar una funci√≥n de p√©rdida (error) calculada en la capa de salida.

En el contexto del **aprendizaje global vs. local**, el Multilayer Perceptron es el paradigma de un sistema de **aprendizaje global**. La red aprende una **aproximaci√≥n de funci√≥n global** que mapea las entradas a las salidas, buscando minimizar la funci√≥n de p√©rdida en todo el conjunto de datos de entrenamiento. A diferencia de los sistemas de aprendizaje local que dividen expl√≠citamente el problema global en m√∫ltiples problemas m√°s peque√±os, el MLP ajusta todos sus pesos de forma interconectada para aprender una representaci√≥n distribuida de los patrones en los datos. Si los datos no se distribuyen linealmente, el MLP es excepcionalmente capaz de modelar estas relaciones complejas gracias a sus capas ocultas y funciones de activaci√≥n no lineales. Esto aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos lineales o m√°s simples, ya que el MLP puede construir representaciones internas de gran complejidad para aproximar casi cualquier funci√≥n continua. Hoy en d√≠a, los MLP son la base de muchas arquitecturas de "Deep Learning".


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas (normalizar) y categ√≥reas (dummies)",
  "‚úÖ Captura relaciones no lineales profundas",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (puede requerir robustez ante outliers)",
  "‚ö†Ô∏è Afecta la convergencia si es muy alta",
  "‚ö†Ô∏è Baja (modelo tipo 'caja negra')",
  "‚ö†Ô∏è Depende de arquitectura y tama√±o del dataset",
  "‚úÖ Recomendable (k-fold o repeated CV)",
  "‚ùå No conviene con pocos datos o ruido elevado"
)

detalles <- c(
  "Red neuronal con m√∫ltiples capas ocultas y funci√≥n de activaci√≥n no lineal.",
  "Clasificaci√≥n con softmax/sigmoide; regresi√≥n con capa lineal en salida.",
  "Debe escalarse cada caracter√≠stica; las categ√≥ricas convierten a variables indicadoras.",
  "Aprende patrones complejos combinando m√∫ltiples capas y neuronas.",
  "No impone distribuci√≥n espec√≠fica de errores, se optimiza con optimizadores basados en gradiente.",
  "Funciona mejor si las muestras son independientes; sensibles a dependencia temporal sin ajustes.",
  "No requiere varianza constante, ajusta pesos en cada mini-batch o lote.",
  "Outliers pueden causar gradientes explosivos o desaparecidos sin mecanismos de robustez.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; batch normalization ayuda.",
  "Dif√≠cil de interpretar cada peso/neuronas; se usan t√©cnicas como SHAP o LIME para explicaci√≥n.",
  "El tiempo de entrenamiento aumenta con cada capa, neuronas y epochs; GPUs aceleran el proceso.",
  "Crucial para ajustar hiperpar√°metros: n√∫mero de capas, neuronas por capa, tasa de aprendizaje, regularizaci√≥n.",
  "No √∫til para datasets muy peque√±os (sobreajuste) o altamente ruidosos sin regularizaci√≥n."
)

tabla_mp <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mp %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MP",
             subtitle = "Multilayer Perceptron (MP)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Convolutional Neural Network (CNN)  {-}   

**Convolutional Neural Networks (CNNs)**, tambi√©n conocidas como **ConvNets**, son una clase especializada de **redes neuronales profundas** que han demostrado ser excepcionalmente efectivas en tareas de **visi√≥n por computadora** (como clasificaci√≥n de im√°genes, detecci√≥n de objetos, reconocimiento facial) y, m√°s recientemente, en procesamiento de lenguaje natural. La idea fundamental de una CNN es imitar el funcionamiento del c√≥rtex visual en el cerebro humano, utilizando **capas de convoluci√≥n** para detectar autom√°ticamente patrones y caracter√≠sticas jer√°rquicas directamente de los datos de entrada sin necesidad de una extracci√≥n manual de caracter√≠sticas.

A diferencia de los Multilayer Perceptrons (MLPs) que conectan cada neurona de una capa con cada neurona de la siguiente capa (lo que resulta en una enorme cantidad de par√°metros para datos de alta dimensi√≥n como im√°genes), las CNNs aprovechan tres ideas arquitect√≥nicas clave:

1.  **Capas de Convoluci√≥n:** Estas capas aplican un peque√±o conjunto de **filtros (kernels)** a la entrada (ej., una imagen). Cada filtro "se desliza" por la entrada (operaci√≥n de convoluci√≥n) y calcula un producto punto entre sus valores y los valores de la regi√≥n de la entrada que est√° cubriendo. Esto genera un **mapa de caracter√≠sticas** que resalta la presencia de patrones espec√≠ficos (bordes, texturas, formas) en diferentes ubicaciones de la entrada. La ventaja es que los mismos filtros se aplican en m√∫ltiples ubicaciones, lo que reduce dr√°sticamente el n√∫mero de par√°metros y captura la **localidad** de los patrones y la **invarianza traslacional**.
2.  **Capas de Pooling (Submuestreo):** Estas capas se insertan peri√≥dicamente entre las capas convolucionales. Su funci√≥n es reducir la dimensionalidad espacial de los mapas de caracter√≠sticas (ej., reduciendo el n√∫mero de p√≠xeles), lo que ayuda a hacer que el modelo sea m√°s robusto a peque√±as variaciones o distorsiones en la posici√≥n de las caracter√≠sticas. Las operaciones comunes son el **max pooling** (tomar el valor m√°ximo de una regi√≥n) o el **average pooling** (tomar el promedio).
3.  **Capas Totalmente Conectadas (Dense):** Despu√©s de varias capas convolucionales y de pooling, los mapas de caracter√≠sticas finales se aplanan en un vector y se conectan a una o m√°s capas totalmente conectadas (similares a las de un MLP). Estas capas finales realizan la clasificaci√≥n o regresi√≥n bas√°ndose en las caracter√≠sticas de alto nivel extra√≠das por las capas anteriores.

El entrenamiento de una CNN se realiza utilizando el algoritmo de **Back-Propagation** y descenso de gradiente (con sus variantes como SGD, Adam, etc.), ajustando los pesos de los filtros y las conexiones de las capas densas para minimizar una funci√≥n de p√©rdida.

En el contexto del **aprendizaje global vs. local**, las CNNs son un ejemplo sobresaliente de un sistema de **aprendizaje global** que, en sus capas iniciales, se beneficia de la detecci√≥n de patrones **locales**. Cada filtro de convoluci√≥n aprende a detectar un patr√≥n local espec√≠fico (un borde vertical, una esquina, etc.) que se repite en diferentes partes de la imagen (lo que es una forma de "regresi√≥n ponderada localmente" en el sentido de que el filtro "aplica" su conocimiento local a diferentes ventanas de entrada). Sin embargo, la combinaci√≥n jer√°rquica de m√∫ltiples capas convolucionales y de pooling, seguida de capas totalmente conectadas, permite que la red construya representaciones cada vez m√°s abstractas y globales del contenido de la imagen. Esto significa que si los datos no se distribuyen linealmente, las CNNs pueden aprender a modelar relaciones extremadamente complejas y no lineales al componer caracter√≠sticas locales en representaciones globales. La arquitectura de CNNs resuelve la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos m√°s simples al permitir que la red aprenda caracter√≠sticas relevantes de forma autom√°tica y jer√°rquica, adapt√°ndose a las complejidades inherentes de datos como im√°genes y videos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Im√°genes (matrices de p√≠xeles) y datos con estructura espacial",
  "‚úÖ Captura relaciones locales y espaciales mediante filtros convolucionales",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias son independientes)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (artefactos o ruido en im√°genes puede afectar)",
  "‚ö†Ô∏è No se eval√∫a colinealidad de predictores, maneja correlaciones espaciales",
  "‚ö†Ô∏è Baja (modelo tipo ‚Äòcaja negra‚Äô, usar t√©cnicas como Grad-CAM para interpretaci√≥n)",
  "‚ö†Ô∏è Lento sin GPU, entrenamiento intensivo en c√≥mputo",
  "‚úÖ Robusto si se aplica k-fold o validaci√≥n en conjunto de im√°genes",
  "‚ùå No funciona bien con pocos datos o sin estructura espacial significativa"
)

detalles <- c(
  "Red neuronal profunda especializada en procesar datos con estructura de grilla (ej. im√°genes).",
  "En clasificaci√≥n utiliza softmax; en regresi√≥n, capa lineal para valores continuos.",
  "Requiere tensores de entrada (canales, altura, ancho); funciones de preprocesamiento para im√°genes.",
  "Filtros convolucionales extraen caracter√≠sticas locales, max-pooling disminuye dimensionalidad manteniendo informaci√≥n relevante.",
  "No impone ninguna distribuci√≥n en los errores, optimiza funci√≥n de p√©rdida directamente.",
  "Ideal si las muestras son independientes; sensible a dependencias temporales o espaciales no modeladas.",
  "No requiere varianza constante pues se basa en convoluciones y pooling, no en un modelo param√©trico de error.",
  "Ruido o artefactos en p√≠xeles pueden alterar el aprendizaje de filtros, es importante usar t√©cnicas de regularizaci√≥n.",
  "La red aprende filtros que capturan patrones locales, por lo que no es necesario verificar colinealidad expl√≠citamente.",
  "Dif√≠cil de interpretar cada filtro y capa; se utilizan mapas de activaci√≥n o Grad-CAM para entender qu√© regiones influyen en la predicci√≥n.",
  "El entrenamiento con m√∫ltiples capas convolucionales y millones de par√°metros es intensivo en GPU/TPU.",
  "Validaci√≥n cruzada o separaci√≥n de conjuntos (train/validation/test) ayuda a evitar overfitting.",
  "No es apropiado para datasets muy peque√±os sin aumentar datos (data augmentation) o sin informaci√≥n espacial clara."
)

tabla_cnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles) 

tabla_mp %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CNN",
             subtitle = "Convolutional Neural Network (CNN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Recurrent Neural Networks (RNNs) {-}   

**Recurrent Neural Networks (RNNs)** son un tipo de **red neuronal artificial** dise√±ado espec√≠ficamente para manejar **datos secuenciales** o temporales, donde la informaci√≥n de pasos anteriores en la secuencia es relevante para la predicci√≥n actual. A diferencia de las redes de propagaci√≥n hacia adelante (como MLP o CNN) que asumen que las entradas son independientes entre s√≠, las RNNs tienen "memoria" o **conexiones recurrentes** que les permiten mantener un **estado interno** que encapsula informaci√≥n de pasos de tiempo anteriores. Esta caracter√≠stica las hace ideales para tareas como el procesamiento de lenguaje natural (PLN), el reconocimiento de voz, la traducci√≥n autom√°tica y la predicci√≥n de series de tiempo.

La idea fundamental de una RNN es que una **unidad recurrente** aplica la misma funci√≥n de transformaci√≥n a cada elemento de una secuencia, con la particularidad de que la salida de la unidad en un paso de tiempo dado se realimenta como entrada para el mismo proceso en el siguiente paso de tiempo. Esto permite que la red "recuerde" y utilice informaci√≥n pasada al procesar la secuencia actual.

El funcionamiento b√°sico de una RNN en un paso de tiempo ($t$) implica:

1.  **Entrada actual ($x_t$):** El elemento actual de la secuencia.
2.  **Estado oculto anterior ($h_{t-1}$):** La "memoria" o estado interno de la red del paso de tiempo anterior.
3.  **C√°lculo del Estado Oculto Actual ($h_t$):** Se combina la entrada actual y el estado oculto anterior, y se aplica una funci√≥n de activaci√≥n (ej., tanh o ReLU).
    $$h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$
    Donde $W_{hh}$ son los pesos de la conexi√≥n recurrente, $W_{xh}$ son los pesos de la entrada, y $b_h$ es el sesgo.
4.  **Salida Actual ($y_t$):** Se genera una salida a partir del estado oculto actual.
    $$y_t = W_{hy} h_t + b_y$$
    Donde $W_{hy}$ son los pesos de la salida y $b_y$ es el sesgo.

Este proceso de actualizaci√≥n de estado y salida se repite para cada elemento de la secuencia. La "memoria" de la RNN est√° codificada en el estado oculto que se pasa de un paso de tiempo al siguiente.

El entrenamiento de las RNNs se realiza mediante una variante del algoritmo de Back-Propagation llamada **Back-Propagation Through Time (BPTT)**. BPTT desenrolla la red a lo largo del tiempo, tratando cada paso de tiempo como una capa separada, y luego aplica la retropropagaci√≥n de manera similar a c√≥mo se entrena un MLP, pero propagando los errores a trav√©s de las conexiones recurrentes. Sin embargo, las RNNs simples pueden sufrir de problemas como el **desvanecimiento del gradiente** (vanishing gradient) o el **explosi√≥n del gradiente** (exploding gradient) para secuencias largas, lo que llev√≥ al desarrollo de arquitecturas m√°s avanzadas como **LSTM (Long Short-Term Memory)** y **GRU (Gated Recurrent Unit)**.

En el contexto del **aprendizaje global vs. local**, las RNNs son sistemas de **aprendizaje global** que est√°n dise√±ados para aprender y modelar **dependencias temporales y patrones secuenciales** en un dominio global. A diferencia de los m√©todos de regresi√≥n ponderada localmente como LOESS, que se enfocan en ajustar curvas en regiones espec√≠ficas de datos, las RNNs intentan aprender una funci√≥n de mapeo compleja que considera toda la secuencia hist√≥rica para producir una predicci√≥n. Si los datos (secuenciales) no se distribuyen linealmente, las RNNs son extremadamente efectivas para capturar estas relaciones no lineales y dependencias a largo plazo. Al tener un estado interno que recuerda informaci√≥n pasada, abordan directamente la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos est√°ticos o lineales, ya que pueden adaptar sus predicciones din√°micamente en funci√≥n del contexto secuencial, lo que las convierte en una herramienta fundamental para el an√°lisis de series de tiempo y el procesamiento de lenguaje.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado secuencial",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n) en secuencias",
  "‚úÖ Series temporales y datos secuenciales (texto, audio, series) convertidos a vectores",
  "‚úÖ Captura dependencias temporales y de largo plazo entre pasos de la secuencia",
  "‚ùå No requiere supuestos de normalidad de residuos",
  "‚ö†Ô∏è Ideal si las secuencias son independientes entre s√≠; no modela dependencia ex√≥gena autom√°ticamente",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers en la serie pueden sesgar el entrenamiento si no se detectan)",
  "‚ö†Ô∏è La colinealidad en caracter√≠sticas secuenciales puede afectar la convergencia (usar embeddings o reducci√≥n)",
  "‚ö†Ô∏è Baja (‚Äúcaja negra‚Äù con muchas capas; usar t√©cnicas de atenci√≥n o visualizaci√≥n de activaciones)",
  "‚ö†Ô∏è Lento sin GPU/TPU; entrenamiento costoso para secuencias largas o redes profundas",
  "‚ö†Ô∏è Usar validaci√≥n cronol√≥gica (time series split) es m√°s apropiado que k-fold cl√°sico",
  "‚ùå No conviene con muy pocas muestras temporales, secuencias extremadamente largas sin truncar, o datos muy ruidosos"
)

detalles <- c(
  "Red neuronal recurrente que procesa datos en pasos temporales manteniendo un estado interno.",
  "En clasificaci√≥n, etiqueta cada elemento o secuencia; en regresi√≥n, predice valores continuos a lo largo del tiempo.",
  "Las entradas deben transformarse en vectores o embeddings; por ejemplo, texto a √≠ndices, series normalizadas.",
  "La arquitectura RNN (LSTM, GRU) retiene informaci√≥n de pasos anteriores para afectar salidas posteriores.",
  "No impone distribuci√≥n en errores, ya que se optimiza v√≠a descenso de gradiente sobre secuencias.",
  "Funciona mejor si cada secuencia (serie) es independiente; para datos con autocorrelaci√≥n compleja, usar variantes especializadas.",
  "No requiere varianza constante, pues se basa en propagaci√≥n de estado y no en un t√©rmino de error param√©trico.",
  "Valores at√≠picos en la serie pueden provocar gradientes explosivos o desvanecidos sin mecanismos como clipping.",
  "La representaci√≥n internal de patrones secuenciales puede verse afectada si hay caracter√≠sticas muy correlacionadas; usar regularizaci√≥n.",
  "Dif√≠cil interpretar pesos internos; se usan mec√°nicas como atenci√≥n (attention) o visualizaci√≥n de celdas LSTM.",
  "El entrenamiento con backpropagation through time es intensivo; GPUs o TPUs aceleran enormemente el proceso.",
  "Para series temporales, se prefiere validaci√≥n basada en ventanas de tiempo (rolling/expanding window) en lugar de random split.",
  "No apto si las secuencias son muy cortas o muy pocas, o hay mucho ruido sin filtrado; en esos casos, usar modelos estad√≠sticos simples."
)

tabla_rnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rnn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RNN",
             subtitle = "Recurrent Neural Networks (RNNs)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Transformers  {-}  

Los **Transformers** son una arquitectura de **red neuronal profunda** que ha revolucionado el campo del **Procesamiento de Lenguaje Natural (PLN)** y, m√°s recientemente, se ha expandido a la visi√≥n por computadora y otras √°reas. Introducidos en el art√≠culo "Attention Is All You Need" (Vaswani et al., 2017), la idea fundamental de los Transformers es prescindir de la naturaleza recurrente de las RNNs y las convolucionales de las CNNs, bas√°ndose enteramente en un mecanismo llamado **auto-atenci√≥n (self-attention)** para capturar dependencias de largo alcance en las secuencias de entrada.

Antes de los Transformers, las RNNs eran el modelo dominante para datos secuenciales. Sin embargo, las RNNs ten√≠an limitaciones como la dificultad para capturar dependencias a muy largo plazo (problema del gradiente desvanecido) y la imposibilidad de paralelizar completamente el procesamiento de secuencias (debido a su naturaleza secuencial). Los Transformers resuelven estos problemas al permitir que cada elemento de la secuencia interact√∫e directamente con todos los dem√°s elementos de la secuencia, sin importar su distancia.

Los componentes clave de un Transformer incluyen:

1.  **Mecanismo de Auto-Atenci√≥n (Self-Attention):** Este es el coraz√≥n del Transformer. Para cada token (palabra) en una secuencia, el mecanismo de auto-atenci√≥n calcula una puntuaci√≥n de "relevancia" entre ese token y todos los dem√°s tokens de la secuencia. Esto permite que el modelo "pese" la importancia de cada token al generar la representaci√≥n de otro token. Este proceso se implementa a trav√©s de tres vectores para cada token: **Query (Q)**, **Key (K)** y **Value (V)**.
    $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
    Donde $d_k$ es la dimensi√≥n de los vectores Key.

2.  **Atenci√≥n Multi-Cabeza (Multi-Head Attention):** Para mejorar la capacidad del modelo de enfocarse en diferentes aspectos de la secuencia, el mecanismo de auto-atenci√≥n se aplica m√∫ltiples veces en paralelo con diferentes conjuntos de matrices de pesos (cabezas). Las salidas de estas cabezas se concatenan y se transforman linealmente.

3.  **Capas Feed-Forward (Posici√≥n por Posici√≥n):** Despu√©s del mecanismo de atenci√≥n, hay una red neuronal de propagaci√≥n hacia adelante (un MLP simple) que se aplica de forma independiente a cada posici√≥n en la secuencia.

4.  **Codificador-Decodificador (Encoder-Decoder Architecture):** El Transformer original consta de un **codificador** y un **decodificador**.
    * El **codificador** toma la secuencia de entrada y genera una representaci√≥n. Consiste en m√∫ltiples capas id√©nticas, cada una con una capa de auto-atenci√≥n multi-cabeza y una capa feed-forward.
    * El **decodificador** toma la representaci√≥n del codificador y genera la secuencia de salida (por ejemplo, la traducci√≥n). Tambi√©n consiste en m√∫ltiples capas, cada una con auto-atenci√≥n multi-cabeza, atenci√≥n multi-cabeza (que atiende a la salida del codificador) y una capa feed-forward.

5.  **Codificaci√≥n Posicional (Positional Encoding):** Dado que los Transformers procesan secuencias en paralelo y no tienen una noci√≥n inherente de la posici√≥n de los tokens (a diferencia de las RNNs), se a√±ade informaci√≥n de la posici√≥n de cada token a sus incrustaciones de entrada.

En el contexto del **aprendizaje global vs. local**, los Transformers son un sistema de **aprendizaje global** que, gracias a su mecanismo de atenci√≥n, pueden aprender **dependencias a largo alcance** y relaciones complejas que son inherentemente globales en la secuencia. Aunque los c√°lculos individuales de atenci√≥n pueden verse como una forma de ponderaci√≥n de la importancia local de los tokens, la red en su conjunto construye una representaci√≥n global de la secuencia. Si los datos (secuenciales) no se distribuyen linealmente, los Transformers son excepcionalmente capaces de modelar estas relaciones no lineales y dependencias a trav√©s de su capacidad para "observar" toda la secuencia a la vez y ponderar la relevancia de cada parte. Esto resuelve de manera fundamental la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos secuenciales anteriores, ya que la arquitectura de atenci√≥n les permite aprender patrones complejos y no lineales en datos secuenciales sin las restricciones de memoria de las RNNs, lo que los convierte en la arquitectura dominante para tareas de PLN avanzadas.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado (frecuentemente secuencial o multitarea)",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n en secuencias)",
  "‚úÖ Texto, secuencias, im√°genes en vectores o embeddings",
  "‚úÖ Captura dependencia secuencial y global mediante mecanismos de atenci√≥n",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚ö†Ô∏è Ideal si las muestras o secuencias son independientes; para datos correlacionados usar variantes espec√≠ficas",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers en embeddings o entradas ruidosas pueden afectar atenci√≥n)",
  "‚ö†Ô∏è La colinealidad en embedding space puede ralentizar aprendizaje; usar regularizaci√≥n",
  "‚ö†Ô∏è Baja (modelo de ‚Äôcaja negra‚Äô, requieren m√©todos como attention visualization o interpretabilidad basada en pesos)",
  "‚ö†Ô∏è Lento sin hardware especializado (secuencialidad en atenci√≥n puede ser costosa)",
  "‚ö†Ô∏è Validaci√≥n temporal o k-fold anidada, seg√∫n tarea; en NLP se prefiere holdout sobre texto sin mezclar",
  "‚ùå No es apropiado con muy pocos datos de entrenamiento o sin estructura secuencial clara"
)

detalles <- c(
  "Arquitectura basada en capas de atenci√≥n para procesar secuencias completas en paralelo.",
  "Modelos como BERT, GPT, T5 pueden usarse para tareas de clasificaci√≥n, traducci√≥n, regresi√≥n de valores continuos en secuencias.",
  "Entradas requieren tokenizaci√≥n y conversi√≥n a embeddings; pueden combinarse varias modalidades.",
  "La auto‚Äêatenci√≥n global permite capturar relaciones a largo y corto plazo sin sesgo posicional estricto.",
  "No impone distribuci√≥n param√©trica de errores; se entrena con optimizadores basados en p√©rdidas cross‚Äêentropy o MSE.",
  "Se prefiere que las secuencias en el batch no sean dependientes; para series de tiempo, usar variantes como Time‚ÄêSeries Transformer.",
  "No se modela varianza del error; el entrenamiento se enfoca en minimizar funci√≥n de p√©rdida directa.",
  "Ruido en texto (typos) o en datos num√©ricos de entrada puede inducir atenci√≥n err√°tica; usar limpieza de datos y regularizaci√≥n.",
  "Los embeddings pueden contener informaci√≥n redundante de caracter√≠sticas correlacionadas; ajustar tama√±o de embedding y regularizaci√≥n.",
  "Interpretabilidad limitada; se usan t√©cnicas como visualizaci√≥n de mapas de atenci√≥n, LIME, SHAP para entender decisiones.",
  "El c√≥mputo de atenci√≥n es O(n¬≤) en longitud de secuencia; GPUs/TPUs o variantes eficientes (Linformer, Performer) alivian costo.",
  "Para tareas de texto, a veces se usa train/validation/test sin CV cl√°sica; para tareas generales, k-fold anidada ayuda a elegir hiperpar√°metros.",
  "No es adecuado con datasets muy peque√±os, sin preentrenamiento o sin estructuras secuenciales definidas."
)

tabla_transformers <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_transformers %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir transformers",
             subtitle = "Transformers")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```





<!--chapter:end:04-neural-networks.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üß© **5. Reducci√≥n de Dimensionalidad** {-}   

**Ejemplos:** PCA, t-SNE, UMAP   
**Cu√°ndo usarlo:**   

* Visualizaci√≥n de datos de alta dimensi√≥n.
* Preprocesamiento para eliminar ruido o multicolinealidad.

**Ventajas:** Mejora desempe√±o y velocidad de otros modelos.    
**Limitaciones:** Puede perder interpretabilidad; no siempre mejora modelos.

---

## Principal Component Analysis (PCA)  {-}   

**Principal Component Analysis (PCA)** es una t√©cnica fundamental de **reducci√≥n de dimensionalidad** no supervisada que se utiliza para transformar un conjunto de variables posiblemente correlacionadas en un conjunto de variables no correlacionadas llamadas **componentes principales**. La idea fundamental de PCA es identificar las direcciones (ejes) en el espacio de los datos a lo largo de las cuales la **varianza de los datos es m√°xima**. Estas nuevas direcciones son ortogonales entre s√≠ y representan la mayor parte de la informaci√≥n (varianza) contenida en el conjunto de datos original con un n√∫mero reducido de dimensiones.

El proceso de PCA generalmente implica los siguientes pasos:

1.  **Estandarizaci√≥n de los Datos:** Es crucial estandarizar los datos (centrar en cero y escalar a varianza unitaria) antes de aplicar PCA, ya que PCA es sensible a la escala de las variables.
2.  **C√°lculo de la Matriz de Covarianza:** Se calcula la matriz de covarianza de los datos estandarizados, que describe la relaci√≥n entre cada par de variables.
3.  **C√°lculo de Eigenvalores y Eigenvectores:** Se calculan los **eigenvalores** y **eigenvectores** de la matriz de covarianza.
    * Los **eigenvectores** representan las **direcciones de los componentes principales** (los nuevos ejes en el espacio de los datos). El primer eigenvector corresponde a la direcci√≥n de mayor varianza, el segundo a la segunda mayor varianza, y as√≠ sucesivamente.
    * Los **eigenvalores** corresponden a la **magnitud de la varianza** a lo largo de cada eigenvector. Un eigenvalor m√°s grande indica que el componente principal asociado explica una mayor proporci√≥n de la varianza total de los datos.
4.  **Selecci√≥n de Componentes Principales:** Se ordenan los eigenvalores de mayor a menor y se seleccionan los $k$ eigenvectores correspondientes a los $k$ eigenvalores m√°s grandes. Estos $k$ eigenvectores forman la nueva base del subespacio dimensional reducido. La elecci√≥n de $k$ se puede basar en la varianza acumulada deseada o en un gr√°fico de codo (scree plot).
5.  **Proyecci√≥n de Datos:** Finalmente, se proyectan los datos originales sobre estos $k$ componentes principales para obtener un nuevo conjunto de datos con menor dimensionalidad pero que retiene la mayor parte de la varianza original.

En el contexto del **aprendizaje global vs. local**, PCA es inherentemente un m√©todo de **aprendizaje global**. Busca una transformaci√≥n lineal que capture la estructura de varianza **global** de todo el conjunto de datos. No divide el problema en subproblemas locales ni se enfoca en regiones espec√≠ficas de los datos. La idea es que, si los datos no se distribuyen linealmente, PCA (en su forma lineal) **no puede capturar relaciones no lineales complejas** de manera efectiva. Est√° limitado a encontrar las direcciones ortogonales de m√°xima varianza en un espacio lineal.

A diferencia de LOESS, que aplica una forma de regresi√≥n ponderada localmente para ajustar curvas suaves en datos no lineales, PCA no est√° dise√±ado para eso. Precisamente porque busca una representaci√≥n lineal global, si la estructura subyacente de los datos es intr√≠nsecamente no lineal, PCA puede no ser capaz de proporcionar una reducci√≥n de dimensionalidad suficientemente buena o capturar las relaciones esenciales. En tales casos, se recurre a t√©cnicas de reducci√≥n de dimensionalidad no lineal como t-SNE o UMAP, o a variantes de PCA como Kernel PCA. No obstante, PCA sigue siendo una herramienta fundamental y muy √∫til para la visualizaci√≥n de datos, la reducci√≥n de ruido y la preparaci√≥n de datos para otros algoritmos, especialmente cuando la varianza se explica bien por relaciones lineales o cuando la dimensionalidad es un problema para la eficiencia computacional.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è No supervisado (reducci√≥n de dimensiones)",
  "‚ùå No aplica (no hay target a predecir)",
  "‚úÖ Num√©ricas (requiere escalado), categ√≥ricas procesadas como dummies",
  "‚úÖ Captura correlaciones lineales entre predictores",
  "‚ùå No requiere supuestos de distribuci√≥n en residuos",
  "‚ö†Ô∏è Ideal si las observaciones son independientes, aunque no cr√≠tico",
  "‚úÖ Deseable (datos homogenizados tras escalado)",
  "‚ö†Ô∏è Moderado (outliers pueden distorsionar componentes principales)",
  "‚úÖ Sensible a colinealidad (reduce variables correlacionadas a componentes)",
  "‚ö†Ô∏è Media (componentes lineales son interpretables, pero combinaciones pueden no serlo)",
  "‚úÖ R√°pido en datasets medianos; escalable con √°lgebra lineal optimizada",
  "‚ö†Ô∏è No se aplica CV cl√°sico; se puede usar reconstrucci√≥n de error o validaci√≥n por bloques",
  "‚ùå No funciona bien si las relaciones son no lineales o datos muy ruidosos sin preprocesar"
)

detalles <- c(
  "M√©todo no supervisado para reducir la dimensi√≥n del espacio de predictores.",
  "No predice variables, se centra en variabilidad interna de los datos.",
  "Todas las variables num√©ricas deben escalarse; las categ√≥ricas convertir a variables indicadoras.",
  "Busca direcciones (componentes) que maximizan varianza lineal entre predictores.",
  "No impone supuestos sobre errores; se basa en descomposici√≥n de la matriz de covarianza.",
  "Mejor si las muestras no est√°n correlacionadas en el tiempo o espacialmente.",
  "Escalar y homogeneizar mejora el c√°lculo de componentes principales.",
  "Outliers extremos pueden sesgar la direcci√≥n de los componentes principales.",
  "Reduce colinealidad al combinar variables correlacionadas en componentes ortogonales.",
  "Componentes iniciales pueden interpretarse mediante pesos, pero componentes posteriores son combinaciones lineales complejas.",
  "Computaci√≥n depende de descomposici√≥n de matrices (SVD), es eficiente con optimizaci√≥n BLAS.",
  "Se puede evaluar n√∫mero √≥ptimo de componentes con validaci√≥n de reconstrucci√≥n o bootstrap de SVD.",
  "No apto para relaciones no lineales complejas; en tal caso usar Kernel PCA o m√©todos no lineales."
)

tabla_pca <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pca %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PCA",
             subtitle = "Principal Component Analysis (PCA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Partial Least Squares Regression (PLSR)  {-}    

**Partial Least Squares Regression (PLSR)** es una t√©cnica de **regresi√≥n multivariada** que combina caracter√≠sticas de la **regresi√≥n por m√≠nimos cuadrados ordinarios (OLS)** y el **an√°lisis de componentes principales (PCA)**. Se utiliza para modelar la relaci√≥n entre un conjunto de variables predictoras (X) y uno o m√°s conjuntos de variables de respuesta (Y), siendo particularmente √∫til en situaciones donde hay un gran n√∫mero de variables predictoras, **multicolinealidad** (altas correlaciones entre las variables predictoras), o cuando el n√∫mero de predictoras excede el n√∫mero de observaciones.

La idea fundamental de PLSR es encontrar un conjunto de **componentes latentes** (tambi√©n conocidos como "factores" o "variables latentes") tanto en el espacio de las variables X como en el de las variables Y. Estos componentes se construyen de tal manera que **maximizan la covarianza** entre las variables predictoras y las variables de respuesta. A diferencia de PCA, que solo busca componentes que expliquen la m√°xima varianza en X, PLSR busca componentes que sean relevantes para explicar la varianza en X *y* que tambi√©n est√©n altamente correlacionados con Y. Una vez que se extraen estos componentes, se realiza una regresi√≥n de m√≠nimos cuadrados ordinarios de Y sobre estos componentes latentes.

El proceso general de PLSR implica:

1.  **Extracci√≥n de Componentes Latentes:** PLSR construye iterativamente un conjunto de componentes latentes. En cada paso:
    * Identifica una combinaci√≥n lineal de las variables X (un componente de X) y una combinaci√≥n lineal de las variables Y (un componente de Y) que tienen la mayor covarianza entre s√≠.
    * Estos componentes representan las direcciones en el espacio de datos que explican la mayor cantidad de la relaci√≥n entre X y Y.
    * Una vez que se extrae un componente, la varianza explicada por ese componente se "deflacta" (se elimina) de las matrices X e Y, y el proceso se repite con los residuos para encontrar el siguiente componente ortogonal.
2.  **Regresi√≥n:** Una vez que se ha determinado el n√∫mero √≥ptimo de componentes latentes (a menudo a trav√©s de validaci√≥n cruzada), se realiza una regresi√≥n lineal est√°ndar de las variables Y sobre estos componentes latentes de X.

**Ventajas clave de PLSR:**

* **Manejo de Multicolinealidad:** Es muy efectivo en la reducci√≥n de dimensionalidad y el manejo de predictoras altamente correlacionadas, donde la regresi√≥n OLS fallar√≠a o producir√≠a estimaciones inestables.
* **Manejo de Datos de Alta Dimensionalidad:** Funciona bien cuando el n√∫mero de variables predictoras es mayor que el n√∫mero de observaciones.
* **Enfoque Predictivo:** Se centra en desarrollar modelos con una fuerte capacidad predictiva.

En el contexto del **aprendizaje global vs. local**, PLSR es un algoritmo de **aprendizaje global**. Busca una transformaci√≥n lineal global de los datos originales en un espacio de componentes latentes, donde se maximiza la covarianza entre los conjuntos de variables predictoras y de respuesta. La relaci√≥n que modela es una relaci√≥n lineal global en este espacio transformado.

Si bien PLSR no es un m√©todo de **regresi√≥n ponderada localmente** como LOESS (que ajusta modelos simples a subconjuntos locales de datos), comparte con ellos el objetivo de modelar relaciones complejas. Sin embargo, lo hace de una manera diferente. En lugar de dividir el espacio de caracter√≠sticas y aplicar modelos locales, PLSR transforma el espacio de caracter√≠sticas de forma global para encontrar una representaci√≥n de menor dimensionalidad que sea √≥ptima para la predicci√≥n. Cuando los datos no se distribuyen linealmente, PLSR puede no ser la herramienta m√°s adecuada en su forma lineal b√°sica, ya que sigue siendo una t√©cnica lineal. Sin embargo, al encontrar las direcciones m√°s relevantes en el espacio de los datos, puede capturar aspectos importantes de la estructura de los datos que son √∫tiles incluso si la relaci√≥n subyacente es no lineal. Para manejar la no linealidad expl√≠citamente, existen extensiones como **Nonlinear Partial Least Squares (NPLS)** o **Kernel PLS (KPLS)**, que introducen funciones kernel para mapear los datos a un espacio de caracter√≠sticas de mayor dimensi√≥n donde la relaci√≥n podr√≠a ser linealmente modelable por PLS.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado (regresi√≥n y clasificaci√≥n con adaptaci√≥n)",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n si se transforma)",
  "‚úÖ Num√©ricas (requiere escalado), categ√≥ricas como dummies",
  "‚úÖ Captura relaciones lineales y reduce dimensiones simult√°neamente",
  "‚ùå No requiere estrictamente, pero mejora con residuos normales",
  "‚úÖ Deseable, aunque no cr√≠tico",
  "‚úÖ Deseable para homogeneizar varianza tras escalado",
  "‚ö†Ô∏è Moderado (outliers pueden influir en componentes latentes)",
  "‚úÖ Dise√±ado para alta colinealidad entre predictores",
  "‚ö†Ô∏è Media (componentes latentes son interpretables, pero relaciones pueden ser complejas)",
  "‚ö†Ô∏è Moderada (depende de n√∫mero de componentes y tama√±o del dataset)",
  "‚úÖ Usar k-fold para elegir n√∫mero de componentes √≥ptimos",
  "‚ùå No funciona bien si relaciones son muy no lineales o datos muy ruidosos sin preprocesar"
)

detalles <- c(
  "Modelo que proyecta predictores y respuesta a espacios latentes para maximizar covarianza.",
  "PLSR encuentra componentes que explican varianza en X y covarianza con Y.",
  "Todas las variables num√©ricas deben escalarse; convertir categ√≥ricas en indicadores.",
  "Combina reducci√≥n de dimensi√≥n (PCA-like) con regresi√≥n en componentes latentes.",
  "No impone supuestos estrictos, pero residuos normales facilitan inferencia estad√≠stica.",
  "Mejor si muestras son independientes; RLSR en datos correlacionados requiere cuidado.",
  "Escalar y homogeneizar predictores e incluso respuesta mejora la estabilidad.",
  "Outliers extremos pueden distorsionar c√°lculo de componentes; usar robust PLSR para mitigarlo.",
  "PLSR maneja colinealidad al construir pocas componentes que representan grupos de variables correlacionadas.",
  "Componentes latentes tienen pesos interpretables, pero interpretar combinaciones puede ser complejo.",
  "El m√©todo usa descomposici√≥n de matrices; eficiente con BLAS/LAPACK optimizado.",
  "Validaci√≥n cruzada ayuda a determinar el n√∫mero √≥ptimo de componentes latentes a usar.",
  "No es adecuado para relaciones puramente no lineales; en ese caso usar Kernel PLSR o m√©todos no lineales."
)

tabla_plsr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_plsr %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PLSR",
             subtitle = "Partial Least Squares Regression (PLSR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Sammon Mapping  {-}   

**Sammon Mapping**, tambi√©n conocida como **Sammon's Nonlinear Mapping (NLM)**, es una t√©cnica de **reducci√≥n de dimensionalidad no lineal** utilizada principalmente para **visualizaci√≥n de datos**. La idea fundamental de Sammon Mapping es proyectar un conjunto de datos de un espacio de alta dimensi√≥n a un espacio de menor dimensi√≥n (t√≠picamente 2D o 3D) de tal manera que se preserve lo mejor posible la **estructura de las distancias entre los puntos** del espacio original. Es decir, si dos puntos estaban cerca en el espacio de alta dimensi√≥n, se espera que tambi√©n est√©n cerca en el espacio de baja dimensi√≥n, y viceversa.

A diferencia de t√©cnicas lineales como PCA, que se enfocan en preservar la varianza o la covarianza, Sammon Mapping se centra en preservar las **distancias por pares** entre los puntos. Para lograr esto, utiliza un algoritmo iterativo que busca minimizar una **funci√≥n de error de Sammon**. Esta funci√≥n mide la diferencia entre las distancias por pares en el espacio de alta dimensi√≥n y las distancias correspondientes en el espacio de baja dimensi√≥n, ponderando m√°s los errores para los puntos que est√°n m√°s cerca en el espacio original. Esto significa que Sammon Mapping presta especial atenci√≥n a preservar la estructura local del conjunto de datos.

La funci√≥n de error de Sammon ($E$) se define como:
$$E = \frac{1}{\sum_{i < j} d_{ij}^* } \sum_{i < j} \frac{(d_{ij}^* - d_{ij})^2}{d_{ij}^*}$$
Donde:
* $d_{ij}^*$ es la distancia euclidiana entre el punto $i$ y el punto $j$ en el espacio de alta dimensi√≥n.
* $d_{ij}$ es la distancia euclidiana entre el punto $i$ y el punto $j$ en el espacio de baja dimensi√≥n.

El algoritmo busca iterativamente las coordenadas de los puntos en el espacio de baja dimensi√≥n que minimizan esta funci√≥n de error, utilizando un m√©todo de **descenso de gradiente**.

En el contexto del **aprendizaje global vs. local**, Sammon Mapping es una t√©cnica que busca un compromiso entre la preservaci√≥n de la estructura **local** y **global** de los datos. Sin embargo, su √©nfasis en ponderar los errores de las distancias peque√±as (puntos cercanos) hace que sea m√°s sensible a la **estructura local** del conjunto de datos. La idea es que, si los datos no se distribuyen linealmente, Sammon Mapping puede generar una representaci√≥n en baja dimensi√≥n que revele agrupaciones y patrones que no ser√≠an evidentes con t√©cnicas lineales como PCA. A diferencia de un m√©todo de regresi√≥n ponderada localmente como LOESS, que se enfoca en ajustar una curva, Sammon Mapping se dedica a la visualizaci√≥n de la estructura de similitud. Al intentar preservar las relaciones de distancia no lineales, aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en las proyecciones lineales, ya que se adapta a la curvatura y la estructura intr√≠nseca no lineal de los datos para ofrecer una visualizaci√≥n m√°s fiel. Aunque puede ser computacionalmente intensivo para conjuntos de datos muy grandes, es una herramienta valiosa para explorar la estructura oculta en datos complejos.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (distancias euclidianas)",
  "‚úÖ No lineal, mantiene distancias entre puntos",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è S√≠, es sensible a valores at√≠picos",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Interpretaci√≥n visual en 2D o 3D; no en componentes",
  "‚ùå Lento para conjuntos grandes (algoritmo iterativo)",
  "‚ö†Ô∏è Se puede validar visualmente o con estr√©s",
  "‚ùå Mal desempe√±o en datos ruidosos o de alta dimensi√≥n sin estructura"

)

detalles <- c(
  "M√©todo no supervisado para proyectar datos de alta dimensi√≥n en espacios de menor dimensi√≥n preservando distancias.",
  "No busca predecir, sino representar relaciones de cercan√≠a entre observaciones.",
  "Usa distancias entre puntos; solo variables num√©ricas tienen sentido.",
  "A diferencia de PCA, Sammon busca preservar distancias relativas entre puntos originales y proyectados.",
  "No genera residuos como un modelo predictivo, por lo tanto no se aplica la normalidad.",
  "No hay modelo de error porque no hay predicci√≥n.",
  "No aplica el supuesto de homoscedasticidad.",
  "Valores extremos alteran las distancias y distorsionan el mapa resultante.",
  "Como es una t√©cnica de reducci√≥n, no le afecta multicolinealidad directamente.",
  "El mapa resultante puede interpretarse en t√©rminos de proximidad, no de pesos o coeficientes.",
  "Implementaci√≥n cl√°sica es iterativa y costosa computacionalmente en datasets grandes.",
  "Puede usarse estr√©s (error entre distancias originales y proyectadas) como m√©trica de calidad.",
  "Si las distancias no reflejan bien la estructura real (por ruido o dimensiones irrelevantes), el m√©todo falla en representar datos √∫tiles."
)

tabla_sammon <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_sammon %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir sammon mapping",
             subtitle = "Sammon Mapping")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Multidimensional Scaling (MDS)  {-}    


**Multidimensional Scaling (MDS)** es una t√©cnica de **reducci√≥n de dimensionalidad** utilizada principalmente para la **visualizaci√≥n de datos** y el an√°lisis exploratorio. La idea fundamental de MDS es representar un conjunto de objetos (puntos de datos) en un espacio de baja dimensi√≥n (t√≠picamente 2D o 3D) de tal manera que las **distancias o disimilitudes entre los objetos en el espacio de baja dimensi√≥n reflejen lo m√°s fielmente posible sus distancias o disimilitudes originales** en el espacio de alta dimensi√≥n o en funci√≥n de una matriz de disimilitudes dada.

A diferencia de PCA, que opera directamente sobre una matriz de datos original (variables x observaciones), MDS t√≠picamente toma como entrada una **matriz de disimilitud** (o similitud) entre cada par de objetos. Esta matriz de disimilitud puede ser calculada a partir de las caracter√≠sticas de los objetos (ej., distancia euclidiana, distancia de Manhattan) o puede ser una medida subjetiva de disimilitud (ej., juicios humanos sobre la similitud entre productos).

Existen varias variantes de MDS, pero las m√°s comunes son:

1.  **MDS Cl√°sico (PCoA - Principal Coordinates Analysis):** Es la forma m√°s b√°sica de MDS. Asume que las disimilitudes son distancias euclidianas y es matem√°ticamente equivalente a PCA cuando se trabaja con una matriz de covarianza derivada de datos centrados. Sin embargo, su input es una matriz de disimilitudes, no directamente los datos originales.
2.  **MDS M√©trica:** Minimiza una funci√≥n de costo que mide la diferencia entre las disimilitudes observadas y las distancias en el espacio de baja dimensi√≥n. Utiliza una m√©trica de distancia espec√≠fica (ej., euclidiana).
3.  **MDS No M√©trica:** Es m√°s flexible y se utiliza cuando las disimilitudes originales son solo de naturaleza ordinal (es decir, solo sabemos que un par es "m√°s dis√≠mil" que otro, pero no la magnitud exacta de la diferencia). En lugar de preservar las distancias exactas, intenta preservar el **orden de las disimilitudes**. Minimiza una funci√≥n de costo que mide la monoton√≠a de la relaci√≥n entre las disimilitudes originales y las distancias en el espacio de baja dimensi√≥n.

El algoritmo MDS busca iterativamente las coordenadas de los puntos en el espacio de baja dimensi√≥n que minimizan una **funci√≥n de estr√©s (stress function)** o **funci√≥n de ajuste (badness-of-fit function)**. Esta funci√≥n cuantifica qu√© tan bien las distancias en la representaci√≥n de baja dimensi√≥n se ajustan a las disimilitudes originales.

En el contexto del **aprendizaje global vs. local**, MDS es un sistema que busca una representaci√≥n **global** de la estructura de las distancias. A diferencia de LOESS, que ajusta modelos a subconjuntos locales de datos, MDS intenta preservar la totalidad de las relaciones de disimilitud en una proyecci√≥n de menor dimensi√≥n. Sin embargo, su capacidad para manejar la no linealidad depende de la variante:

* **MDS Cl√°sico/M√©trico** son esencialmente m√©todos **lineales** en cuanto a la forma en que proyectan las distancias. Si los datos no se distribuyen linealmente y sus disimilitudes no son bien representadas por distancias euclidianas en un espacio de baja dimensi√≥n lineal, estos m√©todos pueden no proporcionar una buena visualizaci√≥n de la estructura intr√≠nseca. En este sentido, "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" si la relaci√≥n subyacente es fuertemente no lineal.
* **MDS No M√©trica**, al preservar el orden de las disimilitudes en lugar de las magnitudes exactas, tiene una capacidad mucho mayor para descubrir y visualizar **estructuras no lineales** en los datos. Se adapta a la forma en que las disimilitudes se "escalan" y, por lo tanto, puede ser visto como un m√©todo que busca una representaci√≥n de funci√≥n global que acomoda la no linealidad inherente en las relaciones de disimilitud.

En resumen, MDS es una herramienta poderosa para revelar la estructura latente en conjuntos de datos bas√°ndose en sus disimilitudes, y su variante no m√©trica es particularmente valiosa cuando se sospecha que las relaciones subyacentes son no lineales. 


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (requiere matriz de distancias)",
  "‚úÖ No lineal en MDS no cl√°sico; lineal en MDS cl√°sico",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è S√≠, valores at√≠picos afectan distancias",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Interpretaci√≥n visual en 2D o 3D, no en ejes significativos",
  "‚ùå Lento si se usan distancias complejas o muchos puntos",
  "‚ö†Ô∏è Validaci√≥n mediante 'stress' y visualizaci√≥n",
  "‚ùå Mal desempe√±o con datos sin estructura o ruido elevado"
)

detalles <- c(
  "M√©todo no supervisado que proyecta datos de alta dimensi√≥n en espacios de 2D o 3D preservando distancias entre puntos.",
  "No busca predecir una variable, solo representar relaciones de cercan√≠a entre observaciones.",
  "Se basa en distancias euclidianas u otras m√©tricas aplicadas a datos num√©ricos.",
  "MDS cl√°sico es lineal; el no cl√°sico (por ejemplo metric o non-metric MDS) puede modelar relaciones no lineales.",
  "No se modelan residuos, por lo que no aplica la normalidad.",
  "No hay errores de predicci√≥n, por tanto no aplica este supuesto.",
  "No hay varianzas residuales, por lo que este supuesto tampoco aplica.",
  "Valores extremos modifican distancias y distorsionan la representaci√≥n espacial.",
  "Al no haber regresores, la multicolinealidad no es un problema.",
  "El mapa generado se interpreta por proximidad relativa, no por pesos o coeficientes.",
  "Puede ser costoso computacionalmente si hay muchos puntos o si se optimiza la funci√≥n de estr√©s.",
  "Se eval√∫a qu√© tan bien se preservan las distancias originales con la m√©trica de estr√©s o visualmente.",
  "No funciona bien si los datos no tienen estructura clara, est√°n muy dispersos o contienen ruido irrelevante."
)

tabla_mds <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mds %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MDS",
             subtitle = "Multidimensional Scaling (MDS)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Projection Pursuit (PP)  {-}    

**Projection Pursuit (PP)** es una t√©cnica de **reducci√≥n de dimensionalidad y modelado exploratorio** utilizada para encontrar proyecciones interesantes de datos de alta dimensi√≥n en espacios de baja dimensi√≥n (t√≠picamente 1D o 2D). La idea fundamental de Projection Pursuit es que, en datos de alta dimensi√≥n, las estructuras interesantes (como agrupaciones, frentes, o anomal√≠as) a menudo se ocultan cuando se observan solo las proyecciones marginales de las variables individuales. PP busca **proyecciones "interesantes"** que revelen estas estructuras no gaussianas y no lineales, que podr√≠an pasar desapercibidas con t√©cnicas lineales como PCA.

A diferencia de PCA, que busca las proyecciones que maximizan la varianza (lo cual es √≥ptimo si los datos son elipsoidales o gaussianos), PP busca proyecciones que se desv√≠an de la distribuci√≥n normal. Esto se logra mediante la definici√≥n de una **funci√≥n de "√≠ndice de proyecci√≥n"** que cuantifica la "interesante" o "no-gaussiana" de una proyecci√≥n. El algoritmo entonces busca activamente las direcciones que maximizan este √≠ndice.

El proceso general de Projection Pursuit es iterativo:

1.  **Definici√≥n del √çndice de Proyecci√≥n:** Se elige una funci√≥n de √≠ndice de proyecci√≥n que penalice la normalidad y recompense las desviaciones (ej., asimetr√≠a, curtosis, o medidas basadas en entrop√≠a). Por ejemplo, un √≠ndice de proyecci√≥n podr√≠a buscar distribuciones multimodales o con colas pesadas.
2.  **Optimizaci√≥n:** El algoritmo utiliza t√©cnicas de optimizaci√≥n num√©rica (como descenso de gradiente) para encontrar la direcci√≥n de proyecci√≥n (un vector que define la combinaci√≥n lineal de las caracter√≠sticas originales) que maximiza el √≠ndice de proyecci√≥n.
3.  **Extracci√≥n de Componentes:** Una vez que se encuentra una proyecci√≥n interesante, los datos se proyectan sobre esta direcci√≥n. A menudo, el proceso se repite para encontrar proyecciones ortogonales adicionales, cada una revelando diferentes aspectos de la estructura de los datos.

**Aplicaciones de PP:**

* **Visualizaci√≥n de Datos:** Para encontrar proyecciones 1D o 2D que revelen patrones o agrupaciones.
* **Clasificaci√≥n y Regresi√≥n:** Puede ser una etapa de preprocesamiento para encontrar caracter√≠sticas m√°s informativas, o incluso puede ser la base para construir modelos (como en **Projection Pursuit Regression - PPR**).

En el contexto del **aprendizaje global vs. local**, Projection Pursuit es una t√©cnica que busca descubrir y modelar estructuras **globales** interesantes en los datos a trav√©s de una serie de **proyecciones lineales, pero la combinaci√≥n de estas proyecciones para capturar la no linealidad es la clave**. Aunque las proyecciones individuales son lineales, el proceso de buscar iterativamente las proyecciones "m√°s interesantes" y luego combinarlas permite a PP descubrir y representar relaciones **no lineales** en el conjunto de datos.

Si los datos no se distribuyen linealmente, PP es una herramienta poderosa porque **no asume linealidad** en la estructura subyacente. En lugar de ello, busca activamente las direcciones que revelan la no linealidad y la no-gaussianidad. Esto es precisamente lo que lo diferencia de PCA. La idea de que "si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n de manera ponderada localmente" es algo que PPR (una extensi√≥n de PP para regresi√≥n) aborda. PPR construye un modelo como una suma de funciones suaves no lineales de varias proyecciones uni-dimensionales de los predictores, lo que permite capturar relaciones complejas que no pueden ser modeladas por una regresi√≥n lineal simple. Por lo tanto, Projection Pursuit aborda la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos lineales o simples, al buscar activamente y explotar las caracter√≠sticas no lineales y de "inter√©s" en las proyecciones de los datos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (requiere matriz de datos)",
  "‚úÖ Detecta proyecciones no lineales con estructura interesante",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede ser sensible a valores extremos",
  "‚ö†Ô∏è Puede verse afectado si hay alta redundancia",
  "‚ö†Ô∏è Interpretaci√≥n m√°s dif√≠cil que PCA; proyecciones no son ortogonales",
  "‚ùå Puede ser lento por b√∫squeda iterativa de proyecciones",
  "‚ö†Ô∏è Validaci√≥n subjetiva o basada en heur√≠sticas de inter√©s",
  "‚ùå No √∫til si no hay estructuras no gaussianas en los datos"
)

detalles <- c(
  "M√©todo no supervisado que busca proyecciones de los datos donde se maximice cierta 'interesantitud' (varianza no gaussiana, agrupamientos, etc.).",
  "No est√° dise√±ado para predicci√≥n, sino para exploraci√≥n visual o estructural.",
  "Se aplica a datos num√©ricos, generalmente estandarizados, buscando direcciones relevantes.",
  "A diferencia del PCA (que busca m√°xima varianza), PP busca patrones como colas pesadas, clusters, o distribuciones no normales.",
  "No es un modelo predictivo, por tanto no se calculan residuos.",
  "No se modela el error; se enfoca en la estructura interna de los datos.",
  "No tiene varianzas residuales, por lo que no aplica homoscedasticidad.",
  "Proyecciones pueden verse distorsionadas por valores extremos.",
  "Variables muy correlacionadas pueden dominar las proyecciones si no se controlan.",
  "Proyecciones son dif√≠ciles de interpretar directamente; pueden requerir an√°lisis posterior.",
  "Requiere m√©todos num√©ricos iterativos para encontrar direcciones de inter√©s, lo que lo vuelve computacionalmente intensivo.",
  "Puede usarse validaci√≥n visual (por ejemplo, si se detectan agrupamientos) o criterios como 'kurtosis'.",
  "Si los datos son gaussianos y no contienen patrones relevantes, PP no encuentra proyecciones √∫tiles."
)

tabla_pp <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pp %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PP",
             subtitle = "Projection Pursuit (PP) ")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Principal Component Regression (PCR)  {-}    

**Principal Component Regression (PCR)** es una t√©cnica de **regresi√≥n** que aborda el problema de la **multicolinealidad** (altas correlaciones entre las variables predictoras) en la regresi√≥n lineal. La idea fundamental de PCR es utilizar el **An√°lisis de Componentes Principales (PCA)** como un paso previo a la regresi√≥n. En lugar de realizar la regresi√≥n directamente sobre las variables predictoras originales, PCR primero transforma las variables predictoras en un conjunto de **componentes principales no correlacionados** y luego realiza la regresi√≥n sobre un subconjunto de estos componentes.

El proceso de Principal Component Regression consta de los siguientes pasos:

1.  **An√°lisis de Componentes Principales (PCA) en las Predictoras:**
    * Primero, se aplica PCA **√∫nicamente a las variables predictoras ($X$)**. Esto genera un nuevo conjunto de variables, los **componentes principales**, que son combinaciones lineales ortogonales de las predictoras originales.
    * Los componentes principales se ordenan por la cantidad de varianza que explican en las variables $X$. El primer componente explica la mayor varianza, el segundo la siguiente mayor varianza, y as√≠ sucesivamente.

2.  **Selecci√≥n de Componentes Principales:**
    * Se selecciona un subconjunto de los componentes principales (normalmente los primeros $k$ componentes que explican una gran proporci√≥n de la varianza total en $X$, o que se consideran m√°s relevantes). La elecci√≥n de $k$ es crucial y a menudo se determina mediante validaci√≥n cruzada.
    * Es importante destacar que, a diferencia de PLSR (Partial Least Squares Regression), PCA en PCR solo considera la varianza de las variables predictoras, no su relaci√≥n con la variable de respuesta.

3.  **Regresi√≥n Lineal M√∫ltiple:**
    * Finalmente, se realiza una **regresi√≥n lineal m√∫ltiple** est√°ndar de la variable de respuesta ($Y$) sobre el subconjunto seleccionado de componentes principales. Dado que los componentes principales son ortogonales (no correlacionados), los problemas de multicolinealidad se eliminan eficazmente.

**Ventajas clave de PCR:**

* **Manejo de Multicolinealidad:** Elimina el problema de la multicolinealidad, lo que lleva a estimaciones de coeficientes m√°s estables y una mejor interpretabilidad en el contexto de los componentes principales.
* **Reducci√≥n de Dimensionalidad:** Puede ser muy √∫til cuando hay un gran n√∫mero de variables predictoras, ya que permite trabajar con un conjunto reducido de componentes que capturan la mayor parte de la informaci√≥n.
* **Reducci√≥n de Ruido:** Al centrarse en los componentes que explican la mayor varianza, puede ayudar a reducir el impacto del ruido en las variables predictoras.

En el contexto del **aprendizaje global vs. local**, Principal Component Regression es un m√©todo de **aprendizaje global**. PCA (la primera etapa de PCR) busca una transformaci√≥n lineal **global** de los datos que capture la m√°xima varianza. La regresi√≥n posterior tambi√©n es un modelo lineal global sobre estos componentes transformados.

Si bien PCR es una mejora sobre la regresi√≥n OLS cuando hay multicolinealidad, sigue siendo fundamentalmente un m√©todo **lineal**. La idea es que, si los datos no se distribuyen linealmente, PCR, en su forma lineal, **no puede capturar relaciones no lineales complejas** entre las variables originales y la variable de respuesta. Su capacidad para manejar la no linealidad es limitada a lo que puede ser capturado por las combinaciones lineales de las caracter√≠sticas originales. Precisamente porque busca una relaci√≥n lineal global en un espacio transformado linealmente, puede enfrentar la situaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" si la relaci√≥n subyacente entre las variables predictoras y la variable de respuesta es intr√≠nsecamente no lineal. Para tales casos, se necesitar√≠an m√©todos de regresi√≥n no lineal o algoritmos de *machine learning* que puedan modelar la no linealidad, como los √°rboles de decisi√≥n, SVM con kernels no lineales, o redes neuronales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (combinaci√≥n de PCA + regresi√≥n)",
  "‚úÖ Variable continua (num√©rica)",
  "‚úÖ Num√©ricas (se aplica PCA primero)",
  "‚úÖ Puede capturar relaciones lineales (con reducci√≥n de dimensionalidad)",
  "‚ö†Ô∏è Requiere verificar residuos del modelo final",
  "‚ö†Ô∏è Se deben revisar los residuos como en regresi√≥n cl√°sica",
  "‚ö†Ô∏è Requiere diagn√≥stico posterior a la regresi√≥n",
  "‚ö†Ô∏è PCA puede estar influenciada por outliers",
  "‚úÖ Reduce multicolinealidad usando componentes ortogonales",
  "‚ö†Ô∏è Menos interpretable (usa componentes, no variables originales)",
  "‚úÖ Eficiente, especialmente con datos de alta dimensi√≥n",
  "‚úÖ Puede usar validaci√≥n cruzada para elegir n√∫mero de componentes",
  "‚ùå Si las primeras componentes no explican bien la variable respuesta"
)

detalles <- c(
  "Modelo supervisado que aplica PCA a los predictores y luego ajusta una regresi√≥n lineal sobre los componentes principales seleccionados.",
  "Se requiere que la variable dependiente sea num√©rica (continua).",
  "Se espera que los predictores sean num√©ricos para aplicar PCA adecuadamente.",
  "PCR puede detectar relaciones lineales al reducir la dimensionalidad primero y luego ajustar la regresi√≥n.",
  "Aunque el PCA es no supervisado, los residuos de la regresi√≥n deben ser normales para cumplir los supuestos de OLS.",
  "Es necesario revisar la independencia de errores como en cualquier regresi√≥n lineal.",
  "Tambi√©n deben analizarse posibles problemas de heterocedasticidad en los residuos.",
  "Outliers pueden influir en los componentes principales y, por lo tanto, en el modelo final.",
  "PCR es muy √∫til cuando los predictores est√°n altamente correlacionados.",
  "Interpretar los resultados puede ser dif√≠cil porque las componentes no corresponden a variables originales.",
  "El proceso es r√°pido incluso con muchos predictores, ya que PCA reduce la dimensi√≥n.",
  "Usualmente se usa validaci√≥n cruzada para determinar cu√°ntas componentes usar.",
  "No es efectivo si los primeros componentes (con mayor varianza) no est√°n relacionados con la variable dependiente."
)

tabla_pcr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pcr %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PCR",
             subtitle = "Principal Component Regression (PCR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Partial Least Squares Discriminant Analysis (PLSDA)  {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (binaria o multicategor√≠a)",
  "‚úÖ Num√©ricas (se proyectan a componentes)",
  "‚úÖ Captura relaciones lineales y no lineales a trav√©s de proyecciones",
  "‚ùå No aplica directamente (modelo de clasificaci√≥n)",
  "‚ùå No aplica como en regresi√≥n cl√°sica",
  "‚ùå No se eval√∫a como en modelos de regresi√≥n",
  "‚ö†Ô∏è Algo sensible a outliers (pueden influir en componentes)",
  "‚úÖ Muy √∫til si hay multicolinealidad",
  "‚ö†Ô∏è Menos interpretable que modelos cl√°sicos; depende de componentes",
  "‚úÖ Eficiente, especialmente con datos de alta dimensi√≥n",
  "‚úÖ Se recomienda usar validaci√≥n cruzada para elegir el n√∫mero de componentes",
  "‚ùå Si las proyecciones no separan bien las clases o hay mucho ruido"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n basado en PLS (Partial Least Squares) que proyecta los datos para maximizar la separaci√≥n entre clases.",
  "Se requiere que la variable dependiente sea categ√≥rica. PLS-DA funciona bien con 2 o m√°s clases.",
  "Las variables predictoras deben ser num√©ricas para que el modelo pueda proyectarlas en componentes latentes.",
  "El modelo encuentra combinaciones de predictores que mejor separan las clases en el espacio proyectado.",
  "No se eval√∫a normalidad de residuos como en modelos de regresi√≥n; la salida es de clasificaci√≥n.",
  "Tampoco aplica la independencia cl√°sica de errores ya que se clasifican observaciones.",
  "El supuesto de homoscedasticidad no es relevante aqu√≠.",
  "Outliers pueden afectar la construcci√≥n de componentes, distorsionando la separaci√≥n de clases.",
  "PLS-DA es √∫til cuando los predictores est√°n altamente correlacionados, ya que crea componentes ortogonales.",
  "Los componentes no son directamente interpretables como las variables originales, aunque se pueden analizar los pesos de carga.",
  "Es un algoritmo relativamente eficiente, especialmente para conjuntos con muchas variables.",
  "La validaci√≥n cruzada es cr√≠tica para seleccionar el n√∫mero √≥ptimo de componentes y evitar overfitting.",
  "No funciona bien si las clases no est√°n bien separadas en el espacio proyectado o si hay demasiado ruido en los datos."
)

tabla_plsda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_plsda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PLSDA",
             subtitle = "Partial Least Squares Discriminant Analysis (PLSDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Mixture Discriminant Analysis (MDA)  {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas",
  "‚úÖ No lineal (usa mezclas de gaussianas para modelar clases)",
  "‚ùå No aplica como en regresi√≥n",
  "‚ùå No se eval√∫a como en regresi√≥n",
  "‚ö†Ô∏è Supone varianza homog√©nea dentro de componentes, pero puede variar entre clases",
  "‚ö†Ô∏è Puede ser sensible a outliers (afectan las medias y covarianzas)",
  "‚ö†Ô∏è Puede verse afectado, aunque usa reducci√≥n dimensional",
  "‚ö†Ô∏è Moderadamente interpretable (depende de componentes gaussianos)",
  "‚ö†Ô∏è M√°s lento que LDA/QDA, pero m√°s flexible",
  "‚úÖ Recomendable para elegir n√∫mero de componentes y evitar sobreajuste",
  "‚ùå Mal desempe√±o si la distribuci√≥n dentro de clases no es bien modelada por gaussianas"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n que combina regresi√≥n discriminante con mezclas gaussianas dentro de cada clase.",
  "Se usa para clasificar observaciones en grupos definidos por una variable categ√≥rica.",
  "Requiere predictores num√©ricos para ajustar distribuciones normales multivariadas.",
  "Modela cada clase como una combinaci√≥n de distribuciones gaussianas, permitiendo formas no lineales.",
  "No hay residuos como en regresi√≥n, ya que se trata de una tarea de clasificaci√≥n.",
  "No eval√∫a independencia cl√°sica de errores; se enfoca en estimar la densidad condicional.",
  "Permite varianza distinta entre componentes, pero se puede ajustar homogeneidad seg√∫n implementaci√≥n.",
  "Outliers pueden afectar las medias y varianzas estimadas de las mezclas gaussianas.",
  "La multicolinealidad puede dificultar la estimaci√≥n de matrices de covarianza.",
  "Interpretar los componentes internos (medias y pesos) puede ser complejo, pero ofrece buena visualizaci√≥n.",
  "Es m√°s lento que LDA o QDA por su naturaleza iterativa y uso de EM (Expectation-Maximization).",
  "Se puede usar validaci√≥n cruzada para seleccionar el n√∫mero √≥ptimo de mezclas por clase.",
  "Si las clases no se ajustan bien a combinaciones de gaussianas, el modelo pierde precisi√≥n."
)

tabla_mda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MDA",
             subtitle = "Mixture Discriminant Analysis (MDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Quadratic Discriminant Analysis (QDA) {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas",
  "‚úÖ Modela separaci√≥n cuadr√°tica entre clases",
  "‚ùå No aplica (clasificaci√≥n, no regresi√≥n)",
  "‚ùå No aplica (se asume independencia dentro de clases)",
  "‚ùå No se asume homoscedasticidad (cada clase tiene su propia matriz de covarianza)",
  "‚ö†Ô∏è Puede ser muy sensible a outliers (afectan las matrices de covarianza)",
  "‚ö†Ô∏è Puede verse afectado, especialmente si hay pocos datos",
  "‚úÖ Relativamente interpretable (fronteras no lineales entre clases)",
  "‚ö†Ô∏è M√°s costoso que LDA; ineficiente con pocos datos o muchas variables",
  "‚úÖ Recomendado para evitar overfitting, especialmente con pocos datos",
  "‚ùå Si hay pocos datos por clase, estimar matrices de covarianza es inestable"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n que permite que cada clase tenga su propia matriz de covarianza.",
  "Se utiliza para predecir a qu√© clase pertenece una observaci√≥n con base en sus caracter√≠sticas.",
  "Requiere predictores num√©ricos continuos, ya que calcula medias y covarianzas.",
  "A diferencia de LDA, permite fronteras no lineales al no asumir varianzas iguales entre clases.",
  "No tiene residuos como en regresi√≥n, por lo que el supuesto de normalidad de errores no aplica.",
  "No aplica el supuesto de independencia de errores; se enfoca en la distribuci√≥n conjunta por clase.",
  "Cada clase tiene su propia varianza y covarianza, lo que lo hace m√°s flexible que LDA.",
  "Valores extremos pueden distorsionar la estimaci√≥n de medias y covarianzas de cada clase.",
  "Multicolinealidad puede dificultar la inversi√≥n de la matriz de covarianza en clases peque√±as.",
  "Los coeficientes y decisiones son interpretables en t√©rminos de separaciones estad√≠sticas entre clases.",
  "M√°s lento y costoso computacionalmente que LDA, especialmente con muchas variables.",
  "La validaci√≥n cruzada ayuda a prevenir sobreajuste y a seleccionar caracter√≠sticas relevantes.",
  "Con clases poco representadas o muchas variables, las matrices de covarianza pueden volverse inestables."
)

tabla_qda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles) 

tabla_qda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir QDA",
             subtitle = "Quadratic Discriminant Analysis (QDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Regularized Discriminant Analysis (RDA)  {-}   

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas",
  "‚úÖ No lineal (transici√≥n entre LDA y QDA)",
  "‚ùå No aplica (no es un modelo de regresi√≥n)",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Controla la homoscedasticidad mediante regularizaci√≥n",
  "‚ö†Ô∏è Puede ser sensible, aunque la regularizaci√≥n reduce impacto",
  "‚úÖ Reduce impacto mediante regularizaci√≥n de covarianzas",
  "‚ö†Ô∏è Menos interpretable que LDA/QDA puro, pero con mayor flexibilidad",
  "‚úÖ M√°s eficiente que QDA en conjuntos peque√±os o ruidosos",
  "‚úÖ Muy √∫til para evitar overfitting, sobre todo con validaci√≥n cruzada",
  "‚ùå Puede no mejorar sobre LDA/QDA si no hay problemas de varianza o sobreajuste"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n que combina LDA y QDA usando par√°metros de regularizaci√≥n.",
  "Clasifica observaciones en clases discretas bas√°ndose en variables num√©ricas predictoras.",
  "Requiere variables num√©ricas para calcular medias y covarianzas por clase.",
  "Introduce par√°metros de mezcla que ajustan la matriz de covarianza hacia la identidad (como ridge) y hacia la covarianza com√∫n.",
  "No genera residuos como un modelo de regresi√≥n, por lo tanto el supuesto no aplica.",
  "No se enfoca en errores independientes, sino en distribuciones de clase.",
  "La regularizaci√≥n suaviza las diferencias entre covarianzas, mitigando problemas de homoscedasticidad.",
  "Los valores at√≠picos pueden influir en la estimaci√≥n, pero se reduce con regularizaci√≥n.",
  "Mejor manejo de multicolinealidad que QDA gracias a la matriz regularizada.",
  "La interpretaci√≥n depende de los valores de regularizaci√≥n elegidos; m√°s flexible pero menos directa.",
  "Reduce complejidad computacional respecto a QDA; √∫til con pocas observaciones por clase.",
  "Es com√∫n usar validaci√≥n cruzada para seleccionar los par√°metros de regularizaci√≥n √≥ptimos.",
  "No aporta mejoras significativas si los supuestos de LDA o QDA se cumplen perfectamente sin sobreajuste."
)

tabla_rda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_rda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RDA",
             subtitle = "Regularized Discriminant Analysis (RDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Flexible Discriminant Analysis (FDA)  {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas (puede usar transformaciones)",
  "‚úÖ No lineal (usa regresi√≥n flexible en el espacio transformado)",
  "‚ùå No aplica (no es regresi√≥n de residuos)",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è FDA suaviza este supuesto al modelar relaciones no lineales",
  "‚ö†Ô∏è Puede ser sensible a outliers, dependiendo del m√©todo de ajuste",
  "‚ö†Ô∏è Puede mitigar multicolinealidad si se usa penalizaci√≥n",
  "‚ö†Ô∏è Menos interpretable que LDA, pero permite mayor flexibilidad",
  "‚ö†Ô∏è Menor eficiencia que LDA por mayor complejidad computacional",
  "‚úÖ Validaci√≥n cruzada √∫til para seleccionar transformaciones o suavizados",
  "‚ùå En datos con pocos casos o ruido excesivo puede sobreajustarse"
)

detalles <- c(
  "Extensi√≥n de LDA que permite relaciones no lineales entre predictores y clases mediante t√©cnicas como splines o regresi√≥n flexible.",
  "Clasifica observaciones en clases categ√≥ricas bas√°ndose en predictores transformados.",
  "Admite variables num√©ricas, las cuales pueden ser transformadas de forma no lineal.",
  "Usa regresi√≥n no lineal flexible (como splines) para modelar relaciones complejas en el espacio de discriminaci√≥n.",
  "No genera residuos como regresi√≥n tradicional; es un modelo de clasificaci√≥n.",
  "No se enfoca en errores secuenciales o dependientes.",
  "Relaja la homocedasticidad al no asumir distribuci√≥n gaussiana estricta.",
  "Puede verse afectado por valores extremos, seg√∫n el m√©todo de suavizado.",
  "La transformaci√≥n flexible puede reducir colinealidad, pero no siempre la elimina.",
  "Los coeficientes y funciones discriminantes pueden ser dif√≠ciles de interpretar si se usan transformaciones complejas.",
  "Mayor costo computacional que LDA, pero m√°s potente en patrones no lineales.",
  "Se recomienda CV para evaluar desempe√±o y evitar overfitting en el proceso de ajuste flexible.",
  "Si los datos no requieren flexibilidad o el tama√±o muestral es bajo, FDA puede ser innecesariamente complejo."
)

tabla_fda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_fda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir FDA",
             subtitle = "Flexible Discriminant Analysis (FDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Linear Discriminant Analysis (LDA)  {-}   

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas (preferentemente)",
  "‚úÖ Asume relaciones lineales entre variables y clases",
  "‚úÖ Supone normalidad multivariante de los predictores dentro de cada clase",
  "‚úÖ Supone independencia entre observaciones",
  "‚úÖ Asume varianza-covarianza igual entre clases (homocedasticidad)",
  "‚ö†Ô∏è Sensible a valores at√≠picos",
  "‚ö†Ô∏è Puede verse afectado negativamente por alta colinealidad",
  "‚úÖ Alta, coeficientes discriminantes son interpretables",
  "‚úÖ Muy eficiente computacionalmente",
  "‚úÖ Se recomienda para evaluar estabilidad y evitar sobreajuste",
  "‚ùå Mal desempe√±o si no se cumplen supuestos de normalidad y homocedasticidad"
)

detalles <- c(
  "Modelo supervisado cl√°sico para clasificaci√≥n que encuentra combinaciones lineales de predictores que separan clases.",
  "Requiere una variable categ√≥rica como objetivo, con dos o m√°s clases.",
  "Mejor con predictores num√©ricos continuos; categ√≥ricos requieren codificaci√≥n previa.",
  "Calcula funciones discriminantes lineales que maximizan la separaci√≥n entre clases.",
  "Cada grupo debe seguir una distribuci√≥n normal multivariante para resultados √≥ptimos.",
  "Las observaciones deben ser independientes para validez de inferencia.",
  "Supone igual matriz de covarianzas entre grupos; si no se cumple, usar QDA.",
  "Outliers influyen en la media y la varianza estimada, distorsionando fronteras.",
  "Multicolinealidad puede hacer que los coeficientes discriminantes sean inestables.",
  "Las funciones discriminantes se interpretan como direcciones de m√°xima separaci√≥n.",
  "Requiere bajo costo computacional y se entrena r√°pidamente.",
  "Se puede usar validaci√≥n cruzada para elegir el n√∫mero de componentes o verificar precisi√≥n.",
  "Cuando los datos no cumplen normalidad ni homocedasticidad, el modelo pierde precisi√≥n."
)

tabla_lda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LDA",
             subtitle = "Linear Discriminant Analysis (LDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



<!--chapter:end:05-dimensionality_reduction.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üß¨ **6. Bayesianos** {-}  

**Ejemplos:** Naive Bayes, Bayesian Networks  
**Cu√°ndo usarlo:**   

* Clasificaci√≥n r√°pida con supuestos simples.
* Problemas de texto o spam detection.

**Ventajas:** Muy r√°pidos, bien fundamentados.   
**Limitaciones:** Supone independencia de variables (no siempre cierto).

---

## Naive Bayes (NB) {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Categ√≥ricas o num√©ricas (seg√∫n variante del modelo)",
  "‚ùå Asume independencia condicional entre predictores",
  "‚ùå No aplica (no es regresi√≥n)",
  "‚úÖ Observaciones deben ser independientes",
  "‚ùå No se requiere homocedasticidad",
  "‚ö†Ô∏è Puede ser sensible si se usa con predictores num√©ricos y hay valores extremos",
  "‚úÖ No afectado por multicolinealidad debido a suponer independencia",
  "‚úÖ Alta, se pueden interpretar probabilidades y efectos de cada variable",
  "‚úÖ Muy r√°pido incluso con grandes conjuntos de datos",
  "‚úÖ Puede usarse para afinar y evaluar desempe√±o del modelo",
  "‚ùå Bajo rendimiento si los predictores no son realmente independientes o las distribuciones asumidas no se cumplen"
)

detalles <- c(
  "Clasificador probabil√≠stico basado en el teorema de Bayes con asunci√≥n de independencia condicional entre predictores.",
  "Funciona para clasificaci√≥n en m√∫ltiples clases categ√≥ricas.",
  "Puede trabajar con variables categ√≥ricas (Multinomial NB) o num√©ricas (Gaussian NB).",
  "Supone que las variables predictoras son independientes entre s√≠ dentro de cada clase.",
  "No genera residuos en el sentido cl√°sico porque no es un modelo de regresi√≥n.",
  "Las observaciones deben ser independientes para que las probabilidades se combinen correctamente.",
  "No requiere igualdad de varianzas; Gaussian NB asume varianza igual por clase pero se puede ajustar.",
  "Los valores at√≠picos pueden distorsionar la estimaci√≥n de probabilidades si hay predictores num√©ricos.",
  "La independencia entre predictores hace que la multicolinealidad no sea problema.",
  "Los resultados pueden interpretarse en t√©rminos de probabilidades a posteriori por clase.",
  "Muy eficiente para entrenamiento y predicci√≥n, incluso con muchos atributos.",
  "Puede validarse usando k-fold o leave-one-out para asegurar estabilidad del modelo.",
  "El supuesto fuerte de independencia condicional rara vez se cumple completamente, lo que puede afectar la precisi√≥n."
)

tabla_nb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_nb %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir NB",
             subtitle = "Naive Bayes (NB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Averaged One - Dependence Estimators (AODE)  {-}   

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica",
  "‚úÖ Categ√≥ricas principalmente",
  "‚ö†Ô∏è Modela dependencias limitadas entre atributos (mejora sobre NB)",
  "‚ùå No aplica (no es regresi√≥n)",
  "‚úÖ Requiere independencia entre instancias",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede verse afectado por outliers si se usan variables num√©ricas sin tratamiento",
  "‚úÖ Menos afectado que Naive Bayes por dependencias entre atributos",
  "‚ö†Ô∏è Moderadamente interpretable (combinaci√≥n de varios modelos NB con 1 dependencia)",
  "‚ö†Ô∏è M√°s costoso que NB, pero a√∫n eficiente",
  "‚úÖ Puede validarse mediante k-fold cross-validation",
  "‚ùå Desempe√±a mal con muchos atributos irrelevantes o con pocos datos por combinaci√≥n de atributos"
)

detalles <- c(
  "Clasificador bayesiano que promedia modelos con una √∫nica dependencia entre pares de atributos para mejorar sobre Naive Bayes.",
  "Dise√±ado para problemas de clasificaci√≥n con clases categ√≥ricas.",
  "Se usa t√≠picamente con variables categ√≥ricas, aunque puede adaptarse a discretizadas.",
  "Relaja el supuesto de independencia total de Naive Bayes permitiendo una √∫nica dependencia por atributo.",
  "No es un modelo de regresi√≥n, por lo que no aplica el supuesto de normalidad de residuos.",
  "Las observaciones deben ser independientes para que las estimaciones sean v√°lidas.",
  "No supone homoscedasticidad debido a su naturaleza probabil√≠stica.",
  "Los valores at√≠picos pueden afectar la calidad de la estimaci√≥n de probabilidades.",
  "Tolera mejor la multicolinealidad al permitir dependencias limitadas entre atributos.",
  "La interpretaci√≥n es m√°s compleja que NB, pero a√∫n comprensible por su estructura promedio.",
  "Requiere m√°s tiempo de c√≥mputo que NB, pero sigue siendo razonablemente eficiente.",
  "La validaci√≥n cruzada es √∫til para evaluar el desempe√±o y generalizaci√≥n del modelo.",
  "El rendimiento cae si hay muchos atributos irrelevantes o datos escasos por combinaci√≥n de atributos."
)

tabla_aode <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_aode %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir AODE",
             subtitle = "Averaged One - Dependence Estimators (AODE)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Bayesian Belief Network (BBN)  {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado y no supervisado (estructuras probabil√≠sticas)",
  "‚úÖ Categ√≥rica o continua (depende de implementaci√≥n)",
  "‚úÖ Categ√≥ricas o continuas discretizadas",
  "‚úÖ Modela relaciones condicionales entre variables (gr√°ficamente)",
  "‚ùå No aplica (no es modelo de regresi√≥n lineal)",
  "‚úÖ Requiere independencia condicional entre nodos seg√∫n la red",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede ser sensible a outliers si se estiman mal las distribuciones",
  "‚úÖ Puede manejar correlaci√≥n entre variables de forma expl√≠cita en la red",
  "‚úÖ Alta interpretabilidad visual con grafos dirigidos ac√≠clicos",
  "‚ö†Ô∏è Costoso computacionalmente en grandes redes o aprendizaje estructural",
  "‚úÖ Validaci√≥n cruzada puede aplicarse en tareas supervisadas (clasificaci√≥n)",
  "‚ùå Mal rendimiento si hay muchas variables irrelevantes o dependencias no detectadas"
)

detalles <- c(
  "Modelo probabil√≠stico gr√°fico que representa relaciones condicionales entre variables mediante una red bayesiana (DAG).",
  "Puede usarse para clasificaci√≥n, predicci√≥n o inferencia probabil√≠stica.",
  "Se adapta a datos categ√≥ricos principalmente, pero tambi√©n se puede usar con discretizaci√≥n de continuas.",
  "Captura relaciones condicionales entre variables expl√≠citamente como conexiones dirigidas.",
  "No genera residuos como los modelos de regresi√≥n, por lo que la normalidad no aplica.",
  "Las dependencias condicionales deben estar bien modeladas en la estructura de la red.",
  "No hay un modelo de varianza/residuos tradicional como para aplicar homoscedasticidad.",
  "Distribuciones err√≥neas o mal estimadas pueden afectar resultados, especialmente con valores extremos.",
  "El modelo representa expl√≠citamente la correlaci√≥n entre variables mediante arcos.",
  "La estructura de la red permite ver c√≥mo interact√∫an las variables entre s√≠.",
  "El aprendizaje estructural y de par√°metros puede ser costoso en t√©rminos computacionales.",
  "Si se usa para clasificaci√≥n, la validaci√≥n cruzada es una forma est√°ndar de evaluaci√≥n.",
  "BBN requiere una buena estructura; datos mal preparados o muy ruidosos deterioran su capacidad explicativa."
)

tabla_bbn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bbn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir BBN",
             subtitle = "Bayesian Belief Network (BBN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Gaussian Naive Bayes (GNB) {-}    

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas (asume distribuci√≥n normal por clase)",
  "‚ùå No modela relaciones entre predictores (independencia asumida)",
  "‚ùå No aplica (no hay residuos como en regresi√≥n)",
  "‚úÖ Asume independencia condicional entre predictores",
  "‚úÖ Cada predictor se modela con varianza homog√©nea por clase",
  "‚ö†Ô∏è Sensible a outliers porque afectan media y varianza de la normal",
  "‚ö†Ô∏è Alta multicolinealidad viola el supuesto de independencia",
  "‚úÖ Altamente interpretable: muestra probabilidades y distribuci√≥n por clase",
  "‚úÖ Muy r√°pido y eficiente, incluso con grandes datasets",
  "‚úÖ Se puede validar f√°cilmente con k-fold o hold-out",
  "‚ùå Si los predictores no son aproximadamente normales por clase, el rendimiento baja"
)

detalles <- c(
  "Clasificador probabil√≠stico que asume que cada predictor sigue una distribuci√≥n normal dentro de cada clase.",
  "Se utiliza para predecir clases categ√≥ricas a partir de predictores continuos.",
  "Cada variable num√©rica se modela con una distribuci√≥n Gaussiana separada por clase.",
  "No considera correlaciones entre predictores; cada uno contribuye de manera independiente.",
  "No genera residuos como un modelo de regresi√≥n, as√≠ que no aplica normalidad de errores.",
  "El supuesto clave es independencia condicional entre predictores dado la clase.",
  "Cada variable tiene su propia media y varianza por clase, sin heterocedasticidad.",
  "Los valores at√≠picos pueden distorsionar los par√°metros estimados de la distribuci√≥n normal.",
  "Altamente correlacionadas violan la independencia condicional asumida y afectan rendimiento.",
  "F√°cil de explicar: se basa en la probabilidad de cada clase dado cada predictor.",
  "Uno de los algoritmos m√°s r√°pidos para clasificaci√≥n supervisada.",
  "Evaluaci√≥n est√°ndar con validaci√≥n cruzada o conjunto de prueba.",
  "Si las variables no tienen forma aproximadamente normal dentro de clases, el modelo puede clasificarlas mal."
)

tabla_gnb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_gnb %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GNB",
             subtitle = "Gaussian Naive Bayes (GNB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Multinomial Naive Bayes (MNB) {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Discretas, conteos (ej. frecuencia de palabras)",
  "‚ùå No modela relaciones entre predictores (asume independencia condicional)",
  "‚ùå No aplica (no hay residuos)",
  "‚úÖ Asume independencia condicional entre predictores",
  "‚ùå No aplica (no se modela varianza)",
  "‚ö†Ô∏è Menos sensible a outliers que Gaussian NB, pero a√∫n puede verse afectado",
  "‚ö†Ô∏è Multicolinealidad viola el supuesto de independencia y puede degradar el rendimiento",
  "‚úÖ Muy interpretable: probabilidades por clase y variable",
  "‚úÖ Extremadamente eficiente en problemas de texto y alta dimensi√≥n",
  "‚úÖ Puede usarse k-fold o validaci√≥n simple",
  "‚ùå No es adecuado para variables continuas o datos que no representen conteos"
)

detalles <- c(
  "Clasificador basado en probabilidad que modela la distribuci√≥n multinomial de conteos por clase.",
  "Usado t√≠picamente para clasificaci√≥n de texto, spam detection, y otros problemas con datos categ√≥ricos o de conteo.",
  "Funciona mejor con variables que representan frecuencia (n√∫mero de veces que aparece un t√©rmino).",
  "Asume que los predictores son independientes condicionalmente dados la clase, sin correlaci√≥n entre ellos.",
  "No hay residuos como en modelos de regresi√≥n, por lo tanto no aplica este supuesto.",
  "La independencia condicional de los predictores es un supuesto fundamental del modelo.",
  "Como no se modela la varianza expl√≠citamente, el supuesto de homoscedasticidad no aplica.",
  "Outliers tienen menor efecto porque se espera que los datos est√©n en formato de conteo (discretos).",
  "Predictores altamente correlacionados pueden afectar negativamente la precisi√≥n del modelo.",
  "La probabilidad condicional de cada clase y predictor es f√°cil de interpretar.",
  "Muy r√°pido incluso en datasets grandes con miles de caracter√≠sticas (como texto).",
  "La precisi√≥n puede evaluarse con validaci√≥n cruzada como en otros clasificadores.",
  "Si las variables son continuas o no reflejan bien los conteos, el modelo puede fallar."
)

tabla_mnb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mnb %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MNB",
             subtitle = "Multinomial Naive Bayes (MNB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Bayesian Network (BN)  {-}   

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado o no supervisado",
  "‚úÖ Categ√≥rica o continua (depende del tipo de red)",
  "‚úÖ Categ√≥ricas y/o continuas",
  "‚úÖ Modela relaciones condicionales entre variables (estructura dirigida)",
  "‚ùå No aplica (no hay residuos t√≠picos)",
  "‚ö†Ô∏è Depende de la estructura de la red",
  "‚ùå No aplica como en regresi√≥n cl√°sica",
  "‚ö†Ô∏è Puede ser sensible si afecta las probabilidades condicionales",
  "‚ö†Ô∏è Puede causar redundancia si no se ajusta bien la estructura",
  "‚úÖ Muy interpretables si se visualiza la red y se conocen las dependencias",
  "‚ö†Ô∏è Aprendizaje de estructura puede ser computacionalmente costoso",
  "‚úÖ Puede usarse validaci√≥n cruzada para evaluar rendimiento predictivo",
  "‚ùå Cuando hay muchas variables y poca informaci√≥n para definir relaciones"
)

detalles <- c(
  "Modelo probabil√≠stico que representa relaciones condicionales entre variables mediante un grafo dirigido ac√≠clico (DAG).",
  "Puede predecir una variable (modo supervisado) o descubrir estructura entre variables (modo no supervisado).",
  "Acepta variables mixtas, aunque muchas implementaciones requieren discretizaci√≥n.",
  "Cada nodo representa una variable y las conexiones modelan dependencias condicionales.",
  "No genera residuos como los modelos de regresi√≥n, por lo que no aplica este supuesto.",
  "Algunas estructuras pueden implicar independencia condicional; otras no.",
  "No se eval√∫a homoscedasticidad, pues no hay predicci√≥n de error num√©rico.",
  "Valores at√≠picos pueden sesgar las probabilidades estimadas si no se controlan.",
  "Si hay variables redundantes o fuertemente correlacionadas, se debe ajustar la estructura de la red para evitar errores.",
  "La red permite interpretar c√≥mo influyen unas variables sobre otras, ideal para razonamiento causal.",
  "El ajuste de par√°metros es eficiente, pero aprender la estructura de la red puede ser lento.",
  "Puede evaluarse el desempe√±o con k-fold o validaci√≥n simple en tareas supervisadas.",
  "Cuando no se tiene informaci√≥n previa o datos suficientes, la red puede no capturar relaciones reales."
)

tabla_bn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir BN",
             subtitle = "Bayesian Network (BN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



<!--chapter:end:06-bayesian.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üßÆ **7. Regularizaci√≥n** {-}  

**Ejemplos:** L1 (Lasso), L2 (Ridge), Elastic Net   
**Cu√°ndo usarlo:**   

* Para evitar sobreajuste en modelos lineales o redes neuronales.
* Cuando tienes muchas variables (alta dimensionalidad).

**Ventajas:** Penaliza modelos complejos.   
**Limitaciones:** Puede eliminar variables √∫tiles si se usa en exceso.

---

## Ridge Regression  {-}   

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal (como OLS)",
  "‚ö†Ô∏è Supuesto deseable pero no estricto",
  "‚úÖ Supuesto necesario",
  "‚úÖ Supuesto necesario",
  "‚ö†Ô∏è Puede verse afectado, pero menos que OLS",
  "‚úÖ Dise√±ado para mitigarla mediante penalizaci√≥n",
  "‚ö†Ô∏è Menos interpretable que OLS (coeficientes sesgados)",
  "‚úÖ Eficiente incluso con muchas variables",
  "‚úÖ Requiere validaci√≥n para ajustar par√°metro lambda",
  "‚ùå Si la relaci√≥n no es lineal o hay muchas variables irrelevantes"
)

detalles <- c(
  "Extensi√≥n de la regresi√≥n lineal que agrega penalizaci√≥n L2 para reducir sobreajuste y manejar multicolinealidad.",
  "Se utiliza cuando se desea predecir una variable num√©rica continua.",
  "Las variables deben ser num√©ricas y estar estandarizadas para que la penalizaci√≥n tenga sentido.",
  "Asume relaci√≥n lineal entre predictores y variable respuesta, como la regresi√≥n lineal.",
  "La normalidad es deseable para inferencia, pero no indispensable para predicci√≥n.",
  "Se espera independencia entre observaciones para que el modelo sea v√°lido.",
  "Es importante que los errores tengan varianza constante para predicciones fiables.",
  "Reduce varianza, pero valores extremos a√∫n pueden afectar los resultados.",
  "La penalizaci√≥n reduce varianza al achicar coeficientes, √∫til con predictores correlacionados.",
  "Coeficientes penalizados dificultan la interpretaci√≥n directa, pero mejoran estabilidad.",
  "R√°pido y adecuado para problemas con muchas variables; incluso p > n.",
  "Se usa validaci√≥n cruzada para elegir el mejor valor de lambda (par√°metro de regularizaci√≥n).",
  "No se recomienda cuando la relaci√≥n entre variables es no lineal o se requiere interpretaci√≥n clara."
)

tabla_ridge <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_ridge %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir ridge",
             subtitle = "Ridge Regression")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Least Absolute Shrinkage and Selection Operator (LASSO)  {-}  

```{r, echo = FALSE}

```


## Elastic Net  {-}  

```{r, echo = FALSE}

```


## Least Angle Regression (LARS)  {-}   

```{r, echo = FALSE}

```



<!--chapter:end:07-regularization.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üîç **8. Instance-Based (Basados en Instancias)** {-}  

**Ejemplos:** K-Nearest Neighbors (KNN)   
**Cu√°ndo usarlo:**   

* Pocos datos, con patrones locales claros.  
* Cuando la similitud entre casos es importante.

**Ventajas:** Simple y eficaz en problemas de baja dimensi√≥n.   
**Limitaciones:** Escala mal con muchos datos; sensible al ruido.

---

## k - Nearest Neighbour (kNN)  {-}   

```{r, echo = FALSE}

```


## Learning Vector Quantization (LVQ)  {-}   

```{r, echo = FALSE}

```


## Self - Organizing Map (SOM)  {-} 

```{r, echo = FALSE}

```


## Locally Weigted Learning (LWL)  {-}   

```{r, echo = FALSE}

```


<!--chapter:end:08-instance_based.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üìè **9. Clustering (No Supervisado)** {-}  

**Ejemplos:** K-Means, DBSCAN, Hierarchical Clustering  
**Cu√°ndo usarlo:**   

* Agrupar datos sin etiquetas previas.
* Descubrir estructuras ocultas o segmentos de mercado.

**Ventajas:** √ötil en exploraci√≥n y reducci√≥n de complejidad.   
**Limitaciones:** Requiere elegir n√∫mero de grupos (excepto DBSCAN); puede ser sensible a escala.

---

## k-Means  {-}  

```{r, echo = FALSE}

```


## k-Medians  {-}   

```{r, echo = FALSE}

```

## Expectation Maximization  {-}  

```{r, echo = FALSE}

```

## Hierarchical Clustering  {-}  

```{r, echo = FALSE}

```

## DBSCAN  {-}    

```{r, echo = FALSE}

```


<!--chapter:end:09-clustering.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üìê **10. Sistemas Basados en Reglas (Rule-Based Systems)** {-}

**Ejemplos:** RuleFit, Decision Rules, l√≥gica difusa
**Cu√°ndo usarlo:**

* Interpretabilidad es clave (por ejemplo, decisiones legales o m√©dicas).
* Incorporar conocimiento experto.

**Ventajas:** F√°cil de entender y auditar.   
**Limitaciones:** No tan precisos como otros m√©todos en datos complejos.

---

## Cubist  {-}  

```{r, echo = FALSE}

```



## One Rule (OneR)  {-}   

```{r, echo = FALSE}

```


## Zero Rule (ZeroR)  {-}    

```{r, echo = FALSE}

```


## Repeated Incremental Pruning to Produce Error Reduction (RIPPER)  {-} 

```{r, echo = FALSE}

```


## Rule Fit  {-}   

```{r, echo = FALSE}

```


## Decision Rules  {-}  

```{r, echo = FALSE}

```


<!--chapter:end:10-rule_based_systems.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
`r if (knitr::is_html_output()) '
# References {-}
'`

Sagi, S. (2019). ML Algorithms: One SD (œÉ). The obvious questions to ask when‚Ä¶ | by Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6 

Kuhn, M. (2019). The caret Package. https://topepo.github.io/caret/index.html

<!--chapter:end:11-references.Rmd-->

