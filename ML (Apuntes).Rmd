---
title: "Machine Learning (Apuntes) "
author: "Diana Villasana Ocampo"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  Apuntes personales
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---

# url: your book url like https://bookdown.org/yihui/bookdown

Placeholder


## üìå Cuadro {.unnumbered}
## Modelos de Machine Learning {.unnumbered}

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üîç 1. Regresi√≥n {-}   

**Ejemplos:** Regresi√≥n Lineal Simple, Regresi√≥n Ridge, Regresi√≥n Lasso.  
**Uso:** Ideal para **predecir valores num√©ricos continuos** (como precios o temperaturas) y cuando esperas **relaciones lineales** entre tus variables.   
**Ventajas:** Es un modelo **simple** de entender y altamente **interpretable**.  
**Limitaciones:** Su desempe√±o es bajo cuando las relaciones entre las variables son **no lineales** o muy complejas.  

---

## Ordinary Least Squares Regression (`OLSR`) {-}    

[Ordinary Least Squares Regression (`OLSR`) en R](https://dvillasanao.github.io/ML_Examples/Output/Regression/01_01.OLSR.html)  
[Ordinary Least Squares Regression (`OLSR`) en Python](https://dvillasanao.github.io/ML_Examples/R/Regression/01_01_OLSR_py.html)   

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/01_image_OLSR.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/OLSR.png"))
```


La **Regresi√≥n por M√≠nimos Cuadrados Ordinarios (OLS)** es una t√©cnica fundamental en estad√≠stica y Machine Learning para modelar la **relaci√≥n lineal** entre una **variable dependiente** (a predecir) y una o m√°s **variables independientes**. Su objetivo es encontrar la **l√≠nea (o hiperplano) que mejor se ajusta** a los datos, minimizando la **suma de los cuadrados de las diferencias** entre los valores reales y los predichos por el modelo. Es decir, busca los coeficientes que hacen que la distancia (al cuadrado) de los puntos a la l√≠nea sea m√≠nima.

Los coeficientes de OLS se pueden calcular directamente con una f√≥rmula matem√°tica, sin necesidad de procesos iterativos complejos, bajo ciertos supuestos como la linealidad de la relaci√≥n y la independencia de los errores.

En el contexto del **aprendizaje global vs. local**, OLS es un ejemplo claro de un modelo de **aprendizaje global**. OLS busca una **√∫nica ecuaci√≥n** o un conjunto de coeficientes que describan la relaci√≥n entre las variables para **todo el conjunto de datos**. La l√≠nea o hiperplano que encuentra es una soluci√≥n global que se aplica de manera uniforme en todo el espacio de caracter√≠sticas. Esto la hace muy interpretable y computacionalmente eficiente, pero limitada si la relaci√≥n real entre las variables no es estrictamente lineal en todo el dominio de los datos.


## Linear Regression {.unnumbered}   


La **Regresi√≥n Lineal** es uno de los algoritmos m√°s fundamentales y ampliamente utilizados en el campo del **Machine Learning y la estad√≠stica**. Es un modelo **supervisado** que busca establecer una **relaci√≥n lineal** entre una **variable de respuesta (o dependiente)** continua y una o m√°s **variables predictoras (o independientes)**.

**Concepto y Ecuaci√≥n:**

La idea central de la regresi√≥n lineal es encontrar la **l√≠nea (o hiperplano en m√∫ltiples dimensiones)** que mejor se ajusta a los datos, de modo que se pueda predecir el valor de la variable dependiente bas√°ndose en los valores de las variables predictoras.

* **Regresi√≥n Lineal Simple:** Implica una √∫nica variable predictora. La ecuaci√≥n de la l√≠nea es:
    $$Y = \beta_0 + \beta_1 X + \epsilon$$  
    
Donde:
    * $Y$ es la variable de respuesta.
    * $X$ es la variable predictora.
    * $\beta_0$ es el **intercepto** (el valor de $Y$ cuando $X$ es 0).
    * $\beta_1$ es el **coeficiente de la pendiente** (cu√°nto cambia $Y$ por cada unidad de cambio en $X$).
    * $\epsilon$ es el **t√©rmino de error** o residual (la parte de $Y$ que el modelo no puede explicar).

* **Regresi√≥n Lineal M√∫ltiple:** Implica dos o m√°s variables predictoras. La ecuaci√≥n se extiende a:   
    $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \epsilon$$  
    
Donde:
    * $X_1, X_2, ..., X_p$ son las $p$ variables predictoras.
    * $\beta_1, \beta_2, ..., \beta_p$ son los coeficientes de cada variable predictora.

**C√≥mo Funciona (M√≠nimos Cuadrados Ordinarios - OLS):**

El m√©todo m√°s com√∫n para estimar los coeficientes ($\beta$s) en la regresi√≥n lineal es el de **M√≠nimos Cuadrados Ordinarios (OLS)**. OLS funciona minimizando la **suma de los cuadrados de los residuos**. Un residuo es la diferencia entre el valor real de $Y$ y el valor predicho por el modelo ($\hat{Y}$).

$$\text{Minimizar: } \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} (Y_i - (\beta_0 + \beta_1 X_{i1} + ... + \beta_p X_{ip}))^2$$

Al minimizar esta suma de cuadrados, OLS encuentra los coeficientes que hacen que la l√≠nea de regresi√≥n est√© lo m√°s cerca posible de la mayor√≠a de los puntos de datos.

**Supuestos Clave:**  

La validez de los resultados de la regresi√≥n lineal tradicional se basa en varios supuestos:

* **Linealidad:** La relaci√≥n entre las variables $X$ y $Y$ es lineal.
* **Independencia:** Las observaciones son independientes entre s√≠.
* **Normalidad de los Residuos:** Los residuos se distribuyen normalmente.
* **Homocedasticidad:** La varianza de los residuos es constante a lo largo de todos los niveles de las variables predictoras.
* **No Multicolinealidad Perfecta:** Las variables predictoras no deben estar perfectamente correlacionadas entre s√≠.

**Uso y Limitaciones:**

La regresi√≥n lineal es popular por su **simplicidad, interpretabilidad** y por ser un buen punto de partida para muchos problemas de predicci√≥n. Sin embargo, su principal limitaci√≥n es que solo puede modelar **relaciones lineales**. Si la relaci√≥n subyacente entre las variables es no lineal, una regresi√≥n lineal puede no capturarla adecuadamente y dar resultados inexactos.

**Aprendizaje Global vs. Local:**

La Regresi√≥n Lineal es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** La Regresi√≥n Lineal aprende un **√∫nico conjunto de coeficientes** que define una **l√≠nea (o hiperplano) global** que se aplica a todo el espacio de caracter√≠sticas. Esta l√≠nea busca representar la **relaci√≥n lineal promedio o general** entre las variables predictoras y la variable de respuesta a lo largo de todo el rango de los datos. La predicci√≥n para cualquier nueva instancia se realiza utilizando la misma ecuaci√≥n lineal, sin importar en qu√© parte del espacio de caracter√≠sticas se encuentre. No hay adaptaciones o modelos separados para diferentes subregiones de los datos; el modelo es una funci√≥n que describe una tendencia general y global.

* **Rigidez de la Linealidad:** Debido a su naturaleza global y lineal, la regresi√≥n lineal no puede capturar relaciones **no lineales o interacciones complejas** entre las variables predictoras de forma inherente. Si la relaci√≥n real en los datos es no lineal, el modelo lineal intentar√° ajustarla con la mejor l√≠nea recta posible, lo que podr√≠a llevar a un bajo rendimiento.


## Regresi√≥n Log√≠stica (Logit) {.unnumbered}

```{r, echo = FALSE,out.width='50%', fig.align='center', fig.cap="Elaboraci√≥n propia"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Logit.png"))
```

La **Regresi√≥n Log√≠stica** es un modelo estad√≠stico usado principalmente para problemas de **clasificaci√≥n binaria**, donde el objetivo es predecir la **probabilidad** de que una instancia pertenezca a una de dos clases (por ejemplo, "s√≠" o "no", "0" o "1"). A pesar de su nombre, no predice un valor continuo, sino una probabilidad.

Este modelo utiliza una **funci√≥n sigmoide (o log√≠stica)** para transformar una combinaci√≥n lineal de las variables de entrada en un valor entre 0 y 1, que se interpreta como una probabilidad. Los coeficientes del modelo se aprenden maximizando la verosimilitud de observar los datos, generalmente a trav√©s de algoritmos como el descenso de gradiente.

En el contexto del **aprendizaje global vs. local**, la Regresi√≥n Log√≠stica es un modelo de **aprendizaje global**. Esto significa que busca un **√∫nico conjunto de coeficientes** que definen una frontera de decisi√≥n (un hiperplano) que se aplica a todo el espacio de caracter√≠sticas para separar las clases. Asume una relaci√≥n lineal entre las variables de entrada y el logaritmo de la probabilidad, y una vez entrenado, usa esta relaci√≥n global para hacer predicciones en cualquier parte del espacio de datos. Si bien es eficiente y muy interpretable, su naturaleza global puede limitar su rendimiento en casos donde las fronteras de decisi√≥n son inherentemente no lineales o muy complejas.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Lineal entre log-odds y predictores",
  "‚ùå No es requisito",
  "‚úÖ Necesaria",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠",
  "‚ö†Ô∏è Puede afectar",
  "‚úÖ Alta (coeficientes interpretables)",
  "‚úÖ Alta",
  "‚úÖ Compatible",
  "‚ùå Respuesta no binaria o multiclase sin ajuste"
)
detalles <- c(
  "Clasificaci√≥n binaria",
  "Ej. √©xito/fracaso, s√≠/no",
  "Convertir categ√≥ricas a dummies",
  "Relaci√≥n entre log(p/(1-p)) y X debe ser lineal",
  "No se exige normalidad en errores",
  "Independencia entre observaciones",
  "Idealmente varianza constante",
  "Outliers pueden alterar los coeficientes",
  "Usar VIF y regularizaci√≥n si hay problema",
  "Coeficientes en t√©rminos de odds/log-odds",
  "R√°pido y estable para datasets medianos",
  "K-fold funciona muy bien",
  "Evitar si hay multiclase sin ajuste"
)
tabla_logit <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

require(gt)
tabla_logit %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir logit",
             subtitle = "Regresi√≥n log√≠stica") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Generalized Linear Model (GLM) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center', fig.cap="Elaboraci√≥n propia"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/GLM.png"))
```

## Least Angle Regression (LARS)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/LARS.png"))
```

**Least Angle Regression (LARS)** es un algoritmo de **regresi√≥n lineal** desarrollado por Bradley Efron, Trevor Hastie, Iain Johnstone y Robert Tibshirani. Es particularmente interesante porque puede considerarse como una **versi√≥n m√°s eficiente y paso a paso de LASSO** (Least Absolute Shrinkage and Selection Operator) y es √∫til para **seleccionar caracter√≠sticas** y manejar datos de alta dimensi√≥n.

A diferencia de OLS, que calcula todos los coeficientes de una vez, o de Lasso, que requiere optimizaci√≥n m√°s compleja, LARS opera de manera incremental. Su idea central es avanzar los coeficientes de forma que su √°ngulo con el vector de residuos sea siempre el mismo y que sea el "m√°s peque√±o" posible.

El proceso de LARS se puede resumir as√≠:

1.  **Inicio:** Todos los coeficientes se inicializan en cero.
2.  **Identificaci√≥n del Predictor m√°s Correlacionado:** El algoritmo encuentra la variable predictora que est√° m√°s correlacionada con la variable de respuesta (o con el residuo actual).
3.  **Movimiento en la Direcci√≥n del Predictor:** El coeficiente de esa variable predictora se mueve gradualmente desde cero en la direcci√≥n del signo de su correlaci√≥n. A medida que el coeficiente se mueve, el residuo cambia.
4.  **Activaci√≥n de Nuevos Predictores:** Cuando otra variable predictora alcanza la misma correlaci√≥n con el residuo actual que la variable que ya est√° activa, el algoritmo cambia de direcci√≥n. Ahora, los coeficientes de *ambas* variables activas se mueven juntas en un "√°ngulo equiestad√≠stico" de tal manera que permanecen igualmente correlacionadas con el residuo.
5.  **Proceso Iterativo:** Este proceso contin√∫a, a√±adiendo nuevas variables al conjunto de variables "activas" (es decir, aquellas con coeficientes distintos de cero) a medida que estas alcanzan la misma correlaci√≥n con el residuo. Los coeficientes se mueven de forma coordinada.
6.  **Criterio de Parada:** El algoritmo se detiene cuando todos los predictores han sido incluidos en el modelo, o cuando se alcanza un n√∫mero predefinido de pasos o de variables.

**Relaci√≥n con otros modelos:**
* Si LARS se detiene cuando los coeficientes de las variables no activas son menores o iguales a la correlaci√≥n actual de las variables activas (y los coeficientes de las variables no activas se fijan en cero si su correlaci√≥n es menor), entonces genera la **soluci√≥n completa del camino de LASSO**.
* Tambi√©n puede generar el camino de soluciones para la **Ridge Regression** si se modifica ligeramente.

LARS es eficiente porque solo requiere un n√∫mero de pasos igual al n√∫mero de variables, o menos si se detiene antes.

**Aprendizaje Global vs. Local:**

Least Angle Regression (LARS) es un modelo de **aprendizaje global**.

* **Aspecto Global:** LARS construye un **modelo lineal global** paso a paso. Aunque el algoritmo a√±ade variables una por una y ajusta sus coeficientes de manera incremental, el modelo resultante en cada paso es una ecuaci√≥n de regresi√≥n lineal que se aplica a todo el conjunto de datos. La decisi√≥n de qu√© variable a√±adir y c√≥mo ajustar los coeficientes se basa en las correlaciones globales entre las variables predictoras y la respuesta (o el residuo). La finalidad es encontrar los coeficientes √≥ptimos para una funci√≥n de regresi√≥n que se aplica a todo el espacio de caracter√≠sticas.

* **Selecci√≥n de Caracter√≠sticas Globalmente:** La capacidad de LARS para realizar selecci√≥n de caracter√≠sticas (al igual que LASSO) es un proceso global. Se identifican las variables m√°s influyentes en el contexto de todo el conjunto de datos, y su inclusi√≥n en el modelo contribuye a la formaci√≥n de una relaci√≥n global entre los predictores y la respuesta. No se construyen modelos separados para diferentes subregiones de los datos; en cambio, se construye un √∫nico modelo global de manera progresiva.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal",
  "‚ö†Ô∏è Deseable para inferencia cl√°sica",
  "‚úÖ Necesaria",
  "‚úÖ Supuesto importante",
  "‚ö†Ô∏è Afectado por valores extremos",
  "‚úÖ Maneja bien multicolinealidad como LASSO",
  "‚úÖ Muy interpretable (secuencia de modelos anidados)",
  "‚úÖ Muy eficiente, especialmente con muchas variables",
  "‚úÖ √ötil para elegir n√∫mero de variables con validaci√≥n cruzada",
  "‚ùå Datos ruidosos o no lineales; ‚ùå si hay muchas interacciones no capturadas"
)

detalles <- c(
  "Algoritmo de regresi√≥n eficiente que selecciona variables secuencialmente como alternativa a LASSO.",
  "Busca predecir una variable continua usando m√∫ltiples predictores.",
  "Usa variables num√©ricas estandarizadas; es sensible a escala.",
  "Asume relaci√≥n lineal entre predictores y respuesta.",
  "No exige normalidad para predicci√≥n, pero s√≠ para inferencia estad√≠stica.",
  "Errores deben ser independientes entre s√≠.",
  "Supone varianza constante de los errores.",
  "Puede verse afectado si hay valores extremos en las variables.",
  "Muy √∫til cuando los predictores est√°n correlacionados; elige uno a la vez.",
  "Produce una ruta de modelos f√°cilmente interpretable con selecci√≥n progresiva.",
  "M√°s r√°pido que LASSO al generar trayectorias de coeficientes.",
  "Puede aplicarse validaci√≥n cruzada para seleccionar el mejor modelo en la secuencia.",
  "No captura relaciones no lineales o interacciones sin modificaci√≥n previa del modelo."
)

tabla_lars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lars %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LARS",
             subtitle = "Least Angle Regression (LARS)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Locally Estimated Scatterplot Smoothing (`LOESS`) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
knitr::include_graphics(paste0(here::here(), "/img/Regression/LOESS.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/LOESS_1.png"))
```

**LOESS (Locally Estimated Scatterplot Smoothing)**, o LOWESS, es una t√©cnica de **regresi√≥n no param√©trica** para crear una curva suave que se ajusta a los datos en un diagrama de dispersi√≥n. Su gran ventaja es que **no asume una forma funcional global** espec√≠fica para la relaci√≥n entre las variables, lo que la hace muy flexible para identificar tendencias y patrones no lineales.

El principio de LOESS es simple: para estimar el valor suavizado en un punto, se seleccionan los **puntos de datos cercanos** (definido por un par√°metro de **"span"** o ancho de banda), se les asignan **pesos** (dando m√°s peso a los puntos m√°s cercanos), y luego se ajusta un **polinomio de bajo grado** (com√∫nmente lineal o cuadr√°tico) a esos puntos usando m√≠nimos cuadrados ponderados. Este proceso se repite para cada punto de inter√©s para construir la curva.

En el contexto del **aprendizaje global vs. local**, LOESS es un modelo de **aprendizaje puramente local**. Su flexibilidad reside en que **ajusta m√∫ltiples modelos simples y locales** (regresiones ponderadas) en diferentes vecindarios de los datos. No busca una √∫nica ecuaci√≥n global que describa la relaci√≥n en todo el conjunto de datos. Esto le permite adaptarse maravillosamente a las variaciones en las relaciones y curvaturas de los datos, lo que es especialmente √∫til cuando los datos no se distribuyen linealmente. Sin embargo, su naturaleza local implica que no produce una f√≥rmula expl√≠cita del modelo, y puede ser computacionalmente m√°s intensivo para conjuntos de datos muy grandes.  


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua",
  "‚úÖ Num√©ricas (usualmente 1 o 2 predictores)",
  "‚úÖ No lineal y suave",
  "‚ùå No necesaria",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è Muy sensible",
  "‚ùå No aplica (pocos predictores)",
  "‚úÖ Muy interpretable gr√°ficamente",
  "‚ö†Ô∏è Lento en grandes vol√∫menes de datos",
  "‚úÖ Puede usarse para elegir 'span'",
  "‚ùå Datos grandes o con ruido fuerte"
)
detalles <- c(
  "Modelo no param√©trico local",
  "Regresi√≥n para valores continuos",
  "Generalmente 1 o 2 variables num√©ricas",
  "Ajuste por vecindad, suaviza la curva",
  "No asume distribuci√≥n espec√≠fica",
  "Supuesto deseable si hay dependencias temporales",
  "Ideal si la varianza no cambia mucho localmente",
  "Altamente afectado por outliers locales",
  "No es una t√©cnica multivariable compleja",
  "La curva ajustada se interpreta visualmente",
  "Computacionalmente costoso con datos grandes",
  "Ayuda a seleccionar el mejor 'span'",
  "Poco eficaz en alta dimensi√≥n o datos muy dispersos"
)
tabla_loess <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
tabla_loess %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LOESS",
             subtitle = "Locally Estimated Scatterplot Smoothing (LOESS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Multivariate Adaptive Regression Splines (`MARS`) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/MARS.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/MARS_1.png"))
```

**Multivariate Adaptive Regression Splines (MARS)** es un algoritmo de **regresi√≥n no param√©trica** que extiende los modelos lineales para manejar relaciones no lineales y complejas. Desarrollado por Jerome Friedman, MARS construye su modelo al **dividir el espacio de entrada en m√∫ltiples regiones y ajustar una funci√≥n lineal simple (o de orden superior) a cada regi√≥n**.

El proceso de MARS consta de dos fases: una **fase de adelante** que a√±ade iterativamente **funciones base** (pares de funciones "hinge" o bisagra) y **nudos** (puntos de corte) para capturar no linealidades e interacciones entre variables, y una **fase de atr√°s** que poda las funciones base menos significativas utilizando criterios como la **Validaci√≥n Cruzada Generalizada (GCV)** para prevenir el sobreajuste. Esto permite a MARS ser adaptable a las particularidades de los datos.

En el contexto del **aprendizaje global vs. local**, MARS se sit√∫a como un modelo de **aprendizaje adaptativo que combina aspectos globales y locales**. Es "local" en el sentido de que sus funciones base y nudos dividen el espacio de datos en regiones, y dentro de cada regi√≥n se aplica una relaci√≥n simple. Sin embargo, es "global" porque la suma de todas estas funciones base forma una √∫nica ecuaci√≥n que describe la relaci√≥n en todo el conjunto de datos y se aplica de forma consistente. Esto significa que si los datos no se distribuyen linealmente, MARS puede aprender y modelar estas relaciones complejas de forma adaptativa, encontrando autom√°ticamente d√≥nde y c√≥mo las relaciones cambian, ofreciendo una soluci√≥n que es tanto flexible como interpretable.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua o categ√≥rica (binaria con extensi√≥n)",
  "‚úÖ Num√©ricas (categ√≥ricas con dummies)",
  "‚úÖ No lineal (autom√°tico)",
  "‚ùå No requerida",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠ (aunque algo robusto)",
  "‚ö†Ô∏è Puede afectar",
  "‚ö†Ô∏è Media (modelo tipo caja negra)",
  "‚úÖ Razonable para tama√±os medianos",
  "‚úÖ Recomendado (ej. repeated k-fold)",
  "‚ùå Relaci√≥n puramente lineal o muchos factores irrelevantes"
)
detalles <- c(
  "Regresi√≥n flexible no lineal",
  "Ideal para regresi√≥n continua (tambi√©n clasificaci√≥n con `earth`)",
  "Crea autom√°ticamente 'splines' por variable",
  "Crea funciones por tramos con 'nudos'",
  "No exige distribuci√≥n espec√≠fica de errores",
  "Mejor si los datos no est√°n correlacionados temporalmente",
  "Idealmente errores con varianza constante",
  "Puede ser sensible a valores extremos",
  "Detecta interacciones, pero VIF sigue siendo √∫til",
  "Coeficientes no tan interpretables como OLS",
  "M√°s lento que OLS pero m√°s flexible",
  "CV ayuda a elegir n√∫mero √≥ptimo de t√©rminos",
  "Tiene riesgo de sobreajuste si no se controla bien"
)
tabla_mars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_mars %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MARS",
             subtitle = "Splines de Regresi√≥n Adaptativa Multivariante (MARS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Polynomial Regression {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center', fig.cap="Elaboraci√≥n propia"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Polynomial Regression.png"))
```

## Quantile Regression {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center', fig.cap="Elaboraci√≥n propia"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Quantile Regression.png"))
```

## Stepwise Regression {.unnumbered}

```{r, echo = FALSE,out.width='50%', fig.align='center', fig.cap="Elaboraci√≥n propia"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/StepW.png"))
```

La **Regresi√≥n por Pasos (Stepwise Regression)** es una t√©cnica para construir un modelo de regresi√≥n lineal (o a veces otros modelos lineales generalizados) seleccionando las variables predictoras de forma iterativa y autom√°tica. Su objetivo es encontrar un subconjunto √≥ptimo de variables que mejore la capacidad predictiva del modelo sin incluir variables irrelevantes o redundantes. Esto ayuda a simplificar el modelo, mejorar la interpretabilidad y reducir el riesgo de sobreajuste.

Existen tres estrategias principales para la regresi√≥n por pasos:

1.  **Selecci√≥n Hacia Adelante (Forward Selection):**
    * Comienza con un modelo que no incluye ninguna variable predictora (solo el intercepto).
    * En cada paso, eval√∫a todas las variables predictoras disponibles que a√∫n no est√°n en el modelo.
    * A√±ade al modelo la variable que, al ser incluida, produce la mayor mejora estad√≠stica (generalmente medida por un valor p bajo, un R-cuadrado ajustado mayor, o un criterio de informaci√≥n como AIC o BIC).
    * El proceso contin√∫a hasta que ninguna de las variables restantes mejora el modelo por encima de un umbral predefinido.

2.  **Eliminaci√≥n Hacia Atr√°s (Backward Elimination):**
    * Comienza con un modelo que incluye **todas** las variables predictoras posibles.
    * En cada paso, eval√∫a las variables predictoras que actualmente est√°n en el modelo.
    * Elimina del modelo la variable que es menos significativa estad√≠sticamente (generalmente medida por un valor p alto, o una reducci√≥n en el R-cuadrado ajustado o un aumento en AIC/BIC).
    * El proceso contin√∫a hasta que la eliminaci√≥n de cualquier variable empeorar√≠a significativamente el modelo.

3.  **H√≠brida (Mixed / Bidirectional Stepwise):**
    * Combina la selecci√≥n hacia adelante y la eliminaci√≥n hacia atr√°s.
    * En cada paso, el algoritmo puede tanto a√±adir una variable si mejora el modelo, como eliminar una variable que ya est√° en el modelo si se vuelve redundante o no significativa. Esto permite que el modelo reconsidere variables que fueron a√±adidas o eliminadas en pasos anteriores. Es el enfoque m√°s com√∫n y robusto.

**Criterios de Selecci√≥n:**  

La decisi√≥n de a√±adir o eliminar una variable en cada paso se basa en criterios estad√≠sticos, siendo los m√°s comunes:
* **Valores p:** Umbrales para la significancia estad√≠stica de los coeficientes.
* **$R^2$ ajustado:** Mide la proporci√≥n de varianza explicada por el modelo, penalizando la inclusi√≥n de variables innecesarias.
* **Criterio de Informaci√≥n de Akaike (AIC):** Penaliza la complejidad del modelo (n√∫mero de par√°metros) en relaci√≥n con su bondad de ajuste.
* **Criterio de Informaci√≥n Bayesiano (BIC):** Similar al AIC, pero con una penalizaci√≥n m√°s fuerte por la complejidad.

**Ventajas y Desventajas:**

* **Ventajas:** Puede ayudar a construir modelos m√°s parsimoniosos, mejorar la interpretabilidad y reducir la multicolinealidad.
* **Desventajas:**
    * **Sobreajuste:** Puede llevar a sobreajuste si se usa de forma acr√≠tica, ya que el algoritmo se optimiza para los datos de entrenamiento.
    * **Problemas de Significancia Estad√≠stica:** Los valores p y otras m√©tricas pueden no ser confiables debido a la selecci√≥n de caracter√≠sticas basada en los datos.
    * **Inestabilidad:** El conjunto de variables seleccionadas puede ser muy sensible a peque√±as perturbaciones en los datos o a la elecci√≥n del criterio de selecci√≥n.
    * **Ignora el Conocimiento del Dominio:** Puede seleccionar variables que son estad√≠sticamente significativas pero que carecen de sentido pr√°ctico o causal.
    * **No Maneja Interacciones Complejas:** Es fundamentalmente un m√©todo para seleccionar variables para un modelo lineal y no est√° dise√±ado para descubrir relaciones no lineales o interacciones complejas.

Debido a sus desventajas, la regresi√≥n por pasos se utiliza con m√°s cautela hoy en d√≠a. A menudo se prefieren m√©todos de regularizaci√≥n (como Lasso o Elastic Net) para la selecci√≥n de caracter√≠sticas, ya que son m√°s estables y realizan la selecci√≥n de forma m√°s robusta.

**Aprendizaje Global vs. Local:**

La Regresi√≥n por Pasos es un modelo de **aprendizaje global**.

* **Aspecto Global:** La regresi√≥n por pasos construye un **√∫nico modelo de regresi√≥n lineal global** que busca explicar la relaci√≥n entre las variables predictoras y la respuesta en todo el conjunto de datos. La selecci√≥n de variables se realiza para optimizar el rendimiento de este modelo global. Los coeficientes finales que se obtienen definen una funci√≥n lineal que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar en qu√© parte del espacio de caracter√≠sticas se encuentre.

* **Proceso de Selecci√≥n (Global):** Aunque el proceso es iterativo y a√±ade/elimina variables, la decisi√≥n en cada paso se basa en c√≥mo esa adici√≥n/eliminaci√≥n afecta la bondad de ajuste o la complejidad del modelo en **todo el conjunto de datos**. No se ajustan modelos separados o locales para diferentes regiones.



## Support Vector Machine (SVM) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/SVM.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/SVM_1.png"))
```


**Support Vector Machine (SVM)** es un potente y vers√°til algoritmo de **Machine Learning** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**, aunque es m√°s conocido por su aplicaci√≥n en clasificaci√≥n. Su objetivo principal es encontrar el **hiperplano √≥ptimo** que separe las clases en el espacio de caracter√≠sticas con el **margen** m√°s grande posible. Los puntos de datos m√°s cercanos a este hiperplano se llaman **vectores de soporte**, y son cruciales para definir la frontera de decisi√≥n.

Para manejar datos que no son linealmente separables, SVM utiliza el **"truco del kernel"**. Este truco permite a SVM mapear impl√≠citamente los datos a un espacio de mayor dimensi√≥n donde las clases podr√≠an ser linealmente separables, sin necesidad de calcular expl√≠citamente las coordenadas. Funciones kernel comunes como el **Radial Basis Function (RBF) o Gaussiano** permiten a SVM modelar fronteras de decisi√≥n no lineales complejas en el espacio original de baja dimensi√≥n.

En el contexto del **aprendizaje global vs. local**, SVM se clasifica principalmente como un modelo de **aprendizaje global**. Esto se debe a que busca un **√∫nico hiperplano √≥ptimo** (o una frontera de decisi√≥n no lineal definida por el kernel) que se aplica a la totalidad del espacio de caracter√≠sticas. Una vez entrenado, el modelo predice evaluando la posici√≥n de un nuevo punto con respecto a esta frontera global. Sin embargo, hay un matiz "local" en su funcionamiento: la determinaci√≥n de este hiperplano depende **cr√≠ticamente solo de los vectores de soporte**, que son los puntos de datos "m√°s dif√≠ciles" cercanos a la frontera. Los puntos que est√°n lejos del margen no influyen en la definici√≥n del modelo. Aunque la frontera de decisi√≥n es una funci√≥n global que se aplica en todas partes, su construcci√≥n est√° influenciada por estos puntos localmente relevantes, permitiendo a SVM adaptar su aproximaci√≥n incluso cuando las relaciones en los datos no se distribuyen linealmente, al encontrar la separaci√≥n √≥ptima en un espacio transformado.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas (categor√≠as deben codificarse)",
  "‚úÖ Capta relaciones no lineales (kernel)",
  "‚ùå No requiere",
  "‚úÖ Idealmente s√≠",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠, especialmente sin margen amplio",
  "‚úÖ Puede manejarla bien",
  "‚ùå Baja (modelo es una caja negra)",
  "‚ö†Ô∏è Lento con muchos datos o predictores",
  "‚úÖ Esencial para elegir kernel y par√°metros",
  "‚ùå Datos con mucho ruido o solapamiento entre clases"
)
detalles <- c(
  "Modelo supervisado que maximiza el margen entre clases",
  "Clasificaci√≥n binaria, multiclase o regresi√≥n (SVR)",
  "Requiere escalar o estandarizar las variables num√©ricas",
  "Puede usar kernel para resolver problemas no lineales",
  "No requiere supuestos cl√°sicos como normalidad",
  "Mejor si los datos son independientes",
  "Puede usarse aunque haya heterocedasticidad",
  "Los outliers cercanos al margen afectan el modelo",
  "Los kernels pueden reducir el efecto de multicolinealidad",
  "Dif√≠cil de explicar; es un modelo de tipo caja negra",
  "Puede ser costoso computacionalmente con datos grandes",
  "Par√°metros como C y gamma se ajustan v√≠a validaci√≥n cruzada",
  "No es ideal si hay ruido o datos mal etiquetados"
)
tabla_svm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_svm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir SVM",
             subtitle = "Support Vector Machine (SVM) ") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

<!--chapter:end:01-regression.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üå≤ 2. √Årboles de Decisi√≥n y Derivados {-}   

**Ejemplos:** √Årbol de Decisi√≥n, Random Forest, Gradient Boosting.   
**Uso:** Excelentes para **datos tabulares** con relaciones no lineales, incluyendo variables categ√≥ricas y num√©ricas. Son una buena opci√≥n cuando la **interpretabilidad** es clave.   
**Ventajas:** Pueden manejar diversos tipos de datos y, los √°rboles individuales, son **f√°ciles de interpretar**.   
**Limitaciones:** Los √°rboles simples pueden **sobreajustarse**, y su rendimiento baja con datos muy ruidosos si no se usan m√©todos de ensamble.   

---


## C4.5  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/C4.5.png"))
```

**C4.5** es una extensi√≥n del algoritmo **ID3**, tambi√©n desarrollado por Ross Quinlan, y es uno de los algoritmos de **√°rboles de decisi√≥n** m√°s influyentes y ampliamente utilizados para tareas de **clasificaci√≥n**. Fue dise√±ado para abordar algunas de las limitaciones de su predecesor, ID3, y se ha convertido en un est√°ndar de facto en el aprendizaje autom√°tico para construir modelos predictivos interpretables.

Al igual que ID3, C4.5 construye un √°rbol de clasificaci√≥n seleccionando en cada nodo el atributo que mejor divide el conjunto de datos. Sin embargo, en lugar de usar solo la **ganancia de informaci√≥n**, C4.5 utiliza la **relaci√≥n de ganancia** (Gain Ratio). La relaci√≥n de ganancia normaliza la ganancia de informaci√≥n por la entrop√≠a intr√≠nseca del atributo, lo que ayuda a mitigar el sesgo de ID3 hacia atributos con muchos valores. Adem√°s, C4.5 introduce varias mejoras significativas:

* **Manejo de atributos continuos:** Puede discretizar atributos num√©ricos continuos dividiendo el rango en intervalos.
* **Manejo de valores faltantes:** Puede manejar datos con valores ausentes asignando una probabilidad fraccionada a cada rama posible.
* **Poda del √°rbol:** Implementa una t√©cnica de poda para reducir el sobreajuste, lo que implica eliminar ramas del √°rbol que no aportan significativamente a la clasificaci√≥n o que representan ruido en los datos.

En el contexto del **aprendizaje global vs. local**, C4.5, al igual que ID3 y CART, opera como un sistema de **aprendizaje local**. La construcci√≥n del √°rbol se logra a trav√©s de decisiones de divisi√≥n que se optimizan localmente en cada nodo, buscando la m√°xima homogeneidad o pureza en los subconjuntos resultantes. Esto le permite a C4.5 manejar eficazmente relaciones no lineales entre las variables independientes y dependientes. La idea es que, si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se puede aplicar de forma efectiva mediante esta **regresi√≥n ponderada localmente**, donde el algoritmo divide el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Al centrarse en divisiones √≥ptimas a nivel de subconjuntos, C4.5 ofrece una alternativa robusta a los m√©todos de aproximaci√≥n de funciones globales, que a veces pueden fallar en proporcionar una buena aproximaci√≥n cuando la relaci√≥n entre las variables no es lineal.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Categ√≥ricas y num√©ricas",
  "‚úÖ Captura relaciones no lineales",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es relevante",
  "‚ö†Ô∏è Moderadamente (puede hacer overfitting con ruido)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol interpretable)",
  "‚úÖ Relativamente r√°pido",
  "‚úÖ Recomendable para evitar sobreajuste",
  "‚ùå Demasiadas categor√≠as o ruido en datos"
)

detalles <- c(
  "Modelo supervisado tipo √°rbol de decisi√≥n",
  "Clasifica variables categ√≥ricas en ramas l√≥gicas",
  "Divide por puntos de corte para variables num√©ricas",
  "No asume forma funcional entre predictores y respuesta",
  "No necesita normalidad de errores",
  "Mejor si las observaciones son independientes",
  "No requiere varianzas constantes",
  "Datos ruidosos pueden afectar las ramas",
  "No se ve afectado por correlaciones entre predictores",
  "Salida f√°cil de visualizar y explicar",
  "Escala bien para tama√±os de muestra medianos",
  "Evita sobreajuste con poda y validaci√≥n",
  "Muchas clases con pocos datos pueden sobreajustar"
)

tabla_c45 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_c45 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir C4.5") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## C5.0  {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/C5.0.png"))
```

**C5.0** es la versi√≥n m√°s reciente y avanzada de los algoritmos de √°rboles de decisi√≥n desarrollados por Ross Quinlan, sucediendo a ID3 y C4.5. Es un algoritmo propietario (aunque se ofrece una versi√≥n de c√≥digo abierto bajo ciertas licencias) y es ampliamente reconocido por su **rapidez**, **precisi√≥n** y **eficiencia** en la construcci√≥n de **√°rboles de decisi√≥n** y **reglas de clasificaci√≥n** para tareas de **clasificaci√≥n**.

Al igual que sus predecesores, C5.0 construye un √°rbol de clasificaci√≥n mediante la divisi√≥n recursiva de los datos en subconjuntos m√°s homog√©neos. Sin embargo, C5.0 incorpora mejoras significativas que lo hacen superior en muchos aspectos:

* **Velocidad y eficiencia:** Es notablemente m√°s r√°pido y m√°s eficiente en el uso de memoria que C4.5, lo que le permite manejar conjuntos de datos mucho m√°s grandes.
* **Impulso (Boosting):** C5.0 puede usar la t√©cnica de **boosting** (espec√≠ficamente, una variante de AdaBoost) para crear m√∫ltiples √°rboles de decisi√≥n y combinarlos para producir una predicci√≥n m√°s robusta y precisa. Esto reduce significativamente los errores de clasificaci√≥n y mejora la generalizaci√≥n.
* **Poda mejorada:** Ofrece t√©cnicas de poda m√°s sofisticadas para evitar el sobreajuste y producir √°rboles m√°s peque√±os y comprensibles.
* **Manejo de valores faltantes y atributos continuos:** Al igual que C4.5, maneja de manera efectiva valores faltantes y atributos num√©ricos continuos.
* **Generaci√≥n de reglas:** Adem√°s de √°rboles de decisi√≥n, C5.0 puede generar conjuntos de **reglas de clasificaci√≥n** concisas, que a menudo son m√°s f√°ciles de interpretar que un √°rbol completo.

En el contexto de la **regresi√≥n localmente ponderada**, C5.0, como los dem√°s algoritmos de √°rboles de decisi√≥n, opera bajo la premisa de un **aprendizaje local**. La construcci√≥n del √°rbol implica tomar decisiones de divisi√≥n √≥ptimas en cada nodo, bas√°ndose en la informaci√≥n local de ese subconjunto de datos. Si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n, que es su enfoque principal) se puede aplicar eficazmente al dividir el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Cada divisi√≥n en el √°rbol se puede ver como una forma de **regresi√≥n ponderada localmente**, donde el algoritmo se enfoca en aproximar la relaci√≥n dentro de un subespacio espec√≠fico del conjunto de datos. Esto convierte a C5.0 en una potente alternativa a los m√©todos de aproximaci√≥n de funciones globales, especialmente cuando la relaci√≥n entre las variables independientes y dependientes no es lineal y se busca un modelo interpretable y robusto.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Categ√≥ricas y num√©ricas",
  "‚úÖ Captura relaciones no lineales",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es relevante",
  "‚ö†Ô∏è Moderadamente (puede generar ramas excesivas)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol f√°cil de visualizar)",
  "‚úÖ Relativamente r√°pido en training",
  "‚úÖ Recomendable para evitar sobreajuste",
  "‚ùå Clases muy desbalanceadas sin ajuste"
)

detalles <- c(
  "Algoritmo de √°rbol de decisi√≥n avanzado basado en C4.5",
  "Clasifica en m√∫ltiples categor√≠as (tambi√©n multiclase)",
  "Divide autom√°ticamente variables num√©ricas con puntos de corte",
  "No asume funci√≥n lineal: usa ganancia de informaci√≥n y boosting",
  "No exige normalidad de errores",
  "Mejor si las instancias son independientes",
  "No requiere varianzas constantes",
  "Los valores extremos pueden influir en ramas profundas",
  "No se ve afectado por correlaci√≥n alta entre predictores",
  "Salida clara con reglas y pesos de boosting",
  "M√°s r√°pido que C4.5 y con opciones de boosting",
  "Usar k-fold o repeated CV para determinar par√°metros √≥ptimos",
  "Muchos atributos irrelevantes pueden generar sobreajuste"
)

tabla_c50 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_c50 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir C5.0") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

``` 


## Classification and Regression Tree (CART)  {-} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CART.png"))
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CART_1.png"))
```

**Classification and Regression Tree (CART)** es un m√©todo no param√©trico que se utiliza para construir **√°rboles de decisi√≥n** tanto para problemas de **clasificaci√≥n** como de **regresi√≥n**. La idea central es dividir recursivamente el espacio de las caracter√≠sticas en regiones m√°s peque√±as y manejables, creando as√≠ un modelo con forma de √°rbol que es f√°cil de interpretar.

A diferencia de los modelos lineales o algunos algoritmos de aprendizaje global, CART no asume una relaci√≥n lineal entre las variables. En su lugar, el algoritmo identifica los mejores **puntos de divisi√≥n** en las variables predictoras para maximizar la **homogeneidad** de las respuestas dentro de cada regi√≥n resultante. Para problemas de clasificaci√≥n, esto se mide com√∫nmente con m√©tricas como la **impureza Gini** o la **ganancia de informaci√≥n**, mientras que para la regresi√≥n, se busca minimizar la **suma de los cuadrados de los residuos**.

Mientras que muchos algoritmos (como las redes neuronales cl√°sicas o las m√°quinas de vectores de soporte) son sistemas de **aprendizaje global** que buscan minimizar una funci√≥n de p√©rdida √∫nica para todo el conjunto de datos, CART se puede considerar m√°s como un sistema de **aprendizaje local**. Construye el modelo tomando decisiones de divisi√≥n locales en cada nodo del √°rbol, lo que le permite capturar relaciones complejas y no lineales en los datos. Esto es particularmente √∫til cuando una aproximaci√≥n de funci√≥n global √∫nica podr√≠a no ser suficiente para modelar la relaci√≥n entre las variables. Una de las ventajas de CART es su capacidad para manejar diferentes tipos de datos (num√©ricos y categ√≥ricos) y su interpretabilidad, ya que la ruta desde la ra√≠z hasta una hoja del √°rbol representa un conjunto de reglas de decisi√≥n.



```{r, echo =FALSE}

criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y Categ√≥ricas",
  "‚úÖ No lineal y con interacciones",
  "‚ùå No requiere",
  "‚ö†Ô∏è Puede verse afectado",
  "‚ö†Ô∏è No necesario pero deseable",
  "‚ö†Ô∏è S√≠, en puntos de corte",
  "‚úÖ No se ve afectado",
  "‚úÖ Alta (gr√°fico del √°rbol)",
  "‚úÖ R√°pido en datasets medianos",
  "‚úÖ Muy usado para poda y ajuste",
  "‚ùå Muy profundo (overfitting), datos muy ruidosos"
)

detalles <- c(
  "Algoritmo basado en divisiones binarias",
  "Puede predecir clases o valores continuos",
  "Acepta todo tipo de variables predictoras",
  "Captura relaciones complejas y no lineales",
  "No requiere distribuci√≥n normal",
  "Idealmente los errores deben ser independientes",
  "La varianza constante mejora resultados",
  "Puede generar divisiones extremas por valores at√≠picos",
  "No necesita preocuparse por colinealidad",
  "F√°cil de entender, especialmente con √°rboles peque√±os",
  "Escalable pero no √≥ptimo en grandes vol√∫menes sin poda",
  "Usa poda y validaci√≥n cruzada para evitar sobreajuste",
  "Tiende al sobreajuste si no se poda o se regulariza"
)

tabla_cart <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

require(gt) 

tabla_cart %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CART",
             subtitle = "Classification and Regression Tree (CART)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Chi-squared Automatic Interaction Detection (CHAID)  {-}    

```{r echo=FALSE, fig.show="hold", out.width="48%", fig.cap="https://select-statistics.co.uk/blog/chaid-chi-square-automatic-interaction-detector/"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CHAID.png"))
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CHAID_1.png"))
```



**Chi-squared Automatic Interaction Detection (CHAID)** es un algoritmo de **√°rboles de decisi√≥n** utilizado principalmente para tareas de **clasificaci√≥n** y, en menor medida, para la **regresi√≥n** (aunque se aplica m√°s com√∫nmente a variables dependientes categ√≥ricas). La idea fundamental de CHAID es construir un √°rbol de decisi√≥n al encontrar las mejores divisiones en las variables predictoras que maximicen la significancia estad√≠stica de la relaci√≥n con la variable dependiente.

A diferencia de ID3, C4.5 o CART, que utilizan medidas de impureza como la entrop√≠a o el √≠ndice Gini, CHAID se basa en pruebas estad√≠sticas de **chi-cuadrado ($\chi^2$)** para identificar las divisiones √≥ptimas. Cuando la variable dependiente es nominal o ordinal, CHAID eval√∫a cada variable predictora para encontrar la combinaci√≥n de categor√≠as que sea m√°s significativamente diferente de otras combinaciones en t√©rminos de la distribuci√≥n de la variable dependiente. El algoritmo fusiona las categor√≠as de una variable predictora si no son significativamente diferentes, y luego selecciona la variable predictora y la divisi√≥n que resultan en el valor m√°s bajo de $p$ (es decir, la mayor significancia estad√≠stica) de la prueba $\chi^2$. Para variables dependientes continuas, se utiliza una prueba F.

En el contexto del **aprendizaje global vs. local**, CHAID opera como un sistema de **aprendizaje local**. La construcci√≥n del √°rbol es un proceso iterativo y recursivo donde las decisiones de divisi√≥n se toman en cada nodo bas√°ndose en la significancia estad√≠stica local de la interacci√≥n entre las variables predictoras y la variable dependiente. Esto le permite a CHAID descubrir relaciones complejas y no lineales en los datos. La idea es que, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n (o clasificaci√≥n) de manera efectiva mediante lo que se denomina **regresi√≥n ponderada localmente**. Esto se logra al dividir el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples, donde cada rama del √°rbol representa una regi√≥n del espacio de caracter√≠sticas donde las interacciones son evaluadas y modeladas localmente. Esto hace de CHAID una alternativa robusta a los m√©todos de aproximaci√≥n de funciones globales, especialmente cuando se busca un modelo interpretable y se quieren identificar las interacciones entre las variables de una manera estad√≠sticamente rigurosa.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n multinivel)",
  "‚úÖ Categ√≥ricas nativas (num√©ricas requieren binarizaci√≥n o discretizaci√≥n)",
  "‚úÖ No lineal (explora interacciones autom√°ticas con œá¬≤)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente (outliers categ√≥ricos pueden crear nodos muy peque√±os)",
  "‚úÖ Robusto a multicolinealidad (usa œá¬≤, no varianzas)",
  "‚úÖ Media (√°rboles con muchos nodos pueden resultar complejos)",
  "‚ö†Ô∏è Razonablemente r√°pido en datasets moderados, lento si hay muy altas cardinalidades",
  "‚úÖ Recomendable para determinar profundidad y grado de interacci√≥n",
  "‚ùå Variable objetivo continua o muchos niveles con pocas observaciones"
)

detalles <- c(
  "Construye un √°rbol de decisi√≥n usando pruebas œá¬≤ para detectar interacciones entre predictores y variable objetivo.",
  "Dise√±ado para clasificar en m√∫ltiples categor√≠as sin orden; puede manejar targets con m√°s de dos niveles.",
  "Funciona mejor con predictores categ√≥ricos; las variables num√©ricas deben transformarse en categor√≠as mediante binning.",
  "No asume ninguna forma funcional; detecta autom√°ticamente relaciones complejas basadas en œá¬≤.",
  "No depende de supuestos de normalidad de errores ni de forma de distribuci√≥n de residuos.",
  "Las instancias deben ser independientes; no es ideal para datos con fuerte dependencia temporal sin procesar.",
  "Homoscedasticidad no se eval√∫a, ya que no se basa en un t√©rmino de error param√©trico como OLS.",
  "Los valores extremos en variables categ√≥ricas con pocas observaciones pueden crear ramas muy espec√≠ficas, pero CHAID maneja cardinalidades moderadas.",
  "Al basarse en œá¬≤, CHAID no se ve afectado directamente por colinealidad, aunque variables muy correlacionadas pueden crear redundancia en las divisiones.",
  "Cada divisi√≥n se basa en pruebas de œá¬≤; el √°rbol resultante puede interpretarse visualmente, pero muchos niveles pueden reducir claridad.",
  "La creaci√≥n recursiva de nodos por agrupaci√≥n de categor√≠as es eficiente para conjuntos de datos moderados; puede volverse lento si hay muchas categor√≠as de predictores.",
  "Se usa validaci√≥n cruzada para podar el √°rbol y elegir el nivel √≥ptimo de interacci√≥n, equilibrando sesgo y varianza.",
  "No es adecuado si la variable objetivo es continua (sin discretizar) o si hay demasiados niveles con muy pocos casos en cada uno."
)

tabla_chaid <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_chaid %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CHAID",
             subtitle = "Chi-squared Automatic Interaction Detection (CHAID)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Conditional Decision Trees (Conditional Inference Trees - CITs)  {-}   

**Conditional Decision Trees**, often referred to as **Conditional Inference Trees (CITs)**, represent a class of **√°rboles de decisi√≥n** que abordan una limitaci√≥n importante de los algoritmos de √°rboles de decisi√≥n tradicionales como CART, ID3, y C4.5: el **sesgo en la selecci√≥n de variables**. Mientras que los algoritmos tradicionales pueden favorecer variables predictoras con muchas categor√≠as o valores continuos (debido a que estas variables tienen m√°s "oportunidades" de generar una divisi√≥n que parezca √≥ptima), los CITs emplean un enfoque basado en **pruebas estad√≠sticas** para seleccionar la mejor divisi√≥n.

La idea fundamental de los Conditional Decision Trees es que cada divisi√≥n en el √°rbol se basa en la **significancia estad√≠stica** de la asociaci√≥n entre las variables predictoras y la variable de respuesta. En lugar de seleccionar el atributo que maximiza una medida de impureza (como la ganancia de informaci√≥n o la impureza Gini), los CITs realizan una serie de **pruebas de inferencia condicional** (t√≠picamente **pruebas de permutaci√≥n**).

El algoritmo opera de la siguiente manera:
1.  En cada nodo, se eval√∫a una **hip√≥tesis nula** de independencia entre cada variable predictora y la variable de respuesta.
2.  Se calcula el valor de $p$ para cada variable predictora.
3.  La variable predictora con el valor de $p$ m√°s peque√±o (es decir, la asociaci√≥n m√°s estad√≠sticamente significativa) es seleccionada para la divisi√≥n, siempre y cuando este valor de $p$ sea menor que un umbral de significancia predefinido.
4.  Una vez seleccionada la variable, se encuentra la mejor divisi√≥n binaria (generalmente) dentro de esa variable para ese nodo.
5.  Este proceso se repite recursivamente hasta que no haya m√°s variables significativas para dividir o se alcance un criterio de parada.

En el contexto del **aprendizaje global vs. local**, los Conditional Decision Trees se pueden considerar como un enfoque de **aprendizaje local** con un fuerte respaldo estad√≠stico. Aunque el √°rbol resultante es un modelo global, cada decisi√≥n de divisi√≥n se toma localmente bas√°ndose en la inferencia estad√≠stica sobre la relaci√≥n entre las variables en ese subconjunto de datos. Esto significa que si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se aplica de forma efectiva mediante lo que se denomina **regresi√≥n ponderada localmente**. Al utilizar pruebas de significancia para las divisiones, los CITs evitan el problema de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en una √∫nica aproximaci√≥n global, ya que las divisiones se determinan por la evidencia estad√≠stica local. Esto los convierte en una alternativa robusta que ofrece una selecci√≥n de variables menos sesgada y modelos con una mayor interpretabilidad estad√≠stica.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ No lineal, usa tests condicionales para particionar",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No relevante",
  "‚ö†Ô∏è Moderadamente (consume tests basados en permutaciones)",
  "‚úÖ Robusto a colinealidad",
  "‚úÖ Alta (cada divisi√≥n est√° basada en criterios estad√≠sticos claros)",
  "‚ö†Ô∏è M√°s lento que CART en datasets grandes",
  "‚úÖ Recomendable para podar y evitar sobreajuste",
  "‚ùå Datos muy peque√±os por nodo o variables irrelevantes"
)

detalles <- c(
  "Construye √°rboles basados en test de independencia condicional (ctree).",
  "Permite tanto regresi√≥n (valor continuo) como clasificaci√≥n multinivel.",
  "Acepta variables num√©ricas y categ√≥ricas sin necesidad de dummies.",
  "Detecta relaciones complejas y no lineales usando tests basados en permutaciones.",
  "No exige que los residuos sigan una distribuci√≥n espec√≠fica.",
  "Ideal si las observaciones no est√°n correlacionadas en el tiempo.",
  "No requiere homoscedasticidad porque no se basa en un modelo param√©trico de error.",
  "Los outliers pueden afectar el c√°lculo de los tests, aunque es m√°s robusto que CART.",
  "El algoritmo ctree no se ve afectado por predictores altamente correlacionados.",
  "Los √°rboles generados son f√°ciles de visualizar y explicar.",
  "Para cada divisi√≥n realiza m√∫ltiples tests, por lo que es m√°s lento en datos muy grandes.",
  "Usar k-fold o repeated CV para elegir la profundidad y evitar sobreajuste.",
  "No es apto si tienes muy pocas observaciones en cada parto o muchas variables irrelevantes."
)

tabla_ctree <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_ctree %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir ctree",
             subtitle = "Conditional Decision Trees") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```

## Cubist  {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Cubist.png"))
```

**Cubist** es un algoritmo de **Machine Learning** desarrollado por RuleQuest Research (autores de C4.5 y See5/C5.0), principalmente para tareas de **regresi√≥n**. Es una extensi√≥n de los modelos de **√°rboles de decisi√≥n** que combina la simplicidad de las reglas con la precisi√≥n de los modelos locales, lo que lo hace muy potente para datos complejos con muchas caracter√≠sticas.

En esencia, Cubist construye un **modelo de reglas con un modelo lineal adjunto a cada regla**. Opera en dos fases principales:

1.  **Construcci√≥n del √Årbol de Reglas:**
    * Similar a un √°rbol de decisi√≥n, Cubist construye una estructura de √°rbol dividiendo los datos en subconjuntos basados en los valores de las caracter√≠sticas.
    * Sin embargo, en lugar de hojas que contienen un valor constante (como en los √°rboles de regresi√≥n tradicionales), cada hoja de este √°rbol se transforma en un **conjunto de reglas**.
    * A cada regla se le asocia un **modelo lineal multivariado local** (o un "modelo de comit√©" de reglas, donde varias reglas contribuyen a la predicci√≥n). Este modelo lineal se entrena solo con los datos que satisfacen las condiciones de esa regla.

2.  **Ajuste del Modelo de Reglas y Predicci√≥n:**
    * Para cada nueva instancia de predicci√≥n, Cubist identifica las reglas que se aplican a esa instancia.
    * La predicci√≥n final se calcula combinando las predicciones de los modelos lineales de las reglas que se aplican, y luego se ajusta un poco esa predicci√≥n mediante un **"comit√©" de vecinos** (ajustes locales adicionales basados en ejemplos similares), si est√° configurado para ello. Esta etapa de ajuste lo hace a√∫n m√°s robusto.

Cubist es valorado por su capacidad para manejar **relaciones complejas y no lineales** en los datos. Proporciona un modelo que es m√°s interpretable que una "caja negra" (como una red neuronal profunda) debido a su base en reglas, pero mucho m√°s preciso que los modelos lineales o los √°rboles de regresi√≥n simples, gracias a sus modelos lineales locales y ajustes.


**Aprendizaje Global vs. Local:**

Cubist es un algoritmo que combina de manera muy efectiva aspectos de **aprendizaje global y local**.

* **Aspecto Global (Estructura de Reglas):** La fase de construcci√≥n del √°rbol y la derivaci√≥n de las reglas crean una **estructura global** que divide el espacio de caracter√≠sticas. Este conjunto de reglas abarca todo el dominio de los datos y determina qu√© modelo local se aplicar√° a una instancia. Es una forma de particionar el espacio de caracter√≠sticas de manera jer√°rquica para establecer un marco de predicci√≥n general.

* **Aspecto Local (Modelos Lineales y Ajustes):** Aqu√≠ es donde Cubist brilla en su capacidad de aprendizaje local:
    * **Modelos Lineales Locales:** Cada regla tiene asociado un **modelo lineal que se entrena solo con los datos que caen dentro de esa regla**. Esto permite a Cubist capturar **relaciones locales y no lineales** de manera precisa. En lugar de una √∫nica relaci√≥n lineal global, el modelo se adapta a las particularidades de diferentes subregiones de los datos.
    * **Ajuste Basado en Vecinos:** Si se activa la opci√≥n de "comit√©" o el ajuste basado en vecinos (conocido como `committees` o `neighbors`), el modelo refina a√∫n m√°s su predicci√≥n incorporando la informaci√≥n de los ejemplos de entrenamiento m√°s cercanos al punto de consulta. Esto es una forma de **"regresi√≥n ponderada localmente"**, donde la predicci√≥n final se ajusta en funci√≥n de los patrones observados en el vecindario inmediato del punto de inter√©s.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n basada en reglas)",
  "‚úÖ Num√©rica (regresi√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Modelo aditivo basado en reglas y ajustes lineales locales",
  "‚ö†Ô∏è Requiere an√°lisis de residuos, no siempre normalidad estricta",
  "‚ö†Ô∏è Asume independencia, como otros modelos supervisados",
  "‚ö†Ô∏è Puede tener heteroscedasticidad",
  "‚ö†Ô∏è Moderadamente sensible a outliers",
  "‚ö†Ô∏è Puede manejar correlaci√≥n, pero multicolinealidad puede afectar interpretabilidad",
  "‚úÖ Moderada: reglas explican el modelo, pero menos transparente que modelos lineales",
  "‚ö†Ô∏è Relativamente r√°pido, pero depende del n√∫mero de reglas",
  "‚úÖ Compatible con validaci√≥n cruzada para evaluar rendimiento",
  "‚ùå No funciona bien con datos muy peque√±os o ruido extremo"
)

detalles <- c(
  "Combina t√©cnicas de √°rboles de decisi√≥n con modelos lineales locales para predicci√≥n precisa.",
  "Predice variables continuas mediante reglas que dividen el espacio y ajustes lineales en cada regi√≥n.",
  "Puede manejar variables predictoras mixtas (num√©ricas y categ√≥ricas).",
  "Modelo flexible que ajusta m√∫ltiples reglas para capturar relaciones no lineales y locales.",
  "Evaluar residuos para verificar supuestos; no es tan r√≠gido como OLS.",
  "Como modelo supervisado, se espera independencia entre observaciones.",
  "Puede tolerar algo de heteroscedasticidad, pero afecta precisi√≥n de intervalos.",
  "Outliers pueden afectar algunas reglas locales y coeficientes.",
  "Multicolinealidad puede dificultar interpretaci√≥n de coeficientes locales.",
  "Las reglas pueden interpretarse, pero el modelo global puede ser complejo.",
  "La velocidad depende del tama√±o del dataset y n√∫mero de reglas generadas.",
  "Se usa validaci√≥n cruzada para seleccionar par√°metros y validar modelo.",
  "No es ideal para datasets muy peque√±os o con mucho ruido no estructurado."
)

tabla_cubist <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_cubist %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir cubist",
             subtitle = "Cubist")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Decision Stump  {-} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Decision Stump.png"))
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Decision Stump_1.png"))
```

Un **Decision Stump** es el tipo de **√°rbol de decisi√≥n** m√°s simple y fundamental, compuesto por un **√∫nico nodo de decisi√≥n (la ra√≠z)** que se conecta directamente a los **nodos hoja**. La idea es que un *decision stump* toma una decisi√≥n de clasificaci√≥n o regresi√≥n bas√°ndose en una sola caracter√≠stica o atributo de entrada.

Aunque parece demasiado simple, la l√≥gica es que, a pesar de su simplicidad, un *decision stump* identifica el mejor umbral o categor√≠a dentro de una √∫nica variable para separar los datos de la manera m√°s efectiva posible. Para problemas de clasificaci√≥n, esto significa encontrar la caracter√≠stica que, por s√≠ sola, maximice alguna medida de **pureza** (como la ganancia de informaci√≥n, la impureza Gini, o la significancia chi-cuadrado) o minimice el error de clasificaci√≥n. Para regresi√≥n, buscar√° el punto de divisi√≥n en una sola caracter√≠stica que minimice la suma de los cuadrados de los errores.

En el contexto del **aprendizaje local vs. global**, un *decision stump* es inherentemente un sistema de **aprendizaje local**. Su "aprendizaje" se limita a encontrar la mejor divisi√≥n dentro de una √∫nica variable, lo que es una forma extrema de **regresi√≥n ponderada localmente**. Si los datos no se distribuyen linealmente, un *decision stump* no puede por s√≠ mismo modelar relaciones complejas. Sin embargo, su valor no reside en ser un modelo predictivo robusto por s√≠ mismo, sino en ser un **"clasificador d√©bil"** o **"regresor d√©bil"** que puede ser combinado en **conjuntos de modelos (ensembles)** m√°s potentes. Por ejemplo, los *decision stumps* son los bloques de construcci√≥n m√°s comunes para algoritmos de **boosting** como **AdaBoost**. En estos casos, m√∫ltiples *decision stumps* se entrenan secuencialmente, cada uno enfoc√°ndose en los errores que cometieron los *stumps* anteriores, sumando sus "aprendizajes locales" para formar un modelo global m√°s preciso. Esto contrarresta la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n simplificada)",
  "‚úÖ Num√©ricas y/o categ√≥ricas",
  "‚ö†Ô∏è Captura solo una divisi√≥n (muy simple, un solo nodo interno)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No relevante",
  "‚úÖ Relativamente robusto (poca complejidad)",
  "‚úÖ Ignora colinealidad (usa solo una variable)",
  "‚úÖ Muy alta (un solo umbral para dividir)",
  "‚úÖ Extremadamente r√°pido",
  "‚úÖ Se puede usar k-fold para evaluar estabilidad",
  "‚ùå No funciona bien si la relaci√≥n es compleja o no hay un buen umbral √∫nico"
)

detalles <- c(
  "Modelo de √°rbol con un solo nivel de decisi√≥n (un umbral en una sola variable).",
  "En clasificaci√≥n predice una clase binaria; en regresi√≥n, un valor medio para cada divisi√≥n.",
  "Selecciona la mejor variable con el punto de corte que maximiza ganancia (clasificaci√≥n) o reduce varianza (regresi√≥n).",
  "Solo ajusta un umbral, por lo que no modela interacciones ni no linealidades complejas.",
  "No hay supuestos param√©tricos de distribuci√≥n de errores.",
  "Mejor si las instancias no est√°n correlacionadas (por ejemplo, no aplica a series de tiempo sin agrupar).",
  "La varianza constante no se eval√∫a, pues el modelo es no param√©trico y muy simple.",
  "Un solo punto de corte es menos sensible a outliers en comparaci√≥n con √°rboles profundos, pero a√∫n puede verse afectado si un outlier define el umbral.",
  "Como solo usa una variable, no se ve afectado por correlaciones altas entre predictores.",
  "El modelo entero es resumido en un √∫nico umbral; f√°cil de explicar.",
  "Muy r√°pido de entrenar y predecir, pues solo se eval√∫a un umbral en un predictor.",
  "Es √∫til para comprobar si hay una √∫nica variable con gran poder predictivo; k-fold ayuda a validar que el umbral se mantenga estable.",
  "No sirve si el problema requiere varias divisiones, interacciones o relaciones no lineales profundas."
)

tabla_stump <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_stump %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Decision Stump",
             subtitle = "Decision Stump") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Guided Trees / Hybrid Trees {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Hybrid Trees.png"))
```

## Iterative Dichotomiser 3 (ID3)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/ID3.png"))
```

**Iterative Dichotomiser 3 (ID3)** es un algoritmo cl√°sico para construir **√°rboles de decisi√≥n**, dise√±ado principalmente para tareas de **clasificaci√≥n**. Fue uno de los primeros algoritmos de √°rboles de decisi√≥n desarrollados por Ross Quinlan. La idea central de ID3 es construir un √°rbol de clasificaci√≥n seleccionando en cada nodo del √°rbol el atributo que mejor divide el conjunto de datos en subconjuntos m√°s puros y homog√©neos.

ID3 opera de forma **iterativa** y **dicot√≥mica** (aunque puede manejar atributos con m√°s de dos categor√≠as), dividiendo el conjunto de datos en cada paso bas√°ndose en el atributo m√°s informativo. La selecci√≥n del "mejor" atributo se basa en m√©tricas de **teor√≠a de la informaci√≥n**, principalmente la **ganancia de informaci√≥n** (Information Gain). La ganancia de informaci√≥n mide la reducci√≥n en la **entrop√≠a** (una medida de la impureza o desorden de un conjunto de datos) que se logra al dividir los datos seg√∫n un atributo particular. El atributo con la mayor ganancia de informaci√≥n es elegido como el nodo de decisi√≥n en cada nivel del √°rbol.

A diferencia de los sistemas de aprendizaje global que buscan minimizar funciones de p√©rdida globales (como el error cuadr√°tico medio), ID3 es un algoritmo de **aprendizaje local** en el sentido de que toma decisiones de divisi√≥n √≥ptimas en cada nodo bas√°ndose en la informaci√≥n disponible en ese subconjunto de datos. Aunque la construcci√≥n del √°rbol es un proceso global, cada paso de la divisi√≥n se optimiza localmente para maximizar la pureza de los subconjuntos resultantes. Esto le permite a ID3 capturar relaciones no lineales entre las variables, ya que no asume una distribuci√≥n lineal de los datos. En esencia, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n (o clasificaci√≥n, en este caso) de manera ponderada localmente al dividir el espacio de caracter√≠sticas en regiones m√°s manejables. Sin embargo, una desventaja de ID3 es que tiende a favorecer atributos con muchos valores y puede ser propenso al sobreajuste.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Principalmente categ√≥ricas (num√©ricas requieren discretizaci√≥n)",
  "‚úÖ No lineal (basado en ganancia de informaci√≥n)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente (valores at√≠picos pueden generar ramas poco representativas)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol simple de interpretar)",
  "‚úÖ R√°pido con datos moderados y discretizados",
  "‚úÖ Recomendable para equilibrar datos y evitar overfitting",
  "‚ùå Respuesta continua, muchos valores faltantes o ruido elevado"
)

detalles <- c(
  "Construye un √°rbol de decisi√≥n dividiendo por ganancia de informaci√≥n (entrop√≠a).",
  "Clasifica muestras en categor√≠as discretas, ej. S√≠/No, A/B/C.",
  "Mejor con variables categ√≥ricas nativas; las num√©ricas deben transformarse en rangos.",
  "No asume ninguna relaci√≥n funcional: usa particiones basadas en criterios de informaci√≥n.",
  "No hay residuos en el sentido param√©trico; no exige distribuci√≥n normal.",
  "Las instancias deben ser independientes; no orientado a series temporales.",
  "No requiere varianzas constantes porque no hay t√©rmino de error param√©trico.",
  "Los outliers categ√≥ricos pueden crear nodos muy peque√±os no representativos.",
  "ID3 ignora correlaciones altas, pero demasiadas variables correlacionadas pueden ralentizar la b√∫squeda de mejores divisiones.",
  "Cada nodo muestra la regla de divisi√≥n; el √°rbol global es f√°cil de visualizar para pocos niveles.",
  "La construcci√≥n recursiva es eficiente para datos discretizados; se vuelve lento si hay muchas categor√≠as o atributos.",
  "Se usa para podar y seleccionar profundidad √≥ptima del √°rbol, equilibrando sesgo y varianza.",
  "No es recomendable si la variable objetivo es continua o si hay mucho ruido sin transformar."
)

tabla_id3 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_id3 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir IDE3",
             subtitle = "Iterative Dichotomiser 3 (ID3)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## M5 (Model Tree) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/M5 (Model Tree).png"))
```

**M5**, a menudo referida como **M5'** o **M5P** (su implementaci√≥n en el software Weka), es un algoritmo de **√°rboles de decisi√≥n** espec√≠ficamente dise√±ado para **tareas de regresi√≥n**, es decir, para predecir valores num√©ricos continuos. Desarrollado por Ross Quinlan en 1992 y luego mejorado por Wang y Witten en 1997, M5 se destaca de los √°rboles de regresi√≥n tradicionales (como los de CART que solo tienen valores constantes en las hojas) al incorporar **modelos de regresi√≥n lineal** en sus nodos hoja.

La idea fundamental de M5 es combinar la interpretabilidad de un √°rbol de decisi√≥n con la capacidad predictiva de los modelos de regresi√≥n lineal. Funciona en dos etapas principales:

1.  **Construcci√≥n del √Årbol:** M5 construye un √°rbol de decisi√≥n de forma recursiva, similar a otros algoritmos de √°rboles. Sin embargo, en lugar de usar medidas de impureza para clasificaci√≥n, utiliza la **reducci√≥n de la desviaci√≥n est√°ndar (SDR)** como criterio de divisi√≥n. El algoritmo selecciona el atributo y el punto de divisi√≥n que maximizan la reducci√≥n de la desviaci√≥n est√°ndar del valor objetivo en los subconjuntos resultantes. Este proceso contin√∫a hasta que el n√∫mero de instancias en un nodo es muy peque√±o o la desviaci√≥n est√°ndar es muy baja.

2.  **Poda y Suavizado:** Una vez construido el √°rbol inicial, M5 lo **poda** para evitar el sobreajuste. En lugar de reemplazar los nodos con un valor constante, los nodos hoja (y a veces nodos internos) son reemplazados por **modelos de regresi√≥n lineal multivariados**. Estos modelos lineales se construyen utilizando los atributos relevantes para esa rama del √°rbol. Adem√°s, M5 aplica un proceso de **suavizado** para compensar las discontinuidades bruscas que podr√≠an surgir entre las predicciones de modelos lineales adyacentes. Este suavizado ajusta el valor predicho en una hoja bas√°ndose en las predicciones de los modelos en los nodos a lo largo de la ruta desde la ra√≠z hasta esa hoja.

En el contexto del **aprendizaje global vs. local**, M5 es un h√≠brido interesante. Por un lado, la construcci√≥n del √°rbol se basa en decisiones de divisi√≥n **locales**, buscando la mejor reducci√≥n de la desviaci√≥n est√°ndar en cada nodo. Esto permite a M5 modelar relaciones no lineales, ya que "si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n de manera ponderada localmente". El √°rbol divide el problema de regresi√≥n global en m√∫ltiples subproblemas m√°s peque√±os. Por otro lado, al tener **modelos de regresi√≥n lineal** en las hojas, M5 incorpora un componente de **aproximaci√≥n de funci√≥n local** m√°s sofisticado que un simple valor constante. Estos modelos lineales son "locales" para la regi√≥n de datos que representa esa hoja, pero internamente son modelos globales para esa subregi√≥n. Esto permite a M5 ofrecer una alternativa potente a las aproximaciones de funciones puramente globales, especialmente cuando las relaciones entre las variables son complejas y se benefician de una combinaci√≥n de particionamiento del espacio y modelado lineal dentro de esas particiones.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua",
  "‚úÖ Num√©ricas (categ√≥ricas procesar como dummies)",
  "‚úÖ Lineal por segmentos (√°rbol + regresi√≥n en hojas)",
  "‚ùå No requiere estrictamente",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es requisito",
  "‚ö†Ô∏è Moderadamente (outliers pueden distorsionar regresiones locales)",
  "‚úÖ Relativamente robusto (regresi√≥n en hojas mitiga algo la colinealidad)",
  "‚ö†Ô∏è Media (√°rbol complejo, hojas lineales m√°s interpretables)",
  "‚ö†Ô∏è Moderado (depende de n√∫mero de nodos y atributos)",
  "‚úÖ Recomendable para optimizar n√∫mero de nodos y hojas",
  "‚ùå Muchos nodos con pocos casos o ruido elevado"
)

detalles <- c(
  "Modelo de √°rbol de regresi√≥n con ajustes lineales en cada hoja.",
  "Predice valores continuos, p. ej., precio, consumo, etc.",
  "Requiere que variables categ√≥ricas se conviertan a indicadores antes de ajuste.",
  "Combina particiones basadas en atributos con regresiones m√∫ltiples en hojas.",
  "No exige que los residuos en cada hoja sean normales, aunque mejora inferencia.",
  "Ideal si las observaciones son independientes; en series de tiempo hay que agrupar.",
  "La varianza constante no es cr√≠tica, cada hoja ajusta localmente.",
  "Los extremos pueden afectar las regresiones locales; poda puede mitigar esto.",
  "El m√©todo divide el espacio antes de ajustar, reduciendo efectos de colinealidad.",
  "El √°rbol completo puede ser grande, pero cada hoja contiene una funci√≥n lineal clara.",
  "Construcci√≥n y poda del √°rbol m√°s costosas que OLS, pero razonables para tama√±os medianos.",
  "Ayuda a determinar n√∫mero √≥ptimo de hojas y complejidad del √°rbol.",
  "Si hay muy pocas observaciones por hoja o ruido demasiado alto, las regresiones locales fallan."
)

tabla_m5 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_m5 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir M5",
             subtitle = "M5 model tree algorithm") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Oblique Decision Trees {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Oblique Decision Trees.png"))
```

## Specific Implementations/Libraries {-}  


<!--chapter:end:02-decision_tree.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üåü 3. M√©todos de Ensamble {-}   

**Ejemplos:** Random Forest, Gradient Boosting (XGBoost, LightGBM, AdaBoost).   
**Uso:** Excelentes para **clasificaci√≥n y regresi√≥n en datos tabulares**, especialmente en **competencias de datos** por su alto rendimiento.   
**Ventajas:** Ofrecen **alta precisi√≥n** y son muy **robustos**.   
**Limitaciones:** Pueden ser **dif√≠ciles de interpretar** y suelen ser **computacionalmente m√°s costosos**.   

---

## Adaptive Boosting (AdaBoost)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/AdaBoost.png"))
```


**AdaBoost (Adaptive Boosting)** es uno de los algoritmos de **boosting** m√°s influyentes y el primero en ser propuesto con √©xito, desarrollado por Yoav Freund y Robert Schapire en 1995. Es una t√©cnica de **aprendizaje conjunto (ensemble learning)** utilizada principalmente para **clasificaci√≥n**, aunque sus principios pueden extenderse a la regresi√≥n. La idea fundamental de AdaBoost es construir un modelo fuerte combinando secuencialmente las predicciones de m√∫ltiples **clasificadores "d√©biles" o "base"**, y lo hace prestando m√°s atenci√≥n a los ejemplos que los modelos anteriores clasificaron incorrectamente.

El funcionamiento de AdaBoost se basa en un sistema de **re-ponderaci√≥n de datos** en cada iteraci√≥n:

1.  **Inicializaci√≥n de Pesos:** Se asigna un peso inicial igual a cada ejemplo de entrenamiento.
2.  **Entrenamiento del Clasificador D√©bil:** En cada iteraci√≥n, se entrena un clasificador d√©bil (a menudo un **Decision Stump**, que es un √°rbol de decisi√≥n de un solo nivel) en el conjunto de datos actual. Este clasificador se enfoca en minimizar el error ponderado.
3.  **C√°lculo del Error Ponderado:** Se calcula el error del clasificador d√©bil, teniendo en cuenta los pesos de los ejemplos. Los ejemplos mal clasificados tienen un mayor impacto en este error.
4.  **Actualizaci√≥n de Pesos de Datos:** Los pesos de los ejemplos mal clasificados por el clasificador actual son **aumentados**, mientras que los pesos de los ejemplos correctamente clasificados son **disminuidos**. Esto asegura que el siguiente clasificador d√©bil se enfoque m√°s en los ejemplos que son dif√≠ciles de clasificar.
5.  **C√°lculo del Peso del Clasificador:** Se asigna un peso (o "contribuci√≥n") al clasificador d√©bil actual en funci√≥n de su precisi√≥n. Los clasificadores m√°s precisos reciben un peso mayor en la predicci√≥n final del conjunto.
6.  **Combinaci√≥n de Predicciones:** Las predicciones finales del modelo AdaBoost se obtienen mediante una **suma ponderada** de las predicciones de todos los clasificadores d√©biles.

En el contexto del **aprendizaje global vs. local**, AdaBoost es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada clasificador d√©bil que se entrena en una iteraci√≥n puede verse como una forma de **regresi√≥n ponderada localmente** (o, m√°s precisamente, clasificaci√≥n ponderada localmente), ya que ajusta su enfoque bas√°ndose en los ejemplos que el modelo combinado anterior no pudo clasificar bien. Al iterar y ajustar los pesos de los datos, AdaBoost se enfoca progresivamente en las regiones del espacio de caracter√≠sticas donde el modelo actual tiene un rendimiento deficiente. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de clasificaci√≥n (y por extensi√≥n, las ideas de regresi√≥n) de manera altamente adaptativa. La capacidad de AdaBoost para concentrarse en los "errores" m√°s dif√≠ciles aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. El resultado es un clasificador global muy preciso y robusto, capaz de modelar relaciones complejas y no lineales, que es una combinaci√≥n ponderada de muchas decisiones locales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n adaptada)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para algunas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones mediante reponderaci√≥n iterativa",
  "‚ùå No requiere supuestos de normalidad en los residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers pueden obtener demasiado peso durante iteraciones)",
  "‚úÖ Robusto (reduce colinealidad al iterar sobre subconjuntos ponderados)",
  "‚ö†Ô∏è Baja (modelo resultante es especie de ‚Äôcaja negra‚Äô)",
  "‚ö†Ô∏è Lento con muchas iteraciones o datos grandes",
  "‚úÖ Recomendable para ajustar tasa de aprendizaje y n√∫mero de iteraciones",
  "‚ùå No es ideal con datos muy ruidosos o clases extremadamente desbalanceadas sin t√©cnicas adicionales"
)

detalles <- c(
  "Ensamble supervisado que combina varios modelos d√©biles (ej. √°rboles simples) ajustando pesos seg√∫n errores anteriores.",
  "En clasificaci√≥n ajusta pesos para mal clasificados; en regresi√≥n, adapta predicci√≥n por minimizaci√≥n de p√©rdida.",
  "Puede trabajar con datos mixtos; para variables categ√≥ricas suele usar codificaci√≥n de dummies.",
  "Cada iteraci√≥n repondera observaciones dif√≠ciles, enfoc√°ndose en patrones que previos modelos no capturaron.",
  "No impone distribuci√≥n de errores; se basa en funci√≥n de p√©rdida, no en supuestos param√©tricos.",
  "Funciona mejor si cada observaci√≥n es independiente; sensible a dependencias temporales si no se corrige.",
  "No requiere varianza constante, ya que funciona sobre el error iterativo en lugar de residuos tradicionales.",
  "Outliers dif√≠ciles de clasificar tienden a recibir mayor peso, lo que puede sesgar el ensamble si no se controla el learning rate.",
  "La reponderaci√≥n de muestras aten√∫a el efecto de predictores correlacionados, pues cada iteraci√≥n puede focalizarse en subconjuntos distintos.",
  "Es complejo desentra√±ar la contribuci√≥n de cada modelo d√©bil; se pueden usar m√©tricas de importancia o SHAP para interpretaci√≥n.",
  "Cada iteraci√≥n entrena un modelo d√©bil; muchas iteraciones o modelos complejos pueden ralentizar el entrenamiento.",
  "K-fold o repeated CV ayudan a elegir tasa de aprendizaje (learning rate) y n√∫mero de iteraciones (trials).",
  "No conviene con instancias altamente ruidosas: se puede sobreajustar r√°pidamente si la tasa de aprendizaje es alta o no se regula iteraciones."
)

tabla_adaboost <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_adaboost %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir adaboost",
             subtitle = "AdaBoost") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Boosting  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Boosting.png"))
```

**Boosting** es una t√©cnica de **aprendizaje conjunto (ensemble learning)** que busca transformar un conjunto de **modelos "d√©biles" o "base"** en un **modelo "fuerte" o "preciso"**. La idea fundamental es construir modelos de forma **secuencial** e **iterativa**, donde cada nuevo modelo se centra en corregir los errores o deficiencias de los modelos construidos en las iteraciones anteriores. A diferencia del *bagging* (como en Random Forest), donde los modelos se entrenan de forma independiente, el *boosting* es intr√≠nsecamente secuencial y adaptativo.

El concepto clave de Boosting radica en la asignaci√≥n de **pesos** o en el enfoque en los **errores residuales**:

1.  **Iteraciones Secuenciales:** El proceso comienza con un modelo base inicial (a menudo simple, como un *decision stump*).
2.  **Enfoque en los Errores:** En cada iteraci√≥n subsiguiente, el algoritmo presta m√°s atenci√≥n a los ejemplos que fueron clasificados (o predichos) incorrectamente por los modelos anteriores, o a los errores residuales no explicados. Esto se logra ya sea **re-ponderando** los datos (dando m√°s peso a los ejemplos mal clasificados) o **ajustando** el nuevo modelo para que prediga los residuos de los modelos anteriores.
3.  **Combinaci√≥n Ponderada:** Las predicciones de todos los modelos d√©biles se combinan, generalmente a trav√©s de una suma ponderada, donde los modelos m√°s precisos reciben un mayor peso en la predicci√≥n final.

La fuerza del boosting radica en su capacidad para reducir el **sesgo** y la **varianza** del modelo final, al construir un modelo complejo a partir de componentes simples que se complementan entre s√≠.

En el contexto del **aprendizaje global vs. local**, Boosting es una estrategia de **aprendizaje global** que opera construyendo una serie de aproximaciones **locales**. Cada modelo "d√©bil" que se entrena en una iteraci√≥n puede verse como una forma de **regresi√≥n ponderada localmente** (o clasificaci√≥n ponderada localmente), ya que se enfoca en una parte espec√≠fica del espacio de las caracter√≠sticas o en los datos con mayor error. El proceso iterativo de Boosting busca corregir estos errores localizados. Si los datos no se distribuyen linealmente, el boosting permite que el concepto de regresi√≥n (o clasificaci√≥n) se aplique de manera muy flexible y potente. La capacidad de concentrarse en los "errores" residuales aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Al ensamblar muchos modelos d√©biles que se adaptan a los errores de los anteriores, Boosting construye un modelo final que es una aproximaci√≥n de funci√≥n global altamente adaptable y precisa. Algoritmos como AdaBoost y Gradient Boosting Machines (GBM) son ejemplos prominentes de esta t√©cnica.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para algunas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones complejas mediante aprendizaje secuencial",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio para muchos algoritmos",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (puede ajustar demasiado a outliers si no se controla)",
  "‚úÖ Robusto (cada iteraci√≥n utiliza un subconjunto ponderado de datos)",
  "‚ö†Ô∏è Baja (modelo en su conjunto es tipo caja negra)",
  "‚ö†Ô∏è Lento con muchos √°rboles o altas iteraciones",
  "‚úÖ Recomendable con k-fold o repeated CV para ajustar tasa de aprendizaje y n√∫mero de iteraciones",
  "‚ùå Si se tienen pocos datos, alto ruido o target muy desbalanceado sin ajuste"
)

detalles <- c(
  "Ensamble supervisado que combina varios modelos d√©biles (ej. √°rboles peque√±os) de forma secuencial",
  "En clasificaci√≥n se usan votaciones ponderadas; en regresi√≥n se suman predicciones graduadas",
  "Acepta variables mixtas; algunas bibliotecas requieren convertir categ√≥ricas en dummies",
  "Construye modelos d√©biles en cada iteraci√≥n, enfoc√°ndose en muestras mal clasificadas o con alto residuo",
  "No exige distribuci√≥n de errores, ya que se basa en funci√≥n de p√©rdida sin supuestos param√©tricos",
  "Mejor si los ejemplos son independientes; puede usar t√©cnicas especiales para datos correlacionados",
  "No asume varianza constante, usa funci√≥n de p√©rdida directa para optimizar",
  "Los outliers pueden recibir peso excesivo en iteraciones posteriores, por lo que es necesario regularizar o usar robust loss",
  "La selecci√≥n de variables se hace impl√≠citamente, reduciendo el impacto de colinealidad",
  "Dif√≠cil de interpretar directamente; se pueden usar m√©tricas de importancia, SHAP o partial dependence para explicaci√≥n",
  "Cada iteraci√≥n entrena un modelo d√©bil, por lo que puede ser costoso si el n√∫mero de iteraciones es alto",
  "CV ayuda a determinar la tasa de aprendizaje (learning rate), n√∫mero de iteraciones y complejidad de base learners",
  "No es adecuado si hay muy pocos ejemplos, alta dimensionalidad con poco se√±al o target extremadamente desequilibrado"
)

tabla_boosting <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_boosting %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Boosting",
             subtitle = "Boosting") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Bootstrapped Aggregation (Bagging)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Bagging.png"))
```

**Bootstrapped Aggregation (Bagging)** es una t√©cnica de **aprendizaje conjunto (ensemble learning)** dise√±ada para mejorar la estabilidad y precisi√≥n de los algoritmos de aprendizaje autom√°tico, particularmente para reducir la **varianza** en los modelos. Fue introducida por Leo Breiman en 1996 y es la base de algoritmos muy populares como **Random Forest**. La idea fundamental de Bagging es entrenar m√∫ltiples versiones de un mismo modelo base en diferentes subconjuntos del conjunto de datos original y luego combinar sus predicciones.

El proceso central de Bagging implica dos pasos clave:

1.  **Muestreo Bootstrap:** En lugar de entrenar un √∫nico modelo en todo el conjunto de datos de entrenamiento, Bagging crea **m√∫ltiples conjuntos de datos de arranque (bootstrap samples)**. Cada muestra de arranque se crea seleccionando aleatoriamente, **con reemplazo**, un n√∫mero de observaciones igual al tama√±o del conjunto de datos original. Esto significa que algunos puntos de datos pueden aparecer varias veces en una muestra de arranque, mientras que otros pueden no aparecer en absoluto. Este muestreo aleatorio introduce diversidad entre los conjuntos de entrenamiento para cada modelo.

2.  **Agregaci√≥n (Aggregation):** Una vez que se han entrenado **m√∫ltiples modelos base independientes** (por ejemplo, √°rboles de decisi√≥n) en cada una de estas muestras de arranque, sus predicciones se combinan. Para tareas de **clasificaci√≥n**, la combinaci√≥n se realiza mediante **votaci√≥n por mayor√≠a** (la clase m√°s votada). Para tareas de **regresi√≥n**, las predicciones se promedian. Esta agregaci√≥n de predicciones de modelos diversos reduce la varianza y, por lo tanto, hace que el modelo final sea m√°s robusto y menos propenso al sobreajuste que un solo modelo entrenado en todo el conjunto de datos.

En el contexto del **aprendizaje global vs. local**, Bagging es una estrategia que combina las ventajas de los modelos de **aprendizaje local** para construir una **aproximaci√≥n de funci√≥n global** m√°s estable. Cada modelo base (ej. un √°rbol de decisi√≥n) que se entrena en una muestra de arranque puede considerarse un sistema de aprendizaje local, ya que toma decisiones basadas en el subconjunto de datos que le ha sido asignado. Sin embargo, al entrenar estos m√∫ltiples modelos en paralelo y luego agregarlos, Bagging construye un modelo final que es una aproximaci√≥n de funci√≥n global altamente adaptable. La ventaja principal es que, si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se puede aplicar eficazmente mediante esta forma de **regresi√≥n ponderada localmente** (donde los "pesos" son impl√≠citos a trav√©s de la agregaci√≥n de predicciones de modelos entrenados en subconjuntos aleatorios de datos). Bagging aborda el problema de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo al promediar o votar las predicciones de m√∫ltiples modelos, lo que reduce la varianza y mejora la generalizaci√≥n.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Captura relaciones no lineales al promediar m√∫ltiples modelos",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚úÖ Robusto (cada bootstrap reduce el impacto de outliers)",
  "‚úÖ Robusto (la agregaci√≥n mitiga colinealidad)",
  "‚ö†Ô∏è Moderada (dif√≠cil interpretar conjunto de modelos)",
  "‚ö†Ô∏è Moderado (depende del n√∫mero de √°rboles y tama√±o del dataset)",
  "‚úÖ Recomendable usar k-fold",
  "‚ùå No es ideal con muy pocos datos o si los base learners son demasiado simples"
)

detalles <- c(
  "Ensamble supervisado que ajusta varios modelos (usualmente √°rboles) sobre muestras bootstrap y promedia predicciones.",
  "En clasificaci√≥n predice la clase m√°s votada; en regresi√≥n, promedia los valores predichos.",
  "Acepta todo tipo de variables; las categ√≥ricas deben codificarse adecuadamente.",
  "Al promediar m√∫ltiples modelos, reduce varianza y captura no linealidades impl√≠citamente.",
  "No impone supuestos sobre la distribuci√≥n de errores.",
  "Los datos deben ser independientes; funciona peor en datos con fuerte autocorrelaci√≥n sin ajuste.",
  "No requiere varianza constante puesto que se basa en agregaci√≥n de m√∫ltiples predicciones.",
  "Cada muestra bootstrap y √°rbol es menos sensible a valores extremos; la agregaci√≥n aumenta robustez.",
  "La selecci√≥n aleatoria de subconjuntos y bootstrap reduce el efecto de predictores correlacionados.",
  "El modelo final es un conjunto de muchos √°rboles, lo que dificulta su explicaci√≥n directa.",
  "Entrenar cientos de √°rboles toma tiempo, pero es paralelizable; la predicci√≥n es relativamente r√°pida.",
  "CV ayuda a ajustar par√°metros como n√∫mero de √°rboles y profundidad m√°xima de cada √°rbol.",
  "Con pocos ejemplos, los bootstrap no aportan diversidad suficiente; si los base learners son muy simples, no capturan bien patrones complejos."
)

tabla_bagging <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bagging %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir bagging",
             subtitle = "Bootstrapped Aggregation (Bagging) ") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## CatBoost {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/CatBoost.png"))
```

## Extreme Gradient Boosting (XGBoost)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/XGBoost.png"))
```

**XGBoost (Extreme Gradient Boosting)** es una implementaci√≥n optimizada y altamente eficiente del algoritmo de **Gradient Boosting Machines (GBM)**, ampliamente reconocida por su **velocidad**, **rendimiento** y **escalabilidad** en problemas de **clasificaci√≥n** y **regresi√≥n**. Gan√≥ una inmensa popularidad debido a su √©xito en numerosas competiciones de *machine learning* (como Kaggle). Aunque se basa en los principios de GBM, XGBoost introduce varias mejoras clave que lo hacen superior en muchos escenarios.

La idea fundamental de XGBoost, al igual que GBM, es construir un modelo aditivo de forma **secuencial**, donde cada nuevo √°rbol intenta corregir los errores residuales del conjunto de √°rboles previos. Sin embargo, XGBoost optimiza este proceso con las siguientes caracter√≠sticas:

1.  **Paralelizaci√≥n:** Aunque el *boosting* es inherentemente secuencial, XGBoost permite la paralelizaci√≥n de la construcci√≥n de los √°rboles individuales. Por ejemplo, en el paso de b√∫squeda de la mejor divisi√≥n, puede evaluar las posibles divisiones en paralelo a trav√©s de m√∫ltiples n√∫cleos de CPU.
2.  **Regularizaci√≥n:** Incorpora t√©rminos de **regularizaci√≥n L1 (Lasso)** y **L2 (Ridge)** en la funci√≥n de costo para controlar la complejidad del modelo y evitar el sobreajuste. Esto es crucial para la generalizaci√≥n.
3.  **Manejo de Valores Faltantes:** Tiene una capacidad incorporada para manejar valores faltantes en los datos, permitiendo al algoritmo aprender la mejor direcci√≥n para los valores ausentes.
4.  **Poda por Profundidad (Depth-First Search):** A diferencia de muchos algoritmos de √°rboles que crecen nivel por nivel, XGBoost puede usar un enfoque de poda por profundidad, lo que a menudo resulta en √°rboles m√°s eficientes.
5.  **Cach√©-Aware Computing:** Optimiza el acceso a la memoria para manejar grandes conjuntos de datos de manera eficiente.
6.  **Flexibilidad de Funci√≥n de P√©rdida:** Permite el uso de funciones de p√©rdida personalizadas, lo que lo hace adaptable a una amplia gama de problemas.

En el contexto del **aprendizaje global vs. local**, XGBoost es una poderosa estrategia de **aprendizaje global** que se construye iterativamente a partir de componentes de **aprendizaje local**. Cada √°rbol de regresi√≥n (o clasificaci√≥n) individual es un "aprendiz d√©bil" que se enfoca en las deficiencias del modelo acumulado. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) de manera altamente sofisticada mediante esta **regresi√≥n ponderada localmente**. Al centrarse en los errores residuales y optimizar el proceso de manera rigurosa, XGBoost aborda de manera excepcional la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su combinaci√≥n de precisi√≥n, velocidad y capacidad para manejar grandes conjuntos de datos lo ha convertido en uno de los algoritmos m√°s populares y efectivos en la pr√°ctica del *machine learning*.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para dummies o label encoding)",
  "‚úÖ Captura no linealidades e interacciones complejas v√≠a √°rboles en boosting",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (puede sobreajustar a outliers si no regula)",
  "‚úÖ Robusto (reduce efecto de colinealidad al usar √°rboles secuenciales)",
  "‚ö†Ô∏è Baja (modelo complejo y tipo ‚Äôcaja negra‚Äô)",
  "‚úÖ Muy r√°pido y escalable (implementaci√≥n optimizada, paralelizable)",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajustar hiperpar√°metros",
  "‚ùå No es ideal con datos muy peque√±os, ruido alto o target extremadamente desbalanceado sin ajuste"
)

detalles <- c(
  "Ensamble supervisado que combina m√∫ltiples √°rboles d√©biles optimizados con gradiente descendente acelerado.",
  "En regresi√≥n predice valores continuos; en clasificaci√≥n combina probabilidades o clases mediante log-loss o multiclass objectives.",
  "Acepta variables mixtas; las categ√≥ricas deben convertirse a formatos compatibles (p. ej. factor numerico, one-hot encoding).",
  "Cada iteraci√≥n ajusta un nuevo √°rbol enfoc√°ndose en los residuos del modelo anterior, capturando patrones complejos.",
  "No impone distribuci√≥n param√©trica de errores, ya que optimiza funciones de p√©rdida directamente.",
  "Funciona mejor si cada muestra es independiente; sensible a series de tiempo sin preparaci√≥n apropiada.",
  "No requiere varianza constante, porque basa la optimizaci√≥n en gradientes del loss, no en supuestos de error.",
  "Los outliers dif√≠ciles de predecir pueden recibir demasiado peso en iteraciones sucesivas; usar _learning_rate_ bajo y _max_depth_ peque√±o para regular.",
  "Los √°rboles reducen el impacto de variables altamente correlacionadas, aunque m√∫ltiples iteraciones pueden a√∫n privilegiar caracter√≠sticas correlacionadas.",
  "Dif√≠cil interpretar directamente cada √°rbol; se utilizan m√©tricas de importancia, SHAP values o partial dependence plots para explicaci√≥n.",
  "Implementaci√≥n en C++ altamente optimizada (CPU/GPU), permite entrenamiento muy r√°pido incluso con millones de filas.",
  "Validaci√≥n cruzada anidada o simple ayuda a elegir hiperpar√°metros como `eta` (learning_rate), `nrounds` (n√∫mero de √°rboles), `max_depth`, `subsample`, `colsample_bytree`.",
  "No conviene con datasets muy peque√±os, ya que puede sobreajustar; tampoco si el ruido es muy alto y no se regula bien la complejidad."
)

tabla_xgboost <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_xgboost %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir xgboost",
             subtitle = "XGBoost")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Gradient Boosting Machines (GBM)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/GBM.png"))
```

**Gradient Boosting Machines (GBM)** es un algoritmo de **aprendizaje conjunto (ensemble learning)** extremadamente potente y vers√°til, utilizado para **clasificaci√≥n**, **regresi√≥n** y otras tareas predictivas. A diferencia de Random Forest que construye √°rboles de forma independiente en paralelo (bagging), GBM construye los √°rboles de forma **secuencial** y aditiva. La idea central es que cada nuevo √°rbol en el conjunto intenta corregir los errores residuales (residuos) del conjunto de √°rboles construidos previamente.

El concepto fundamental detr√°s de GBM es el **impulso (boosting)**, donde los modelos "d√©biles" (generalmente √°rboles de decisi√≥n, a menudo √°rboles poco profundos o "stumps") se combinan para formar un modelo "fuerte". GBM logra esto de una manera espec√≠fica:

1.  **Modelo Inicial:** Comienza con una predicci√≥n inicial para todos los datos (por ejemplo, el valor promedio para regresi√≥n o la probabilidad logar√≠tmica para clasificaci√≥n).
2.  **C√°lculo de Residuos (Pseudo-residuos):** En cada iteraci√≥n, el algoritmo calcula los **residuos** (o m√°s precisamente, los "pseudo-residuos" o gradientes negativos de la funci√≥n de p√©rdida) entre los valores reales y las predicciones actuales del modelo. Estos residuos representan los "errores" que el modelo actual no ha podido capturar.
3.  **Entrenamiento de un Nuevo √Årbol:** Se entrena un nuevo √°rbol de decisi√≥n para **predecir estos residuos**. Este √°rbol es t√≠picamente peque√±o y d√©bil, dise√±ado para centrarse en las √°reas donde el modelo actual tiene los mayores errores.
4.  **Actualizaci√≥n del Modelo:** La predicci√≥n de este nuevo √°rbol se a√±ade a la predicci√≥n acumulada del modelo existente, multiplicada por una **tasa de aprendizaje (learning rate)**. Esta tasa de aprendizaje controla el tama√±o del paso de cada √°rbol, evitando que el modelo se sobreajuste r√°pidamente.
5.  **Iteraci√≥n:** Este proceso se repite para un n√∫mero predefinido de iteraciones, o hasta que una m√©trica de rendimiento deje de mejorar. Cada nuevo √°rbol contribuye a reducir los errores restantes.

En el contexto del **aprendizaje global vs. local**, GBM es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol individual en el proceso de boosting es un sistema de aprendizaje local (como los *decision stumps* o √°rboles poco profundos) que se enfoca en una parte espec√≠fica del error. Sin embargo, la combinaci√≥n aditiva y secuencial de estos modelos "d√©biles" produce un modelo predictivo global altamente sofisticado y preciso. Si los datos no se distribuyen linealmente, GBM aplica el concepto de regresi√≥n (o clasificaci√≥n) mediante una forma incremental y adaptativa de **regresi√≥n ponderada localmente**. Al centrarse en los errores residuales, GBM aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su capacidad para minimizar la funci√≥n de p√©rdida de forma gradual y dirigida lo hace excepcionalmente eficaz para modelar relaciones complejas y no lineales, a menudo logrando un rendimiento superior en muchos problemas del mundo real.


```{r, echo = FALSE}

criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n)",
  "‚úÖ Captura no linealidades e interacciones complejas v√≠a boosting",
  "‚ùå No requiere",
  "‚úÖ Deseable pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (los outliers pueden influir en √°rboles individuales)",
  "‚úÖ Robusto (los arboles manejan colinealidad localmente)",
  "‚ö†Ô∏è Baja (modelo de tipo caja negra con varios √°rboles secuenciales)",
  "‚ö†Ô∏è Lento con muchos √°rboles, datos grandes o par√°metros altos",
  "‚úÖ Recomendable con k-fold o repeated CV para ajuste de hiperpar√°metros",
  "‚ùå Si se tienen pocos datos, muchas categor√≠as o ruido alto"
)

detalles <- c(
  "Ensamble de √°rboles secuenciales donde cada √°rbol corrige errores del anterior.",
  "En clasificaci√≥n se combinan probabilidades; en regresi√≥n se promedian predicciones.",
  "Funciona con muchas variables y aprende la importancia autom√°ticamente.",
  "Construye √°rboles d√©biles que se enfocan en los errores residuales previos.",
  "No impone supuestos en la distribuci√≥n de los errores.",
  "Los datos deben ser observaciones independientes; sensible a dependencias temporales.",
  "No requiere varianza constante puesto que se basa en √°rboles.",
  "Los outliers pueden exagerar los gradientes y forzar ajustes extremos en √°rboles individuales.",
  "Los √°rboles reducen impacto de colinealidad, pero m√∫ltiples √°rboles pueden still complicarla.",
  "Dif√≠cil de interpretar el conjunto; se pueden usar importance plots o SHAP para explicaci√≥n.",
  "La construcci√≥n secuencial de cientos de √°rboles puede ser costosa en tiempo y memoria.",
  "La validaci√≥n cruzada ayuda a determinar tasa de aprendizaje, n√∫mero de √°rboles y profundidad.",
  "No es ideal cuando se tienen muy pocos datos o categor√≠as con pocos ejemplos."
)

tabla_gbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_gbm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GBM",
             subtitle = "Gradient Boosting Machines (GBM)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## Gradient Boosted Regression Trees (GBRT)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/GBRT.png"))
```


**Gradient Boosted Regression Trees (GBRT)**, a menudo conocida como **Gradient Boosting Machines (GBM)** cuando los modelos base son √°rboles de decisi√≥n de regresi√≥n, es una t√©cnica de **aprendizaje conjunto (ensemble learning)** extremadamente potente y ampliamente utilizada para tareas de **regresi√≥n** (predicci√≥n de valores num√©ricos continuos) y tambi√©n puede adaptarse para **clasificaci√≥n**. Su fortaleza radica en su capacidad para construir un modelo predictivo robusto y preciso mediante la combinaci√≥n secuencial de m√∫ltiples √°rboles de decisi√≥n "d√©biles".

La idea central de GBRT se basa en el principio de **boosting**, donde cada nuevo √°rbol en el conjunto se entrena para **corregir los errores residuales** (la diferencia entre los valores reales y las predicciones acumuladas del modelo hasta ese momento) de los √°rboles construidos en las iteraciones anteriores. Este proceso es iterativo y aditivo:

1.  **Modelo Inicial:** El proceso comienza con una predicci√≥n inicial simple para todos los datos, a menudo el valor promedio de la variable objetivo.
2.  **C√°lculo de Pseudo-Residuos:** En cada iteraci√≥n, GBRT calcula los "pseudo-residuos", que son los **gradientes negativos de la funci√≥n de p√©rdida** con respecto a la predicci√≥n actual. Para la p√©rdida cuadr√°tica media (com√∫n en regresi√≥n), estos pseudo-residuos son simplemente los errores tradicionales (valor real - predicci√≥n).
3.  **Entrenamiento de un √Årbol de Regresi√≥n:** Se entrena un nuevo **√°rbol de decisi√≥n de regresi√≥n** (que es un "aprendiz d√©bil", a menudo un √°rbol poco profundo o un *decision stump*) para **predecir estos pseudo-residuos**. El √°rbol busca los mejores puntos de divisi√≥n para reducir estos errores.
4.  **Actualizaci√≥n del Modelo:** La predicci√≥n de este nuevo √°rbol de regresi√≥n se a√±ade a la predicci√≥n acumulada del modelo existente, pero se escala por una **tasa de aprendizaje (learning rate)**. Esta tasa de aprendizaje es un hiperpar√°metro crucial que controla la "contribuci√≥n" de cada nuevo √°rbol y ayuda a prevenir el sobreajuste.
5.  **Iteraci√≥n:** Los pasos 2 a 4 se repiten para un n√∫mero predefinido de iteraciones. Cada nuevo √°rbol se enfoca en las deficiencias del modelo combinado anterior, refinando gradualmente la predicci√≥n.

En el contexto del **aprendizaje global vs. local**, GBRT es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol de regresi√≥n individual es un sistema de aprendizaje local que divide el espacio de caracter√≠sticas y aprende patrones en subregiones. Sin embargo, el proceso de boosting, al combinar estos √°rboles secuencialmente para reducir los errores residuales globales, construye una **aproximaci√≥n de funci√≥n global** altamente flexible y precisa. La clave es que, si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n de manera muy efectiva a trav√©s de esta **regresi√≥n ponderada localmente**. Al centrarse en los errores que el modelo actual no puede explicar, GBRT aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Es excepcionalmente potente para capturar relaciones complejas y no lineales, y es ampliamente utilizado en diversas aplicaciones, desde la predicci√≥n de precios hasta la optimizaci√≥n de rutas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para ciertas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones complejas mediante boosting de √°rboles",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers pueden influir en √°rboles individuales)",
  "‚úÖ Robusto (los √°rboles reducen el impacto de colinealidad localmente)",
  "‚ö†Ô∏è Baja (modelo en su conjunto es ‚Äúcaja negra‚Äù)",
  "‚ö†Ô∏è Lento con muchos √°rboles o datos extensos",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajuste de hiperpar√°metros",
  "‚ùå No es ideal si hay muy pocos datos o ruido excesivo"
)

detalles <- c(
  "Ensamble de √°rboles de regresi√≥n secuenciales donde cada √°rbol corrige errores del anterior mediante gradiente.",
  "Predice valores continuos sumando las predicciones ponderadas de m√∫ltiples √°rboles d√©biles.",
  "Funciona con variables mixtas; las categ√≥ricas suelen transformarse en dummies.",
  "Cada nuevo √°rbol se enfoca en los residuos del modelo anterior, capturando patrones complejos.",
  "No impone distribuci√≥n normal porque optimiza una funci√≥n de p√©rdida (por ejemplo, MSE) directamente.",
  "Mejor si las observaciones no est√°n correlacionadas en el tiempo; ajustar para series si es necesario.",
  "No requiere varianza constante puesto que se basa en √°rboles, no en un modelo param√©trico de errores.",
  "Los valores extremos pueden provocar ajustes excesivos en √°rboles individuales; usar tasa de aprendizaje baja ayuda a mitigar.",
  "Los √°rboles reducen el impacto de variables correlacionadas, aunque m√∫ltiples iteraciones pueden complicar interpretaciones.",
  "Dif√≠cil de interpretar directamente; se puede usar importancia de variables o herramientas como SHAP para explicaci√≥n.",
  "Cada iteraci√≥n entrena un √°rbol nuevo; muchos √°rboles o gran profundidad de √°rbol incrementan el tiempo de entrenamiento.",
  "CV ayuda a determinar par√°metros como tasa de aprendizaje (`learning_rate`), n√∫mero de √°rboles (`n.trees`) y profundidad m√°xima (`max_depth`).",
  "No es adecuado cuando el dataset es muy peque√±o o extremadamente ruidoso, ya que puede sobreajustar f√°cilmente."
)

tabla_gbrt <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_gbrt %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GBRT",
             subtitle = "Gradient Boosted Regression Trees (GBRT)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Isolation Forest {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Isolation Forest.png"))
```

## Light Gradient Boosting Machine (LightGBM)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/LightGBM.png"))
```

**LightGBM (Light Gradient Boosting Machine)** es otro algoritmo de **Gradient Boosting Machines (GBM)** de alto rendimiento, desarrollado por Microsoft. Est√° dise√±ado para ser **extremadamente r√°pido** y **eficiente** en el uso de memoria, especialmente con grandes conjuntos de datos, sin sacrificar una precisi√≥n significativa. Al igual que XGBoost, ha ganado popularidad en competiciones de *machine learning* por su velocidad y capacidad para manejar grandes vol√∫menes de datos.

La idea fundamental de LightGBM es la misma que la de otros algoritmos de boosting: construir un modelo aditivo de forma **secuencial**, donde cada nuevo √°rbol intenta corregir los errores residuales del modelo combinado anterior. Sin embargo, LightGBM introduce varias optimizaciones clave para lograr su notable eficiencia:

1.  **Gradient-based One-Side Sampling (GOSS):** A diferencia de XGBoost que usa todas las instancias para cada iteraci√≥n, GOSS se enfoca en las instancias que tienen un **mayor gradiente** (es decir, las que contribuyen m√°s al error). Descarta las instancias con gradientes peque√±os o las muestrea con menos frecuencia, lo que acelera el entrenamiento sin perder demasiada precisi√≥n.
2.  **Exclusive Feature Bundling (EFB):** EFB agrupa caracter√≠sticas mutuamente exclusivas (es decir, caracter√≠sticas que rara vez toman valores distintos de cero al mismo tiempo) en un solo "bundle". Esto reduce el n√∫mero de caracter√≠sticas y acelera el c√°lculo del histograma sin afectar la precisi√≥n.
3.  **Histogram-based Algorithm:** En lugar de construir √°rboles en una forma de pre-orden que es com√∫n en muchos algoritmos (lo que puede ser lento al enumerar todos los puntos de divisi√≥n), LightGBM utiliza un **algoritmo basado en histogramas**. Convierte los valores de las caracter√≠sticas continuas en *bins* discretos. Esto acelera significativamente el proceso de b√∫squeda del mejor punto de divisi√≥n.
4.  **Leaf-wise (Best-first) Tree Growth:** A diferencia de la mayor√≠a de los √°rboles de decisi√≥n que crecen nivel por nivel (como en XGBoost), LightGBM crece el √°rbol **"hoja por hoja" (leaf-wise)**. Esto significa que en cada paso, selecciona la hoja con la mayor reducci√≥n de p√©rdida y la divide. Este enfoque puede llevar a √°rboles m√°s profundos y asim√©tricos que pueden ser m√°s precisos para el mismo n√∫mero de nodos, aunque puede ser m√°s propenso al sobreajuste (lo cual se mitiga con la regularizaci√≥n).

En el contexto del **aprendizaje global vs. local**, LightGBM, al igual que otros algoritmos de boosting, es una estrategia de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol que se entrena es un "aprendiz d√©bil" que se enfoca en las deficiencias residuales del modelo acumulado. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) de manera muy eficiente mediante esta **regresi√≥n ponderada localmente**. Al centrarse en los errores y optimizar los c√°lculos, LightGBM aborda de manera sobresaliente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su √©nfasis en la velocidad y la eficiencia lo hace ideal para conjuntos de datos muy grandes o escenarios donde el tiempo de entrenamiento es una preocupaci√≥n cr√≠tica.  



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n apropiada)",
  "‚úÖ Captura no linealidades e interacciones mediante √°rboles en boosting",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (los outliers pueden influir en pesos de hojas)",
  "‚úÖ Robusto (usa histogram-based split que aten√∫a colinealidad)",
  "‚ö†Ô∏è Baja (modelo complejo tipo ‚Äòcaja negra‚Äô)",
  "‚úÖ Muy r√°pido y escalable (optimized gradient-based)",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajustar hiperpar√°metros",
  "‚ùå No conviene con datos muy peque√±os o muy ruidosos sin regularizaci√≥n"
)

detalles <- c(
  "Ensamble supervisado que entrenan √°rboles de decisi√≥n usando histogram-based gradient boosting.",
  "En regresi√≥n predice valores continuos; en clasificaci√≥n maximiza log-loss u otras funciones objetivo.",
  "Acepta variables mixtas; las categ√≥ricas deben convertirse a formato num√©rico o usar encoding interno.",
  "Cada iteraci√≥n ajusta un √°rbol enfoc√°ndose en los residuos del anterior, capturando patrones complejos.",
  "No impone distribuci√≥n param√©trica de errores; optimiza la funci√≥n de p√©rdida directamente.",
  "Funciona mejor si las muestras son independientes; sensible a series de tiempo sin preparaci√≥n adecuada.",
  "No requiere varianza constante, dado que es un m√©todo basado en √°rbol, no en supuestos de error.",
  "Los valores extremos pueden afectar el c√°lculo de gradientes y splits; usar regularizaci√≥n y par√°metros de manejo de outliers.",
  "La divisi√≥n basada en histogramas reduce el impacto de predictores altamente correlacionados.",
  "Dif√≠cil interpretar cada √°rbol; se utilizan m√©tricas de importancia y herramientas como SHAP para explicaci√≥n.",
  "Implementaci√≥n en C++ altamente optimizada que permite entrenamiento muy r√°pido incluso con grandes vol√∫menes de datos.",
  "CV ayuda a elegir par√°metros como `learning_rate`, `num_leaves`, `max_depth`, `feature_fraction`, `bagging_fraction`.",
  "No es ideal si el dataset es muy peque√±o, pues el boosting puede sobreajustar; tampoco con mucho ruido sin regularizaci√≥n adecuada."
)

tabla_lightgbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lightgbm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LightGBM",
             subtitle = "LightGBM")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Random Forest  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Bagging.png"))
```

**Random Forest** es un algoritmo de **aprendizaje conjunto (ensemble learning)** altamente popular y potente, utilizado tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**. Fue desarrollado por Leo Breiman en 2001 y se basa en la idea de combinar las predicciones de m√∫ltiples **√°rboles de decisi√≥n** para lograr una mayor precisi√≥n y robustez que un solo √°rbol. La fuerza de Random Forest reside en dos conceptos clave: **bagging (bootstrap aggregation)** y la **aleatoriedad en la selecci√≥n de caracter√≠sticas**.

La idea fundamental detr√°s de Random Forest es construir un "bosque" de √°rboles de decisi√≥n de una manera espec√≠fica:

1.  **Bagging (Bootstrap Aggregation):** En lugar de entrenar un solo √°rbol en todo el conjunto de datos, Random Forest entrena cada √°rbol en una **muestra de arranque (bootstrap sample)** diferente. Una muestra de arranque es un subconjunto del conjunto de datos original, muestreado con reemplazo. Esto significa que algunos puntos de datos pueden aparecer varias veces en una muestra, mientras que otros pueden no aparecer en absoluto. Este muestreo genera diversidad entre los √°rboles.

2.  **Aleatoriedad en la Selecci√≥n de Caracter√≠sticas:** Cuando cada √°rbol se construye, en cada paso de divisi√≥n (nodo), Random Forest no considera todas las caracter√≠sticas disponibles. En cambio, solo considera un **subconjunto aleatorio de caracter√≠sticas** para encontrar la mejor divisi√≥n. Esta aleatoriedad adicional (adem√°s del muestreo de arranque) descorrelaciona a√∫n m√°s los √°rboles, lo que es crucial para el rendimiento del algoritmo. Si los √°rboles estuvieran altamente correlacionados, el error de un √°rbol promedio no se reducir√≠a al promediar.

Una vez que se han construido numerosos √°rboles (t√≠picamente cientos o miles), las predicciones se combinan: para **clasificaci√≥n**, se utiliza la **votaci√≥n por mayor√≠a** (la clase m√°s votada por los √°rboles individuales); para **regresi√≥n**, se calcula el **promedio** de las predicciones de todos los √°rboles.

En el contexto del **aprendizaje global vs. local**, Random Forest se puede considerar como un sistema de **aprendizaje global** que se construye a partir de componentes de **aprendizaje local**. Cada √°rbol individual en el bosque es un sistema de aprendizaje local (como CART, que divide el problema en subproblemas m√°s peque√±os). Sin embargo, al combinar las predicciones de muchos de estos √°rboles, Random Forest logra una **aproximaci√≥n de funci√≥n global** muy robusta y flexible. La ventaja es que, si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) mediante una forma sofisticada de **regresi√≥n ponderada localmente**. La combinaci√≥n de √°rboles diversos y descorrelacionados mitiga la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Random Forest sobresale en capturar relaciones complejas y no lineales, manejar grandes conjuntos de datos con muchas caracter√≠sticas y es menos propenso al sobreajuste que un solo √°rbol de decisi√≥n grande.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n)",
  "‚úÖ Captura relaciones no lineales e interacciones complejas",
  "‚ùå No requiere",
  "‚úÖ Deseable pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚úÖ Robusto a outliers (por agregaci√≥n)",
  "‚úÖ Robusto (selecciona subconjuntos aleatorios)",
  "‚ö†Ô∏è Moderada (dif√≠cil interpretar cientos de √°rboles)",
  "‚ö†Ô∏è Lento con muchos √°rboles o datos grandes",
  "‚úÖ Recomendado usar k-fold",
  "‚ùå Puede sobreajustar si no se ajustan hiperpar√°metros (e.g. profundidad, n√∫mero de √°rboles)"
)

detalles <- c(
  "Ensamble de √°rboles de decisi√≥n, cada uno entrenado en una muestra bootstrap y usando un subconjunto aleatorio de predictores.",
  "En clasificaci√≥n predice la clase mayoritaria entre √°rboles; en regresi√≥n, el promedio de predicciones.",
  "Acepta muchas variables y selecciona autom√°ticamente las m√°s relevantes por importancia.",
  "Al generar m√∫ltiples √°rboles, capta interacciones no lineales sin necesidad de especificarlas.",
  "No hay supuestos sobre la distribuci√≥n de los errores.",
  "Los √°rboles individuales pueden manejar correlaci√≥n leve; el ensamble mitiga la dependencia.",
  "No necesita homogeneidad de varianza en los errores residuales.",
  "Cada √°rbol es poco sensible a outliers, y la agregaci√≥n mejora robustez.",
  "Reduce el problema de colinealidad al seleccionar subconjuntos de variables por √°rbol.",
  "Es dif√≠cil de explicar, aunque se pueden usar m√©tricas de importancia de variables.",
  "Puede volverse lento si se entrenan miles de √°rboles en datasets muy grandes.",
  "Cross-validation ayuda a evitar overfitting y evaluar generalizaci√≥n.",
  "No es ideal si el interpretabilidad es cr√≠tica o el tiempo computacional es limitado."
)

tabla_rf <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rf %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir random forest",
             subtitle = "Random Forest") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Stacked Generlization (Blending) {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Blending.png"))
```

**Stacked Generalization**, com√∫nmente conocido como **Stacking**, y su variante **Blending**, son t√©cnicas avanzadas de **aprendizaje conjunto (ensemble learning)** que buscan combinar las predicciones de m√∫ltiples modelos de aprendizaje autom√°tico para obtener un rendimiento predictivo superior al de cualquier modelo individual. La idea fundamental es que, en lugar de simplemente promediar o votar las predicciones, se entrena un **modelo de segundo nivel (meta-modelo)** para aprender a combinar √≥ptimamente las predicciones de los modelos de primer nivel (modelos base).

El proceso de Stacking generalmente implica dos o m√°s "capas" de modelos:

1.  **Modelos Base (Nivel 0):** En la primera capa, se entrenan m√∫ltiples modelos de aprendizaje autom√°tico diversos (pueden ser de diferentes tipos, como √°rboles de decisi√≥n, m√°quinas de vectores de soporte, redes neuronales, etc.). Estos modelos base se entrenan sobre el conjunto de datos de entrenamiento original (o en particiones del mismo).

2.  **Generaci√≥n de Meta-Caracter√≠sticas:** Las predicciones generadas por estos modelos base sobre un conjunto de datos "fuera de muestra" (que no se us√≥ para entrenar los modelos base, t√≠picamente a trav√©s de validaci√≥n cruzada k-fold) se utilizan como **nuevas caracter√≠sticas** o "meta-caracter√≠sticas". Estas meta-caracter√≠sticas, junto con la variable objetivo original, forman un nuevo conjunto de datos de entrenamiento para el meta-modelo.

3.  **Meta-Modelo (Nivel 1):** En la segunda capa, se entrena un **meta-modelo** (a menudo un modelo m√°s simple, como regresi√≥n lineal, regresi√≥n log√≠stica o un √°rbol de decisi√≥n poco profundo) utilizando estas meta-caracter√≠sticas como entrada y la variable objetivo original como salida. El meta-modelo aprende la relaci√≥n entre las predicciones de los modelos base y la respuesta verdadera, y por lo tanto, c√≥mo "pesar" o "combinar" esas predicciones de la mejor manera.

**Blending** es una variaci√≥n m√°s sencilla de Stacking. La principal diferencia es c√≥mo se generan las meta-caracter√≠sticas para el meta-modelo. En Blending, se reserva una **subdivisi√≥n de validaci√≥n (holdout set)** del conjunto de entrenamiento original. Los modelos base se entrenan en la parte restante del conjunto de entrenamiento, y luego sus predicciones sobre este conjunto de validaci√≥n se utilizan directamente como meta-caracter√≠sticas para entrenar el meta-modelo. Esto simplifica el proceso de validaci√≥n cruzada, pero el meta-modelo se entrena con menos datos.

En el contexto del **aprendizaje global vs. local**, Stacking/Blending es una estrategia de **aprendizaje global** que explota el poder de m√∫ltiples **aproximaciones de funci√≥n local** (los modelos base) para construir un modelo final altamente sofisticado. Cada modelo base, dependiendo de su naturaleza, puede ser un sistema de aprendizaje local que descubre patrones en subregiones de datos. Sin embargo, el meta-modelo aprende una funci√≥n de combinaci√≥n global sobre las predicciones de estos modelos base. Si los datos no se distribuyen linealmente, Stacking/Blending aplica el concepto de regresi√≥n (o clasificaci√≥n) de una manera muy flexible. Al permitir que un modelo de segundo nivel aprenda a combinar las predicciones de diversos modelos, supera la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Es particularmente eficaz en competiciones de machine learning donde se busca el m√°ximo rendimiento, ya que aprovecha las fortalezas complementarias de diferentes algoritmos. Sin embargo, puede ser computacionalmente intensivo y m√°s dif√≠cil de interpretar que los modelos individuales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua (depende de los modelos base)",
  "‚úÖ Num√©ricas y/o categ√≥ricas (codificaci√≥n seg√∫n modelos base)",
  "‚úÖ Captura relaciones complejas v√≠a combinaci√≥n de modelos base",
  "‚ùå No exige supuestos de normalidad en residuos",
  "‚úÖ Deseable, pero no obligatorio (mejor si observaciones independientes)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers afectan modelos base individuales)",
  "‚ö†Ô∏è Puede verse afectado (depende de base learners y correlated features)",
  "‚ö†Ô∏è Baja (modelo meta dif√≠cil de interpretar directamente)",
  "‚ö†Ô∏è Lento en entrenamiento y predicci√≥n, seg√∫n n√∫mero de base learners",
  "‚úÖ Esencial (usar CV anidada para entrenar meta-modelo)",
  "‚ùå Si datos muy escasos o muy ruidosos, riesgo de sobreajuste"
)

detalles <- c(
  "Ensamble supervisado que combina varias predicciones (base learners) mediante un modelo meta.",
  "El meta-modelo acepta la salida de modelos base; puede predecir clases o valores continuos.",
  "Usa predictores originales para los base learners; algunos requieren dummies, otros no.",
  "Aprende patrones no lineales e interacciones complejas a trav√©s de m√∫ltiples capas.",
  "No impone distribuci√≥n normal: cada base learner tiene sus propios supuestos.",
  "Ideal si cada muestra es independiente; sensibles a dependencias en validaciones cruzadas.",
  "No requiere varianza constante, ya que se basa en agregaci√≥n de predicciones.",
  "Modelos base (p. ej. ARBOTS, SVM) pueden verse influenciados por valores extremos;",
  "Modelos base diversificados reducen colinealidad, pero meta-modelo puede verse afectado.",
  "Dif√≠cil atribuir importancia directa; se pueden usar t√©cnicas como SHAP para interpretaci√≥n.",
  "Entrenamiento de m√∫ltiples base learners y meta-modelo incrementa tiempo; predicci√≥n tambi√©n m√°s lenta.",
  "Usar validaci√≥n cruzada anidada: inner folds para entrenar base learners y stacking, outer folds para evaluar.",
  "No recomendable si hay muy pocos datos (stacking requiere dividir en folds) o si los base learners no aportan diversidad."
)

tabla_stacking <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_stacking %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir blending",
             subtitle = "Stacked Generlizaation (Blending)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```







<!--chapter:end:03-ensemble.Rmd-->


# üß† 4. Redes Neuronales {-}  

Placeholder


## Autoenconder  {-}  
## Back - Propagation  {-}  
## Convolutional Neural Network (CNN)  {-}   
## Hopfield Network  {-}    
## Generative Adversarial Networks (GANs) {-}  
## Long Short-Term Memory (LSTM) / Gated Recurrent Unit (GRU) {-}   
## Multilayer Perceptron (MP)  {-}     
## Perceptron  {-}     
## Radial Basis Function Network (RBFN)  {-}   
## Recurrent Neural Networks (RNNs) {-}  
## Restricted Boltzmann Machine (RBM) {-}   
## Transformers  {-}  

<!--chapter:end:04-neural-networks.Rmd-->


# üß© 5. Reducci√≥n de Dimensionalidad {-}  

Placeholder


## Flexible Discriminant Analysis (FDA)  {-}   
## Independent Component Analysis (ICA) {-}   
## Kernel PCA (KPCA) {-}   
## Linear Discriminant Analysis (LDA)  {-}    
## Locally Linear Embedding (LLE) {-}    
## Mixture Discriminant Analysis (MDA)  {-}   
## Multidimensional Scaling (MDS)  {-}    
## Quadratic Discriminant Analysis (QDA) {-}  
## Partial Least Squares Regression (PLSR)  {-}    
## Partial Least Squares Discriminant Analysis (PLSDA)  {-}   
## Principal Component Analysis (PCA)  {-}   
## Principal Component Regression (PCR)  {-}   
## Projection Pursuit (PP)  {-}     
## Sammon Mapping  {-}  
## Regularized Discriminant Analysis (RDA)  {-}   
## -	t-Distributed Stochastic Neighbor Embedding (t-SNE) {-}
## Uniform Manifold Approximation and Projection (UMAP) {-}   

<!--chapter:end:05-dimensionality_reduction.Rmd-->


# üß¨ 6. Modelos Bayesianos {-}  

Placeholder


## Averaged One - Dependence Estimators (AODE)  {-}    
## Bayesian Network (BN)  {-}    
## Bayesian Belief Network (BBN)  {-} 
## Bayesian Linear Regression {-}   
## Gaussian Naive Bayes (GNB) {-}   
## Hidden Markov Models (HMMs) {-}   
## Kalman Filter {-}   
## Multinomial Naive Bayes (MNB) {-}   
## Naive Bayes (NB) {-}   
## Particle Filter {-}   

<!--chapter:end:06-bayesian.Rmd-->


# üßÆ 7. Regularizaci√≥n {-}  

Placeholder


## Elastic Net  {-}   
## Ridge Regression  {-}  
## Least Absolute Shrinkage and Selection Operator (LASSO)  {-}  

<!--chapter:end:07-regularization.Rmd-->


# üîç 8. Modelos Basados en Instancias {-}  

Placeholder


## Case-Based Reasoning (CBR) {-}   
## k - Nearest Neighbour (kNN)  {-}    
## Kernel Regression / Nadaraya-Watson Estimator   
## Learning Vector Quantization (LVQ)  {-} 
## Locally Weighted Learning (LWL)  {-}   
## Self - Organizing Map (SOM)  {-}   
## Prototype-Based Learning (General Concept) {-}     

<!--chapter:end:08-instance_based.Rmd-->


# üìè 9. Clustering (Aprendizaje No Supervisado) {-}  

Placeholder


## Affinity Propagation {-}   
## Agglomerative Clustering {-}   
## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)  {-}    
## Expectation Maximization (EM) {-}  
## Gaussian Mixture Models (GMMs) {-}   
## Hierarchical Clustering (hclust) {-} 
## k-Means  {-}   
## k-Medians  {-}    
## Mean-Shift {-}   
## Ordering Points To Identify the Clustering Structure (OPTICS) {-}   
## Spectral Clustering {-}   

<!--chapter:end:09-clustering.Rmd-->


# üìê 10. Sistemas Basados en Reglas {-}  

Placeholder


## Associative Classification (e.g., CBA, CMAR, FP-Growth based methods) {-}     
## CN2 Algorithm {-}  
## Decision List/Decision Tree to Rules {-}  
## Decision Rules  {-}   
## L√≥gica Difusa (Fuzzy Logic) {-}
## Minsky's Perceptron {-}    
## One Rule (OneR)  {-}   
## Repeated Incremental Pruning to Produce Error Reduction (RIPPER)  {-}  
## Rule Fit  {-}    
## Zero Rule (ZeroR)  {-}    

<!--chapter:end:10-rule_based_systems.Rmd-->


# ü§ñ 4. Deep Learning {-}  

Placeholder


## Deep Boltzman Machine (DBM) {-}  
## Deep Belief Networks (DBNet) {-}   
## Reinforcement Learning (DL-based RL) {-}  
## Stacked Auto-Enconders {-}  
## Variational Autoencoders (VAEs) {-}

<!--chapter:end:11-deep-learning.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
`r if (knitr::is_html_output()) '
# References {-}
'`

Sagi, S. (2019). ML Algorithms: One SD (œÉ). The obvious questions to ask when‚Ä¶ | by Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6 

Kuhn, M. (2019). The caret Package. https://topepo.github.io/caret/index.html

<!--chapter:end:12-references.Rmd-->

