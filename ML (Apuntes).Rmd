---
title: "Machine Learning (Apuntes) "
author: "Diana Villasana Ocampo"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  Apuntes personales
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---
```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```

```{r, echo = FALSE, include = FALSE}
require(dplyr)
require(gt)
require(caret)
```

# Machine Learning {.unnumbered}

El machine learning ha experimentado un crecimiento significativo durante la √∫ltima d√©cada. Este desarrollo se atribuye a tres factores fundamentales: el **incremento sustancial en la disponibilidad de datos** (Big Data), la **evoluci√≥n de las capacidades computacionales** y el **perfeccionamiento de algoritmos avanzados**. En la actualidad, el machine learning constituye un elemento transformador en diversos sectores: desde aplicaciones m√©dicas para el diagn√≥stico de enfermedades, hasta la optimizaci√≥n de estrategias financieras. Su capacidad anal√≠tica y de procesamiento de datos lo posiciona como un recurso esencial para la planificaci√≥n estrat√©gica, la optimizaci√≥n de procesos y el desarrollo de soluciones personalizadas.

**¬øCu√°ndo Implementar Machine Learning?**

La implementaci√≥n del machine learning resulta particularmente efectiva en los siguientes contextos:

-   **Disponibilidad de datos a gran escala:** La eficacia del modelo se incrementa proporcionalmente con la cantidad de datos relevantes disponibles.\
-   **Presencia de relaciones complejas entre variables:** En situaciones donde la multiplicidad de variables dificulta la definici√≥n de reglas convencionales.\
-   **Necesidad de adaptaci√≥n din√°mica:** Los sistemas de machine learning permiten una optimizaci√≥n continua mediante la incorporaci√≥n de nueva informaci√≥n.\
-   **Requerimientos de automatizaci√≥n avanzada:** Facilita la ejecuci√≥n de tareas complejas, desde el an√°lisis visual hasta la generaci√≥n de pron√≥sticos predictivos.

```{r echo=FALSE, fig.align='center', out.width="80%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/ML Categories_Page_1.png"))
```

## üìå Cuadro {.unnumbered}

```{r, echo = FALSE}
algoritmos_ml <- data.frame(
                            Tipo = c(
                              "Regressi√≥n",
                              "√Årboles / Decision Tree",
                              "Ensambles",
                              "Deep Learning",
                              "Reducci√≥n de Dim.",
                              "Bayesianos",
                              "Regularizaci√≥n",
                              "Instance-Based",
                              "Clustering",
                              "Rule-Based"
                            ),
                            Problema_tipico = c(
                              "Valores num√©ricos",
                              "Clasificaci√≥n, regresi√≥n",
                              "Clasificaci√≥n, regresi√≥n",
                              "Im√°genes, texto, audio",
                              "Visualizaci√≥n, preprocesamiento",
                              "Clasificaci√≥n r√°pida",
                              "Evitar overfitting",
                              "Clasificaci√≥n",
                              "Agrupamiento no supervisado",
                              "Interpretabilidad"
                            ),
                            Ventajas = c(
                              "Simplicidad",
                              "Interpretabilidad",
                              "Precisi√≥n",
                              "Modelos complejos",
                              "Mejora eficiencia",
                              "Velocidad",
                              "Generalizaci√≥n",
                              "Simple, no requiere entrenamiento",
                              "Descubrir estructuras ocultas",
                              "L√≥gica clara"
                            ),
                            Cuando_usarlo = c(
                              "Relaciones lineales",
                              "Datos tabulares",
                              "Alto rendimiento, Kaggle",
                              "Datos grandes y no estructurados",
                              "Datos con muchas variables",
                              "Texto, spam detection",
                              "Modelos lineales con muchas variables",
                              "Pocos datos y relaciones claras",
                              "Segmentaci√≥n sin etiquetas",
                              "Reglas conocidas, decisiones explicables"
                            ),
                            stringsAsFactors = FALSE
                          )

require(gt)

algoritmos_ml %>% 
 gt() %>%
  tab_header(title = "Modelos y cuando usarlos") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     fmt_integer(columns = names(data)[4:22], 
                sep_mark = " ") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Cuando_") ~ px(300),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```

## Modelos de Machine Learning {.unnumbered}

Modelos disponibles en la paqueter√≠a `caret` en R.

**Enlace**: <https://topepo.github.io/caret/model-training-and-tuning.html>

```{r list_setup, include=FALSE}
library(DT)
library(caret)
mods <- getModelInfo()

num_param <- unlist(lapply(mods, function(x) paste(as.character(x$param$parameter), sep = "", collapse = ", ")))
num_param[num_param == "parameter"] <- "None"
num_param <- data.frame(model = names(num_param), num_param = num_param)

mod_type <- unlist(lapply(mods, function(x) paste(sort(x$type), sep = "", collapse = ", ")))
mod_type <- data.frame(model = names(mod_type), type = mod_type)

libs <- unlist(lapply(mods, function(x) paste(x$library, sep = "", collapse = ", ")))
libs <- data.frame(model = names(libs), libraries = libs)

mod_name <- unlist(lapply(mods, function(x) x$label))
mod_name <- data.frame(model = names(mod_name), name = mod_name)

model_info <- merge(mod_name, mod_type, all = TRUE)
model_info <- merge(model_info, libs, all = TRUE)
model_info <- merge(model_info, num_param, all = TRUE)
model_info <- model_info[, c('name', 'model', 'type', 'libraries', 'num_param')]
model_info <- model_info[order(model_info$name),]
```

::: {style="height:400px;overflow:auto;"}
```{r list_table, echo = FALSE}
model_info %>%
 gt() %>%
  tab_header(title = "Modelos de Machine Learning",
             subtitle = "Disponibles en la paqueter√≠a caret") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("name") ~ px(350),
                   starts_with("num_param") ~ px(350),
                   everything() ~ px(200)) %>%
         cols_label(name = md("**Model**"),
                    model = md("**Method Value**"),
                    type = md("**Type**"),
                    libraries = md("**Libraries**"),
                    num_param = md("**Tunning Parameters**")) %>%
         as_raw_html()
```
:::

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üîç 1. Regresi√≥n {-}   

**Ejemplos:** Regresi√≥n Lineal Simple, Regresi√≥n Ridge, Regresi√≥n Lasso.  
**Uso:** Ideal para **predecir valores num√©ricos continuos** (como precios o temperaturas) y cuando esperas **relaciones lineales** entre tus variables.   
**Ventajas:** Es un modelo **simple** de entender y altamente **interpretable**.  
**Limitaciones:** Su desempe√±o es bajo cuando las relaciones entre las variables son **no lineales** o muy complejas.  

---

## Ordinary Least Squares Regression (`OLSR`) {-}    

[Ordinary Least Squares Regression (`OLSR`) en R](https://dvillasanao.github.io/ML_Examples/Output/Regression/01_01.OLSR.html)  
[Ordinary Least Squares Regression (`OLSR`) en Python](https://dvillasanao.github.io/ML_Examples/R/Regression/01_01_OLSR_py.html)   

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/01_image_OLSR.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/OLSR.png"))
```


La **Regresi√≥n por M√≠nimos Cuadrados Ordinarios (OLS)** es una t√©cnica fundamental en estad√≠stica y Machine Learning para modelar la **relaci√≥n lineal** entre una **variable dependiente** (a predecir) y una o m√°s **variables independientes**. Su objetivo es encontrar la **l√≠nea (o hiperplano) que mejor se ajusta** a los datos, minimizando la **suma de los cuadrados de las diferencias** entre los valores reales y los predichos por el modelo. Es decir, busca los coeficientes que hacen que la distancia (al cuadrado) de los puntos a la l√≠nea sea m√≠nima.

Los coeficientes de OLS se pueden calcular directamente con una f√≥rmula matem√°tica, sin necesidad de procesos iterativos complejos, bajo ciertos supuestos como la linealidad de la relaci√≥n y la independencia de los errores.

En el contexto del **aprendizaje global vs. local**, OLS es un ejemplo claro de un modelo de **aprendizaje global**. OLS busca una **√∫nica ecuaci√≥n** o un conjunto de coeficientes que describan la relaci√≥n entre las variables para **todo el conjunto de datos**. La l√≠nea o hiperplano que encuentra es una soluci√≥n global que se aplica de manera uniforme en todo el espacio de caracter√≠sticas. Esto la hace muy interpretable y computacionalmente eficiente, pero limitada si la relaci√≥n real entre las variables no es estrictamente lineal en todo el dominio de los datos.


## Linear Regression {.unnumbered}   


La **Regresi√≥n Lineal** es uno de los algoritmos m√°s fundamentales y ampliamente utilizados en el campo del **Machine Learning y la estad√≠stica**. Es un modelo **supervisado** que busca establecer una **relaci√≥n lineal** entre una **variable de respuesta (o dependiente)** continua y una o m√°s **variables predictoras (o independientes)**.

**Concepto y Ecuaci√≥n:**

La idea central de la regresi√≥n lineal es encontrar la **l√≠nea (o hiperplano en m√∫ltiples dimensiones)** que mejor se ajusta a los datos, de modo que se pueda predecir el valor de la variable dependiente bas√°ndose en los valores de las variables predictoras.

* **Regresi√≥n Lineal Simple:** Implica una √∫nica variable predictora. La ecuaci√≥n de la l√≠nea es:
    $$Y = \beta_0 + \beta_1 X + \epsilon$$  
    
Donde:
    * $Y$ es la variable de respuesta.
    * $X$ es la variable predictora.
    * $\beta_0$ es el **intercepto** (el valor de $Y$ cuando $X$ es 0).
    * $\beta_1$ es el **coeficiente de la pendiente** (cu√°nto cambia $Y$ por cada unidad de cambio en $X$).
    * $\epsilon$ es el **t√©rmino de error** o residual (la parte de $Y$ que el modelo no puede explicar).

* **Regresi√≥n Lineal M√∫ltiple:** Implica dos o m√°s variables predictoras. La ecuaci√≥n se extiende a:   
    $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \epsilon$$  
    
Donde:
    * $X_1, X_2, ..., X_p$ son las $p$ variables predictoras.
    * $\beta_1, \beta_2, ..., \beta_p$ son los coeficientes de cada variable predictora.

**C√≥mo Funciona (M√≠nimos Cuadrados Ordinarios - OLS):**

El m√©todo m√°s com√∫n para estimar los coeficientes ($\beta$s) en la regresi√≥n lineal es el de **M√≠nimos Cuadrados Ordinarios (OLS)**. OLS funciona minimizando la **suma de los cuadrados de los residuos**. Un residuo es la diferencia entre el valor real de $Y$ y el valor predicho por el modelo ($\hat{Y}$).

$$\text{Minimizar: } \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} (Y_i - (\beta_0 + \beta_1 X_{i1} + ... + \beta_p X_{ip}))^2$$

Al minimizar esta suma de cuadrados, OLS encuentra los coeficientes que hacen que la l√≠nea de regresi√≥n est√© lo m√°s cerca posible de la mayor√≠a de los puntos de datos.

**Supuestos Clave:**  

La validez de los resultados de la regresi√≥n lineal tradicional se basa en varios supuestos:

* **Linealidad:** La relaci√≥n entre las variables $X$ y $Y$ es lineal.
* **Independencia:** Las observaciones son independientes entre s√≠.
* **Normalidad de los Residuos:** Los residuos se distribuyen normalmente.
* **Homocedasticidad:** La varianza de los residuos es constante a lo largo de todos los niveles de las variables predictoras.
* **No Multicolinealidad Perfecta:** Las variables predictoras no deben estar perfectamente correlacionadas entre s√≠.

**Uso y Limitaciones:**

La regresi√≥n lineal es popular por su **simplicidad, interpretabilidad** y por ser un buen punto de partida para muchos problemas de predicci√≥n. Sin embargo, su principal limitaci√≥n es que solo puede modelar **relaciones lineales**. Si la relaci√≥n subyacente entre las variables es no lineal, una regresi√≥n lineal puede no capturarla adecuadamente y dar resultados inexactos.

**Aprendizaje Global vs. Local:**

La Regresi√≥n Lineal es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** La Regresi√≥n Lineal aprende un **√∫nico conjunto de coeficientes** que define una **l√≠nea (o hiperplano) global** que se aplica a todo el espacio de caracter√≠sticas. Esta l√≠nea busca representar la **relaci√≥n lineal promedio o general** entre las variables predictoras y la variable de respuesta a lo largo de todo el rango de los datos. La predicci√≥n para cualquier nueva instancia se realiza utilizando la misma ecuaci√≥n lineal, sin importar en qu√© parte del espacio de caracter√≠sticas se encuentre. No hay adaptaciones o modelos separados para diferentes subregiones de los datos; el modelo es una funci√≥n que describe una tendencia general y global.

* **Rigidez de la Linealidad:** Debido a su naturaleza global y lineal, la regresi√≥n lineal no puede capturar relaciones **no lineales o interacciones complejas** entre las variables predictoras de forma inherente. Si la relaci√≥n real en los datos es no lineal, el modelo lineal intentar√° ajustarla con la mejor l√≠nea recta posible, lo que podr√≠a llevar a un bajo rendimiento.


## Regresi√≥n Log√≠stica (Logit) {.unnumbered}

```{r, echo = FALSE,out.width='50%', fig.align='center', fig.cap="Elaboraci√≥n propia"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Logit.png"))
```

La **Regresi√≥n Log√≠stica** es un modelo estad√≠stico usado principalmente para problemas de **clasificaci√≥n binaria**, donde el objetivo es predecir la **probabilidad** de que una instancia pertenezca a una de dos clases (por ejemplo, "s√≠" o "no", "0" o "1"). A pesar de su nombre, no predice un valor continuo, sino una probabilidad.

Este modelo utiliza una **funci√≥n sigmoide (o log√≠stica)** para transformar una combinaci√≥n lineal de las variables de entrada en un valor entre 0 y 1, que se interpreta como una probabilidad. Los coeficientes del modelo se aprenden maximizando la verosimilitud de observar los datos, generalmente a trav√©s de algoritmos como el descenso de gradiente.

En el contexto del **aprendizaje global vs. local**, la Regresi√≥n Log√≠stica es un modelo de **aprendizaje global**. Esto significa que busca un **√∫nico conjunto de coeficientes** que definen una frontera de decisi√≥n (un hiperplano) que se aplica a todo el espacio de caracter√≠sticas para separar las clases. Asume una relaci√≥n lineal entre las variables de entrada y el logaritmo de la probabilidad, y una vez entrenado, usa esta relaci√≥n global para hacer predicciones en cualquier parte del espacio de datos. Si bien es eficiente y muy interpretable, su naturaleza global puede limitar su rendimiento en casos donde las fronteras de decisi√≥n son inherentemente no lineales o muy complejas.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Lineal entre log-odds y predictores",
  "‚ùå No es requisito",
  "‚úÖ Necesaria",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠",
  "‚ö†Ô∏è Puede afectar",
  "‚úÖ Alta (coeficientes interpretables)",
  "‚úÖ Alta",
  "‚úÖ Compatible",
  "‚ùå Respuesta no binaria o multiclase sin ajuste"
)
detalles <- c(
  "Clasificaci√≥n binaria",
  "Ej. √©xito/fracaso, s√≠/no",
  "Convertir categ√≥ricas a dummies",
  "Relaci√≥n entre log(p/(1-p)) y X debe ser lineal",
  "No se exige normalidad en errores",
  "Independencia entre observaciones",
  "Idealmente varianza constante",
  "Outliers pueden alterar los coeficientes",
  "Usar VIF y regularizaci√≥n si hay problema",
  "Coeficientes en t√©rminos de odds/log-odds",
  "R√°pido y estable para datasets medianos",
  "K-fold funciona muy bien",
  "Evitar si hay multiclase sin ajuste"
)
tabla_logit <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

require(gt)
tabla_logit %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir logit",
             subtitle = "Regresi√≥n log√≠stica") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Generalized Linear Model (GLM) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/GLM.png"))
```

## Least Angle Regression (LARS)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/LARS.png"))
```

**Least Angle Regression (LARS)** es un algoritmo de **regresi√≥n lineal** desarrollado por Bradley Efron, Trevor Hastie, Iain Johnstone y Robert Tibshirani. Es particularmente interesante porque puede considerarse como una **versi√≥n m√°s eficiente y paso a paso de LASSO** (Least Absolute Shrinkage and Selection Operator) y es √∫til para **seleccionar caracter√≠sticas** y manejar datos de alta dimensi√≥n.

A diferencia de OLS, que calcula todos los coeficientes de una vez, o de Lasso, que requiere optimizaci√≥n m√°s compleja, LARS opera de manera incremental. Su idea central es avanzar los coeficientes de forma que su √°ngulo con el vector de residuos sea siempre el mismo y que sea el "m√°s peque√±o" posible.

El proceso de LARS se puede resumir as√≠:

1.  **Inicio:** Todos los coeficientes se inicializan en cero.
2.  **Identificaci√≥n del Predictor m√°s Correlacionado:** El algoritmo encuentra la variable predictora que est√° m√°s correlacionada con la variable de respuesta (o con el residuo actual).
3.  **Movimiento en la Direcci√≥n del Predictor:** El coeficiente de esa variable predictora se mueve gradualmente desde cero en la direcci√≥n del signo de su correlaci√≥n. A medida que el coeficiente se mueve, el residuo cambia.
4.  **Activaci√≥n de Nuevos Predictores:** Cuando otra variable predictora alcanza la misma correlaci√≥n con el residuo actual que la variable que ya est√° activa, el algoritmo cambia de direcci√≥n. Ahora, los coeficientes de *ambas* variables activas se mueven juntas en un "√°ngulo equiestad√≠stico" de tal manera que permanecen igualmente correlacionadas con el residuo.
5.  **Proceso Iterativo:** Este proceso contin√∫a, a√±adiendo nuevas variables al conjunto de variables "activas" (es decir, aquellas con coeficientes distintos de cero) a medida que estas alcanzan la misma correlaci√≥n con el residuo. Los coeficientes se mueven de forma coordinada.
6.  **Criterio de Parada:** El algoritmo se detiene cuando todos los predictores han sido incluidos en el modelo, o cuando se alcanza un n√∫mero predefinido de pasos o de variables.

**Relaci√≥n con otros modelos:**
* Si LARS se detiene cuando los coeficientes de las variables no activas son menores o iguales a la correlaci√≥n actual de las variables activas (y los coeficientes de las variables no activas se fijan en cero si su correlaci√≥n es menor), entonces genera la **soluci√≥n completa del camino de LASSO**.
* Tambi√©n puede generar el camino de soluciones para la **Ridge Regression** si se modifica ligeramente.

LARS es eficiente porque solo requiere un n√∫mero de pasos igual al n√∫mero de variables, o menos si se detiene antes.

**Aprendizaje Global vs. Local:**

Least Angle Regression (LARS) es un modelo de **aprendizaje global**.

* **Aspecto Global:** LARS construye un **modelo lineal global** paso a paso. Aunque el algoritmo a√±ade variables una por una y ajusta sus coeficientes de manera incremental, el modelo resultante en cada paso es una ecuaci√≥n de regresi√≥n lineal que se aplica a todo el conjunto de datos. La decisi√≥n de qu√© variable a√±adir y c√≥mo ajustar los coeficientes se basa en las correlaciones globales entre las variables predictoras y la respuesta (o el residuo). La finalidad es encontrar los coeficientes √≥ptimos para una funci√≥n de regresi√≥n que se aplica a todo el espacio de caracter√≠sticas.

* **Selecci√≥n de Caracter√≠sticas Globalmente:** La capacidad de LARS para realizar selecci√≥n de caracter√≠sticas (al igual que LASSO) es un proceso global. Se identifican las variables m√°s influyentes en el contexto de todo el conjunto de datos, y su inclusi√≥n en el modelo contribuye a la formaci√≥n de una relaci√≥n global entre los predictores y la respuesta. No se construyen modelos separados para diferentes subregiones de los datos; en cambio, se construye un √∫nico modelo global de manera progresiva.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal",
  "‚ö†Ô∏è Deseable para inferencia cl√°sica",
  "‚úÖ Necesaria",
  "‚úÖ Supuesto importante",
  "‚ö†Ô∏è Afectado por valores extremos",
  "‚úÖ Maneja bien multicolinealidad como LASSO",
  "‚úÖ Muy interpretable (secuencia de modelos anidados)",
  "‚úÖ Muy eficiente, especialmente con muchas variables",
  "‚úÖ √ötil para elegir n√∫mero de variables con validaci√≥n cruzada",
  "‚ùå Datos ruidosos o no lineales; ‚ùå si hay muchas interacciones no capturadas"
)

detalles <- c(
  "Algoritmo de regresi√≥n eficiente que selecciona variables secuencialmente como alternativa a LASSO.",
  "Busca predecir una variable continua usando m√∫ltiples predictores.",
  "Usa variables num√©ricas estandarizadas; es sensible a escala.",
  "Asume relaci√≥n lineal entre predictores y respuesta.",
  "No exige normalidad para predicci√≥n, pero s√≠ para inferencia estad√≠stica.",
  "Errores deben ser independientes entre s√≠.",
  "Supone varianza constante de los errores.",
  "Puede verse afectado si hay valores extremos en las variables.",
  "Muy √∫til cuando los predictores est√°n correlacionados; elige uno a la vez.",
  "Produce una ruta de modelos f√°cilmente interpretable con selecci√≥n progresiva.",
  "M√°s r√°pido que LASSO al generar trayectorias de coeficientes.",
  "Puede aplicarse validaci√≥n cruzada para seleccionar el mejor modelo en la secuencia.",
  "No captura relaciones no lineales o interacciones sin modificaci√≥n previa del modelo."
)

tabla_lars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lars %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LARS",
             subtitle = "Least Angle Regression (LARS)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Locally Estimated Scatterplot Smoothing (`LOESS`) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
knitr::include_graphics(paste0(here::here(), "/img/Regression/LOESS.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/LOESS_1.png"))
```

**LOESS (Locally Estimated Scatterplot Smoothing)**, o LOWESS, es una t√©cnica de **regresi√≥n no param√©trica** para crear una curva suave que se ajusta a los datos en un diagrama de dispersi√≥n. Su gran ventaja es que **no asume una forma funcional global** espec√≠fica para la relaci√≥n entre las variables, lo que la hace muy flexible para identificar tendencias y patrones no lineales.

El principio de LOESS es simple: para estimar el valor suavizado en un punto, se seleccionan los **puntos de datos cercanos** (definido por un par√°metro de **"span"** o ancho de banda), se les asignan **pesos** (dando m√°s peso a los puntos m√°s cercanos), y luego se ajusta un **polinomio de bajo grado** (com√∫nmente lineal o cuadr√°tico) a esos puntos usando m√≠nimos cuadrados ponderados. Este proceso se repite para cada punto de inter√©s para construir la curva.

En el contexto del **aprendizaje global vs. local**, LOESS es un modelo de **aprendizaje puramente local**. Su flexibilidad reside en que **ajusta m√∫ltiples modelos simples y locales** (regresiones ponderadas) en diferentes vecindarios de los datos. No busca una √∫nica ecuaci√≥n global que describa la relaci√≥n en todo el conjunto de datos. Esto le permite adaptarse maravillosamente a las variaciones en las relaciones y curvaturas de los datos, lo que es especialmente √∫til cuando los datos no se distribuyen linealmente. Sin embargo, su naturaleza local implica que no produce una f√≥rmula expl√≠cita del modelo, y puede ser computacionalmente m√°s intensivo para conjuntos de datos muy grandes.  


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua",
  "‚úÖ Num√©ricas (usualmente 1 o 2 predictores)",
  "‚úÖ No lineal y suave",
  "‚ùå No necesaria",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è Muy sensible",
  "‚ùå No aplica (pocos predictores)",
  "‚úÖ Muy interpretable gr√°ficamente",
  "‚ö†Ô∏è Lento en grandes vol√∫menes de datos",
  "‚úÖ Puede usarse para elegir 'span'",
  "‚ùå Datos grandes o con ruido fuerte"
)
detalles <- c(
  "Modelo no param√©trico local",
  "Regresi√≥n para valores continuos",
  "Generalmente 1 o 2 variables num√©ricas",
  "Ajuste por vecindad, suaviza la curva",
  "No asume distribuci√≥n espec√≠fica",
  "Supuesto deseable si hay dependencias temporales",
  "Ideal si la varianza no cambia mucho localmente",
  "Altamente afectado por outliers locales",
  "No es una t√©cnica multivariable compleja",
  "La curva ajustada se interpreta visualmente",
  "Computacionalmente costoso con datos grandes",
  "Ayuda a seleccionar el mejor 'span'",
  "Poco eficaz en alta dimensi√≥n o datos muy dispersos"
)
tabla_loess <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
tabla_loess %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LOESS",
             subtitle = "Locally Estimated Scatterplot Smoothing (LOESS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Multivariate Adaptive Regression Splines (`MARS`) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/MARS.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/MARS_1.png"))
```

**Multivariate Adaptive Regression Splines (MARS)** es un algoritmo de **regresi√≥n no param√©trica** que extiende los modelos lineales para manejar relaciones no lineales y complejas. Desarrollado por Jerome Friedman, MARS construye su modelo al **dividir el espacio de entrada en m√∫ltiples regiones y ajustar una funci√≥n lineal simple (o de orden superior) a cada regi√≥n**.

El proceso de MARS consta de dos fases: una **fase de adelante** que a√±ade iterativamente **funciones base** (pares de funciones "hinge" o bisagra) y **nudos** (puntos de corte) para capturar no linealidades e interacciones entre variables, y una **fase de atr√°s** que poda las funciones base menos significativas utilizando criterios como la **Validaci√≥n Cruzada Generalizada (GCV)** para prevenir el sobreajuste. Esto permite a MARS ser adaptable a las particularidades de los datos.

En el contexto del **aprendizaje global vs. local**, MARS se sit√∫a como un modelo de **aprendizaje adaptativo que combina aspectos globales y locales**. Es "local" en el sentido de que sus funciones base y nudos dividen el espacio de datos en regiones, y dentro de cada regi√≥n se aplica una relaci√≥n simple. Sin embargo, es "global" porque la suma de todas estas funciones base forma una √∫nica ecuaci√≥n que describe la relaci√≥n en todo el conjunto de datos y se aplica de forma consistente. Esto significa que si los datos no se distribuyen linealmente, MARS puede aprender y modelar estas relaciones complejas de forma adaptativa, encontrando autom√°ticamente d√≥nde y c√≥mo las relaciones cambian, ofreciendo una soluci√≥n que es tanto flexible como interpretable.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua o categ√≥rica (binaria con extensi√≥n)",
  "‚úÖ Num√©ricas (categ√≥ricas con dummies)",
  "‚úÖ No lineal (autom√°tico)",
  "‚ùå No requerida",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠ (aunque algo robusto)",
  "‚ö†Ô∏è Puede afectar",
  "‚ö†Ô∏è Media (modelo tipo caja negra)",
  "‚úÖ Razonable para tama√±os medianos",
  "‚úÖ Recomendado (ej. repeated k-fold)",
  "‚ùå Relaci√≥n puramente lineal o muchos factores irrelevantes"
)
detalles <- c(
  "Regresi√≥n flexible no lineal",
  "Ideal para regresi√≥n continua (tambi√©n clasificaci√≥n con `earth`)",
  "Crea autom√°ticamente 'splines' por variable",
  "Crea funciones por tramos con 'nudos'",
  "No exige distribuci√≥n espec√≠fica de errores",
  "Mejor si los datos no est√°n correlacionados temporalmente",
  "Idealmente errores con varianza constante",
  "Puede ser sensible a valores extremos",
  "Detecta interacciones, pero VIF sigue siendo √∫til",
  "Coeficientes no tan interpretables como OLS",
  "M√°s lento que OLS pero m√°s flexible",
  "CV ayuda a elegir n√∫mero √≥ptimo de t√©rminos",
  "Tiene riesgo de sobreajuste si no se controla bien"
)
tabla_mars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_mars %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MARS",
             subtitle = "Splines de Regresi√≥n Adaptativa Multivariante (MARS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Polynomial Regression {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Polynomial Regression.png"))
```

## Quantile Regression {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Quantile Regression.png"))
```

## Stepwise Regression {.unnumbered}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/StepW.png"))
```

La **Regresi√≥n por Pasos (Stepwise Regression)** es una t√©cnica para construir un modelo de regresi√≥n lineal (o a veces otros modelos lineales generalizados) seleccionando las variables predictoras de forma iterativa y autom√°tica. Su objetivo es encontrar un subconjunto √≥ptimo de variables que mejore la capacidad predictiva del modelo sin incluir variables irrelevantes o redundantes. Esto ayuda a simplificar el modelo, mejorar la interpretabilidad y reducir el riesgo de sobreajuste.

Existen tres estrategias principales para la regresi√≥n por pasos:

1.  **Selecci√≥n Hacia Adelante (Forward Selection):**
    * Comienza con un modelo que no incluye ninguna variable predictora (solo el intercepto).
    * En cada paso, eval√∫a todas las variables predictoras disponibles que a√∫n no est√°n en el modelo.
    * A√±ade al modelo la variable que, al ser incluida, produce la mayor mejora estad√≠stica (generalmente medida por un valor p bajo, un R-cuadrado ajustado mayor, o un criterio de informaci√≥n como AIC o BIC).
    * El proceso contin√∫a hasta que ninguna de las variables restantes mejora el modelo por encima de un umbral predefinido.

2.  **Eliminaci√≥n Hacia Atr√°s (Backward Elimination):**
    * Comienza con un modelo que incluye **todas** las variables predictoras posibles.
    * En cada paso, eval√∫a las variables predictoras que actualmente est√°n en el modelo.
    * Elimina del modelo la variable que es menos significativa estad√≠sticamente (generalmente medida por un valor p alto, o una reducci√≥n en el R-cuadrado ajustado o un aumento en AIC/BIC).
    * El proceso contin√∫a hasta que la eliminaci√≥n de cualquier variable empeorar√≠a significativamente el modelo.

3.  **H√≠brida (Mixed / Bidirectional Stepwise):**
    * Combina la selecci√≥n hacia adelante y la eliminaci√≥n hacia atr√°s.
    * En cada paso, el algoritmo puede tanto a√±adir una variable si mejora el modelo, como eliminar una variable que ya est√° en el modelo si se vuelve redundante o no significativa. Esto permite que el modelo reconsidere variables que fueron a√±adidas o eliminadas en pasos anteriores. Es el enfoque m√°s com√∫n y robusto.

**Criterios de Selecci√≥n:**  

La decisi√≥n de a√±adir o eliminar una variable en cada paso se basa en criterios estad√≠sticos, siendo los m√°s comunes:
* **Valores p:** Umbrales para la significancia estad√≠stica de los coeficientes.
* **$R^2$ ajustado:** Mide la proporci√≥n de varianza explicada por el modelo, penalizando la inclusi√≥n de variables innecesarias.
* **Criterio de Informaci√≥n de Akaike (AIC):** Penaliza la complejidad del modelo (n√∫mero de par√°metros) en relaci√≥n con su bondad de ajuste.
* **Criterio de Informaci√≥n Bayesiano (BIC):** Similar al AIC, pero con una penalizaci√≥n m√°s fuerte por la complejidad.

**Ventajas y Desventajas:**

* **Ventajas:** Puede ayudar a construir modelos m√°s parsimoniosos, mejorar la interpretabilidad y reducir la multicolinealidad.
* **Desventajas:**
    * **Sobreajuste:** Puede llevar a sobreajuste si se usa de forma acr√≠tica, ya que el algoritmo se optimiza para los datos de entrenamiento.
    * **Problemas de Significancia Estad√≠stica:** Los valores p y otras m√©tricas pueden no ser confiables debido a la selecci√≥n de caracter√≠sticas basada en los datos.
    * **Inestabilidad:** El conjunto de variables seleccionadas puede ser muy sensible a peque√±as perturbaciones en los datos o a la elecci√≥n del criterio de selecci√≥n.
    * **Ignora el Conocimiento del Dominio:** Puede seleccionar variables que son estad√≠sticamente significativas pero que carecen de sentido pr√°ctico o causal.
    * **No Maneja Interacciones Complejas:** Es fundamentalmente un m√©todo para seleccionar variables para un modelo lineal y no est√° dise√±ado para descubrir relaciones no lineales o interacciones complejas.

Debido a sus desventajas, la regresi√≥n por pasos se utiliza con m√°s cautela hoy en d√≠a. A menudo se prefieren m√©todos de regularizaci√≥n (como Lasso o Elastic Net) para la selecci√≥n de caracter√≠sticas, ya que son m√°s estables y realizan la selecci√≥n de forma m√°s robusta.

**Aprendizaje Global vs. Local:**

La Regresi√≥n por Pasos es un modelo de **aprendizaje global**.

* **Aspecto Global:** La regresi√≥n por pasos construye un **√∫nico modelo de regresi√≥n lineal global** que busca explicar la relaci√≥n entre las variables predictoras y la respuesta en todo el conjunto de datos. La selecci√≥n de variables se realiza para optimizar el rendimiento de este modelo global. Los coeficientes finales que se obtienen definen una funci√≥n lineal que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar en qu√© parte del espacio de caracter√≠sticas se encuentre.

* **Proceso de Selecci√≥n (Global):** Aunque el proceso es iterativo y a√±ade/elimina variables, la decisi√≥n en cada paso se basa en c√≥mo esa adici√≥n/eliminaci√≥n afecta la bondad de ajuste o la complejidad del modelo en **todo el conjunto de datos**. No se ajustan modelos separados o locales para diferentes regiones.



## Support Vector Machine (SVM) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/SVM.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/SVM_1.png"))
```


**Support Vector Machine (SVM)** es un potente y vers√°til algoritmo de **Machine Learning** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**, aunque es m√°s conocido por su aplicaci√≥n en clasificaci√≥n. Su objetivo principal es encontrar el **hiperplano √≥ptimo** que separe las clases en el espacio de caracter√≠sticas con el **margen** m√°s grande posible. Los puntos de datos m√°s cercanos a este hiperplano se llaman **vectores de soporte**, y son cruciales para definir la frontera de decisi√≥n.

Para manejar datos que no son linealmente separables, SVM utiliza el **"truco del kernel"**. Este truco permite a SVM mapear impl√≠citamente los datos a un espacio de mayor dimensi√≥n donde las clases podr√≠an ser linealmente separables, sin necesidad de calcular expl√≠citamente las coordenadas. Funciones kernel comunes como el **Radial Basis Function (RBF) o Gaussiano** permiten a SVM modelar fronteras de decisi√≥n no lineales complejas en el espacio original de baja dimensi√≥n.

En el contexto del **aprendizaje global vs. local**, SVM se clasifica principalmente como un modelo de **aprendizaje global**. Esto se debe a que busca un **√∫nico hiperplano √≥ptimo** (o una frontera de decisi√≥n no lineal definida por el kernel) que se aplica a la totalidad del espacio de caracter√≠sticas. Una vez entrenado, el modelo predice evaluando la posici√≥n de un nuevo punto con respecto a esta frontera global. Sin embargo, hay un matiz "local" en su funcionamiento: la determinaci√≥n de este hiperplano depende **cr√≠ticamente solo de los vectores de soporte**, que son los puntos de datos "m√°s dif√≠ciles" cercanos a la frontera. Los puntos que est√°n lejos del margen no influyen en la definici√≥n del modelo. Aunque la frontera de decisi√≥n es una funci√≥n global que se aplica en todas partes, su construcci√≥n est√° influenciada por estos puntos localmente relevantes, permitiendo a SVM adaptar su aproximaci√≥n incluso cuando las relaciones en los datos no se distribuyen linealmente, al encontrar la separaci√≥n √≥ptima en un espacio transformado.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas (categor√≠as deben codificarse)",
  "‚úÖ Capta relaciones no lineales (kernel)",
  "‚ùå No requiere",
  "‚úÖ Idealmente s√≠",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠, especialmente sin margen amplio",
  "‚úÖ Puede manejarla bien",
  "‚ùå Baja (modelo es una caja negra)",
  "‚ö†Ô∏è Lento con muchos datos o predictores",
  "‚úÖ Esencial para elegir kernel y par√°metros",
  "‚ùå Datos con mucho ruido o solapamiento entre clases"
)
detalles <- c(
  "Modelo supervisado que maximiza el margen entre clases",
  "Clasificaci√≥n binaria, multiclase o regresi√≥n (SVR)",
  "Requiere escalar o estandarizar las variables num√©ricas",
  "Puede usar kernel para resolver problemas no lineales",
  "No requiere supuestos cl√°sicos como normalidad",
  "Mejor si los datos son independientes",
  "Puede usarse aunque haya heterocedasticidad",
  "Los outliers cercanos al margen afectan el modelo",
  "Los kernels pueden reducir el efecto de multicolinealidad",
  "Dif√≠cil de explicar; es un modelo de tipo caja negra",
  "Puede ser costoso computacionalmente con datos grandes",
  "Par√°metros como C y gamma se ajustan v√≠a validaci√≥n cruzada",
  "No es ideal si hay ruido o datos mal etiquetados"
)
tabla_svm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_svm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir SVM",
             subtitle = "Support Vector Machine (SVM) ") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

<!--chapter:end:01-regression.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üå≤ 2. √Årboles de Decisi√≥n y Derivados {-}   

**Ejemplos:** √Årbol de Decisi√≥n, Random Forest, Gradient Boosting.   
**Uso:** Excelentes para **datos tabulares** con relaciones no lineales, incluyendo variables categ√≥ricas y num√©ricas. Son una buena opci√≥n cuando la **interpretabilidad** es clave.   
**Ventajas:** Pueden manejar diversos tipos de datos y, los √°rboles individuales, son **f√°ciles de interpretar**.   
**Limitaciones:** Los √°rboles simples pueden **sobreajustarse**, y su rendimiento baja con datos muy ruidosos si no se usan m√©todos de ensamble.   

---


## C4.5  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/C4.5.png"))
```

**C4.5** es una extensi√≥n del algoritmo **ID3**, tambi√©n desarrollado por Ross Quinlan, y es uno de los algoritmos de **√°rboles de decisi√≥n** m√°s influyentes y ampliamente utilizados para tareas de **clasificaci√≥n**. Fue dise√±ado para abordar algunas de las limitaciones de su predecesor, ID3, y se ha convertido en un est√°ndar de facto en el aprendizaje autom√°tico para construir modelos predictivos interpretables.

Al igual que ID3, C4.5 construye un √°rbol de clasificaci√≥n seleccionando en cada nodo el atributo que mejor divide el conjunto de datos. Sin embargo, en lugar de usar solo la **ganancia de informaci√≥n**, C4.5 utiliza la **relaci√≥n de ganancia** (Gain Ratio). La relaci√≥n de ganancia normaliza la ganancia de informaci√≥n por la entrop√≠a intr√≠nseca del atributo, lo que ayuda a mitigar el sesgo de ID3 hacia atributos con muchos valores. Adem√°s, C4.5 introduce varias mejoras significativas:

* **Manejo de atributos continuos:** Puede discretizar atributos num√©ricos continuos dividiendo el rango en intervalos.
* **Manejo de valores faltantes:** Puede manejar datos con valores ausentes asignando una probabilidad fraccionada a cada rama posible.
* **Poda del √°rbol:** Implementa una t√©cnica de poda para reducir el sobreajuste, lo que implica eliminar ramas del √°rbol que no aportan significativamente a la clasificaci√≥n o que representan ruido en los datos.

En el contexto del **aprendizaje global vs. local**, C4.5, al igual que ID3 y CART, opera como un sistema de **aprendizaje local**. La construcci√≥n del √°rbol se logra a trav√©s de decisiones de divisi√≥n que se optimizan localmente en cada nodo, buscando la m√°xima homogeneidad o pureza en los subconjuntos resultantes. Esto le permite a C4.5 manejar eficazmente relaciones no lineales entre las variables independientes y dependientes. La idea es que, si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se puede aplicar de forma efectiva mediante esta **regresi√≥n ponderada localmente**, donde el algoritmo divide el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Al centrarse en divisiones √≥ptimas a nivel de subconjuntos, C4.5 ofrece una alternativa robusta a los m√©todos de aproximaci√≥n de funciones globales, que a veces pueden fallar en proporcionar una buena aproximaci√≥n cuando la relaci√≥n entre las variables no es lineal.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Categ√≥ricas y num√©ricas",
  "‚úÖ Captura relaciones no lineales",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es relevante",
  "‚ö†Ô∏è Moderadamente (puede hacer overfitting con ruido)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol interpretable)",
  "‚úÖ Relativamente r√°pido",
  "‚úÖ Recomendable para evitar sobreajuste",
  "‚ùå Demasiadas categor√≠as o ruido en datos"
)

detalles <- c(
  "Modelo supervisado tipo √°rbol de decisi√≥n",
  "Clasifica variables categ√≥ricas en ramas l√≥gicas",
  "Divide por puntos de corte para variables num√©ricas",
  "No asume forma funcional entre predictores y respuesta",
  "No necesita normalidad de errores",
  "Mejor si las observaciones son independientes",
  "No requiere varianzas constantes",
  "Datos ruidosos pueden afectar las ramas",
  "No se ve afectado por correlaciones entre predictores",
  "Salida f√°cil de visualizar y explicar",
  "Escala bien para tama√±os de muestra medianos",
  "Evita sobreajuste con poda y validaci√≥n",
  "Muchas clases con pocos datos pueden sobreajustar"
)

tabla_c45 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_c45 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir C4.5") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## C5.0  {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/C5.0.png"))
```

**C5.0** es la versi√≥n m√°s reciente y avanzada de los algoritmos de √°rboles de decisi√≥n desarrollados por Ross Quinlan, sucediendo a ID3 y C4.5. Es un algoritmo propietario (aunque se ofrece una versi√≥n de c√≥digo abierto bajo ciertas licencias) y es ampliamente reconocido por su **rapidez**, **precisi√≥n** y **eficiencia** en la construcci√≥n de **√°rboles de decisi√≥n** y **reglas de clasificaci√≥n** para tareas de **clasificaci√≥n**.

Al igual que sus predecesores, C5.0 construye un √°rbol de clasificaci√≥n mediante la divisi√≥n recursiva de los datos en subconjuntos m√°s homog√©neos. Sin embargo, C5.0 incorpora mejoras significativas que lo hacen superior en muchos aspectos:

* **Velocidad y eficiencia:** Es notablemente m√°s r√°pido y m√°s eficiente en el uso de memoria que C4.5, lo que le permite manejar conjuntos de datos mucho m√°s grandes.
* **Impulso (Boosting):** C5.0 puede usar la t√©cnica de **boosting** (espec√≠ficamente, una variante de AdaBoost) para crear m√∫ltiples √°rboles de decisi√≥n y combinarlos para producir una predicci√≥n m√°s robusta y precisa. Esto reduce significativamente los errores de clasificaci√≥n y mejora la generalizaci√≥n.
* **Poda mejorada:** Ofrece t√©cnicas de poda m√°s sofisticadas para evitar el sobreajuste y producir √°rboles m√°s peque√±os y comprensibles.
* **Manejo de valores faltantes y atributos continuos:** Al igual que C4.5, maneja de manera efectiva valores faltantes y atributos num√©ricos continuos.
* **Generaci√≥n de reglas:** Adem√°s de √°rboles de decisi√≥n, C5.0 puede generar conjuntos de **reglas de clasificaci√≥n** concisas, que a menudo son m√°s f√°ciles de interpretar que un √°rbol completo.

En el contexto de la **regresi√≥n localmente ponderada**, C5.0, como los dem√°s algoritmos de √°rboles de decisi√≥n, opera bajo la premisa de un **aprendizaje local**. La construcci√≥n del √°rbol implica tomar decisiones de divisi√≥n √≥ptimas en cada nodo, bas√°ndose en la informaci√≥n local de ese subconjunto de datos. Si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n, que es su enfoque principal) se puede aplicar eficazmente al dividir el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Cada divisi√≥n en el √°rbol se puede ver como una forma de **regresi√≥n ponderada localmente**, donde el algoritmo se enfoca en aproximar la relaci√≥n dentro de un subespacio espec√≠fico del conjunto de datos. Esto convierte a C5.0 en una potente alternativa a los m√©todos de aproximaci√≥n de funciones globales, especialmente cuando la relaci√≥n entre las variables independientes y dependientes no es lineal y se busca un modelo interpretable y robusto.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Categ√≥ricas y num√©ricas",
  "‚úÖ Captura relaciones no lineales",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es relevante",
  "‚ö†Ô∏è Moderadamente (puede generar ramas excesivas)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol f√°cil de visualizar)",
  "‚úÖ Relativamente r√°pido en training",
  "‚úÖ Recomendable para evitar sobreajuste",
  "‚ùå Clases muy desbalanceadas sin ajuste"
)

detalles <- c(
  "Algoritmo de √°rbol de decisi√≥n avanzado basado en C4.5",
  "Clasifica en m√∫ltiples categor√≠as (tambi√©n multiclase)",
  "Divide autom√°ticamente variables num√©ricas con puntos de corte",
  "No asume funci√≥n lineal: usa ganancia de informaci√≥n y boosting",
  "No exige normalidad de errores",
  "Mejor si las instancias son independientes",
  "No requiere varianzas constantes",
  "Los valores extremos pueden influir en ramas profundas",
  "No se ve afectado por correlaci√≥n alta entre predictores",
  "Salida clara con reglas y pesos de boosting",
  "M√°s r√°pido que C4.5 y con opciones de boosting",
  "Usar k-fold o repeated CV para determinar par√°metros √≥ptimos",
  "Muchos atributos irrelevantes pueden generar sobreajuste"
)

tabla_c50 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_c50 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir C5.0") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

``` 


## Classification and Regression Tree (CART)  {-} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CART.png"))
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CART_1.png"))
```

**Classification and Regression Tree (CART)** es un m√©todo no param√©trico que se utiliza para construir **√°rboles de decisi√≥n** tanto para problemas de **clasificaci√≥n** como de **regresi√≥n**. La idea central es dividir recursivamente el espacio de las caracter√≠sticas en regiones m√°s peque√±as y manejables, creando as√≠ un modelo con forma de √°rbol que es f√°cil de interpretar.

A diferencia de los modelos lineales o algunos algoritmos de aprendizaje global, CART no asume una relaci√≥n lineal entre las variables. En su lugar, el algoritmo identifica los mejores **puntos de divisi√≥n** en las variables predictoras para maximizar la **homogeneidad** de las respuestas dentro de cada regi√≥n resultante. Para problemas de clasificaci√≥n, esto se mide com√∫nmente con m√©tricas como la **impureza Gini** o la **ganancia de informaci√≥n**, mientras que para la regresi√≥n, se busca minimizar la **suma de los cuadrados de los residuos**.

Mientras que muchos algoritmos (como las redes neuronales cl√°sicas o las m√°quinas de vectores de soporte) son sistemas de **aprendizaje global** que buscan minimizar una funci√≥n de p√©rdida √∫nica para todo el conjunto de datos, CART se puede considerar m√°s como un sistema de **aprendizaje local**. Construye el modelo tomando decisiones de divisi√≥n locales en cada nodo del √°rbol, lo que le permite capturar relaciones complejas y no lineales en los datos. Esto es particularmente √∫til cuando una aproximaci√≥n de funci√≥n global √∫nica podr√≠a no ser suficiente para modelar la relaci√≥n entre las variables. Una de las ventajas de CART es su capacidad para manejar diferentes tipos de datos (num√©ricos y categ√≥ricos) y su interpretabilidad, ya que la ruta desde la ra√≠z hasta una hoja del √°rbol representa un conjunto de reglas de decisi√≥n.



```{r, echo =FALSE}

criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y Categ√≥ricas",
  "‚úÖ No lineal y con interacciones",
  "‚ùå No requiere",
  "‚ö†Ô∏è Puede verse afectado",
  "‚ö†Ô∏è No necesario pero deseable",
  "‚ö†Ô∏è S√≠, en puntos de corte",
  "‚úÖ No se ve afectado",
  "‚úÖ Alta (gr√°fico del √°rbol)",
  "‚úÖ R√°pido en datasets medianos",
  "‚úÖ Muy usado para poda y ajuste",
  "‚ùå Muy profundo (overfitting), datos muy ruidosos"
)

detalles <- c(
  "Algoritmo basado en divisiones binarias",
  "Puede predecir clases o valores continuos",
  "Acepta todo tipo de variables predictoras",
  "Captura relaciones complejas y no lineales",
  "No requiere distribuci√≥n normal",
  "Idealmente los errores deben ser independientes",
  "La varianza constante mejora resultados",
  "Puede generar divisiones extremas por valores at√≠picos",
  "No necesita preocuparse por colinealidad",
  "F√°cil de entender, especialmente con √°rboles peque√±os",
  "Escalable pero no √≥ptimo en grandes vol√∫menes sin poda",
  "Usa poda y validaci√≥n cruzada para evitar sobreajuste",
  "Tiende al sobreajuste si no se poda o se regulariza"
)

tabla_cart <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

require(gt) 

tabla_cart %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CART",
             subtitle = "Classification and Regression Tree (CART)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Chi-squared Automatic Interaction Detection (CHAID)  {-}    

```{r echo=FALSE, fig.show="hold", out.width="48%", fig.cap="https://select-statistics.co.uk/blog/chaid-chi-square-automatic-interaction-detector/"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CHAID.png"))
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/CHAID_1.png"))
```



**Chi-squared Automatic Interaction Detection (CHAID)** es un algoritmo de **√°rboles de decisi√≥n** utilizado principalmente para tareas de **clasificaci√≥n** y, en menor medida, para la **regresi√≥n** (aunque se aplica m√°s com√∫nmente a variables dependientes categ√≥ricas). La idea fundamental de CHAID es construir un √°rbol de decisi√≥n al encontrar las mejores divisiones en las variables predictoras que maximicen la significancia estad√≠stica de la relaci√≥n con la variable dependiente.

A diferencia de ID3, C4.5 o CART, que utilizan medidas de impureza como la entrop√≠a o el √≠ndice Gini, CHAID se basa en pruebas estad√≠sticas de **chi-cuadrado ($\chi^2$)** para identificar las divisiones √≥ptimas. Cuando la variable dependiente es nominal o ordinal, CHAID eval√∫a cada variable predictora para encontrar la combinaci√≥n de categor√≠as que sea m√°s significativamente diferente de otras combinaciones en t√©rminos de la distribuci√≥n de la variable dependiente. El algoritmo fusiona las categor√≠as de una variable predictora si no son significativamente diferentes, y luego selecciona la variable predictora y la divisi√≥n que resultan en el valor m√°s bajo de $p$ (es decir, la mayor significancia estad√≠stica) de la prueba $\chi^2$. Para variables dependientes continuas, se utiliza una prueba F.

En el contexto del **aprendizaje global vs. local**, CHAID opera como un sistema de **aprendizaje local**. La construcci√≥n del √°rbol es un proceso iterativo y recursivo donde las decisiones de divisi√≥n se toman en cada nodo bas√°ndose en la significancia estad√≠stica local de la interacci√≥n entre las variables predictoras y la variable dependiente. Esto le permite a CHAID descubrir relaciones complejas y no lineales en los datos. La idea es que, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n (o clasificaci√≥n) de manera efectiva mediante lo que se denomina **regresi√≥n ponderada localmente**. Esto se logra al dividir el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples, donde cada rama del √°rbol representa una regi√≥n del espacio de caracter√≠sticas donde las interacciones son evaluadas y modeladas localmente. Esto hace de CHAID una alternativa robusta a los m√©todos de aproximaci√≥n de funciones globales, especialmente cuando se busca un modelo interpretable y se quieren identificar las interacciones entre las variables de una manera estad√≠sticamente rigurosa.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n multinivel)",
  "‚úÖ Categ√≥ricas nativas (num√©ricas requieren binarizaci√≥n o discretizaci√≥n)",
  "‚úÖ No lineal (explora interacciones autom√°ticas con œá¬≤)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente (outliers categ√≥ricos pueden crear nodos muy peque√±os)",
  "‚úÖ Robusto a multicolinealidad (usa œá¬≤, no varianzas)",
  "‚úÖ Media (√°rboles con muchos nodos pueden resultar complejos)",
  "‚ö†Ô∏è Razonablemente r√°pido en datasets moderados, lento si hay muy altas cardinalidades",
  "‚úÖ Recomendable para determinar profundidad y grado de interacci√≥n",
  "‚ùå Variable objetivo continua o muchos niveles con pocas observaciones"
)

detalles <- c(
  "Construye un √°rbol de decisi√≥n usando pruebas œá¬≤ para detectar interacciones entre predictores y variable objetivo.",
  "Dise√±ado para clasificar en m√∫ltiples categor√≠as sin orden; puede manejar targets con m√°s de dos niveles.",
  "Funciona mejor con predictores categ√≥ricos; las variables num√©ricas deben transformarse en categor√≠as mediante binning.",
  "No asume ninguna forma funcional; detecta autom√°ticamente relaciones complejas basadas en œá¬≤.",
  "No depende de supuestos de normalidad de errores ni de forma de distribuci√≥n de residuos.",
  "Las instancias deben ser independientes; no es ideal para datos con fuerte dependencia temporal sin procesar.",
  "Homoscedasticidad no se eval√∫a, ya que no se basa en un t√©rmino de error param√©trico como OLS.",
  "Los valores extremos en variables categ√≥ricas con pocas observaciones pueden crear ramas muy espec√≠ficas, pero CHAID maneja cardinalidades moderadas.",
  "Al basarse en œá¬≤, CHAID no se ve afectado directamente por colinealidad, aunque variables muy correlacionadas pueden crear redundancia en las divisiones.",
  "Cada divisi√≥n se basa en pruebas de œá¬≤; el √°rbol resultante puede interpretarse visualmente, pero muchos niveles pueden reducir claridad.",
  "La creaci√≥n recursiva de nodos por agrupaci√≥n de categor√≠as es eficiente para conjuntos de datos moderados; puede volverse lento si hay muchas categor√≠as de predictores.",
  "Se usa validaci√≥n cruzada para podar el √°rbol y elegir el nivel √≥ptimo de interacci√≥n, equilibrando sesgo y varianza.",
  "No es adecuado si la variable objetivo es continua (sin discretizar) o si hay demasiados niveles con muy pocos casos en cada uno."
)

tabla_chaid <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_chaid %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CHAID",
             subtitle = "Chi-squared Automatic Interaction Detection (CHAID)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Conditional Decision Trees (Conditional Inference Trees - CITs)  {-}   

**Conditional Decision Trees**, often referred to as **Conditional Inference Trees (CITs)**, represent a class of **√°rboles de decisi√≥n** que abordan una limitaci√≥n importante de los algoritmos de √°rboles de decisi√≥n tradicionales como CART, ID3, y C4.5: el **sesgo en la selecci√≥n de variables**. Mientras que los algoritmos tradicionales pueden favorecer variables predictoras con muchas categor√≠as o valores continuos (debido a que estas variables tienen m√°s "oportunidades" de generar una divisi√≥n que parezca √≥ptima), los CITs emplean un enfoque basado en **pruebas estad√≠sticas** para seleccionar la mejor divisi√≥n.

La idea fundamental de los Conditional Decision Trees es que cada divisi√≥n en el √°rbol se basa en la **significancia estad√≠stica** de la asociaci√≥n entre las variables predictoras y la variable de respuesta. En lugar de seleccionar el atributo que maximiza una medida de impureza (como la ganancia de informaci√≥n o la impureza Gini), los CITs realizan una serie de **pruebas de inferencia condicional** (t√≠picamente **pruebas de permutaci√≥n**).

El algoritmo opera de la siguiente manera:
1.  En cada nodo, se eval√∫a una **hip√≥tesis nula** de independencia entre cada variable predictora y la variable de respuesta.
2.  Se calcula el valor de $p$ para cada variable predictora.
3.  La variable predictora con el valor de $p$ m√°s peque√±o (es decir, la asociaci√≥n m√°s estad√≠sticamente significativa) es seleccionada para la divisi√≥n, siempre y cuando este valor de $p$ sea menor que un umbral de significancia predefinido.
4.  Una vez seleccionada la variable, se encuentra la mejor divisi√≥n binaria (generalmente) dentro de esa variable para ese nodo.
5.  Este proceso se repite recursivamente hasta que no haya m√°s variables significativas para dividir o se alcance un criterio de parada.

En el contexto del **aprendizaje global vs. local**, los Conditional Decision Trees se pueden considerar como un enfoque de **aprendizaje local** con un fuerte respaldo estad√≠stico. Aunque el √°rbol resultante es un modelo global, cada decisi√≥n de divisi√≥n se toma localmente bas√°ndose en la inferencia estad√≠stica sobre la relaci√≥n entre las variables en ese subconjunto de datos. Esto significa que si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se aplica de forma efectiva mediante lo que se denomina **regresi√≥n ponderada localmente**. Al utilizar pruebas de significancia para las divisiones, los CITs evitan el problema de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en una √∫nica aproximaci√≥n global, ya que las divisiones se determinan por la evidencia estad√≠stica local. Esto los convierte en una alternativa robusta que ofrece una selecci√≥n de variables menos sesgada y modelos con una mayor interpretabilidad estad√≠stica.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ No lineal, usa tests condicionales para particionar",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No relevante",
  "‚ö†Ô∏è Moderadamente (consume tests basados en permutaciones)",
  "‚úÖ Robusto a colinealidad",
  "‚úÖ Alta (cada divisi√≥n est√° basada en criterios estad√≠sticos claros)",
  "‚ö†Ô∏è M√°s lento que CART en datasets grandes",
  "‚úÖ Recomendable para podar y evitar sobreajuste",
  "‚ùå Datos muy peque√±os por nodo o variables irrelevantes"
)

detalles <- c(
  "Construye √°rboles basados en test de independencia condicional (ctree).",
  "Permite tanto regresi√≥n (valor continuo) como clasificaci√≥n multinivel.",
  "Acepta variables num√©ricas y categ√≥ricas sin necesidad de dummies.",
  "Detecta relaciones complejas y no lineales usando tests basados en permutaciones.",
  "No exige que los residuos sigan una distribuci√≥n espec√≠fica.",
  "Ideal si las observaciones no est√°n correlacionadas en el tiempo.",
  "No requiere homoscedasticidad porque no se basa en un modelo param√©trico de error.",
  "Los outliers pueden afectar el c√°lculo de los tests, aunque es m√°s robusto que CART.",
  "El algoritmo ctree no se ve afectado por predictores altamente correlacionados.",
  "Los √°rboles generados son f√°ciles de visualizar y explicar.",
  "Para cada divisi√≥n realiza m√∫ltiples tests, por lo que es m√°s lento en datos muy grandes.",
  "Usar k-fold o repeated CV para elegir la profundidad y evitar sobreajuste.",
  "No es apto si tienes muy pocas observaciones en cada parto o muchas variables irrelevantes."
)

tabla_ctree <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_ctree %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir ctree",
             subtitle = "Conditional Decision Trees") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```

## Cubist  {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Cubist.png"))
```

**Cubist** es un algoritmo de **Machine Learning** desarrollado por RuleQuest Research (autores de C4.5 y See5/C5.0), principalmente para tareas de **regresi√≥n**. Es una extensi√≥n de los modelos de **√°rboles de decisi√≥n** que combina la simplicidad de las reglas con la precisi√≥n de los modelos locales, lo que lo hace muy potente para datos complejos con muchas caracter√≠sticas.

En esencia, Cubist construye un **modelo de reglas con un modelo lineal adjunto a cada regla**. Opera en dos fases principales:

1.  **Construcci√≥n del √Årbol de Reglas:**
    * Similar a un √°rbol de decisi√≥n, Cubist construye una estructura de √°rbol dividiendo los datos en subconjuntos basados en los valores de las caracter√≠sticas.
    * Sin embargo, en lugar de hojas que contienen un valor constante (como en los √°rboles de regresi√≥n tradicionales), cada hoja de este √°rbol se transforma en un **conjunto de reglas**.
    * A cada regla se le asocia un **modelo lineal multivariado local** (o un "modelo de comit√©" de reglas, donde varias reglas contribuyen a la predicci√≥n). Este modelo lineal se entrena solo con los datos que satisfacen las condiciones de esa regla.

2.  **Ajuste del Modelo de Reglas y Predicci√≥n:**
    * Para cada nueva instancia de predicci√≥n, Cubist identifica las reglas que se aplican a esa instancia.
    * La predicci√≥n final se calcula combinando las predicciones de los modelos lineales de las reglas que se aplican, y luego se ajusta un poco esa predicci√≥n mediante un **"comit√©" de vecinos** (ajustes locales adicionales basados en ejemplos similares), si est√° configurado para ello. Esta etapa de ajuste lo hace a√∫n m√°s robusto.

Cubist es valorado por su capacidad para manejar **relaciones complejas y no lineales** en los datos. Proporciona un modelo que es m√°s interpretable que una "caja negra" (como una red neuronal profunda) debido a su base en reglas, pero mucho m√°s preciso que los modelos lineales o los √°rboles de regresi√≥n simples, gracias a sus modelos lineales locales y ajustes.


**Aprendizaje Global vs. Local:**

Cubist es un algoritmo que combina de manera muy efectiva aspectos de **aprendizaje global y local**.

* **Aspecto Global (Estructura de Reglas):** La fase de construcci√≥n del √°rbol y la derivaci√≥n de las reglas crean una **estructura global** que divide el espacio de caracter√≠sticas. Este conjunto de reglas abarca todo el dominio de los datos y determina qu√© modelo local se aplicar√° a una instancia. Es una forma de particionar el espacio de caracter√≠sticas de manera jer√°rquica para establecer un marco de predicci√≥n general.

* **Aspecto Local (Modelos Lineales y Ajustes):** Aqu√≠ es donde Cubist brilla en su capacidad de aprendizaje local:
    * **Modelos Lineales Locales:** Cada regla tiene asociado un **modelo lineal que se entrena solo con los datos que caen dentro de esa regla**. Esto permite a Cubist capturar **relaciones locales y no lineales** de manera precisa. En lugar de una √∫nica relaci√≥n lineal global, el modelo se adapta a las particularidades de diferentes subregiones de los datos.
    * **Ajuste Basado en Vecinos:** Si se activa la opci√≥n de "comit√©" o el ajuste basado en vecinos (conocido como `committees` o `neighbors`), el modelo refina a√∫n m√°s su predicci√≥n incorporando la informaci√≥n de los ejemplos de entrenamiento m√°s cercanos al punto de consulta. Esto es una forma de **"regresi√≥n ponderada localmente"**, donde la predicci√≥n final se ajusta en funci√≥n de los patrones observados en el vecindario inmediato del punto de inter√©s.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n basada en reglas)",
  "‚úÖ Num√©rica (regresi√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Modelo aditivo basado en reglas y ajustes lineales locales",
  "‚ö†Ô∏è Requiere an√°lisis de residuos, no siempre normalidad estricta",
  "‚ö†Ô∏è Asume independencia, como otros modelos supervisados",
  "‚ö†Ô∏è Puede tener heteroscedasticidad",
  "‚ö†Ô∏è Moderadamente sensible a outliers",
  "‚ö†Ô∏è Puede manejar correlaci√≥n, pero multicolinealidad puede afectar interpretabilidad",
  "‚úÖ Moderada: reglas explican el modelo, pero menos transparente que modelos lineales",
  "‚ö†Ô∏è Relativamente r√°pido, pero depende del n√∫mero de reglas",
  "‚úÖ Compatible con validaci√≥n cruzada para evaluar rendimiento",
  "‚ùå No funciona bien con datos muy peque√±os o ruido extremo"
)

detalles <- c(
  "Combina t√©cnicas de √°rboles de decisi√≥n con modelos lineales locales para predicci√≥n precisa.",
  "Predice variables continuas mediante reglas que dividen el espacio y ajustes lineales en cada regi√≥n.",
  "Puede manejar variables predictoras mixtas (num√©ricas y categ√≥ricas).",
  "Modelo flexible que ajusta m√∫ltiples reglas para capturar relaciones no lineales y locales.",
  "Evaluar residuos para verificar supuestos; no es tan r√≠gido como OLS.",
  "Como modelo supervisado, se espera independencia entre observaciones.",
  "Puede tolerar algo de heteroscedasticidad, pero afecta precisi√≥n de intervalos.",
  "Outliers pueden afectar algunas reglas locales y coeficientes.",
  "Multicolinealidad puede dificultar interpretaci√≥n de coeficientes locales.",
  "Las reglas pueden interpretarse, pero el modelo global puede ser complejo.",
  "La velocidad depende del tama√±o del dataset y n√∫mero de reglas generadas.",
  "Se usa validaci√≥n cruzada para seleccionar par√°metros y validar modelo.",
  "No es ideal para datasets muy peque√±os o con mucho ruido no estructurado."
)

tabla_cubist <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_cubist %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir cubist",
             subtitle = "Cubist")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Decision Stump  {-} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Decision Stump.png"))
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Decision Stump_1.png"))
```

Un **Decision Stump** es el tipo de **√°rbol de decisi√≥n** m√°s simple y fundamental, compuesto por un **√∫nico nodo de decisi√≥n (la ra√≠z)** que se conecta directamente a los **nodos hoja**. La idea es que un *decision stump* toma una decisi√≥n de clasificaci√≥n o regresi√≥n bas√°ndose en una sola caracter√≠stica o atributo de entrada.

Aunque parece demasiado simple, la l√≥gica es que, a pesar de su simplicidad, un *decision stump* identifica el mejor umbral o categor√≠a dentro de una √∫nica variable para separar los datos de la manera m√°s efectiva posible. Para problemas de clasificaci√≥n, esto significa encontrar la caracter√≠stica que, por s√≠ sola, maximice alguna medida de **pureza** (como la ganancia de informaci√≥n, la impureza Gini, o la significancia chi-cuadrado) o minimice el error de clasificaci√≥n. Para regresi√≥n, buscar√° el punto de divisi√≥n en una sola caracter√≠stica que minimice la suma de los cuadrados de los errores.

En el contexto del **aprendizaje local vs. global**, un *decision stump* es inherentemente un sistema de **aprendizaje local**. Su "aprendizaje" se limita a encontrar la mejor divisi√≥n dentro de una √∫nica variable, lo que es una forma extrema de **regresi√≥n ponderada localmente**. Si los datos no se distribuyen linealmente, un *decision stump* no puede por s√≠ mismo modelar relaciones complejas. Sin embargo, su valor no reside en ser un modelo predictivo robusto por s√≠ mismo, sino en ser un **"clasificador d√©bil"** o **"regresor d√©bil"** que puede ser combinado en **conjuntos de modelos (ensembles)** m√°s potentes. Por ejemplo, los *decision stumps* son los bloques de construcci√≥n m√°s comunes para algoritmos de **boosting** como **AdaBoost**. En estos casos, m√∫ltiples *decision stumps* se entrenan secuencialmente, cada uno enfoc√°ndose en los errores que cometieron los *stumps* anteriores, sumando sus "aprendizajes locales" para formar un modelo global m√°s preciso. Esto contrarresta la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n simplificada)",
  "‚úÖ Num√©ricas y/o categ√≥ricas",
  "‚ö†Ô∏è Captura solo una divisi√≥n (muy simple, un solo nodo interno)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No relevante",
  "‚úÖ Relativamente robusto (poca complejidad)",
  "‚úÖ Ignora colinealidad (usa solo una variable)",
  "‚úÖ Muy alta (un solo umbral para dividir)",
  "‚úÖ Extremadamente r√°pido",
  "‚úÖ Se puede usar k-fold para evaluar estabilidad",
  "‚ùå No funciona bien si la relaci√≥n es compleja o no hay un buen umbral √∫nico"
)

detalles <- c(
  "Modelo de √°rbol con un solo nivel de decisi√≥n (un umbral en una sola variable).",
  "En clasificaci√≥n predice una clase binaria; en regresi√≥n, un valor medio para cada divisi√≥n.",
  "Selecciona la mejor variable con el punto de corte que maximiza ganancia (clasificaci√≥n) o reduce varianza (regresi√≥n).",
  "Solo ajusta un umbral, por lo que no modela interacciones ni no linealidades complejas.",
  "No hay supuestos param√©tricos de distribuci√≥n de errores.",
  "Mejor si las instancias no est√°n correlacionadas (por ejemplo, no aplica a series de tiempo sin agrupar).",
  "La varianza constante no se eval√∫a, pues el modelo es no param√©trico y muy simple.",
  "Un solo punto de corte es menos sensible a outliers en comparaci√≥n con √°rboles profundos, pero a√∫n puede verse afectado si un outlier define el umbral.",
  "Como solo usa una variable, no se ve afectado por correlaciones altas entre predictores.",
  "El modelo entero es resumido en un √∫nico umbral; f√°cil de explicar.",
  "Muy r√°pido de entrenar y predecir, pues solo se eval√∫a un umbral en un predictor.",
  "Es √∫til para comprobar si hay una √∫nica variable con gran poder predictivo; k-fold ayuda a validar que el umbral se mantenga estable.",
  "No sirve si el problema requiere varias divisiones, interacciones o relaciones no lineales profundas."
)

tabla_stump <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_stump %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Decision Stump",
             subtitle = "Decision Stump") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Guided Trees / Hybrid Trees {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Hybrid Trees.png"))
```

## Iterative Dichotomiser 3 (ID3)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/ID3.png"))
```

**Iterative Dichotomiser 3 (ID3)** es un algoritmo cl√°sico para construir **√°rboles de decisi√≥n**, dise√±ado principalmente para tareas de **clasificaci√≥n**. Fue uno de los primeros algoritmos de √°rboles de decisi√≥n desarrollados por Ross Quinlan. La idea central de ID3 es construir un √°rbol de clasificaci√≥n seleccionando en cada nodo del √°rbol el atributo que mejor divide el conjunto de datos en subconjuntos m√°s puros y homog√©neos.

ID3 opera de forma **iterativa** y **dicot√≥mica** (aunque puede manejar atributos con m√°s de dos categor√≠as), dividiendo el conjunto de datos en cada paso bas√°ndose en el atributo m√°s informativo. La selecci√≥n del "mejor" atributo se basa en m√©tricas de **teor√≠a de la informaci√≥n**, principalmente la **ganancia de informaci√≥n** (Information Gain). La ganancia de informaci√≥n mide la reducci√≥n en la **entrop√≠a** (una medida de la impureza o desorden de un conjunto de datos) que se logra al dividir los datos seg√∫n un atributo particular. El atributo con la mayor ganancia de informaci√≥n es elegido como el nodo de decisi√≥n en cada nivel del √°rbol.

A diferencia de los sistemas de aprendizaje global que buscan minimizar funciones de p√©rdida globales (como el error cuadr√°tico medio), ID3 es un algoritmo de **aprendizaje local** en el sentido de que toma decisiones de divisi√≥n √≥ptimas en cada nodo bas√°ndose en la informaci√≥n disponible en ese subconjunto de datos. Aunque la construcci√≥n del √°rbol es un proceso global, cada paso de la divisi√≥n se optimiza localmente para maximizar la pureza de los subconjuntos resultantes. Esto le permite a ID3 capturar relaciones no lineales entre las variables, ya que no asume una distribuci√≥n lineal de los datos. En esencia, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n (o clasificaci√≥n, en este caso) de manera ponderada localmente al dividir el espacio de caracter√≠sticas en regiones m√°s manejables. Sin embargo, una desventaja de ID3 es que tiende a favorecer atributos con muchos valores y puede ser propenso al sobreajuste.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Principalmente categ√≥ricas (num√©ricas requieren discretizaci√≥n)",
  "‚úÖ No lineal (basado en ganancia de informaci√≥n)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente (valores at√≠picos pueden generar ramas poco representativas)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol simple de interpretar)",
  "‚úÖ R√°pido con datos moderados y discretizados",
  "‚úÖ Recomendable para equilibrar datos y evitar overfitting",
  "‚ùå Respuesta continua, muchos valores faltantes o ruido elevado"
)

detalles <- c(
  "Construye un √°rbol de decisi√≥n dividiendo por ganancia de informaci√≥n (entrop√≠a).",
  "Clasifica muestras en categor√≠as discretas, ej. S√≠/No, A/B/C.",
  "Mejor con variables categ√≥ricas nativas; las num√©ricas deben transformarse en rangos.",
  "No asume ninguna relaci√≥n funcional: usa particiones basadas en criterios de informaci√≥n.",
  "No hay residuos en el sentido param√©trico; no exige distribuci√≥n normal.",
  "Las instancias deben ser independientes; no orientado a series temporales.",
  "No requiere varianzas constantes porque no hay t√©rmino de error param√©trico.",
  "Los outliers categ√≥ricos pueden crear nodos muy peque√±os no representativos.",
  "ID3 ignora correlaciones altas, pero demasiadas variables correlacionadas pueden ralentizar la b√∫squeda de mejores divisiones.",
  "Cada nodo muestra la regla de divisi√≥n; el √°rbol global es f√°cil de visualizar para pocos niveles.",
  "La construcci√≥n recursiva es eficiente para datos discretizados; se vuelve lento si hay muchas categor√≠as o atributos.",
  "Se usa para podar y seleccionar profundidad √≥ptima del √°rbol, equilibrando sesgo y varianza.",
  "No es recomendable si la variable objetivo es continua o si hay mucho ruido sin transformar."
)

tabla_id3 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_id3 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir IDE3",
             subtitle = "Iterative Dichotomiser 3 (ID3)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## M5 (Model Tree) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/M5 (Model Tree).png"))
```

**M5**, a menudo referida como **M5'** o **M5P** (su implementaci√≥n en el software Weka), es un algoritmo de **√°rboles de decisi√≥n** espec√≠ficamente dise√±ado para **tareas de regresi√≥n**, es decir, para predecir valores num√©ricos continuos. Desarrollado por Ross Quinlan en 1992 y luego mejorado por Wang y Witten en 1997, M5 se destaca de los √°rboles de regresi√≥n tradicionales (como los de CART que solo tienen valores constantes en las hojas) al incorporar **modelos de regresi√≥n lineal** en sus nodos hoja.

La idea fundamental de M5 es combinar la interpretabilidad de un √°rbol de decisi√≥n con la capacidad predictiva de los modelos de regresi√≥n lineal. Funciona en dos etapas principales:

1.  **Construcci√≥n del √Årbol:** M5 construye un √°rbol de decisi√≥n de forma recursiva, similar a otros algoritmos de √°rboles. Sin embargo, en lugar de usar medidas de impureza para clasificaci√≥n, utiliza la **reducci√≥n de la desviaci√≥n est√°ndar (SDR)** como criterio de divisi√≥n. El algoritmo selecciona el atributo y el punto de divisi√≥n que maximizan la reducci√≥n de la desviaci√≥n est√°ndar del valor objetivo en los subconjuntos resultantes. Este proceso contin√∫a hasta que el n√∫mero de instancias en un nodo es muy peque√±o o la desviaci√≥n est√°ndar es muy baja.

2.  **Poda y Suavizado:** Una vez construido el √°rbol inicial, M5 lo **poda** para evitar el sobreajuste. En lugar de reemplazar los nodos con un valor constante, los nodos hoja (y a veces nodos internos) son reemplazados por **modelos de regresi√≥n lineal multivariados**. Estos modelos lineales se construyen utilizando los atributos relevantes para esa rama del √°rbol. Adem√°s, M5 aplica un proceso de **suavizado** para compensar las discontinuidades bruscas que podr√≠an surgir entre las predicciones de modelos lineales adyacentes. Este suavizado ajusta el valor predicho en una hoja bas√°ndose en las predicciones de los modelos en los nodos a lo largo de la ruta desde la ra√≠z hasta esa hoja.

En el contexto del **aprendizaje global vs. local**, M5 es un h√≠brido interesante. Por un lado, la construcci√≥n del √°rbol se basa en decisiones de divisi√≥n **locales**, buscando la mejor reducci√≥n de la desviaci√≥n est√°ndar en cada nodo. Esto permite a M5 modelar relaciones no lineales, ya que "si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n de manera ponderada localmente". El √°rbol divide el problema de regresi√≥n global en m√∫ltiples subproblemas m√°s peque√±os. Por otro lado, al tener **modelos de regresi√≥n lineal** en las hojas, M5 incorpora un componente de **aproximaci√≥n de funci√≥n local** m√°s sofisticado que un simple valor constante. Estos modelos lineales son "locales" para la regi√≥n de datos que representa esa hoja, pero internamente son modelos globales para esa subregi√≥n. Esto permite a M5 ofrecer una alternativa potente a las aproximaciones de funciones puramente globales, especialmente cuando las relaciones entre las variables son complejas y se benefician de una combinaci√≥n de particionamiento del espacio y modelado lineal dentro de esas particiones.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua",
  "‚úÖ Num√©ricas (categ√≥ricas procesar como dummies)",
  "‚úÖ Lineal por segmentos (√°rbol + regresi√≥n en hojas)",
  "‚ùå No requiere estrictamente",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es requisito",
  "‚ö†Ô∏è Moderadamente (outliers pueden distorsionar regresiones locales)",
  "‚úÖ Relativamente robusto (regresi√≥n en hojas mitiga algo la colinealidad)",
  "‚ö†Ô∏è Media (√°rbol complejo, hojas lineales m√°s interpretables)",
  "‚ö†Ô∏è Moderado (depende de n√∫mero de nodos y atributos)",
  "‚úÖ Recomendable para optimizar n√∫mero de nodos y hojas",
  "‚ùå Muchos nodos con pocos casos o ruido elevado"
)

detalles <- c(
  "Modelo de √°rbol de regresi√≥n con ajustes lineales en cada hoja.",
  "Predice valores continuos, p. ej., precio, consumo, etc.",
  "Requiere que variables categ√≥ricas se conviertan a indicadores antes de ajuste.",
  "Combina particiones basadas en atributos con regresiones m√∫ltiples en hojas.",
  "No exige que los residuos en cada hoja sean normales, aunque mejora inferencia.",
  "Ideal si las observaciones son independientes; en series de tiempo hay que agrupar.",
  "La varianza constante no es cr√≠tica, cada hoja ajusta localmente.",
  "Los extremos pueden afectar las regresiones locales; poda puede mitigar esto.",
  "El m√©todo divide el espacio antes de ajustar, reduciendo efectos de colinealidad.",
  "El √°rbol completo puede ser grande, pero cada hoja contiene una funci√≥n lineal clara.",
  "Construcci√≥n y poda del √°rbol m√°s costosas que OLS, pero razonables para tama√±os medianos.",
  "Ayuda a determinar n√∫mero √≥ptimo de hojas y complejidad del √°rbol.",
  "Si hay muy pocas observaciones por hoja o ruido demasiado alto, las regresiones locales fallan."
)

tabla_m5 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_m5 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir M5",
             subtitle = "M5 model tree algorithm") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Oblique Decision Trees {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Decision Tree/Oblique Decision Trees.png"))
```

## Specific Implementations/Libraries {-}  


<!--chapter:end:02-decision_tree.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üåü 3. M√©todos de Ensamble {-}   

**Ejemplos:** Random Forest, Gradient Boosting (XGBoost, LightGBM, AdaBoost).   
**Uso:** Excelentes para **clasificaci√≥n y regresi√≥n en datos tabulares**, especialmente en **competencias de datos** por su alto rendimiento.   
**Ventajas:** Ofrecen **alta precisi√≥n** y son muy **robustos**.   
**Limitaciones:** Pueden ser **dif√≠ciles de interpretar** y suelen ser **computacionalmente m√°s costosos**.   

---

## Adaptive Boosting (AdaBoost)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/AdaBoost.png"))
```


**AdaBoost (Adaptive Boosting)** es uno de los algoritmos de **boosting** m√°s influyentes y el primero en ser propuesto con √©xito, desarrollado por Yoav Freund y Robert Schapire en 1995. Es una t√©cnica de **aprendizaje conjunto (ensemble learning)** utilizada principalmente para **clasificaci√≥n**, aunque sus principios pueden extenderse a la regresi√≥n. La idea fundamental de AdaBoost es construir un modelo fuerte combinando secuencialmente las predicciones de m√∫ltiples **clasificadores "d√©biles" o "base"**, y lo hace prestando m√°s atenci√≥n a los ejemplos que los modelos anteriores clasificaron incorrectamente.

El funcionamiento de AdaBoost se basa en un sistema de **re-ponderaci√≥n de datos** en cada iteraci√≥n:

1.  **Inicializaci√≥n de Pesos:** Se asigna un peso inicial igual a cada ejemplo de entrenamiento.
2.  **Entrenamiento del Clasificador D√©bil:** En cada iteraci√≥n, se entrena un clasificador d√©bil (a menudo un **Decision Stump**, que es un √°rbol de decisi√≥n de un solo nivel) en el conjunto de datos actual. Este clasificador se enfoca en minimizar el error ponderado.
3.  **C√°lculo del Error Ponderado:** Se calcula el error del clasificador d√©bil, teniendo en cuenta los pesos de los ejemplos. Los ejemplos mal clasificados tienen un mayor impacto en este error.
4.  **Actualizaci√≥n de Pesos de Datos:** Los pesos de los ejemplos mal clasificados por el clasificador actual son **aumentados**, mientras que los pesos de los ejemplos correctamente clasificados son **disminuidos**. Esto asegura que el siguiente clasificador d√©bil se enfoque m√°s en los ejemplos que son dif√≠ciles de clasificar.
5.  **C√°lculo del Peso del Clasificador:** Se asigna un peso (o "contribuci√≥n") al clasificador d√©bil actual en funci√≥n de su precisi√≥n. Los clasificadores m√°s precisos reciben un peso mayor en la predicci√≥n final del conjunto.
6.  **Combinaci√≥n de Predicciones:** Las predicciones finales del modelo AdaBoost se obtienen mediante una **suma ponderada** de las predicciones de todos los clasificadores d√©biles.

En el contexto del **aprendizaje global vs. local**, AdaBoost es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada clasificador d√©bil que se entrena en una iteraci√≥n puede verse como una forma de **regresi√≥n ponderada localmente** (o, m√°s precisamente, clasificaci√≥n ponderada localmente), ya que ajusta su enfoque bas√°ndose en los ejemplos que el modelo combinado anterior no pudo clasificar bien. Al iterar y ajustar los pesos de los datos, AdaBoost se enfoca progresivamente en las regiones del espacio de caracter√≠sticas donde el modelo actual tiene un rendimiento deficiente. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de clasificaci√≥n (y por extensi√≥n, las ideas de regresi√≥n) de manera altamente adaptativa. La capacidad de AdaBoost para concentrarse en los "errores" m√°s dif√≠ciles aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. El resultado es un clasificador global muy preciso y robusto, capaz de modelar relaciones complejas y no lineales, que es una combinaci√≥n ponderada de muchas decisiones locales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n adaptada)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para algunas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones mediante reponderaci√≥n iterativa",
  "‚ùå No requiere supuestos de normalidad en los residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers pueden obtener demasiado peso durante iteraciones)",
  "‚úÖ Robusto (reduce colinealidad al iterar sobre subconjuntos ponderados)",
  "‚ö†Ô∏è Baja (modelo resultante es especie de ‚Äôcaja negra‚Äô)",
  "‚ö†Ô∏è Lento con muchas iteraciones o datos grandes",
  "‚úÖ Recomendable para ajustar tasa de aprendizaje y n√∫mero de iteraciones",
  "‚ùå No es ideal con datos muy ruidosos o clases extremadamente desbalanceadas sin t√©cnicas adicionales"
)

detalles <- c(
  "Ensamble supervisado que combina varios modelos d√©biles (ej. √°rboles simples) ajustando pesos seg√∫n errores anteriores.",
  "En clasificaci√≥n ajusta pesos para mal clasificados; en regresi√≥n, adapta predicci√≥n por minimizaci√≥n de p√©rdida.",
  "Puede trabajar con datos mixtos; para variables categ√≥ricas suele usar codificaci√≥n de dummies.",
  "Cada iteraci√≥n repondera observaciones dif√≠ciles, enfoc√°ndose en patrones que previos modelos no capturaron.",
  "No impone distribuci√≥n de errores; se basa en funci√≥n de p√©rdida, no en supuestos param√©tricos.",
  "Funciona mejor si cada observaci√≥n es independiente; sensible a dependencias temporales si no se corrige.",
  "No requiere varianza constante, ya que funciona sobre el error iterativo en lugar de residuos tradicionales.",
  "Outliers dif√≠ciles de clasificar tienden a recibir mayor peso, lo que puede sesgar el ensamble si no se controla el learning rate.",
  "La reponderaci√≥n de muestras aten√∫a el efecto de predictores correlacionados, pues cada iteraci√≥n puede focalizarse en subconjuntos distintos.",
  "Es complejo desentra√±ar la contribuci√≥n de cada modelo d√©bil; se pueden usar m√©tricas de importancia o SHAP para interpretaci√≥n.",
  "Cada iteraci√≥n entrena un modelo d√©bil; muchas iteraciones o modelos complejos pueden ralentizar el entrenamiento.",
  "K-fold o repeated CV ayudan a elegir tasa de aprendizaje (learning rate) y n√∫mero de iteraciones (trials).",
  "No conviene con instancias altamente ruidosas: se puede sobreajustar r√°pidamente si la tasa de aprendizaje es alta o no se regula iteraciones."
)

tabla_adaboost <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_adaboost %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir adaboost",
             subtitle = "AdaBoost") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Boosting  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Boosting.png"))
```

**Boosting** es una t√©cnica de **aprendizaje conjunto (ensemble learning)** que busca transformar un conjunto de **modelos "d√©biles" o "base"** en un **modelo "fuerte" o "preciso"**. La idea fundamental es construir modelos de forma **secuencial** e **iterativa**, donde cada nuevo modelo se centra en corregir los errores o deficiencias de los modelos construidos en las iteraciones anteriores. A diferencia del *bagging* (como en Random Forest), donde los modelos se entrenan de forma independiente, el *boosting* es intr√≠nsecamente secuencial y adaptativo.

El concepto clave de Boosting radica en la asignaci√≥n de **pesos** o en el enfoque en los **errores residuales**:

1.  **Iteraciones Secuenciales:** El proceso comienza con un modelo base inicial (a menudo simple, como un *decision stump*).
2.  **Enfoque en los Errores:** En cada iteraci√≥n subsiguiente, el algoritmo presta m√°s atenci√≥n a los ejemplos que fueron clasificados (o predichos) incorrectamente por los modelos anteriores, o a los errores residuales no explicados. Esto se logra ya sea **re-ponderando** los datos (dando m√°s peso a los ejemplos mal clasificados) o **ajustando** el nuevo modelo para que prediga los residuos de los modelos anteriores.
3.  **Combinaci√≥n Ponderada:** Las predicciones de todos los modelos d√©biles se combinan, generalmente a trav√©s de una suma ponderada, donde los modelos m√°s precisos reciben un mayor peso en la predicci√≥n final.

La fuerza del boosting radica en su capacidad para reducir el **sesgo** y la **varianza** del modelo final, al construir un modelo complejo a partir de componentes simples que se complementan entre s√≠.

En el contexto del **aprendizaje global vs. local**, Boosting es una estrategia de **aprendizaje global** que opera construyendo una serie de aproximaciones **locales**. Cada modelo "d√©bil" que se entrena en una iteraci√≥n puede verse como una forma de **regresi√≥n ponderada localmente** (o clasificaci√≥n ponderada localmente), ya que se enfoca en una parte espec√≠fica del espacio de las caracter√≠sticas o en los datos con mayor error. El proceso iterativo de Boosting busca corregir estos errores localizados. Si los datos no se distribuyen linealmente, el boosting permite que el concepto de regresi√≥n (o clasificaci√≥n) se aplique de manera muy flexible y potente. La capacidad de concentrarse en los "errores" residuales aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Al ensamblar muchos modelos d√©biles que se adaptan a los errores de los anteriores, Boosting construye un modelo final que es una aproximaci√≥n de funci√≥n global altamente adaptable y precisa. Algoritmos como AdaBoost y Gradient Boosting Machines (GBM) son ejemplos prominentes de esta t√©cnica.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para algunas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones complejas mediante aprendizaje secuencial",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio para muchos algoritmos",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (puede ajustar demasiado a outliers si no se controla)",
  "‚úÖ Robusto (cada iteraci√≥n utiliza un subconjunto ponderado de datos)",
  "‚ö†Ô∏è Baja (modelo en su conjunto es tipo caja negra)",
  "‚ö†Ô∏è Lento con muchos √°rboles o altas iteraciones",
  "‚úÖ Recomendable con k-fold o repeated CV para ajustar tasa de aprendizaje y n√∫mero de iteraciones",
  "‚ùå Si se tienen pocos datos, alto ruido o target muy desbalanceado sin ajuste"
)

detalles <- c(
  "Ensamble supervisado que combina varios modelos d√©biles (ej. √°rboles peque√±os) de forma secuencial",
  "En clasificaci√≥n se usan votaciones ponderadas; en regresi√≥n se suman predicciones graduadas",
  "Acepta variables mixtas; algunas bibliotecas requieren convertir categ√≥ricas en dummies",
  "Construye modelos d√©biles en cada iteraci√≥n, enfoc√°ndose en muestras mal clasificadas o con alto residuo",
  "No exige distribuci√≥n de errores, ya que se basa en funci√≥n de p√©rdida sin supuestos param√©tricos",
  "Mejor si los ejemplos son independientes; puede usar t√©cnicas especiales para datos correlacionados",
  "No asume varianza constante, usa funci√≥n de p√©rdida directa para optimizar",
  "Los outliers pueden recibir peso excesivo en iteraciones posteriores, por lo que es necesario regularizar o usar robust loss",
  "La selecci√≥n de variables se hace impl√≠citamente, reduciendo el impacto de colinealidad",
  "Dif√≠cil de interpretar directamente; se pueden usar m√©tricas de importancia, SHAP o partial dependence para explicaci√≥n",
  "Cada iteraci√≥n entrena un modelo d√©bil, por lo que puede ser costoso si el n√∫mero de iteraciones es alto",
  "CV ayuda a determinar la tasa de aprendizaje (learning rate), n√∫mero de iteraciones y complejidad de base learners",
  "No es adecuado si hay muy pocos ejemplos, alta dimensionalidad con poco se√±al o target extremadamente desequilibrado"
)

tabla_boosting <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_boosting %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Boosting",
             subtitle = "Boosting") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Bootstrapped Aggregation (Bagging)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Bagging.png"))
```

**Bootstrapped Aggregation (Bagging)** es una t√©cnica de **aprendizaje conjunto (ensemble learning)** dise√±ada para mejorar la estabilidad y precisi√≥n de los algoritmos de aprendizaje autom√°tico, particularmente para reducir la **varianza** en los modelos. Fue introducida por Leo Breiman en 1996 y es la base de algoritmos muy populares como **Random Forest**. La idea fundamental de Bagging es entrenar m√∫ltiples versiones de un mismo modelo base en diferentes subconjuntos del conjunto de datos original y luego combinar sus predicciones.

El proceso central de Bagging implica dos pasos clave:

1.  **Muestreo Bootstrap:** En lugar de entrenar un √∫nico modelo en todo el conjunto de datos de entrenamiento, Bagging crea **m√∫ltiples conjuntos de datos de arranque (bootstrap samples)**. Cada muestra de arranque se crea seleccionando aleatoriamente, **con reemplazo**, un n√∫mero de observaciones igual al tama√±o del conjunto de datos original. Esto significa que algunos puntos de datos pueden aparecer varias veces en una muestra de arranque, mientras que otros pueden no aparecer en absoluto. Este muestreo aleatorio introduce diversidad entre los conjuntos de entrenamiento para cada modelo.

2.  **Agregaci√≥n (Aggregation):** Una vez que se han entrenado **m√∫ltiples modelos base independientes** (por ejemplo, √°rboles de decisi√≥n) en cada una de estas muestras de arranque, sus predicciones se combinan. Para tareas de **clasificaci√≥n**, la combinaci√≥n se realiza mediante **votaci√≥n por mayor√≠a** (la clase m√°s votada). Para tareas de **regresi√≥n**, las predicciones se promedian. Esta agregaci√≥n de predicciones de modelos diversos reduce la varianza y, por lo tanto, hace que el modelo final sea m√°s robusto y menos propenso al sobreajuste que un solo modelo entrenado en todo el conjunto de datos.

En el contexto del **aprendizaje global vs. local**, Bagging es una estrategia que combina las ventajas de los modelos de **aprendizaje local** para construir una **aproximaci√≥n de funci√≥n global** m√°s estable. Cada modelo base (ej. un √°rbol de decisi√≥n) que se entrena en una muestra de arranque puede considerarse un sistema de aprendizaje local, ya que toma decisiones basadas en el subconjunto de datos que le ha sido asignado. Sin embargo, al entrenar estos m√∫ltiples modelos en paralelo y luego agregarlos, Bagging construye un modelo final que es una aproximaci√≥n de funci√≥n global altamente adaptable. La ventaja principal es que, si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se puede aplicar eficazmente mediante esta forma de **regresi√≥n ponderada localmente** (donde los "pesos" son impl√≠citos a trav√©s de la agregaci√≥n de predicciones de modelos entrenados en subconjuntos aleatorios de datos). Bagging aborda el problema de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo al promediar o votar las predicciones de m√∫ltiples modelos, lo que reduce la varianza y mejora la generalizaci√≥n.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Captura relaciones no lineales al promediar m√∫ltiples modelos",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚úÖ Robusto (cada bootstrap reduce el impacto de outliers)",
  "‚úÖ Robusto (la agregaci√≥n mitiga colinealidad)",
  "‚ö†Ô∏è Moderada (dif√≠cil interpretar conjunto de modelos)",
  "‚ö†Ô∏è Moderado (depende del n√∫mero de √°rboles y tama√±o del dataset)",
  "‚úÖ Recomendable usar k-fold",
  "‚ùå No es ideal con muy pocos datos o si los base learners son demasiado simples"
)

detalles <- c(
  "Ensamble supervisado que ajusta varios modelos (usualmente √°rboles) sobre muestras bootstrap y promedia predicciones.",
  "En clasificaci√≥n predice la clase m√°s votada; en regresi√≥n, promedia los valores predichos.",
  "Acepta todo tipo de variables; las categ√≥ricas deben codificarse adecuadamente.",
  "Al promediar m√∫ltiples modelos, reduce varianza y captura no linealidades impl√≠citamente.",
  "No impone supuestos sobre la distribuci√≥n de errores.",
  "Los datos deben ser independientes; funciona peor en datos con fuerte autocorrelaci√≥n sin ajuste.",
  "No requiere varianza constante puesto que se basa en agregaci√≥n de m√∫ltiples predicciones.",
  "Cada muestra bootstrap y √°rbol es menos sensible a valores extremos; la agregaci√≥n aumenta robustez.",
  "La selecci√≥n aleatoria de subconjuntos y bootstrap reduce el efecto de predictores correlacionados.",
  "El modelo final es un conjunto de muchos √°rboles, lo que dificulta su explicaci√≥n directa.",
  "Entrenar cientos de √°rboles toma tiempo, pero es paralelizable; la predicci√≥n es relativamente r√°pida.",
  "CV ayuda a ajustar par√°metros como n√∫mero de √°rboles y profundidad m√°xima de cada √°rbol.",
  "Con pocos ejemplos, los bootstrap no aportan diversidad suficiente; si los base learners son muy simples, no capturan bien patrones complejos."
)

tabla_bagging <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bagging %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir bagging",
             subtitle = "Bootstrapped Aggregation (Bagging) ") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## CatBoost {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/CatBoost.png"))
```

## Extreme Gradient Boosting (XGBoost)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/XGBoost.png"))
```

**XGBoost (Extreme Gradient Boosting)** es una implementaci√≥n optimizada y altamente eficiente del algoritmo de **Gradient Boosting Machines (GBM)**, ampliamente reconocida por su **velocidad**, **rendimiento** y **escalabilidad** en problemas de **clasificaci√≥n** y **regresi√≥n**. Gan√≥ una inmensa popularidad debido a su √©xito en numerosas competiciones de *machine learning* (como Kaggle). Aunque se basa en los principios de GBM, XGBoost introduce varias mejoras clave que lo hacen superior en muchos escenarios.

La idea fundamental de XGBoost, al igual que GBM, es construir un modelo aditivo de forma **secuencial**, donde cada nuevo √°rbol intenta corregir los errores residuales del conjunto de √°rboles previos. Sin embargo, XGBoost optimiza este proceso con las siguientes caracter√≠sticas:

1.  **Paralelizaci√≥n:** Aunque el *boosting* es inherentemente secuencial, XGBoost permite la paralelizaci√≥n de la construcci√≥n de los √°rboles individuales. Por ejemplo, en el paso de b√∫squeda de la mejor divisi√≥n, puede evaluar las posibles divisiones en paralelo a trav√©s de m√∫ltiples n√∫cleos de CPU.
2.  **Regularizaci√≥n:** Incorpora t√©rminos de **regularizaci√≥n L1 (Lasso)** y **L2 (Ridge)** en la funci√≥n de costo para controlar la complejidad del modelo y evitar el sobreajuste. Esto es crucial para la generalizaci√≥n.
3.  **Manejo de Valores Faltantes:** Tiene una capacidad incorporada para manejar valores faltantes en los datos, permitiendo al algoritmo aprender la mejor direcci√≥n para los valores ausentes.
4.  **Poda por Profundidad (Depth-First Search):** A diferencia de muchos algoritmos de √°rboles que crecen nivel por nivel, XGBoost puede usar un enfoque de poda por profundidad, lo que a menudo resulta en √°rboles m√°s eficientes.
5.  **Cach√©-Aware Computing:** Optimiza el acceso a la memoria para manejar grandes conjuntos de datos de manera eficiente.
6.  **Flexibilidad de Funci√≥n de P√©rdida:** Permite el uso de funciones de p√©rdida personalizadas, lo que lo hace adaptable a una amplia gama de problemas.

En el contexto del **aprendizaje global vs. local**, XGBoost es una poderosa estrategia de **aprendizaje global** que se construye iterativamente a partir de componentes de **aprendizaje local**. Cada √°rbol de regresi√≥n (o clasificaci√≥n) individual es un "aprendiz d√©bil" que se enfoca en las deficiencias del modelo acumulado. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) de manera altamente sofisticada mediante esta **regresi√≥n ponderada localmente**. Al centrarse en los errores residuales y optimizar el proceso de manera rigurosa, XGBoost aborda de manera excepcional la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su combinaci√≥n de precisi√≥n, velocidad y capacidad para manejar grandes conjuntos de datos lo ha convertido en uno de los algoritmos m√°s populares y efectivos en la pr√°ctica del *machine learning*.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para dummies o label encoding)",
  "‚úÖ Captura no linealidades e interacciones complejas v√≠a √°rboles en boosting",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (puede sobreajustar a outliers si no regula)",
  "‚úÖ Robusto (reduce efecto de colinealidad al usar √°rboles secuenciales)",
  "‚ö†Ô∏è Baja (modelo complejo y tipo ‚Äôcaja negra‚Äô)",
  "‚úÖ Muy r√°pido y escalable (implementaci√≥n optimizada, paralelizable)",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajustar hiperpar√°metros",
  "‚ùå No es ideal con datos muy peque√±os, ruido alto o target extremadamente desbalanceado sin ajuste"
)

detalles <- c(
  "Ensamble supervisado que combina m√∫ltiples √°rboles d√©biles optimizados con gradiente descendente acelerado.",
  "En regresi√≥n predice valores continuos; en clasificaci√≥n combina probabilidades o clases mediante log-loss o multiclass objectives.",
  "Acepta variables mixtas; las categ√≥ricas deben convertirse a formatos compatibles (p. ej. factor numerico, one-hot encoding).",
  "Cada iteraci√≥n ajusta un nuevo √°rbol enfoc√°ndose en los residuos del modelo anterior, capturando patrones complejos.",
  "No impone distribuci√≥n param√©trica de errores, ya que optimiza funciones de p√©rdida directamente.",
  "Funciona mejor si cada muestra es independiente; sensible a series de tiempo sin preparaci√≥n apropiada.",
  "No requiere varianza constante, porque basa la optimizaci√≥n en gradientes del loss, no en supuestos de error.",
  "Los outliers dif√≠ciles de predecir pueden recibir demasiado peso en iteraciones sucesivas; usar _learning_rate_ bajo y _max_depth_ peque√±o para regular.",
  "Los √°rboles reducen el impacto de variables altamente correlacionadas, aunque m√∫ltiples iteraciones pueden a√∫n privilegiar caracter√≠sticas correlacionadas.",
  "Dif√≠cil interpretar directamente cada √°rbol; se utilizan m√©tricas de importancia, SHAP values o partial dependence plots para explicaci√≥n.",
  "Implementaci√≥n en C++ altamente optimizada (CPU/GPU), permite entrenamiento muy r√°pido incluso con millones de filas.",
  "Validaci√≥n cruzada anidada o simple ayuda a elegir hiperpar√°metros como `eta` (learning_rate), `nrounds` (n√∫mero de √°rboles), `max_depth`, `subsample`, `colsample_bytree`.",
  "No conviene con datasets muy peque√±os, ya que puede sobreajustar; tampoco si el ruido es muy alto y no se regula bien la complejidad."
)

tabla_xgboost <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_xgboost %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir xgboost",
             subtitle = "XGBoost")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Gradient Boosting Machines (GBM)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/GBM.png"))
```

**Gradient Boosting Machines (GBM)** es un algoritmo de **aprendizaje conjunto (ensemble learning)** extremadamente potente y vers√°til, utilizado para **clasificaci√≥n**, **regresi√≥n** y otras tareas predictivas. A diferencia de Random Forest que construye √°rboles de forma independiente en paralelo (bagging), GBM construye los √°rboles de forma **secuencial** y aditiva. La idea central es que cada nuevo √°rbol en el conjunto intenta corregir los errores residuales (residuos) del conjunto de √°rboles construidos previamente.

El concepto fundamental detr√°s de GBM es el **impulso (boosting)**, donde los modelos "d√©biles" (generalmente √°rboles de decisi√≥n, a menudo √°rboles poco profundos o "stumps") se combinan para formar un modelo "fuerte". GBM logra esto de una manera espec√≠fica:

1.  **Modelo Inicial:** Comienza con una predicci√≥n inicial para todos los datos (por ejemplo, el valor promedio para regresi√≥n o la probabilidad logar√≠tmica para clasificaci√≥n).
2.  **C√°lculo de Residuos (Pseudo-residuos):** En cada iteraci√≥n, el algoritmo calcula los **residuos** (o m√°s precisamente, los "pseudo-residuos" o gradientes negativos de la funci√≥n de p√©rdida) entre los valores reales y las predicciones actuales del modelo. Estos residuos representan los "errores" que el modelo actual no ha podido capturar.
3.  **Entrenamiento de un Nuevo √Årbol:** Se entrena un nuevo √°rbol de decisi√≥n para **predecir estos residuos**. Este √°rbol es t√≠picamente peque√±o y d√©bil, dise√±ado para centrarse en las √°reas donde el modelo actual tiene los mayores errores.
4.  **Actualizaci√≥n del Modelo:** La predicci√≥n de este nuevo √°rbol se a√±ade a la predicci√≥n acumulada del modelo existente, multiplicada por una **tasa de aprendizaje (learning rate)**. Esta tasa de aprendizaje controla el tama√±o del paso de cada √°rbol, evitando que el modelo se sobreajuste r√°pidamente.
5.  **Iteraci√≥n:** Este proceso se repite para un n√∫mero predefinido de iteraciones, o hasta que una m√©trica de rendimiento deje de mejorar. Cada nuevo √°rbol contribuye a reducir los errores restantes.

En el contexto del **aprendizaje global vs. local**, GBM es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol individual en el proceso de boosting es un sistema de aprendizaje local (como los *decision stumps* o √°rboles poco profundos) que se enfoca en una parte espec√≠fica del error. Sin embargo, la combinaci√≥n aditiva y secuencial de estos modelos "d√©biles" produce un modelo predictivo global altamente sofisticado y preciso. Si los datos no se distribuyen linealmente, GBM aplica el concepto de regresi√≥n (o clasificaci√≥n) mediante una forma incremental y adaptativa de **regresi√≥n ponderada localmente**. Al centrarse en los errores residuales, GBM aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su capacidad para minimizar la funci√≥n de p√©rdida de forma gradual y dirigida lo hace excepcionalmente eficaz para modelar relaciones complejas y no lineales, a menudo logrando un rendimiento superior en muchos problemas del mundo real.


```{r, echo = FALSE}

criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n)",
  "‚úÖ Captura no linealidades e interacciones complejas v√≠a boosting",
  "‚ùå No requiere",
  "‚úÖ Deseable pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (los outliers pueden influir en √°rboles individuales)",
  "‚úÖ Robusto (los arboles manejan colinealidad localmente)",
  "‚ö†Ô∏è Baja (modelo de tipo caja negra con varios √°rboles secuenciales)",
  "‚ö†Ô∏è Lento con muchos √°rboles, datos grandes o par√°metros altos",
  "‚úÖ Recomendable con k-fold o repeated CV para ajuste de hiperpar√°metros",
  "‚ùå Si se tienen pocos datos, muchas categor√≠as o ruido alto"
)

detalles <- c(
  "Ensamble de √°rboles secuenciales donde cada √°rbol corrige errores del anterior.",
  "En clasificaci√≥n se combinan probabilidades; en regresi√≥n se promedian predicciones.",
  "Funciona con muchas variables y aprende la importancia autom√°ticamente.",
  "Construye √°rboles d√©biles que se enfocan en los errores residuales previos.",
  "No impone supuestos en la distribuci√≥n de los errores.",
  "Los datos deben ser observaciones independientes; sensible a dependencias temporales.",
  "No requiere varianza constante puesto que se basa en √°rboles.",
  "Los outliers pueden exagerar los gradientes y forzar ajustes extremos en √°rboles individuales.",
  "Los √°rboles reducen impacto de colinealidad, pero m√∫ltiples √°rboles pueden still complicarla.",
  "Dif√≠cil de interpretar el conjunto; se pueden usar importance plots o SHAP para explicaci√≥n.",
  "La construcci√≥n secuencial de cientos de √°rboles puede ser costosa en tiempo y memoria.",
  "La validaci√≥n cruzada ayuda a determinar tasa de aprendizaje, n√∫mero de √°rboles y profundidad.",
  "No es ideal cuando se tienen muy pocos datos o categor√≠as con pocos ejemplos."
)

tabla_gbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_gbm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GBM",
             subtitle = "Gradient Boosting Machines (GBM)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## Gradient Boosted Regression Trees (GBRT)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/GBRT.png"))
```


**Gradient Boosted Regression Trees (GBRT)**, a menudo conocida como **Gradient Boosting Machines (GBM)** cuando los modelos base son √°rboles de decisi√≥n de regresi√≥n, es una t√©cnica de **aprendizaje conjunto (ensemble learning)** extremadamente potente y ampliamente utilizada para tareas de **regresi√≥n** (predicci√≥n de valores num√©ricos continuos) y tambi√©n puede adaptarse para **clasificaci√≥n**. Su fortaleza radica en su capacidad para construir un modelo predictivo robusto y preciso mediante la combinaci√≥n secuencial de m√∫ltiples √°rboles de decisi√≥n "d√©biles".

La idea central de GBRT se basa en el principio de **boosting**, donde cada nuevo √°rbol en el conjunto se entrena para **corregir los errores residuales** (la diferencia entre los valores reales y las predicciones acumuladas del modelo hasta ese momento) de los √°rboles construidos en las iteraciones anteriores. Este proceso es iterativo y aditivo:

1.  **Modelo Inicial:** El proceso comienza con una predicci√≥n inicial simple para todos los datos, a menudo el valor promedio de la variable objetivo.
2.  **C√°lculo de Pseudo-Residuos:** En cada iteraci√≥n, GBRT calcula los "pseudo-residuos", que son los **gradientes negativos de la funci√≥n de p√©rdida** con respecto a la predicci√≥n actual. Para la p√©rdida cuadr√°tica media (com√∫n en regresi√≥n), estos pseudo-residuos son simplemente los errores tradicionales (valor real - predicci√≥n).
3.  **Entrenamiento de un √Årbol de Regresi√≥n:** Se entrena un nuevo **√°rbol de decisi√≥n de regresi√≥n** (que es un "aprendiz d√©bil", a menudo un √°rbol poco profundo o un *decision stump*) para **predecir estos pseudo-residuos**. El √°rbol busca los mejores puntos de divisi√≥n para reducir estos errores.
4.  **Actualizaci√≥n del Modelo:** La predicci√≥n de este nuevo √°rbol de regresi√≥n se a√±ade a la predicci√≥n acumulada del modelo existente, pero se escala por una **tasa de aprendizaje (learning rate)**. Esta tasa de aprendizaje es un hiperpar√°metro crucial que controla la "contribuci√≥n" de cada nuevo √°rbol y ayuda a prevenir el sobreajuste.
5.  **Iteraci√≥n:** Los pasos 2 a 4 se repiten para un n√∫mero predefinido de iteraciones. Cada nuevo √°rbol se enfoca en las deficiencias del modelo combinado anterior, refinando gradualmente la predicci√≥n.

En el contexto del **aprendizaje global vs. local**, GBRT es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol de regresi√≥n individual es un sistema de aprendizaje local que divide el espacio de caracter√≠sticas y aprende patrones en subregiones. Sin embargo, el proceso de boosting, al combinar estos √°rboles secuencialmente para reducir los errores residuales globales, construye una **aproximaci√≥n de funci√≥n global** altamente flexible y precisa. La clave es que, si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n de manera muy efectiva a trav√©s de esta **regresi√≥n ponderada localmente**. Al centrarse en los errores que el modelo actual no puede explicar, GBRT aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Es excepcionalmente potente para capturar relaciones complejas y no lineales, y es ampliamente utilizado en diversas aplicaciones, desde la predicci√≥n de precios hasta la optimizaci√≥n de rutas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para ciertas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones complejas mediante boosting de √°rboles",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers pueden influir en √°rboles individuales)",
  "‚úÖ Robusto (los √°rboles reducen el impacto de colinealidad localmente)",
  "‚ö†Ô∏è Baja (modelo en su conjunto es ‚Äúcaja negra‚Äù)",
  "‚ö†Ô∏è Lento con muchos √°rboles o datos extensos",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajuste de hiperpar√°metros",
  "‚ùå No es ideal si hay muy pocos datos o ruido excesivo"
)

detalles <- c(
  "Ensamble de √°rboles de regresi√≥n secuenciales donde cada √°rbol corrige errores del anterior mediante gradiente.",
  "Predice valores continuos sumando las predicciones ponderadas de m√∫ltiples √°rboles d√©biles.",
  "Funciona con variables mixtas; las categ√≥ricas suelen transformarse en dummies.",
  "Cada nuevo √°rbol se enfoca en los residuos del modelo anterior, capturando patrones complejos.",
  "No impone distribuci√≥n normal porque optimiza una funci√≥n de p√©rdida (por ejemplo, MSE) directamente.",
  "Mejor si las observaciones no est√°n correlacionadas en el tiempo; ajustar para series si es necesario.",
  "No requiere varianza constante puesto que se basa en √°rboles, no en un modelo param√©trico de errores.",
  "Los valores extremos pueden provocar ajustes excesivos en √°rboles individuales; usar tasa de aprendizaje baja ayuda a mitigar.",
  "Los √°rboles reducen el impacto de variables correlacionadas, aunque m√∫ltiples iteraciones pueden complicar interpretaciones.",
  "Dif√≠cil de interpretar directamente; se puede usar importancia de variables o herramientas como SHAP para explicaci√≥n.",
  "Cada iteraci√≥n entrena un √°rbol nuevo; muchos √°rboles o gran profundidad de √°rbol incrementan el tiempo de entrenamiento.",
  "CV ayuda a determinar par√°metros como tasa de aprendizaje (`learning_rate`), n√∫mero de √°rboles (`n.trees`) y profundidad m√°xima (`max_depth`).",
  "No es adecuado cuando el dataset es muy peque√±o o extremadamente ruidoso, ya que puede sobreajustar f√°cilmente."
)

tabla_gbrt <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_gbrt %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GBRT",
             subtitle = "Gradient Boosted Regression Trees (GBRT)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Isolation Forest {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Isolation Forest.png"))
```

## Light Gradient Boosting Machine (LightGBM)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/LightGBM.png"))
```

**LightGBM (Light Gradient Boosting Machine)** es otro algoritmo de **Gradient Boosting Machines (GBM)** de alto rendimiento, desarrollado por Microsoft. Est√° dise√±ado para ser **extremadamente r√°pido** y **eficiente** en el uso de memoria, especialmente con grandes conjuntos de datos, sin sacrificar una precisi√≥n significativa. Al igual que XGBoost, ha ganado popularidad en competiciones de *machine learning* por su velocidad y capacidad para manejar grandes vol√∫menes de datos.

La idea fundamental de LightGBM es la misma que la de otros algoritmos de boosting: construir un modelo aditivo de forma **secuencial**, donde cada nuevo √°rbol intenta corregir los errores residuales del modelo combinado anterior. Sin embargo, LightGBM introduce varias optimizaciones clave para lograr su notable eficiencia:

1.  **Gradient-based One-Side Sampling (GOSS):** A diferencia de XGBoost que usa todas las instancias para cada iteraci√≥n, GOSS se enfoca en las instancias que tienen un **mayor gradiente** (es decir, las que contribuyen m√°s al error). Descarta las instancias con gradientes peque√±os o las muestrea con menos frecuencia, lo que acelera el entrenamiento sin perder demasiada precisi√≥n.
2.  **Exclusive Feature Bundling (EFB):** EFB agrupa caracter√≠sticas mutuamente exclusivas (es decir, caracter√≠sticas que rara vez toman valores distintos de cero al mismo tiempo) en un solo "bundle". Esto reduce el n√∫mero de caracter√≠sticas y acelera el c√°lculo del histograma sin afectar la precisi√≥n.
3.  **Histogram-based Algorithm:** En lugar de construir √°rboles en una forma de pre-orden que es com√∫n en muchos algoritmos (lo que puede ser lento al enumerar todos los puntos de divisi√≥n), LightGBM utiliza un **algoritmo basado en histogramas**. Convierte los valores de las caracter√≠sticas continuas en *bins* discretos. Esto acelera significativamente el proceso de b√∫squeda del mejor punto de divisi√≥n.
4.  **Leaf-wise (Best-first) Tree Growth:** A diferencia de la mayor√≠a de los √°rboles de decisi√≥n que crecen nivel por nivel (como en XGBoost), LightGBM crece el √°rbol **"hoja por hoja" (leaf-wise)**. Esto significa que en cada paso, selecciona la hoja con la mayor reducci√≥n de p√©rdida y la divide. Este enfoque puede llevar a √°rboles m√°s profundos y asim√©tricos que pueden ser m√°s precisos para el mismo n√∫mero de nodos, aunque puede ser m√°s propenso al sobreajuste (lo cual se mitiga con la regularizaci√≥n).

En el contexto del **aprendizaje global vs. local**, LightGBM, al igual que otros algoritmos de boosting, es una estrategia de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol que se entrena es un "aprendiz d√©bil" que se enfoca en las deficiencias residuales del modelo acumulado. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) de manera muy eficiente mediante esta **regresi√≥n ponderada localmente**. Al centrarse en los errores y optimizar los c√°lculos, LightGBM aborda de manera sobresaliente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su √©nfasis en la velocidad y la eficiencia lo hace ideal para conjuntos de datos muy grandes o escenarios donde el tiempo de entrenamiento es una preocupaci√≥n cr√≠tica.  



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n apropiada)",
  "‚úÖ Captura no linealidades e interacciones mediante √°rboles en boosting",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (los outliers pueden influir en pesos de hojas)",
  "‚úÖ Robusto (usa histogram-based split que aten√∫a colinealidad)",
  "‚ö†Ô∏è Baja (modelo complejo tipo ‚Äòcaja negra‚Äô)",
  "‚úÖ Muy r√°pido y escalable (optimized gradient-based)",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajustar hiperpar√°metros",
  "‚ùå No conviene con datos muy peque√±os o muy ruidosos sin regularizaci√≥n"
)

detalles <- c(
  "Ensamble supervisado que entrenan √°rboles de decisi√≥n usando histogram-based gradient boosting.",
  "En regresi√≥n predice valores continuos; en clasificaci√≥n maximiza log-loss u otras funciones objetivo.",
  "Acepta variables mixtas; las categ√≥ricas deben convertirse a formato num√©rico o usar encoding interno.",
  "Cada iteraci√≥n ajusta un √°rbol enfoc√°ndose en los residuos del anterior, capturando patrones complejos.",
  "No impone distribuci√≥n param√©trica de errores; optimiza la funci√≥n de p√©rdida directamente.",
  "Funciona mejor si las muestras son independientes; sensible a series de tiempo sin preparaci√≥n adecuada.",
  "No requiere varianza constante, dado que es un m√©todo basado en √°rbol, no en supuestos de error.",
  "Los valores extremos pueden afectar el c√°lculo de gradientes y splits; usar regularizaci√≥n y par√°metros de manejo de outliers.",
  "La divisi√≥n basada en histogramas reduce el impacto de predictores altamente correlacionados.",
  "Dif√≠cil interpretar cada √°rbol; se utilizan m√©tricas de importancia y herramientas como SHAP para explicaci√≥n.",
  "Implementaci√≥n en C++ altamente optimizada que permite entrenamiento muy r√°pido incluso con grandes vol√∫menes de datos.",
  "CV ayuda a elegir par√°metros como `learning_rate`, `num_leaves`, `max_depth`, `feature_fraction`, `bagging_fraction`.",
  "No es ideal si el dataset es muy peque√±o, pues el boosting puede sobreajustar; tampoco con mucho ruido sin regularizaci√≥n adecuada."
)

tabla_lightgbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lightgbm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LightGBM",
             subtitle = "LightGBM")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Random Forest  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Bagging.png"))
```

**Random Forest** es un algoritmo de **aprendizaje conjunto (ensemble learning)** altamente popular y potente, utilizado tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**. Fue desarrollado por Leo Breiman en 2001 y se basa en la idea de combinar las predicciones de m√∫ltiples **√°rboles de decisi√≥n** para lograr una mayor precisi√≥n y robustez que un solo √°rbol. La fuerza de Random Forest reside en dos conceptos clave: **bagging (bootstrap aggregation)** y la **aleatoriedad en la selecci√≥n de caracter√≠sticas**.

La idea fundamental detr√°s de Random Forest es construir un "bosque" de √°rboles de decisi√≥n de una manera espec√≠fica:

1.  **Bagging (Bootstrap Aggregation):** En lugar de entrenar un solo √°rbol en todo el conjunto de datos, Random Forest entrena cada √°rbol en una **muestra de arranque (bootstrap sample)** diferente. Una muestra de arranque es un subconjunto del conjunto de datos original, muestreado con reemplazo. Esto significa que algunos puntos de datos pueden aparecer varias veces en una muestra, mientras que otros pueden no aparecer en absoluto. Este muestreo genera diversidad entre los √°rboles.

2.  **Aleatoriedad en la Selecci√≥n de Caracter√≠sticas:** Cuando cada √°rbol se construye, en cada paso de divisi√≥n (nodo), Random Forest no considera todas las caracter√≠sticas disponibles. En cambio, solo considera un **subconjunto aleatorio de caracter√≠sticas** para encontrar la mejor divisi√≥n. Esta aleatoriedad adicional (adem√°s del muestreo de arranque) descorrelaciona a√∫n m√°s los √°rboles, lo que es crucial para el rendimiento del algoritmo. Si los √°rboles estuvieran altamente correlacionados, el error de un √°rbol promedio no se reducir√≠a al promediar.

Una vez que se han construido numerosos √°rboles (t√≠picamente cientos o miles), las predicciones se combinan: para **clasificaci√≥n**, se utiliza la **votaci√≥n por mayor√≠a** (la clase m√°s votada por los √°rboles individuales); para **regresi√≥n**, se calcula el **promedio** de las predicciones de todos los √°rboles.

En el contexto del **aprendizaje global vs. local**, Random Forest se puede considerar como un sistema de **aprendizaje global** que se construye a partir de componentes de **aprendizaje local**. Cada √°rbol individual en el bosque es un sistema de aprendizaje local (como CART, que divide el problema en subproblemas m√°s peque√±os). Sin embargo, al combinar las predicciones de muchos de estos √°rboles, Random Forest logra una **aproximaci√≥n de funci√≥n global** muy robusta y flexible. La ventaja es que, si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) mediante una forma sofisticada de **regresi√≥n ponderada localmente**. La combinaci√≥n de √°rboles diversos y descorrelacionados mitiga la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Random Forest sobresale en capturar relaciones complejas y no lineales, manejar grandes conjuntos de datos con muchas caracter√≠sticas y es menos propenso al sobreajuste que un solo √°rbol de decisi√≥n grande.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n)",
  "‚úÖ Captura relaciones no lineales e interacciones complejas",
  "‚ùå No requiere",
  "‚úÖ Deseable pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚úÖ Robusto a outliers (por agregaci√≥n)",
  "‚úÖ Robusto (selecciona subconjuntos aleatorios)",
  "‚ö†Ô∏è Moderada (dif√≠cil interpretar cientos de √°rboles)",
  "‚ö†Ô∏è Lento con muchos √°rboles o datos grandes",
  "‚úÖ Recomendado usar k-fold",
  "‚ùå Puede sobreajustar si no se ajustan hiperpar√°metros (e.g. profundidad, n√∫mero de √°rboles)"
)

detalles <- c(
  "Ensamble de √°rboles de decisi√≥n, cada uno entrenado en una muestra bootstrap y usando un subconjunto aleatorio de predictores.",
  "En clasificaci√≥n predice la clase mayoritaria entre √°rboles; en regresi√≥n, el promedio de predicciones.",
  "Acepta muchas variables y selecciona autom√°ticamente las m√°s relevantes por importancia.",
  "Al generar m√∫ltiples √°rboles, capta interacciones no lineales sin necesidad de especificarlas.",
  "No hay supuestos sobre la distribuci√≥n de los errores.",
  "Los √°rboles individuales pueden manejar correlaci√≥n leve; el ensamble mitiga la dependencia.",
  "No necesita homogeneidad de varianza en los errores residuales.",
  "Cada √°rbol es poco sensible a outliers, y la agregaci√≥n mejora robustez.",
  "Reduce el problema de colinealidad al seleccionar subconjuntos de variables por √°rbol.",
  "Es dif√≠cil de explicar, aunque se pueden usar m√©tricas de importancia de variables.",
  "Puede volverse lento si se entrenan miles de √°rboles en datasets muy grandes.",
  "Cross-validation ayuda a evitar overfitting y evaluar generalizaci√≥n.",
  "No es ideal si el interpretabilidad es cr√≠tica o el tiempo computacional es limitado."
)

tabla_rf <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rf %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir random forest",
             subtitle = "Random Forest") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Stacked Generlization (Blending) {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Ensemble/Blending.png"))
```

**Stacked Generalization**, com√∫nmente conocido como **Stacking**, y su variante **Blending**, son t√©cnicas avanzadas de **aprendizaje conjunto (ensemble learning)** que buscan combinar las predicciones de m√∫ltiples modelos de aprendizaje autom√°tico para obtener un rendimiento predictivo superior al de cualquier modelo individual. La idea fundamental es que, en lugar de simplemente promediar o votar las predicciones, se entrena un **modelo de segundo nivel (meta-modelo)** para aprender a combinar √≥ptimamente las predicciones de los modelos de primer nivel (modelos base).

El proceso de Stacking generalmente implica dos o m√°s "capas" de modelos:

1.  **Modelos Base (Nivel 0):** En la primera capa, se entrenan m√∫ltiples modelos de aprendizaje autom√°tico diversos (pueden ser de diferentes tipos, como √°rboles de decisi√≥n, m√°quinas de vectores de soporte, redes neuronales, etc.). Estos modelos base se entrenan sobre el conjunto de datos de entrenamiento original (o en particiones del mismo).

2.  **Generaci√≥n de Meta-Caracter√≠sticas:** Las predicciones generadas por estos modelos base sobre un conjunto de datos "fuera de muestra" (que no se us√≥ para entrenar los modelos base, t√≠picamente a trav√©s de validaci√≥n cruzada k-fold) se utilizan como **nuevas caracter√≠sticas** o "meta-caracter√≠sticas". Estas meta-caracter√≠sticas, junto con la variable objetivo original, forman un nuevo conjunto de datos de entrenamiento para el meta-modelo.

3.  **Meta-Modelo (Nivel 1):** En la segunda capa, se entrena un **meta-modelo** (a menudo un modelo m√°s simple, como regresi√≥n lineal, regresi√≥n log√≠stica o un √°rbol de decisi√≥n poco profundo) utilizando estas meta-caracter√≠sticas como entrada y la variable objetivo original como salida. El meta-modelo aprende la relaci√≥n entre las predicciones de los modelos base y la respuesta verdadera, y por lo tanto, c√≥mo "pesar" o "combinar" esas predicciones de la mejor manera.

**Blending** es una variaci√≥n m√°s sencilla de Stacking. La principal diferencia es c√≥mo se generan las meta-caracter√≠sticas para el meta-modelo. En Blending, se reserva una **subdivisi√≥n de validaci√≥n (holdout set)** del conjunto de entrenamiento original. Los modelos base se entrenan en la parte restante del conjunto de entrenamiento, y luego sus predicciones sobre este conjunto de validaci√≥n se utilizan directamente como meta-caracter√≠sticas para entrenar el meta-modelo. Esto simplifica el proceso de validaci√≥n cruzada, pero el meta-modelo se entrena con menos datos.

En el contexto del **aprendizaje global vs. local**, Stacking/Blending es una estrategia de **aprendizaje global** que explota el poder de m√∫ltiples **aproximaciones de funci√≥n local** (los modelos base) para construir un modelo final altamente sofisticado. Cada modelo base, dependiendo de su naturaleza, puede ser un sistema de aprendizaje local que descubre patrones en subregiones de datos. Sin embargo, el meta-modelo aprende una funci√≥n de combinaci√≥n global sobre las predicciones de estos modelos base. Si los datos no se distribuyen linealmente, Stacking/Blending aplica el concepto de regresi√≥n (o clasificaci√≥n) de una manera muy flexible. Al permitir que un modelo de segundo nivel aprenda a combinar las predicciones de diversos modelos, supera la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Es particularmente eficaz en competiciones de machine learning donde se busca el m√°ximo rendimiento, ya que aprovecha las fortalezas complementarias de diferentes algoritmos. Sin embargo, puede ser computacionalmente intensivo y m√°s dif√≠cil de interpretar que los modelos individuales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua (depende de los modelos base)",
  "‚úÖ Num√©ricas y/o categ√≥ricas (codificaci√≥n seg√∫n modelos base)",
  "‚úÖ Captura relaciones complejas v√≠a combinaci√≥n de modelos base",
  "‚ùå No exige supuestos de normalidad en residuos",
  "‚úÖ Deseable, pero no obligatorio (mejor si observaciones independientes)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers afectan modelos base individuales)",
  "‚ö†Ô∏è Puede verse afectado (depende de base learners y correlated features)",
  "‚ö†Ô∏è Baja (modelo meta dif√≠cil de interpretar directamente)",
  "‚ö†Ô∏è Lento en entrenamiento y predicci√≥n, seg√∫n n√∫mero de base learners",
  "‚úÖ Esencial (usar CV anidada para entrenar meta-modelo)",
  "‚ùå Si datos muy escasos o muy ruidosos, riesgo de sobreajuste"
)

detalles <- c(
  "Ensamble supervisado que combina varias predicciones (base learners) mediante un modelo meta.",
  "El meta-modelo acepta la salida de modelos base; puede predecir clases o valores continuos.",
  "Usa predictores originales para los base learners; algunos requieren dummies, otros no.",
  "Aprende patrones no lineales e interacciones complejas a trav√©s de m√∫ltiples capas.",
  "No impone distribuci√≥n normal: cada base learner tiene sus propios supuestos.",
  "Ideal si cada muestra es independiente; sensibles a dependencias en validaciones cruzadas.",
  "No requiere varianza constante, ya que se basa en agregaci√≥n de predicciones.",
  "Modelos base (p. ej. ARBOTS, SVM) pueden verse influenciados por valores extremos;",
  "Modelos base diversificados reducen colinealidad, pero meta-modelo puede verse afectado.",
  "Dif√≠cil atribuir importancia directa; se pueden usar t√©cnicas como SHAP para interpretaci√≥n.",
  "Entrenamiento de m√∫ltiples base learners y meta-modelo incrementa tiempo; predicci√≥n tambi√©n m√°s lenta.",
  "Usar validaci√≥n cruzada anidada: inner folds para entrenar base learners y stacking, outer folds para evaluar.",
  "No recomendable si hay muy pocos datos (stacking requiere dividir en folds) o si los base learners no aportan diversidad."
)

tabla_stacking <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_stacking %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir blending",
             subtitle = "Stacked Generlizaation (Blending)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```







<!--chapter:end:03-ensemble.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üß† 4. Redes Neuronales {-}  

**Ejemplos:** MLP, CNN, RNN, Transformers.  
**Uso:** Perfectas para **im√°genes** (CNN), **texto** (Transformers) y **series temporales** (RNN/LSTM), especialmente con **grandes vol√∫menes de datos no estructurados**.  
**Ventajas:** Muy poderosas para datos complejos.  
**Limitaciones:** Requieren **mucha data** y **computaci√≥n**, y tienen **menor interpretabilidad**.  

---

## Autoenconder  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Autoenconder.png"))
```


Un **Autoencoder** es un tipo de **red neuronal artificial** dise√±ado para aprender una **representaci√≥n (o codificaci√≥n) eficiente y comprimida** de los datos de entrada, sin supervisi√≥n humana. Su objetivo principal es la **reducci√≥n de dimensionalidad** o el **aprendizaje de caracter√≠sticas**, lo que lo hace √∫til para tareas como la detecci√≥n de anomal√≠as, la denoising de im√°genes, o la generaci√≥n de datos.

La arquitectura b√°sica de un Autoencoder se compone de dos partes principales:

1.  **Encoder (Codificador):** Esta parte de la red toma los datos de entrada y los transforma en una representaci√≥n de menor dimensi√≥n, a menudo llamada **c√≥digo, representaci√≥n latente, o cuello de botella (bottleneck)**. Es decir, comprime la informaci√≥n esencial de la entrada.
2.  **Decoder (Decodificador):** Esta parte toma la representaci√≥n comprimida (el c√≥digo) del encoder y la reconstruye de nuevo a la dimensi√≥n original de los datos de entrada.

El Autoencoder se entrena para **minimizar la diferencia entre la entrada original y su reconstrucci√≥n** generada por el decoder. Esta diferencia se mide a trav√©s de una **funci√≥n de p√©rdida de reconstrucci√≥n** (como el error cuadr√°tico medio para datos continuos o la entrop√≠a cruzada para datos binarios). Al forzar a la red a reconstruir su propia entrada a partir de una representaci√≥n comprimida, el Autoencoder aprende las caracter√≠sticas m√°s salientes y √∫tiles de los datos de forma no supervisada.

Existen varias variantes de Autoencoders, como los **Autoencoders Denoising** (que aprenden a reconstruir datos limpios a partir de datos con ruido), los **Autoencoders Variacionales (VAEs)** (que aprenden una distribuci√≥n probabil√≠stica de la representaci√≥n latente, √∫tiles para la generaci√≥n de datos), y los **Autoencoders Convolucionales** (que usan capas convolucionales, ideales para im√°genes).


**Aprendizaje Global vs. Local:**

Un Autoencoder se considera principalmente un modelo de **aprendizaje global**, aunque con una perspectiva √∫nica debido a su naturaleza de compresi√≥n y reconstrucci√≥n.

* **Aspecto Global:** Un Autoencoder aprende una **transformaci√≥n global** de los datos. El encoder aprende a mapear todo el espacio de entrada a un espacio de representaci√≥n latente, y el decoder aprende a mapear ese espacio latente de vuelta al espacio de salida. Las ponderaciones y sesgos de la red se ajustan para encontrar esta transformaci√≥n que funciona de manera √≥ptima para todo el conjunto de datos de entrenamiento, permitiendo la reconstrucci√≥n m√°s fiel posible en general. La **funci√≥n de p√©rdida de reconstrucci√≥n** se minimiza a nivel de todo el conjunto de datos, no solo en vecindarios espec√≠ficos.

* **Representaci√≥n Local vs. Reconstrucci√≥n Global:** Aunque el objetivo final es una reconstrucci√≥n global de la entrada, la **representaci√≥n latente (el c√≥digo)** puede verse como una forma de capturar **caracter√≠sticas o patrones importantes** que, en cierto sentido, resumen la informaci√≥n "local" o particular de cada instancia de datos de una manera comprimida. Sin embargo, la forma en que estas caracter√≠sticas se aprenden y se utilizan para la reconstrucci√≥n se rige por un conjunto global de par√°metros de la red. No se entrena un modelo separado para cada vecindario de datos, sino una √∫nica red que aprende una funci√≥n de mapeo para todo el dominio.

En resumen, el Autoencoder aprende una representaci√≥n eficiente y una capacidad de reconstrucci√≥n que se aplica de manera consistente a todos los datos, lo que lo clasifica como un modelo de aprendizaje global que busca una soluci√≥n unificada para el problema de la codificaci√≥n y decodificaci√≥n de datos. 

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (aprendizaje no supervisado)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (o categ√≥ricas codificadas)",
  "‚úÖ Captura relaciones complejas y no lineales",
  "‚ùå No aplica (no es un modelo de regresi√≥n)",
  "‚ùå No aplica (no hay errores residuales)",
  "‚ùå No aplica",
  "‚úÖ S√≠, pueden afectar la reconstrucci√≥n",
  "‚úÖ Puede ayudar a reducir efectos de multicolinealidad",
  "‚ö†Ô∏è Baja interpretabilidad (representaciones latentes)",
  "‚ö†Ô∏è Lento en entrenamiento, especialmente con muchas capas o datos",
  "‚ö†Ô∏è Se puede validar con reconstrucci√≥n y autoevaluaci√≥n",
  "‚ùå Datos con mucha dispersi√≥n o sin estructura latente clara"
)

detalles <- c(
  "Red neuronal no supervisada que aprende a codificar y decodificar los datos para reducir dimensionalidad o detectar anomal√≠as.",
  "No predice una variable externa, sino que reproduce la entrada como salida.",
  "Requiere variables num√©ricas (o una codificaci√≥n previa en caso de categ√≥ricas).",
  "Es capaz de capturar estructuras complejas y no lineales al comprimir los datos.",
  "No tiene residuos como un modelo cl√°sico, pero s√≠ errores de reconstrucci√≥n.",
  "No modela errores independientes, ya que no es un modelo predictivo tradicional.",
  "Tampoco se eval√∫a homoscedasticidad, ya que no hay predicci√≥n como tal.",
  "Outliers distorsionan el entrenamiento, especialmente si no se normaliza.",
  "Ayuda a eliminar redundancias en los datos si est√°n correlacionados.",
  "Las capas internas (representaciones) no son directamente interpretables.",
  "Requiere entrenamiento con varias iteraciones y puede tardar con arquitecturas grandes.",
  "Se eval√∫a con p√©rdida de reconstrucci√≥n o aplicando validaci√≥n cruzada si se integra en modelos supervisados.",
  "Pierde eficacia si los datos no tienen una estructura latente √∫til para codificar."
)

tabla_autoencoder <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_autoencoder %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir autoencoder",
             subtitle = "Autoenconder")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Back - Propagation  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Back - Propagation.png"))
```


**Back-Propagation (Retropropagaci√≥n)** es el algoritmo fundamental de entrenamiento utilizado para ajustar los pesos de las **redes neuronales artificiales multicapa (MLP)**. La idea central de Back-Propagation es calcular la **contribuci√≥n de cada peso al error global de la red** y luego ajustar esos pesos para reducir dicho error, propagando la informaci√≥n del error "hacia atr√°s" desde la capa de salida hasta la capa de entrada.

A diferencia del Perceptron, que solo puede aprender patrones linealmente separables, Back-Propagation permite entrenar redes neuronales profundas con m√∫ltiples capas ocultas y funciones de activaci√≥n no lineales, lo que les permite modelar relaciones complejas y no lineales en los datos.

El funcionamiento de Back-Propagation se divide en dos fases principales que se repiten iterativamente:

1.  **Fase de Propagaci√≥n hacia Adelante (Forward Pass):**
    * Las entradas se pasan a trav√©s de la red, desde la capa de entrada, a trav√©s de las capas ocultas, hasta la capa de salida.
    * En cada neurona, se calcula la suma ponderada de sus entradas (incluido el sesgo) y se aplica la funci√≥n de activaci√≥n (ej. sigmoide, tanh, ReLU) para producir la salida de esa neurona.
    * La salida final de la red se compara con el valor objetivo real para calcular el **error global** (o "costo") de la red, utilizando una funci√≥n de p√©rdida (ej. error cuadr√°tico medio para regresi√≥n, entrop√≠a cruzada para clasificaci√≥n).

2.  **Fase de Retropropagaci√≥n (Backward Pass):**
    * El error global se propaga **hacia atr√°s** desde la capa de salida, a trav√©s de las capas ocultas, hasta la capa de entrada.
    * En cada capa, se calcula el **gradiente** del error con respecto a los pesos de las conexiones de esa capa. Esto implica el uso de la **regla de la cadena** del c√°lculo diferencial para determinar cu√°nto contribuye cada peso al error final.
    * Una vez calculados los gradientes, los pesos de la red se **actualizan** en la direcci√≥n opuesta al gradiente (es decir, en la direcci√≥n de mayor descenso) para reducir el error. Esta actualizaci√≥n se realiza con una **tasa de aprendizaje** que controla el tama√±o del paso.
    $$w_{ij}^{\text{nuevo}} = w_{ij}^{\text{anterior}} - \alpha \cdot \frac{\partial E}{\partial w_{ij}}$$
    Donde $E$ es el error, $w_{ij}$ es el peso de la conexi√≥n entre la neurona $i$ y la neurona $j$, y $\alpha$ es la tasa de aprendizaje.

En el contexto del **aprendizaje global vs. local**, Back-Propagation es el coraz√≥n del entrenamiento de sistemas de **aprendizaje global** por excelencia (las redes neuronales multicapa). La red neuronal busca aprender una **aproximaci√≥n de funci√≥n global** que mapee las entradas a las salidas, minimizando el error en todo el conjunto de datos. Si los datos no se distribuyen linealmente, Back-Propagation permite que la red aprenda relaciones no lineales complejas a trav√©s de sus m√∫ltiples capas y funciones de activaci√≥n no lineales. A diferencia de LOESS o los m√©todos de regresi√≥n ponderada localmente, Back-Propagation no divide expl√≠citamente el problema en m√∫ltiples problemas locales independientes para minimizar funciones de costo locales. En cambio, busca minimizar una funci√≥n de p√©rdida **global** para toda la red. Sin embargo, su capacidad para ajustar un gran n√∫mero de par√°metros (pesos) le permite construir representaciones internas de los datos que pueden ser incre√≠blemente flexibles y adaptables, superando la limitaci√≥n de que "a veces ning√∫n valor de par√°metro [en un modelo simple] puede proporcionar una aproximaci√≥n suficientemente buena". La retropropagaci√≥n es lo que permiti√≥ a las redes neuronales convertirse en poderosas herramientas de aprendizaje autom√°tico.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas (requiere normalizar), Categ√≥ricas como dummies",
  "‚úÖ Captura relaciones no lineales profundas",
  "‚ùå No requiere",
  "‚úÖ Deseable, aunque no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Elevado (puede requerir robustez ante valores extremos)",
  "‚ö†Ô∏è Puede ralentizar convergencia si es muy alta",
  "‚ö†Ô∏è Baja (modelo de ‚Äúcaja negra‚Äù)",
  "‚ö†Ô∏è Depende de la arquitectura y tama√±o del dataset",
  "‚úÖ Recomendable para ajustar tasas de aprendizaje, capas y neuronas",
  "‚ùå No conviene con datos muy peque√±os o alta dimensionalidad sin regularizar"
)

detalles <- c(
  "Algoritmo para entrenar redes neuronales multicapa ajustando pesos por retropropagaci√≥n del error.",
  "En clasificaci√≥n usa softmax o sigmoide en salida; en regresi√≥n, capa lineal para valor continuo.",
  "Debe escalarse cada caracter√≠stica; las categ√≥ricas transformarse a variables indicadoras antes de entrenar.",
  "Aprende funciones arbitrariamente complejas activando m√∫ltiples capas ocultas con funciones no lineales.",
  "No impone distribuci√≥n espec√≠fica en errores, se optimiza v√≠a descenso de gradiente.",
  "Mejor si las observaciones son independientes; sensible a secuencias sin ajustes espec√≠ficos.",
  "No requiere varianza constante, ya que los pesos se ajustan adaptativamente durante el entrenamiento.",
  "Valores extremos pueden causar activaciones saturadas (vanishing/exploding gradients) si no se manejan.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; Batch Normalization ayuda a mitigar.",
  "Dif√≠cil interpretar cada peso individual; se usan t√©cnicas como LIME o SHAP para explicar decisiones.",
  "El tiempo crece con n√∫mero de capas, neuronas y epochs; GPUs aceleran el proceso.",
  "Cross-validation (o k-fold) ayuda a elegir n√∫mero de capas, neuronas por capa, tasa de aprendizaje y regularizaci√≥n.",
  "No funciona bien con datasets peque√±os (overfitting f√°cil) o ruido elevado sin t√©cnicas de regularizaci√≥n."
)

tabla_backprop <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_backprop %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir back-propagation",
             subtitle = "Back - Propagation")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Convolutional Neural Network (CNN)  {-}   


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/CNN.png"))
```

**Convolutional Neural Networks (CNNs)**, tambi√©n conocidas como **ConvNets**, son una clase especializada de **redes neuronales profundas** que han demostrado ser excepcionalmente efectivas en tareas de **visi√≥n por computadora** (como clasificaci√≥n de im√°genes, detecci√≥n de objetos, reconocimiento facial) y, m√°s recientemente, en procesamiento de lenguaje natural. La idea fundamental de una CNN es imitar el funcionamiento del c√≥rtex visual en el cerebro humano, utilizando **capas de convoluci√≥n** para detectar autom√°ticamente patrones y caracter√≠sticas jer√°rquicas directamente de los datos de entrada sin necesidad de una extracci√≥n manual de caracter√≠sticas.

A diferencia de los Multilayer Perceptrons (MLPs) que conectan cada neurona de una capa con cada neurona de la siguiente capa (lo que resulta en una enorme cantidad de par√°metros para datos de alta dimensi√≥n como im√°genes), las CNNs aprovechan tres ideas arquitect√≥nicas clave:

1.  **Capas de Convoluci√≥n:** Estas capas aplican un peque√±o conjunto de **filtros (kernels)** a la entrada (ej., una imagen). Cada filtro "se desliza" por la entrada (operaci√≥n de convoluci√≥n) y calcula un producto punto entre sus valores y los valores de la regi√≥n de la entrada que est√° cubriendo. Esto genera un **mapa de caracter√≠sticas** que resalta la presencia de patrones espec√≠ficos (bordes, texturas, formas) en diferentes ubicaciones de la entrada. La ventaja es que los mismos filtros se aplican en m√∫ltiples ubicaciones, lo que reduce dr√°sticamente el n√∫mero de par√°metros y captura la **localidad** de los patrones y la **invarianza traslacional**.
2.  **Capas de Pooling (Submuestreo):** Estas capas se insertan peri√≥dicamente entre las capas convolucionales. Su funci√≥n es reducir la dimensionalidad espacial de los mapas de caracter√≠sticas (ej., reduciendo el n√∫mero de p√≠xeles), lo que ayuda a hacer que el modelo sea m√°s robusto a peque√±as variaciones o distorsiones en la posici√≥n de las caracter√≠sticas. Las operaciones comunes son el **max pooling** (tomar el valor m√°ximo de una regi√≥n) o el **average pooling** (tomar el promedio).
3.  **Capas Totalmente Conectadas (Dense):** Despu√©s de varias capas convolucionales y de pooling, los mapas de caracter√≠sticas finales se aplanan en un vector y se conectan a una o m√°s capas totalmente conectadas (similares a las de un MLP). Estas capas finales realizan la clasificaci√≥n o regresi√≥n bas√°ndose en las caracter√≠sticas de alto nivel extra√≠das por las capas anteriores.

El entrenamiento de una CNN se realiza utilizando el algoritmo de **Back-Propagation** y descenso de gradiente (con sus variantes como SGD, Adam, etc.), ajustando los pesos de los filtros y las conexiones de las capas densas para minimizar una funci√≥n de p√©rdida.

En el contexto del **aprendizaje global vs. local**, las CNNs son un ejemplo sobresaliente de un sistema de **aprendizaje global** que, en sus capas iniciales, se beneficia de la detecci√≥n de patrones **locales**. Cada filtro de convoluci√≥n aprende a detectar un patr√≥n local espec√≠fico (un borde vertical, una esquina, etc.) que se repite en diferentes partes de la imagen (lo que es una forma de "regresi√≥n ponderada localmente" en el sentido de que el filtro "aplica" su conocimiento local a diferentes ventanas de entrada). Sin embargo, la combinaci√≥n jer√°rquica de m√∫ltiples capas convolucionales y de pooling, seguida de capas totalmente conectadas, permite que la red construya representaciones cada vez m√°s abstractas y globales del contenido de la imagen. Esto significa que si los datos no se distribuyen linealmente, las CNNs pueden aprender a modelar relaciones extremadamente complejas y no lineales al componer caracter√≠sticas locales en representaciones globales. La arquitectura de CNNs resuelve la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos m√°s simples al permitir que la red aprenda caracter√≠sticas relevantes de forma autom√°tica y jer√°rquica, adapt√°ndose a las complejidades inherentes de datos como im√°genes y videos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Im√°genes (matrices de p√≠xeles) y datos con estructura espacial",
  "‚úÖ Captura relaciones locales y espaciales mediante filtros convolucionales",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias son independientes)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (artefactos o ruido en im√°genes puede afectar)",
  "‚ö†Ô∏è No se eval√∫a colinealidad de predictores, maneja correlaciones espaciales",
  "‚ö†Ô∏è Baja (modelo tipo ‚Äòcaja negra‚Äô, usar t√©cnicas como Grad-CAM para interpretaci√≥n)",
  "‚ö†Ô∏è Lento sin GPU, entrenamiento intensivo en c√≥mputo",
  "‚úÖ Robusto si se aplica k-fold o validaci√≥n en conjunto de im√°genes",
  "‚ùå No funciona bien con pocos datos o sin estructura espacial significativa"
)

detalles <- c(
  "Red neuronal profunda especializada en procesar datos con estructura de grilla (ej. im√°genes).",
  "En clasificaci√≥n utiliza softmax; en regresi√≥n, capa lineal para valores continuos.",
  "Requiere tensores de entrada (canales, altura, ancho); funciones de preprocesamiento para im√°genes.",
  "Filtros convolucionales extraen caracter√≠sticas locales, max-pooling disminuye dimensionalidad manteniendo informaci√≥n relevante.",
  "No impone ninguna distribuci√≥n en los errores, optimiza funci√≥n de p√©rdida directamente.",
  "Ideal si las muestras son independientes; sensible a dependencias temporales o espaciales no modeladas.",
  "No requiere varianza constante pues se basa en convoluciones y pooling, no en un modelo param√©trico de error.",
  "Ruido o artefactos en p√≠xeles pueden alterar el aprendizaje de filtros, es importante usar t√©cnicas de regularizaci√≥n.",
  "La red aprende filtros que capturan patrones locales, por lo que no es necesario verificar colinealidad expl√≠citamente.",
  "Dif√≠cil de interpretar cada filtro y capa; se utilizan mapas de activaci√≥n o Grad-CAM para entender qu√© regiones influyen en la predicci√≥n.",
  "El entrenamiento con m√∫ltiples capas convolucionales y millones de par√°metros es intensivo en GPU/TPU.",
  "Validaci√≥n cruzada o separaci√≥n de conjuntos (train/validation/test) ayuda a evitar overfitting.",
  "No es apropiado para datasets muy peque√±os sin aumentar datos (data augmentation) o sin informaci√≥n espacial clara."
)

tabla_cnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles) 

tabla_cnn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CNN",
             subtitle = "Convolutional Neural Network (CNN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```


## Hopfield Network  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Hopfield Network.png"))
```

La **Red de Hopfield** es un tipo de **red neuronal recurrente** o **red neuronal con memoria asociativa**, propuesta por John Hopfield en 1982. A diferencia de las redes neuronales de propagaci√≥n hacia adelante (como el Perceptr√≥n o las MLP entrenadas con Back-Propagation) que se utilizan para el mapeo de entrada a salida, la idea fundamental de una Red de Hopfield es funcionar como un **sistema de memoria asociativa** y un **sistema din√°mico que converge a estados estables**. Su objetivo principal es almacenar y recuperar patrones binarios, as√≠ como resolver problemas de optimizaci√≥n.

El funcionamiento de una Red de Hopfield se basa en los siguientes principios:

1.  **Neuronas Binarias:** La red consta de un conjunto de neuronas (nodos) que son **binarias**, lo que significa que solo pueden tomar dos estados posibles, generalmente $1$ o $-1$.
2.  **Conexiones Ponderadas:** Cada neurona est√° conectada a todas las dem√°s neuronas (excepto a s√≠ misma) mediante **conexiones sim√©tricas y ponderadas**. Los pesos de estas conexiones se calculan de manera que los patrones que se quieren "memorizar" se conviertan en **estados de energ√≠a m√≠nima** de la red. La regla de aprendizaje m√°s com√∫n para establecer estos pesos es la **regla de Hebb**: si dos neuronas se activan juntas para un patr√≥n, el peso entre ellas se incrementa.
3.  **Din√°mica de Activaci√≥n:** Cuando se presenta una entrada a la red (que puede ser un patr√≥n ruidoso o incompleto), las neuronas se actualizan de forma as√≠ncrona o s√≠ncrona. La activaci√≥n de cada neurona se recalcula en funci√≥n de la suma ponderada de las activaciones de las otras neuronas a las que est√° conectada.
    $$S_i = \text{sgn}\left(\sum_{j \neq i} W_{ij} S_j\right)$$
    Donde $S_i$ es el estado de la neurona $i$, $W_{ij}$ es el peso entre la neurona $i$ y $j$, y $\text{sgn}$ es la funci√≥n signo.
4.  **Convergencia a Estados Estables:** Este proceso de actualizaci√≥n se repite hasta que la red alcanza un **estado estable** (un "atractor"), donde las activaciones de las neuronas ya no cambian. Si la red ha sido entrenada correctamente, este estado estable corresponder√° al patr√≥n memorizado m√°s cercano a la entrada inicial (memoria asociativa).
5.  **Funci√≥n de Energ√≠a:** La estabilidad de la red se puede describir mediante una **funci√≥n de energ√≠a de Lyapunov**. Durante la din√°mica de la red, la energ√≠a de la red siempre disminuye hasta que se alcanza un m√≠nimo local (un patr√≥n memorizado).

En el contexto del **aprendizaje global vs. local**, la Red de Hopfield es un sistema de **aprendizaje global** que exhibe un comportamiento de **optimizaci√≥n local**. La regla de aprendizaje (como la regla de Hebb) establece los pesos de todas las conexiones para que los patrones deseados se conviertan en m√≠nimos de energ√≠a en todo el espacio de estados. Es decir, se busca una configuraci√≥n global de pesos para memorizar un conjunto de patrones. Sin embargo, la **din√°mica de recuperaci√≥n** de la red es intr√≠nsecamente un proceso de **convergencia local**: dada una entrada inicial, la red "cae" en el m√≠nimo de energ√≠a m√°s cercano, que corresponde al patr√≥n memorizado.

Si los datos no se distribuyen linealmente, la Red de Hopfield no aplica el concepto de regresi√≥n (o clasificaci√≥n) de la misma manera que LOESS o los √°rboles de decisi√≥n. En cambio, funciona como un sistema de **memoria y recuperaci√≥n de patrones** no lineales. Puede almacenar y recuperar patrones complejos que no son linealmente separables. La red busca una soluci√≥n global (un conjunto de pesos) para almacenar los patrones, y luego, en la recuperaci√≥n, utiliza un proceso de "b√∫squeda" local en el espacio de energ√≠a para converger a un patr√≥n memorizado. Esto aborda la idea de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un modelo de regresi√≥n lineal, ya que la Red de Hopfield no es un modelo de regresi√≥n en s√≠, sino un sistema din√°mico que encuentra estados de equilibrio. Su capacidad para manejar patrones ruidosos o incompletos para recuperar el patr√≥n completo es una de sus principales fortalezas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è No es supervisado en el sentido cl√°sico",
  "‚ö†Ô∏è No hay ‚Äútarget‚Äù continuo o categ√≥rico (almacenamiento de patrones)",
  "‚úÖ Variables binarias o valores discretizados (patrones binarios)",
  "‚úÖ Correlaciona patrones con pesos sim√©tricos entre neuronas",
  "‚ùå No aplica (no hay residuos param√©tricos)",
  "‚úÖ Deseable, aunque no obligatorio (los estados deben actualizarse sin bucles no deseados)",
  "‚ùå No aplica (no modela varianza de errores)",
  "‚ö†Ô∏è Muy sensible (un solo nodo saturado puede distorsionar la red)",
  "‚ö†Ô∏è Puede verse afectado si los patrones de entrenamiento tienen redundancia fuerte",
  "‚ö†Ô∏è Media (la din√°mica de atra√ß√£o es interpretable, pero las conexiones pueden ser complejas)",
  "‚ö†Ô∏è Moderada (dependiendo del n√∫mero de neuronas y estados s√≠ncronos/as√≠ncronos)",
  "‚ùå No se usa tradicionalmente, pero se puede validar estabilidad de memorias con pruebas de convergencia",
  "‚ö†Ô∏è No sirve si los patrones no son binarios o si hay alto ruido en entradas asociativas"
)

detalles <- c(
  "Red neuronal recurrente para recuperaci√≥n asociativa de patrones, no requiere pares X‚Üíy.",
  "No predice una variable externa, recupera patrones completos a partir de entradas parciales o ruidosas.",
  "Requiere que cada elemento del patr√≥n sea binario (¬±1) o est√© discretizado; las variables continuas deben binarizarse.",
  "Los pesos sim√©tricos se calculan por Hebb (p. ej. W = Œ£ p·µ¢ p·µ¢·µÄ), sin umbral expl√≠cito para relaciones lineales.",
  "No hay un t√©rmino de error param√©trico; la din√°mica sigue la funci√≥n de energ√≠a, no un residuo gaussiano.",
  "Es mejor si las actualizaciones de estado son independientes o s√≠ncronas; la dependencia temporal puede generar oscilaciones.",
  "No modela varianza de error, pues busca minimizar energ√≠a, no error cuadr√°tico.",
  "Patrones fuera del rango binario pueden causar saturaci√≥n o estados inestables.",
  "Patrones muy similares (colineales) pueden interferir en recuperaciones correctas (atractores vecinos).",
  "La din√°mica de convergencia hacia un estado estable (atractor) se puede visualizar, pero la topolog√≠a de pesos puede no ser transparente.",
  "Simulaci√≥n de din√°micas es razonable para tama√±os moderados (‚â§1000 neuronas); grandes redes requieren optimizaci√≥n paralela.",
  "No se usa CV tradicional; se analiza la robustez de memorias variando inicializaci√≥n o agregando ruido.",
  "No apto si los datos no pueden discretizarse en patrones binarios, o si se requieren m√∫ltiples clases de salida simult√°neas."
)

tabla_hopfield <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_hopfield %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir hopfield network",
             subtitle = "Hopfield Network")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  

```

## Gated Recurrent Unit (GRU) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/GRU.png"))
```

## Generative Adversarial Networks (GANs) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/GANs.png"))
```

## Long Short-Term Memory (LSTM) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/LSTM.png"))
```

## Multilayer Perceptron (MP)  {-}     

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/MP.png"))
```

El **Multilayer Perceptron (MLP)**, tambi√©n conocido como **red neuronal de propagaci√≥n hacia adelante cl√°sica**, es un tipo fundamental de **red neuronal artificial** utilizada para una amplia gama de tareas de **aprendizaje supervisado**, incluyendo **clasificaci√≥n** y **regresi√≥n**. La idea fundamental del MLP es extender el concepto del Perceptr√≥n simple al incorporar una o m√°s **capas ocultas** entre la capa de entrada y la capa de salida, y utilizando **funciones de activaci√≥n no lineales** en estas capas. Esta arquitectura de m√∫ltiples capas es lo que le confiere a los MLP su capacidad para aprender y modelar relaciones complejas y no lineales en los datos.

La estructura de un MLP t√≠picamente incluye:

1.  **Capa de Entrada:** Recibe las caracter√≠sticas de entrada del problema.
2.  **Capas Ocultas:** Son una o m√°s capas intermedias donde se realizan c√°lculos complejos. Cada neurona en una capa oculta recibe entradas de la capa anterior, calcula una suma ponderada de estas entradas (m√°s un sesgo), y luego aplica una **funci√≥n de activaci√≥n no lineal** (como la funci√≥n sigmoide, tanh o ReLU) a esta suma. Es la no linealidad de estas funciones de activaci√≥n la que permite al MLP aprender relaciones no lineales.
    $$a_j = f\left(\sum_{i=1}^{n} w_{ij} x_i + b_j\right)$$
    Donde $a_j$ es la activaci√≥n de la neurona $j$, $x_i$ son las entradas de la capa anterior, $w_{ij}$ son los pesos, $b_j$ es el sesgo, y $f$ es la funci√≥n de activaci√≥n no lineal.
3.  **Capa de Salida:** Produce la predicci√≥n final de la red. La funci√≥n de activaci√≥n en esta capa depende del tipo de problema (ej., una funci√≥n lineal para regresi√≥n, softmax para clasificaci√≥n multiclase, o sigmoide para clasificaci√≥n binaria).

El entrenamiento de un MLP se realiza t√≠picamente utilizando el algoritmo de **Back-Propagation**, que ajusta los pesos de la red de manera iterativa para minimizar una funci√≥n de p√©rdida (error) calculada en la capa de salida.

En el contexto del **aprendizaje global vs. local**, el Multilayer Perceptron es el paradigma de un sistema de **aprendizaje global**. La red aprende una **aproximaci√≥n de funci√≥n global** que mapea las entradas a las salidas, buscando minimizar la funci√≥n de p√©rdida en todo el conjunto de datos de entrenamiento. A diferencia de los sistemas de aprendizaje local que dividen expl√≠citamente el problema global en m√∫ltiples problemas m√°s peque√±os, el MLP ajusta todos sus pesos de forma interconectada para aprender una representaci√≥n distribuida de los patrones en los datos. Si los datos no se distribuyen linealmente, el MLP es excepcionalmente capaz de modelar estas relaciones complejas gracias a sus capas ocultas y funciones de activaci√≥n no lineales. Esto aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos lineales o m√°s simples, ya que el MLP puede construir representaciones internas de gran complejidad para aproximar casi cualquier funci√≥n continua. Hoy en d√≠a, los MLP son la base de muchas arquitecturas de "Deep Learning".


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas (normalizar) y categ√≥reas (dummies)",
  "‚úÖ Captura relaciones no lineales profundas",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (puede requerir robustez ante outliers)",
  "‚ö†Ô∏è Afecta la convergencia si es muy alta",
  "‚ö†Ô∏è Baja (modelo tipo 'caja negra')",
  "‚ö†Ô∏è Depende de arquitectura y tama√±o del dataset",
  "‚úÖ Recomendable (k-fold o repeated CV)",
  "‚ùå No conviene con pocos datos o ruido elevado"
)

detalles <- c(
  "Red neuronal con m√∫ltiples capas ocultas y funci√≥n de activaci√≥n no lineal.",
  "Clasificaci√≥n con softmax/sigmoide; regresi√≥n con capa lineal en salida.",
  "Debe escalarse cada caracter√≠stica; las categ√≥ricas convierten a variables indicadoras.",
  "Aprende patrones complejos combinando m√∫ltiples capas y neuronas.",
  "No impone distribuci√≥n espec√≠fica de errores, se optimiza con optimizadores basados en gradiente.",
  "Funciona mejor si las muestras son independientes; sensibles a dependencia temporal sin ajustes.",
  "No requiere varianza constante, ajusta pesos en cada mini-batch o lote.",
  "Outliers pueden causar gradientes explosivos o desaparecidos sin mecanismos de robustez.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; batch normalization ayuda.",
  "Dif√≠cil de interpretar cada peso/neuronas; se usan t√©cnicas como SHAP o LIME para explicaci√≥n.",
  "El tiempo de entrenamiento aumenta con cada capa, neuronas y epochs; GPUs aceleran el proceso.",
  "Crucial para ajustar hiperpar√°metros: n√∫mero de capas, neuronas por capa, tasa de aprendizaje, regularizaci√≥n.",
  "No √∫til para datasets muy peque√±os (sobreajuste) o altamente ruidosos sin regularizaci√≥n."
)

tabla_mp <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mp %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MP",
             subtitle = "Multilayer Perceptron (MP)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Perceptron  {-}     

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Perceptron.png"))
```

El **Perceptron** es el algoritmo de **aprendizaje supervisado** m√°s simple y uno de los primeros modelos de **redes neuronales artificiales**, propuesto por Frank Rosenblatt en 1957. Est√° dise√±ado para tareas de **clasificaci√≥n binaria**, es decir, para decidir si una entrada pertenece a una de dos clases posibles. Su idea fundamental es modelar c√≥mo una neurona biol√≥gica podr√≠a tomar decisiones.

El funcionamiento de un Perceptron es bastante directo:

1.  **Entradas y Pesos:** Recibe m√∫ltiples **entradas** (caracter√≠sticas) y a cada entrada se le asigna un **peso**. Estos pesos representan la importancia de cada caracter√≠stica.
2.  **Suma Ponderada:** Las entradas se multiplican por sus respectivos pesos y se suman. A esta suma se le a√±ade un **t√©rmino de sesgo (bias)**.
    $$z = \sum_{i=1}^{n} w_i x_i + b$$
    Donde $x_i$ son las entradas, $w_i$ son los pesos, $b$ es el sesgo, y $n$ es el n√∫mero de entradas.
3.  **Funci√≥n de Activaci√≥n:** El resultado de la suma ponderada ($z$) se pasa a trav√©s de una **funci√≥n de activaci√≥n** (generalmente una funci√≥n escal√≥n o *step function*). Esta funci√≥n decide la salida final, que es 1 si la suma excede un umbral (o 0 si no lo excede). Para el Perceptron original, la salida es binaria.
    $$\text{salida} = \begin{cases} 1 & \text{si } z \geq \text{umbral} \\ 0 & \text{si } z < \text{umbral} \end{cases}$$
4.  **Aprendizaje (Regla de Perceptron):** El Perceptron aprende ajustando sus pesos de forma iterativa. Si la predicci√≥n es incorrecta, los pesos se actualizan para reducir el error en la siguiente iteraci√≥n. La regla de actualizaci√≥n de pesos es:
    $$w_i^{\text{nuevo}} = w_i^{\text{anterior}} + \alpha \cdot (y - \hat{y}) \cdot x_i$$
    Donde $\alpha$ es la tasa de aprendizaje, $y$ es el valor real, y $\hat{y}$ es la predicci√≥n del Perceptron.

En el contexto del **aprendizaje global vs. local**, el Perceptron es un sistema de **aprendizaje global** por naturaleza. Busca encontrar un **hiperplano de separaci√≥n lineal** √∫nico que divida el espacio de caracter√≠sticas en dos regiones. La idea es que, si los datos son **linealmente separables** (es decir, si existe una l√≠nea, plano o hiperplano que puede separar perfectamente las dos clases), el Perceptron est√° garantizado para converger y encontrar esa soluci√≥n.

Sin embargo, precisamente porque busca una soluci√≥n lineal global, si los datos no se distribuyen linealmente (es decir, no son linealmente separables), el Perceptron **no puede encontrar una soluci√≥n convergente** y no puede aprender la relaci√≥n. Esto ilustra la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" cuando se busca una soluci√≥n global r√≠gida. El Perceptron original no puede aplicar el concepto de regresi√≥n ponderada localmente ni adaptarse a complejidades no lineales, a diferencia de modelos posteriores como las redes neuronales multicapa con funciones de activaci√≥n no lineales o los algoritmos de √°rboles de decisi√≥n. A pesar de esta limitaci√≥n, el Perceptron sent√≥ las bases para el desarrollo posterior de redes neuronales m√°s complejas.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas (requiere normalizar), Categ√≥ricas como dummies",
  "‚ö†Ô∏è Aprendizaje lineal: separabilidad lineal requerida",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers pueden cambiar el hiperplano)",
  "‚ö†Ô∏è Afecta la convergencia si est√° muy alta",
  "‚ö†Ô∏è Baja (modelo b√°sico de una capa sin capas ocultas)",
  "‚úÖ Muy r√°pido para datasets medianos",
  "‚úÖ √ötil para evaluar margen de separaci√≥n",
  "‚ùå No sirve si las clases no son linealmente separables"
)

detalles <- c(
  "Red neuronal de una sola capa que ajusta un hiperplano separador.",
  "Dise√±ado para clasificaci√≥n binaria; no predice valores continuos.",
  "Todas las features deben ser num√©ricas y escaladas; las categ√≥ricas deben convertirse en indicadores.",
  "Busca maximizar el margen de separaci√≥n lineal entre dos clases; no captura no linealidades.",
  "No exige distribuci√≥n normal de errores ya que optimiza con perceptr√≥n simple.",
  "Funciona mejor si las instancias son independientes; sensible a dependencias temporales sin ajuste.",
  "No se basa en varianza de errores; el algoritmo actualiza pesos sin supuestos de varianza.",
  "Los valores extremos cercanos al margen pueden forzar ajustes bruscos de pesos.",
  "La colinealidad puede ralentizar la convergencia, aunque no impide la definici√≥n de hiperplano.",
  "F√°cil de entender: el peso de cada caracter√≠stica indica direcci√≥n del hiperplano.",
  "Entrenamiento r√°pido usando regla de aprendizaje por error; escalable a datos medianos.",
  "Se usa CV para ajustar tasa de aprendizaje y n√∫mero de √©pocas para evitar bajo/sobreajuste.",
  "In√∫til si las clases no se pueden separar linealmente; requiere extensiones (por ejemplo, kernel) para no linealidad."
)

tabla_perceptron <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_perceptron %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir perceptron",
             subtitle = "Perceptron")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Radial Basis Function Network (RBFN)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/RBFN.png"))
```

**Radial Basis Function Network (RBFN)** es un tipo de **red neuronal artificial** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**. A diferencia de las redes neuronales multicapa perceptr√≥n tradicionales que utilizan funciones de activaci√≥n sigmoide o ReLU, las RBFN emplean **funciones de base radial** como sus funciones de activaci√≥n en la capa oculta. Su estructura es t√≠picamente m√°s simple que un perceptr√≥n multicapa, consistiendo generalmente en tres capas: una capa de entrada, una capa oculta con neuronas de base radial, y una capa de salida.

La idea fundamental de una RBFN radica en su capacidad para modelar relaciones no lineales al mapear datos de entrada a un espacio de caracter√≠sticas de mayor dimensi√≥n donde pueden ser **linealmente separables** (para clasificaci√≥n) o donde una **funci√≥n lineal** puede aproximar la relaci√≥n (para regresi√≥n). Esto se logra a trav√©s de las neuronas de la capa oculta, cada una de las cuales representa un "centro" en el espacio de caracter√≠sticas.

El funcionamiento de una RBFN implica:

1.  **Capa de Entrada:** Recibe las caracter√≠sticas de entrada.
2.  **Capa Oculta (Neuronas de Base Radial):** Cada neurona en esta capa tiene un **centro** ($c_i$) y un **radio (o desviaci√≥n est√°ndar, $\sigma_i$)**. La funci√≥n de activaci√≥n de estas neuronas (com√∫nmente una **funci√≥n Gaussiana**) calcula la **distancia** entre el vector de entrada ($x$) y el centro de la neurona ($c_i$), y luego aplica la funci√≥n de base radial. Cuanto m√°s cerca est√© la entrada del centro de la neurona, mayor ser√° la activaci√≥n de esa neurona.
    $$\phi_i(x) = \exp\left(-\frac{\|x - c_i\|^2}{2\sigma_i^2}\right)$$
    Donde $\phi_i(x)$ es la salida de la neurona $i$, $\|x - c_i\|$ es la distancia euclidiana entre la entrada $x$ y el centro $c_i$, y $\sigma_i$ es el radio (ancho) de la funci√≥n Gaussiana.
3.  **Capa de Salida:** Las salidas de las neuronas de la capa oculta se combinan linealmente (ponderadas por unos coeficientes, $w_{ij}$) para producir la salida final de la red. Para regresi√≥n, es una suma ponderada; para clasificaci√≥n, a menudo se usa una funci√≥n de activaci√≥n softmax.
    $$y_j = \sum_{i=1}^{M} w_{ij}\phi_i(x)$$
    Donde $y_j$ es la salida $j$, $M$ es el n√∫mero de neuronas ocultas, y $w_{ij}$ son los pesos de la capa de salida.

En el contexto del **aprendizaje global vs. local**, las RBFN son intr√≠nsecamente sistemas de **aprendizaje local**. Cada neurona de la capa oculta es sensible a una **regi√≥n espec√≠fica** del espacio de entrada, definida por su centro y su radio. La red como un todo es una combinaci√≥n de estas respuestas locales. Si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se aplica de forma muy eficaz mediante esta naturaleza de **regresi√≥n ponderada localmente**. Las RBFN pueden aproximar cualquier funci√≥n continua con la suficiente cantidad de neuronas de base radial. Esto aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo global, ya que la red puede adaptarse localmente a las caracter√≠sticas de diferentes regiones del espacio de datos. Son particularmente √∫tiles para problemas de aproximaci√≥n de funciones, series de tiempo y reconocimiento de patrones.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas (requiere normalizaci√≥n), Categ√≥ricas como dummies",
  "‚úÖ Captura no linealidades mediante funciones base radiales",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "‚ùå No asume varianza constante",
  "‚ö†Ô∏è Moderadamente (centros pueden verse alterados por outliers)",
  "‚ö†Ô∏è Puede influir en la selecci√≥n de centros, pero no tan cr√≠tico como en OLS",
  "‚ö†Ô∏è Baja (la capa oculta con RBF es dif√≠cil de interpretar)",
  "‚ö†Ô∏è Moderada (depende de n√∫mero de centros y dimensiones)",
  "‚úÖ Recomendable para ajustar n√∫mero de bases y spread",
  "‚ùå Datos muy grandes o alta dimensionalidad sin reducci√≥n, mucho ruido"
)

detalles <- c(
  "Red neuronal de una capa oculta con funciones radial basis como activaci√≥n.",
  "Para regresi√≥n predice un valor continuo; para clasificaci√≥n usa votaci√≥n o softmax sobre salidas.",
  "Requiere que las caracter√≠sticas num√©ricas est√©n escaladas; las categ√≥ricas deben convertirse a variables indicadoras.",
  "Cada neurona oculta calcula una funci√≥n gaussiana (u otra RBF) centrada en un punto, captando curvas suaves.",
  "No impone distribuci√≥n normal en los errores, pues optimiza en funci√≥n de m√≠nimos cuadrados o cross-entropy.",
  "Funciona mejor si las observaciones son independientes; sensible a estructuras de dependencia sin modelar.",
  "No requiere homocedasticidad ya que no se basa en un modelo param√©trico de error con varianza fija.",
  "Los valores extremos pueden desplazar los centros de las RBF, afectando la forma del modelo.",
  "La colinealidad puede dificultar la determinaci√≥n de centros √≥ptimos, pero no invalida el ajuste.",
  "Las neuronas ocultas representan combinaciones complejas de caracter√≠sticas, por lo que el modelo es tipo 'caja negra'.",
  "El entrenamiento implica fijar o aprender centros y spreads; para muchos centros o dimensiones altas, el costo crece r√°pido.",
  "Se usa CV para elegir el n√∫mero de bases (centros) y el par√°metro de ancho (`sigma` o `spread`) para evitar sobreajuste.",
  "No conviene cuando hay decenas de miles de caracter√≠sticas sin reducci√≥n previa o cuando el ruido es muy alto."
)

tabla_rbfn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rbfn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RBFN",
             subtitle = "Radial Basis Function Network (RBFN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Recurrent Neural Networks (RNNs) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/RNNs.png"))
```

**Recurrent Neural Networks (RNNs)** son un tipo de **red neuronal artificial** dise√±ado espec√≠ficamente para manejar **datos secuenciales** o temporales, donde la informaci√≥n de pasos anteriores en la secuencia es relevante para la predicci√≥n actual. A diferencia de las redes de propagaci√≥n hacia adelante (como MLP o CNN) que asumen que las entradas son independientes entre s√≠, las RNNs tienen "memoria" o **conexiones recurrentes** que les permiten mantener un **estado interno** que encapsula informaci√≥n de pasos de tiempo anteriores. Esta caracter√≠stica las hace ideales para tareas como el procesamiento de lenguaje natural (PLN), el reconocimiento de voz, la traducci√≥n autom√°tica y la predicci√≥n de series de tiempo.

La idea fundamental de una RNN es que una **unidad recurrente** aplica la misma funci√≥n de transformaci√≥n a cada elemento de una secuencia, con la particularidad de que la salida de la unidad en un paso de tiempo dado se realimenta como entrada para el mismo proceso en el siguiente paso de tiempo. Esto permite que la red "recuerde" y utilice informaci√≥n pasada al procesar la secuencia actual.

El funcionamiento b√°sico de una RNN en un paso de tiempo ($t$) implica:

1.  **Entrada actual ($x_t$):** El elemento actual de la secuencia.
2.  **Estado oculto anterior ($h_{t-1}$):** La "memoria" o estado interno de la red del paso de tiempo anterior.
3.  **C√°lculo del Estado Oculto Actual ($h_t$):** Se combina la entrada actual y el estado oculto anterior, y se aplica una funci√≥n de activaci√≥n (ej., tanh o ReLU).
    $$h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$
    Donde $W_{hh}$ son los pesos de la conexi√≥n recurrente, $W_{xh}$ son los pesos de la entrada, y $b_h$ es el sesgo.
4.  **Salida Actual ($y_t$):** Se genera una salida a partir del estado oculto actual.
    $$y_t = W_{hy} h_t + b_y$$
    Donde $W_{hy}$ son los pesos de la salida y $b_y$ es el sesgo.

Este proceso de actualizaci√≥n de estado y salida se repite para cada elemento de la secuencia. La "memoria" de la RNN est√° codificada en el estado oculto que se pasa de un paso de tiempo al siguiente.

El entrenamiento de las RNNs se realiza mediante una variante del algoritmo de Back-Propagation llamada **Back-Propagation Through Time (BPTT)**. BPTT desenrolla la red a lo largo del tiempo, tratando cada paso de tiempo como una capa separada, y luego aplica la retropropagaci√≥n de manera similar a c√≥mo se entrena un MLP, pero propagando los errores a trav√©s de las conexiones recurrentes. Sin embargo, las RNNs simples pueden sufrir de problemas como el **desvanecimiento del gradiente** (vanishing gradient) o el **explosi√≥n del gradiente** (exploding gradient) para secuencias largas, lo que llev√≥ al desarrollo de arquitecturas m√°s avanzadas como **LSTM (Long Short-Term Memory)** y **GRU (Gated Recurrent Unit)**.

En el contexto del **aprendizaje global vs. local**, las RNNs son sistemas de **aprendizaje global** que est√°n dise√±ados para aprender y modelar **dependencias temporales y patrones secuenciales** en un dominio global. A diferencia de los m√©todos de regresi√≥n ponderada localmente como LOESS, que se enfocan en ajustar curvas en regiones espec√≠ficas de datos, las RNNs intentan aprender una funci√≥n de mapeo compleja que considera toda la secuencia hist√≥rica para producir una predicci√≥n. Si los datos (secuenciales) no se distribuyen linealmente, las RNNs son extremadamente efectivas para capturar estas relaciones no lineales y dependencias a largo plazo. Al tener un estado interno que recuerda informaci√≥n pasada, abordan directamente la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos est√°ticos o lineales, ya que pueden adaptar sus predicciones din√°micamente en funci√≥n del contexto secuencial, lo que las convierte en una herramienta fundamental para el an√°lisis de series de tiempo y el procesamiento de lenguaje.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado secuencial",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n) en secuencias",
  "‚úÖ Series temporales y datos secuenciales (texto, audio, series) convertidos a vectores",
  "‚úÖ Captura dependencias temporales y de largo plazo entre pasos de la secuencia",
  "‚ùå No requiere supuestos de normalidad de residuos",
  "‚ö†Ô∏è Ideal si las secuencias son independientes entre s√≠; no modela dependencia ex√≥gena autom√°ticamente",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers en la serie pueden sesgar el entrenamiento si no se detectan)",
  "‚ö†Ô∏è La colinealidad en caracter√≠sticas secuenciales puede afectar la convergencia (usar embeddings o reducci√≥n)",
  "‚ö†Ô∏è Baja (‚Äúcaja negra‚Äù con muchas capas; usar t√©cnicas de atenci√≥n o visualizaci√≥n de activaciones)",
  "‚ö†Ô∏è Lento sin GPU/TPU; entrenamiento costoso para secuencias largas o redes profundas",
  "‚ö†Ô∏è Usar validaci√≥n cronol√≥gica (time series split) es m√°s apropiado que k-fold cl√°sico",
  "‚ùå No conviene con muy pocas muestras temporales, secuencias extremadamente largas sin truncar, o datos muy ruidosos"
)

detalles <- c(
  "Red neuronal recurrente que procesa datos en pasos temporales manteniendo un estado interno.",
  "En clasificaci√≥n, etiqueta cada elemento o secuencia; en regresi√≥n, predice valores continuos a lo largo del tiempo.",
  "Las entradas deben transformarse en vectores o embeddings; por ejemplo, texto a √≠ndices, series normalizadas.",
  "La arquitectura RNN (LSTM, GRU) retiene informaci√≥n de pasos anteriores para afectar salidas posteriores.",
  "No impone distribuci√≥n en errores, ya que se optimiza v√≠a descenso de gradiente sobre secuencias.",
  "Funciona mejor si cada secuencia (serie) es independiente; para datos con autocorrelaci√≥n compleja, usar variantes especializadas.",
  "No requiere varianza constante, pues se basa en propagaci√≥n de estado y no en un t√©rmino de error param√©trico.",
  "Valores at√≠picos en la serie pueden provocar gradientes explosivos o desvanecidos sin mecanismos como clipping.",
  "La representaci√≥n internal de patrones secuenciales puede verse afectada si hay caracter√≠sticas muy correlacionadas; usar regularizaci√≥n.",
  "Dif√≠cil interpretar pesos internos; se usan mec√°nicas como atenci√≥n (attention) o visualizaci√≥n de celdas LSTM.",
  "El entrenamiento con backpropagation through time es intensivo; GPUs o TPUs aceleran enormemente el proceso.",
  "Para series temporales, se prefiere validaci√≥n basada en ventanas de tiempo (rolling/expanding window) en lugar de random split.",
  "No apto si las secuencias son muy cortas o muy pocas, o hay mucho ruido sin filtrado; en esos casos, usar modelos estad√≠sticos simples."
)

tabla_rnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rnn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RNN",
             subtitle = "Recurrent Neural Networks (RNNs)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Restricted Boltzmann Machine (RBM) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/RBM.png"))
```

## Transformers  {-}  


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Transformers.png"))
```

Los **Transformers** son una arquitectura de **red neuronal profunda** que ha revolucionado el campo del **Procesamiento de Lenguaje Natural (PLN)** y, m√°s recientemente, se ha expandido a la visi√≥n por computadora y otras √°reas. Introducidos en el art√≠culo "Attention Is All You Need" (Vaswani et al., 2017), la idea fundamental de los Transformers es prescindir de la naturaleza recurrente de las RNNs y las convolucionales de las CNNs, bas√°ndose enteramente en un mecanismo llamado **auto-atenci√≥n (self-attention)** para capturar dependencias de largo alcance en las secuencias de entrada.

Antes de los Transformers, las RNNs eran el modelo dominante para datos secuenciales. Sin embargo, las RNNs ten√≠an limitaciones como la dificultad para capturar dependencias a muy largo plazo (problema del gradiente desvanecido) y la imposibilidad de paralelizar completamente el procesamiento de secuencias (debido a su naturaleza secuencial). Los Transformers resuelven estos problemas al permitir que cada elemento de la secuencia interact√∫e directamente con todos los dem√°s elementos de la secuencia, sin importar su distancia.

Los componentes clave de un Transformer incluyen:

1.  **Mecanismo de Auto-Atenci√≥n (Self-Attention):** Este es el coraz√≥n del Transformer. Para cada token (palabra) en una secuencia, el mecanismo de auto-atenci√≥n calcula una puntuaci√≥n de "relevancia" entre ese token y todos los dem√°s tokens de la secuencia. Esto permite que el modelo "pese" la importancia de cada token al generar la representaci√≥n de otro token. Este proceso se implementa a trav√©s de tres vectores para cada token: **Query (Q)**, **Key (K)** y **Value (V)**.
    $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
    Donde $d_k$ es la dimensi√≥n de los vectores Key.

2.  **Atenci√≥n Multi-Cabeza (Multi-Head Attention):** Para mejorar la capacidad del modelo de enfocarse en diferentes aspectos de la secuencia, el mecanismo de auto-atenci√≥n se aplica m√∫ltiples veces en paralelo con diferentes conjuntos de matrices de pesos (cabezas). Las salidas de estas cabezas se concatenan y se transforman linealmente.

3.  **Capas Feed-Forward (Posici√≥n por Posici√≥n):** Despu√©s del mecanismo de atenci√≥n, hay una red neuronal de propagaci√≥n hacia adelante (un MLP simple) que se aplica de forma independiente a cada posici√≥n en la secuencia.

4.  **Codificador-Decodificador (Encoder-Decoder Architecture):** El Transformer original consta de un **codificador** y un **decodificador**.
    * El **codificador** toma la secuencia de entrada y genera una representaci√≥n. Consiste en m√∫ltiples capas id√©nticas, cada una con una capa de auto-atenci√≥n multi-cabeza y una capa feed-forward.
    * El **decodificador** toma la representaci√≥n del codificador y genera la secuencia de salida (por ejemplo, la traducci√≥n). Tambi√©n consiste en m√∫ltiples capas, cada una con auto-atenci√≥n multi-cabeza, atenci√≥n multi-cabeza (que atiende a la salida del codificador) y una capa feed-forward.

5.  **Codificaci√≥n Posicional (Positional Encoding):** Dado que los Transformers procesan secuencias en paralelo y no tienen una noci√≥n inherente de la posici√≥n de los tokens (a diferencia de las RNNs), se a√±ade informaci√≥n de la posici√≥n de cada token a sus incrustaciones de entrada.

En el contexto del **aprendizaje global vs. local**, los Transformers son un sistema de **aprendizaje global** que, gracias a su mecanismo de atenci√≥n, pueden aprender **dependencias a largo alcance** y relaciones complejas que son inherentemente globales en la secuencia. Aunque los c√°lculos individuales de atenci√≥n pueden verse como una forma de ponderaci√≥n de la importancia local de los tokens, la red en su conjunto construye una representaci√≥n global de la secuencia. Si los datos (secuenciales) no se distribuyen linealmente, los Transformers son excepcionalmente capaces de modelar estas relaciones no lineales y dependencias a trav√©s de su capacidad para "observar" toda la secuencia a la vez y ponderar la relevancia de cada parte. Esto resuelve de manera fundamental la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos secuenciales anteriores, ya que la arquitectura de atenci√≥n les permite aprender patrones complejos y no lineales en datos secuenciales sin las restricciones de memoria de las RNNs, lo que los convierte en la arquitectura dominante para tareas de PLN avanzadas.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado (frecuentemente secuencial o multitarea)",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n en secuencias)",
  "‚úÖ Texto, secuencias, im√°genes en vectores o embeddings",
  "‚úÖ Captura dependencia secuencial y global mediante mecanismos de atenci√≥n",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚ö†Ô∏è Ideal si las muestras o secuencias son independientes; para datos correlacionados usar variantes espec√≠ficas",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers en embeddings o entradas ruidosas pueden afectar atenci√≥n)",
  "‚ö†Ô∏è La colinealidad en embedding space puede ralentizar aprendizaje; usar regularizaci√≥n",
  "‚ö†Ô∏è Baja (modelo de ‚Äôcaja negra‚Äô, requieren m√©todos como attention visualization o interpretabilidad basada en pesos)",
  "‚ö†Ô∏è Lento sin hardware especializado (secuencialidad en atenci√≥n puede ser costosa)",
  "‚ö†Ô∏è Validaci√≥n temporal o k-fold anidada, seg√∫n tarea; en NLP se prefiere holdout sobre texto sin mezclar",
  "‚ùå No es apropiado con muy pocos datos de entrenamiento o sin estructura secuencial clara"
)

detalles <- c(
  "Arquitectura basada en capas de atenci√≥n para procesar secuencias completas en paralelo.",
  "Modelos como BERT, GPT, T5 pueden usarse para tareas de clasificaci√≥n, traducci√≥n, regresi√≥n de valores continuos en secuencias.",
  "Entradas requieren tokenizaci√≥n y conversi√≥n a embeddings; pueden combinarse varias modalidades.",
  "La auto‚Äêatenci√≥n global permite capturar relaciones a largo y corto plazo sin sesgo posicional estricto.",
  "No impone distribuci√≥n param√©trica de errores; se entrena con optimizadores basados en p√©rdidas cross‚Äêentropy o MSE.",
  "Se prefiere que las secuencias en el batch no sean dependientes; para series de tiempo, usar variantes como Time‚ÄêSeries Transformer.",
  "No se modela varianza del error; el entrenamiento se enfoca en minimizar funci√≥n de p√©rdida directa.",
  "Ruido en texto (typos) o en datos num√©ricos de entrada puede inducir atenci√≥n err√°tica; usar limpieza de datos y regularizaci√≥n.",
  "Los embeddings pueden contener informaci√≥n redundante de caracter√≠sticas correlacionadas; ajustar tama√±o de embedding y regularizaci√≥n.",
  "Interpretabilidad limitada; se usan t√©cnicas como visualizaci√≥n de mapas de atenci√≥n, LIME, SHAP para entender decisiones.",
  "El c√≥mputo de atenci√≥n es O(n¬≤) en longitud de secuencia; GPUs/TPUs o variantes eficientes (Linformer, Performer) alivian costo.",
  "Para tareas de texto, a veces se usa train/validation/test sin CV cl√°sica; para tareas generales, k-fold anidada ayuda a elegir hiperpar√°metros.",
  "No es adecuado con datasets muy peque√±os, sin preentrenamiento o sin estructuras secuenciales definidas."
)

tabla_transformers <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_transformers %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir transformers",
             subtitle = "Transformers")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```





<!--chapter:end:04-neural-networks.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üß© 5. Reducci√≥n de Dimensionalidad {-}  

**Ejemplos:** PCA (An√°lisis de Componentes Principales), t-SNE, UMAP.   
**Uso:** Fundamental para **visualizar datos de alta dimensi√≥n**, haci√©ndolos m√°s comprensibles. Tambi√©n es un paso clave de **preprocesamiento** para eliminar ruido o multicolinealidad antes de aplicar otros modelos.  
**Ventajas:** Puede **mejorar significativamente el rendimiento y la velocidad** de otros algoritmos de *machine learning*.  
**Limitaciones:** A veces se **pierde la interpretabilidad** de los datos originales y no siempre garantiza una mejora en el desempe√±o de los modelos.   

---

## Flexible Discriminant Analysis (FDA)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/FDA.png"))
```

**Flexible Discriminant Analysis (FDA)** es un m√©todo de **clasificaci√≥n** que generaliza el An√°lisis Discriminante Lineal (LDA) para manejar **relaciones no lineales** entre las variables predictoras y las clases. A diferencia de LDA, que asume l√≠mites de decisi√≥n lineales y distribuciones gaussianas con matrices de covarianza iguales, FDA es mucho m√°s adaptable.

FDA logra esta flexibilidad al combinar dos conceptos:
1.  **Optimal Scoring:** Transforma las variables de respuesta categ√≥ricas en valores num√©ricos (scores √≥ptimos) de manera que las clases sean m√°s f√°cilmente separables linealmente.
2.  **Modelos de Regresi√≥n No Param√©tricos:** En lugar de usar una regresi√≥n lineal simple (como en LDA), FDA utiliza m√©todos de regresi√≥n no param√©tricos m√°s flexibles, como las **Multivariate Adaptive Regression Splines (MARS)**. Esto permite que la relaci√≥n entre las variables transformadas y los scores √≥ptimos sea no lineal, lo que a su vez se traduce en fronteras de decisi√≥n no lineales en el espacio original de los datos.

Es decir, FDA toma los datos, los transforma de una manera inteligente para que sean m√°s f√°ciles de separar, y luego aplica una discriminaci√≥n lineal en ese espacio transformado, lo que resulta en una frontera de decisi√≥n compleja y flexible en el espacio original.

En el contexto del **aprendizaje global vs. local**, FDA se considera un modelo que **integra aspectos de ambos**.

* **Aspecto Global:** El objetivo final de FDA es encontrar una **funci√≥n discriminante global** que separe las clases en el espacio transformado. Los scores √≥ptimos y las funciones base del m√©todo de regresi√≥n (como MARS) se aprenden considerando la estructura general de los datos para lograr la mejor separaci√≥n a nivel global. El modelo resultante es una funci√≥n que se aplica de manera consistente a cualquier nueva observaci√≥n.

* **Aspecto Local (debido al uso de modelos no param√©tricos como MARS):** La flexibilidad de FDA proviene de su uso de m√©todos como MARS, que dividen el espacio de las caracter√≠sticas en **regiones locales** y ajustan relaciones simples dentro de cada una. Esto permite que el modelo se adapte a no linealidades y a cambios en la relaci√≥n entre las variables en diferentes partes del espacio de datos. As√≠, si los datos no se distribuyen linealmente, FDA puede construir fronteras de decisi√≥n que capturan esas complejidades al "localizar" las relaciones importantes.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas (puede usar transformaciones)",
  "‚úÖ No lineal (usa regresi√≥n flexible en el espacio transformado)",
  "‚ùå No aplica (no es regresi√≥n de residuos)",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è FDA suaviza este supuesto al modelar relaciones no lineales",
  "‚ö†Ô∏è Puede ser sensible a outliers, dependiendo del m√©todo de ajuste",
  "‚ö†Ô∏è Puede mitigar multicolinealidad si se usa penalizaci√≥n",
  "‚ö†Ô∏è Menos interpretable que LDA, pero permite mayor flexibilidad",
  "‚ö†Ô∏è Menor eficiencia que LDA por mayor complejidad computacional",
  "‚úÖ Validaci√≥n cruzada √∫til para seleccionar transformaciones o suavizados",
  "‚ùå En datos con pocos casos o ruido excesivo puede sobreajustarse"
)

detalles <- c(
  "Extensi√≥n de LDA que permite relaciones no lineales entre predictores y clases mediante t√©cnicas como splines o regresi√≥n flexible.",
  "Clasifica observaciones en clases categ√≥ricas bas√°ndose en predictores transformados.",
  "Admite variables num√©ricas, las cuales pueden ser transformadas de forma no lineal.",
  "Usa regresi√≥n no lineal flexible (como splines) para modelar relaciones complejas en el espacio de discriminaci√≥n.",
  "No genera residuos como regresi√≥n tradicional; es un modelo de clasificaci√≥n.",
  "No se enfoca en errores secuenciales o dependientes.",
  "Relaja la homocedasticidad al no asumir distribuci√≥n gaussiana estricta.",
  "Puede verse afectado por valores extremos, seg√∫n el m√©todo de suavizado.",
  "La transformaci√≥n flexible puede reducir colinealidad, pero no siempre la elimina.",
  "Los coeficientes y funciones discriminantes pueden ser dif√≠ciles de interpretar si se usan transformaciones complejas.",
  "Mayor costo computacional que LDA, pero m√°s potente en patrones no lineales.",
  "Se recomienda CV para evaluar desempe√±o y evitar overfitting en el proceso de ajuste flexible.",
  "Si los datos no requieren flexibilidad o el tama√±o muestral es bajo, FDA puede ser innecesariamente complejo."
)

tabla_fda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_fda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir FDA",
             subtitle = "Flexible Discriminant Analysis (FDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Independent Component Analysis (ICA) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/ICA.png"))
```

## Kernel PCA (KPCA) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/KPCA.png"))
```
  
## Linear Discriminant Analysis (LDA)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/LDA.png"))
```

El **An√°lisis Discriminante Lineal (LDA)** es un m√©todo de **clasificaci√≥n** y **reducci√≥n de dimensionalidad** utilizado para encontrar una combinaci√≥n lineal de caracter√≠sticas que mejor separe dos o m√°s clases de objetos o eventos. Su objetivo principal es modelar la diferencia entre las clases, lo que lo hace muy √∫til para tareas de clasificaci√≥n supervisada.

LDA funciona proyectando los puntos de datos a un espacio de menor dimensi√≥n (generalmente una o pocas dimensiones) de tal manera que las clases est√©n lo m√°s separadas posible. Para lograr esto, busca una direcci√≥n (un eje) que maximice la **separaci√≥n entre las medias de las clases** (varianza entre clases) mientras minimiza la **varianza dentro de cada clase** (varianza intraclase). En un problema de clasificaci√≥n binaria, esto significa encontrar la l√≠nea √≥ptima para proyectar los datos de modo que las dos clases se superpongan lo menos posible.

A diferencia de modelos como la Regresi√≥n Log√≠stica, que buscan modelar la probabilidad de pertenencia a una clase, LDA modela directamente la distribuci√≥n de los datos dentro de cada clase y luego utiliza el Teorema de Bayes para asignar una nueva observaci√≥n a la clase m√°s probable. LDA asume que las **varianzas (o matrices de covarianza) de las clases son iguales** y que los datos est√°n distribuidos normalmente.

**Aprendizaje Global vs. Local:**

El An√°lisis Discriminante Lineal (LDA) es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** LDA busca una **√∫nica transformaci√≥n lineal** o un conjunto de direcciones (ejes) que se aplican a **todos los datos** para lograr la m√°xima separaci√≥n entre las clases en un espacio de menor dimensi√≥n. La frontera de decisi√≥n que resulta de LDA es siempre **lineal** y se define globalmente a partir de las medias y las varianzas combinadas (asumidas como iguales) de todas las clases. El modelo es "fijo" y se aplica uniformemente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n espec√≠fica en el espacio de caracter√≠sticas. No se ajustan modelos diferentes para distintos vecindarios de datos, sino que se aprende una regla de separaci√≥n que es v√°lida para todo el dominio.

Por lo tanto, si los datos no se distribuyen linealmente o las fronteras de decisi√≥n entre las clases son inherentemente no lineales (por ejemplo, si una clase rodea a otra), LDA puede no ser el m√©todo m√°s adecuado. En esos escenarios, modelos de aprendizaje local o m√°s flexibles (como los √°rboles de decisi√≥n, SVM con kernels no lineales, o FDA que extiende LDA para no linealidades) suelen ofrecer un mejor rendimiento.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas (preferentemente)",
  "‚úÖ Asume relaciones lineales entre variables y clases",
  "‚úÖ Supone normalidad multivariante de los predictores dentro de cada clase",
  "‚úÖ Supone independencia entre observaciones",
  "‚úÖ Asume varianza-covarianza igual entre clases (homocedasticidad)",
  "‚ö†Ô∏è Sensible a valores at√≠picos",
  "‚ö†Ô∏è Puede verse afectado negativamente por alta colinealidad",
  "‚úÖ Alta, coeficientes discriminantes son interpretables",
  "‚úÖ Muy eficiente computacionalmente",
  "‚úÖ Se recomienda para evaluar estabilidad y evitar sobreajuste",
  "‚ùå Mal desempe√±o si no se cumplen supuestos de normalidad y homocedasticidad"
)

detalles <- c(
  "Modelo supervisado cl√°sico para clasificaci√≥n que encuentra combinaciones lineales de predictores que separan clases.",
  "Requiere una variable categ√≥rica como objetivo, con dos o m√°s clases.",
  "Mejor con predictores num√©ricos continuos; categ√≥ricos requieren codificaci√≥n previa.",
  "Calcula funciones discriminantes lineales que maximizan la separaci√≥n entre clases.",
  "Cada grupo debe seguir una distribuci√≥n normal multivariante para resultados √≥ptimos.",
  "Las observaciones deben ser independientes para validez de inferencia.",
  "Supone igual matriz de covarianzas entre grupos; si no se cumple, usar QDA.",
  "Outliers influyen en la media y la varianza estimada, distorsionando fronteras.",
  "Multicolinealidad puede hacer que los coeficientes discriminantes sean inestables.",
  "Las funciones discriminantes se interpretan como direcciones de m√°xima separaci√≥n.",
  "Requiere bajo costo computacional y se entrena r√°pidamente.",
  "Se puede usar validaci√≥n cruzada para elegir el n√∫mero de componentes o verificar precisi√≥n.",
  "Cuando los datos no cumplen normalidad ni homocedasticidad, el modelo pierde precisi√≥n."
)

tabla_lda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LDA",
             subtitle = "Linear Discriminant Analysis (LDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Locally Linear Embedding (LLE) {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/LLE.png"))
```
   
## Mixture Discriminant Analysis (MDA)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/MDA.png"))
```

El **An√°lisis Discriminante de Mezclas (MDA)** es una extensi√≥n del An√°lisis Discriminante Lineal (LDA) y del An√°lisis Discriminante Cuadr√°tico (QDA) que aborda la limitaci√≥n de que estas t√©cnicas asumen que cada clase proviene de una √∫nica distribuci√≥n normal (o gaussiana). MDA relaja esta suposici√≥n al permitir que **cada clase sea modelada como una mezcla de m√∫ltiples distribuciones gaussianas**. Esto le otorga una capacidad significativamente mayor para manejar clases con formas complejas o multimodales, que no pueden ser descritas adecuadamente por una sola distribuci√≥n normal.  

MDA funciona de la siguiente manera:  

1.  **Modelado por Componentes de Mezcla:** Para cada clase, MDA estima los par√°metros (media y matriz de covarianza) de varias distribuciones gaussianas ("componentes de mezcla") en lugar de solo una. Es similar al proceso de agrupamiento de mezclas gaussianas (Gaussian Mixture Models - GMM) aplicado dentro de cada clase.  
2.  **Asignaci√≥n a la Clase:** Una vez que se han modelado las distribuciones de mezcla para cada clase, para una nueva observaci√≥n, MDA calcula la probabilidad de que esa observaci√≥n pertenezca a cada componente de mezcla en cada clase. Luego, asigna la observaci√≥n a la clase que maximiza la probabilidad posterior, es decir, la clase que es m√°s probable que haya generado esa observaci√≥n.   
3.  **Fronteras de Decisi√≥n Flexibles:** Al modelar cada clase como una mezcla de gaussianas, MDA puede generar fronteras de decisi√≥n que son mucho m√°s flexibles y no lineales que las de LDA (que son lineales) o QDA (que son cuadr√°ticas). Esto le permite adaptarse a clases con estructuras complejas, que pueden tener "agrupaciones" internas o formas irregulares.  

Los par√°metros del modelo (las medias, covarianzas y pesos de los componentes de mezcla para cada clase) se suelen estimar utilizando un algoritmo iterativo como la **Maximizaci√≥n de Expectativas (Expectation-Maximization - EM)**.   

**Aprendizaje Global vs. Local:**   

El An√°lisis Discriminante de Mezclas (MDA) se encuentra en un punto intermedio, inclin√°ndose hacia un modelo que **combina aspectos de aprendizaje global y local**, con una mayor flexibilidad para capturar la estructura local de los datos en comparaci√≥n con LDA o QDA.  

* **Aspecto Global:** Al igual que LDA, el objetivo final de MDA es crear un **clasificador global** que pueda asignar cualquier nueva observaci√≥n a una de las clases. Las distribuciones de mezcla para cada clase se aprenden a partir de todo el conjunto de datos de entrenamiento para esas clases, y el clasificador resultante se aplica de manera consistente en todo el espacio de caracter√≠sticas. La regla de decisi√≥n final es una funci√≥n que se deriva de las distribuciones aprendidas para todas las clases.  

* **Aspecto Local:** La "flexibilidad" de MDA y su capacidad para manejar no linealidades proviene de su suposici√≥n de que cada clase puede estar compuesta por **m√∫ltiples componentes gaussianos**. Esto significa que, dentro de una misma clase, puede haber sub-agrupaciones o densidades locales que son modeladas individualmente. Al permitir estas m√∫ltiples distribuciones gaussianas dentro de cada clase, MDA puede adaptarse mejor a las caracter√≠sticas y densidades de los datos en diferentes **vecindarios o subregiones** del espacio de caracter√≠sticas. Si los datos no se distribuyen linealmente y tienen formas complejas (como clusters separados dentro de una clase), MDA puede "localizar" y modelar estas estructuras, llevando a fronteras de decisi√≥n mucho m√°s complejas y no lineales que se ajustan mejor a la forma real de las clases.   


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas",
  "‚úÖ No lineal (usa mezclas de gaussianas para modelar clases)",
  "‚ùå No aplica como en regresi√≥n",
  "‚ùå No se eval√∫a como en regresi√≥n",
  "‚ö†Ô∏è Supone varianza homog√©nea dentro de componentes, pero puede variar entre clases",
  "‚ö†Ô∏è Puede ser sensible a outliers (afectan las medias y covarianzas)",
  "‚ö†Ô∏è Puede verse afectado, aunque usa reducci√≥n dimensional",
  "‚ö†Ô∏è Moderadamente interpretable (depende de componentes gaussianos)",
  "‚ö†Ô∏è M√°s lento que LDA/QDA, pero m√°s flexible",
  "‚úÖ Recomendable para elegir n√∫mero de componentes y evitar sobreajuste",
  "‚ùå Mal desempe√±o si la distribuci√≥n dentro de clases no es bien modelada por gaussianas"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n que combina regresi√≥n discriminante con mezclas gaussianas dentro de cada clase.",
  "Se usa para clasificar observaciones en grupos definidos por una variable categ√≥rica.",
  "Requiere predictores num√©ricos para ajustar distribuciones normales multivariadas.",
  "Modela cada clase como una combinaci√≥n de distribuciones gaussianas, permitiendo formas no lineales.",
  "No hay residuos como en regresi√≥n, ya que se trata de una tarea de clasificaci√≥n.",
  "No eval√∫a independencia cl√°sica de errores; se enfoca en estimar la densidad condicional.",
  "Permite varianza distinta entre componentes, pero se puede ajustar homogeneidad seg√∫n implementaci√≥n.",
  "Outliers pueden afectar las medias y varianzas estimadas de las mezclas gaussianas.",
  "La multicolinealidad puede dificultar la estimaci√≥n de matrices de covarianza.",
  "Interpretar los componentes internos (medias y pesos) puede ser complejo, pero ofrece buena visualizaci√≥n.",
  "Es m√°s lento que LDA o QDA por su naturaleza iterativa y uso de EM (Expectation-Maximization).",
  "Se puede usar validaci√≥n cruzada para seleccionar el n√∫mero √≥ptimo de mezclas por clase.",
  "Si las clases no se ajustan bien a combinaciones de gaussianas, el modelo pierde precisi√≥n."
)

tabla_mda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MDA",
             subtitle = "Mixture Discriminant Analysis (MDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Multidimensional Scaling (MDS)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/MDS.png"))
```


El **Escalamiento Multidimensional (MDS)** es una t√©cnica de **reducci√≥n de dimensionalidad** utilizada para visualizar y explorar las **similitudes o disimilitudes** entre un conjunto de objetos. Su objetivo principal es tomar datos de alta dimensi√≥n, donde las relaciones entre los puntos pueden ser dif√≠ciles de entender, y representarlos en un **espacio de menor dimensi√≥n** (t√≠picamente 2D o 3D) de tal manera que las **distancias entre los puntos en el nuevo espacio reflejen lo m√°s fielmente posible las distancias (o disimilitudes) originales** entre los objetos.   

Imagina que tienes una tabla de distancias de viaje entre varias ciudades. MDS intentar√≠a dibujar un mapa de esas ciudades donde las distancias en el mapa se correspondieran lo m√°s posible con las distancias de la tabla.   

El proceso general de MDS implica:   

1.  **Matriz de Disimilitud:** Se necesita una matriz que contenga las disimilitudes (distancias) entre cada par de objetos. Estas disimilitudes pueden ser distancias euclidianas, correlaciones, o cualquier otra medida de qu√© tan diferentes (o similares) son dos objetos.  
2.  **Optimizaci√≥n:** El algoritmo busca una configuraci√≥n de puntos en el espacio de menor dimensi√≥n que minimice una **funci√≥n de "estr√©s" o "ajuste"**. Esta funci√≥n mide qu√© tan bien las distancias en el espacio reducido se corresponden con las disimilitudes originales. Una funci√≥n de estr√©s baja indica un buen ajuste.  
3.  **Visualizaci√≥n:** Los puntos resultantes en el espacio de menor dimensi√≥n pueden ser graficados para revelar patrones, clusters o la estructura subyacente de los datos que no eran evidentes en las dimensiones originales.  

Existen varias variantes de MDS, como el **MDS Cl√°sico (o M√©trica)**, que asume que las disimilitudes son distancias euclidianas y busca una soluci√≥n anal√≠tica, y el **MDS No-M√©trico**, que solo busca preservar el **orden** de las disimilitudes (es decir, si A es m√°s diferente de B que de C, esa relaci√≥n se mantendr√° en el espacio reducido, sin que las distancias exactas tengan que ser iguales).  


**Aprendizaje Global vs. Local:**  

El Escalamiento Multidimensional (MDS) se considera predominantemente una t√©cnica de **aprendizaje global**.   

* **Aspecto Global:** MDS busca una **configuraci√≥n √∫nica de puntos** en el espacio de baja dimensi√≥n que optimice el ajuste de **todas las disimilitudes** en el conjunto de datos de manera simult√°nea. La funci√≥n de estr√©s que se minimiza considera las distancias entre *todos* los pares de puntos, buscando una soluci√≥n que sea globalmente la mejor representaci√≥n de esas relaciones. El objetivo es preservar la estructura general de las distancias en el conjunto de datos completo, no solo las relaciones en vecindarios espec√≠ficos. La soluci√≥n que se encuentra es una "vista a√©rea" o un "mapa" de las relaciones de todo el conjunto de datos.   

Aunque las disimilitudes originales son "locales" en el sentido de que son medidas entre pares de puntos, la forma en que MDS utiliza todas estas medidas para construir un mapa coherente y de baja dimensi√≥n es un proceso global de optimizaci√≥n. No se ajustan modelos separados para diferentes subconjuntos de datos; en su lugar, se busca una representaci√≥n unificada que capture la estructura general de similaridad/disimilitud de todos los datos. Por lo tanto, si los datos tienen una estructura global bien definida basada en distancias, MDS es una herramienta efectiva para revelar esa estructura.   


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (requiere matriz de distancias)",
  "‚úÖ No lineal en MDS no cl√°sico; lineal en MDS cl√°sico",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è S√≠, valores at√≠picos afectan distancias",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Interpretaci√≥n visual en 2D o 3D, no en ejes significativos",
  "‚ùå Lento si se usan distancias complejas o muchos puntos",
  "‚ö†Ô∏è Validaci√≥n mediante 'stress' y visualizaci√≥n",
  "‚ùå Mal desempe√±o con datos sin estructura o ruido elevado"
)

detalles <- c(
  "M√©todo no supervisado que proyecta datos de alta dimensi√≥n en espacios de 2D o 3D preservando distancias entre puntos.",
  "No busca predecir una variable, solo representar relaciones de cercan√≠a entre observaciones.",
  "Se basa en distancias euclidianas u otras m√©tricas aplicadas a datos num√©ricos.",
  "MDS cl√°sico es lineal; el no cl√°sico (por ejemplo metric o non-metric MDS) puede modelar relaciones no lineales.",
  "No se modelan residuos, por lo que no aplica la normalidad.",
  "No hay errores de predicci√≥n, por tanto no aplica este supuesto.",
  "No hay varianzas residuales, por lo que este supuesto tampoco aplica.",
  "Valores extremos modifican distancias y distorsionan la representaci√≥n espacial.",
  "Al no haber regresores, la multicolinealidad no es un problema.",
  "El mapa generado se interpreta por proximidad relativa, no por pesos o coeficientes.",
  "Puede ser costoso computacionalmente si hay muchos puntos o si se optimiza la funci√≥n de estr√©s.",
  "Se eval√∫a qu√© tan bien se preservan las distancias originales con la m√©trica de estr√©s o visualmente.",
  "No funciona bien si los datos no tienen estructura clara, est√°n muy dispersos o contienen ruido irrelevante."
)

tabla_mds <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mds %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MDS",
             subtitle = "Multidimensional Scaling (MDS)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Quadratic Discriminant Analysis (QDA) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/QDA.png"))
```

El **An√°lisis Discriminante Cuadr√°tico (QDA)** es un m√©todo de **clasificaci√≥n** que, al igual que el An√°lisis Discriminante Lineal (LDA), modela la distribuci√≥n de cada clase para clasificar nuevas observaciones. Sin embargo, QDA es una extensi√≥n de LDA que relaja una de sus suposiciones clave: mientras que LDA asume que todas las clases comparten la misma matriz de covarianza (es decir, las distribuciones tienen la misma "forma" o "orientaci√≥n"), **QDA permite que cada clase tenga su propia matriz de covarianza distinta**.  

Esta diferencia es fundamental:  
* **LDA:** Asume que la variaci√≥n de los datos es la misma en todas las clases, lo que resulta en **fronteras de decisi√≥n lineales** entre las clases.  
* **QDA:** Permite que la variaci√≥n de los datos sea diferente para cada clase, lo que resulta en **fronteras de decisi√≥n cuadr√°ticas** entre las clases. Esto significa que las fronteras de decisi√≥n pueden ser curvas (elipsoides, par√°bolas, hip√©rbolas), lo que permite a QDA modelar relaciones m√°s complejas y no lineales entre las variables y las clases.  

El funcionamiento de QDA implica:  
1.  **Modelado de Distribuciones:** Para cada clase, QDA estima la media y la matriz de covarianza espec√≠ficas de esa clase, asumiendo una distribuci√≥n normal multivariada.  
2.  **Clasificaci√≥n:** Para una nueva observaci√≥n, QDA calcula la probabilidad de que esa observaci√≥n provenga de cada clase, utilizando las distribuciones normales modeladas para cada clase. Luego, asigna la observaci√≥n a la clase con la probabilidad posterior m√°s alta (aplicando el Teorema de Bayes).  

**Aprendizaje Global vs. Local:**  

El An√°lisis Discriminante Cuadr√°tico (QDA) es, al igual que LDA, un modelo de **aprendizaje global**.  

* **Aspecto Global:** QDA construye un **clasificador global** basado en las distribuciones de probabilidad aprendidas para cada clase. Las medias y las matrices de covarianza se estiman a partir de todo el conjunto de datos de entrenamiento para cada clase, y estos par√°metros definen una funci√≥n discriminante que se aplica de manera uniforme a cualquier nueva observaci√≥n en el espacio de caracter√≠sticas. La frontera de decisi√≥n, aunque cuadr√°tica y no lineal, es una √∫nica funci√≥n matem√°tica definida a nivel global por los par√°metros del modelo. No se ajustan modelos separados para diferentes vecindarios de datos.  

* **Mayor Flexibilidad Globalmente:** Aunque sigue siendo un modelo global, la capacidad de QDA para tener matrices de covarianza separadas para cada clase le otorga una **mayor flexibilidad para adaptarse a formas de clase m√°s diversas** en comparaci√≥n con LDA. Esto significa que QDA puede modelar situaciones donde las clases tienen diferentes orientaciones o dispersiones en el espacio de caracter√≠sticas, lo que resulta en fronteras de decisi√≥n que pueden capturar ciertas no linealidades de manera global. Sin embargo, sigue asumiendo distribuciones gaussianas para cada clase y una forma cuadr√°tica para las fronteras, lo que puede ser una limitaci√≥n si la verdadera complejidad de los datos es a√∫n mayor o no se ajusta a estas suposiciones.  



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas",
  "‚úÖ Modela separaci√≥n cuadr√°tica entre clases",
  "‚ùå No aplica (clasificaci√≥n, no regresi√≥n)",
  "‚ùå No aplica (se asume independencia dentro de clases)",
  "‚ùå No se asume homoscedasticidad (cada clase tiene su propia matriz de covarianza)",
  "‚ö†Ô∏è Puede ser muy sensible a outliers (afectan las matrices de covarianza)",
  "‚ö†Ô∏è Puede verse afectado, especialmente si hay pocos datos",
  "‚úÖ Relativamente interpretable (fronteras no lineales entre clases)",
  "‚ö†Ô∏è M√°s costoso que LDA; ineficiente con pocos datos o muchas variables",
  "‚úÖ Recomendado para evitar overfitting, especialmente con pocos datos",
  "‚ùå Si hay pocos datos por clase, estimar matrices de covarianza es inestable"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n que permite que cada clase tenga su propia matriz de covarianza.",
  "Se utiliza para predecir a qu√© clase pertenece una observaci√≥n con base en sus caracter√≠sticas.",
  "Requiere predictores num√©ricos continuos, ya que calcula medias y covarianzas.",
  "A diferencia de LDA, permite fronteras no lineales al no asumir varianzas iguales entre clases.",
  "No tiene residuos como en regresi√≥n, por lo que el supuesto de normalidad de errores no aplica.",
  "No aplica el supuesto de independencia de errores; se enfoca en la distribuci√≥n conjunta por clase.",
  "Cada clase tiene su propia varianza y covarianza, lo que lo hace m√°s flexible que LDA.",
  "Valores extremos pueden distorsionar la estimaci√≥n de medias y covarianzas de cada clase.",
  "Multicolinealidad puede dificultar la inversi√≥n de la matriz de covarianza en clases peque√±as.",
  "Los coeficientes y decisiones son interpretables en t√©rminos de separaciones estad√≠sticas entre clases.",
  "M√°s lento y costoso computacionalmente que LDA, especialmente con muchas variables.",
  "La validaci√≥n cruzada ayuda a prevenir sobreajuste y a seleccionar caracter√≠sticas relevantes.",
  "Con clases poco representadas o muchas variables, las matrices de covarianza pueden volverse inestables."
)

tabla_qda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles) 

tabla_qda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir QDA",
             subtitle = "Quadratic Discriminant Analysis (QDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

  
  
## Partial Least Squares Regression (PLSR)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/PLSR.png"))
```

**Partial Least Squares Regression (PLSR)** es una t√©cnica de **regresi√≥n multivariada** que combina caracter√≠sticas de la **regresi√≥n por m√≠nimos cuadrados ordinarios (OLS)** y el **an√°lisis de componentes principales (PCA)**. Se utiliza para modelar la relaci√≥n entre un conjunto de variables predictoras (X) y uno o m√°s conjuntos de variables de respuesta (Y), siendo particularmente √∫til en situaciones donde hay un gran n√∫mero de variables predictoras, **multicolinealidad** (altas correlaciones entre las variables predictoras), o cuando el n√∫mero de predictoras excede el n√∫mero de observaciones.  

La idea fundamental de PLSR es encontrar un conjunto de **componentes latentes** (tambi√©n conocidos como "factores" o "variables latentes") tanto en el espacio de las variables X como en el de las variables Y. Estos componentes se construyen de tal manera que **maximizan la covarianza** entre las variables predictoras y las variables de respuesta. A diferencia de PCA, que solo busca componentes que expliquen la m√°xima varianza en X, PLSR busca componentes que sean relevantes para explicar la varianza en X *y* que tambi√©n est√©n altamente correlacionados con Y. Una vez que se extraen estos componentes, se realiza una regresi√≥n de m√≠nimos cuadrados ordinarios de Y sobre estos componentes latentes.  

El proceso general de PLSR implica:  

1.  **Extracci√≥n de Componentes Latentes:** PLSR construye iterativamente un conjunto de componentes latentes. En cada paso:
    * Identifica una combinaci√≥n lineal de las variables X (un componente de X) y una combinaci√≥n lineal de las variables Y (un componente de Y) que tienen la mayor covarianza entre s√≠.  
    * Estos componentes representan las direcciones en el espacio de datos que explican la mayor cantidad de la relaci√≥n entre X y Y.
    * Una vez que se extrae un componente, la varianza explicada por ese componente se "deflacta" (se elimina) de las matrices X e Y, y el proceso se repite con los residuos para encontrar el siguiente componente ortogonal.  
2.  **Regresi√≥n:** Una vez que se ha determinado el n√∫mero √≥ptimo de componentes latentes (a menudo a trav√©s de validaci√≥n cruzada), se realiza una regresi√≥n lineal est√°ndar de las variables Y sobre estos componentes latentes de X.  

**Ventajas clave de PLSR:**  

* **Manejo de Multicolinealidad:** Es muy efectivo en la reducci√≥n de dimensionalidad y el manejo de predictoras altamente correlacionadas, donde la regresi√≥n OLS fallar√≠a o producir√≠a estimaciones inestables.  
* **Manejo de Datos de Alta Dimensionalidad:** Funciona bien cuando el n√∫mero de variables predictoras es mayor que el n√∫mero de observaciones.   
* **Enfoque Predictivo:** Se centra en desarrollar modelos con una fuerte capacidad predictiva.  

**Aprendizaje Global vs. Local:**  
  
La Regresi√≥n por M√≠nimos Cuadrados Parciales (PLSR) se considera un modelo de **aprendizaje global**.  

* **Aspecto Global:** PLSR construye un **modelo lineal global** que relaciona las variables predictoras con la variable de respuesta a trav√©s de sus componentes latentes. Los componentes PLS se derivan de la estructura de covarianza de **todas las variables** (tanto predictoras como de respuesta) en el conjunto de datos completo, y el modelo de regresi√≥n final se ajusta sobre estos componentes, generando una ecuaci√≥n que se aplica de manera consistente a cualquier nueva observaci√≥n. No se ajustan modelos separados para diferentes vecindarios de datos; en cambio, se busca una transformaci√≥n global de los datos que facilite la predicci√≥n.  

Si bien PLSR no es un m√©todo de **regresi√≥n ponderada localmente** como LOESS (que ajusta modelos simples a subconjuntos locales de datos), comparte con ellos el objetivo de modelar relaciones complejas. Sin embargo, lo hace de una manera diferente. En lugar de dividir el espacio de caracter√≠sticas y aplicar modelos locales, PLSR transforma el espacio de caracter√≠sticas de forma global para encontrar una representaci√≥n de menor dimensionalidad que sea √≥ptima para la predicci√≥n. Cuando los datos no se distribuyen linealmente, PLSR puede no ser la herramienta m√°s adecuada en su forma lineal b√°sica, ya que sigue siendo una t√©cnica lineal. Sin embargo, al encontrar las direcciones m√°s relevantes en el espacio de los datos, puede capturar aspectos importantes de la estructura de los datos que son √∫tiles incluso si la relaci√≥n subyacente es no lineal. Para manejar la no linealidad expl√≠citamente, existen extensiones como **Nonlinear Partial Least Squares (NPLS)** o **Kernel PLS (KPLS)**, que introducen funciones kernel para mapear los datos a un espacio de caracter√≠sticas de mayor dimensi√≥n donde la relaci√≥n podr√≠a ser linealmente modelable por PLS.  


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado (regresi√≥n y clasificaci√≥n con adaptaci√≥n)",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n si se transforma)",
  "‚úÖ Num√©ricas (requiere escalado), categ√≥ricas como dummies",
  "‚úÖ Captura relaciones lineales y reduce dimensiones simult√°neamente",
  "‚ùå No requiere estrictamente, pero mejora con residuos normales",
  "‚úÖ Deseable, aunque no cr√≠tico",
  "‚úÖ Deseable para homogeneizar varianza tras escalado",
  "‚ö†Ô∏è Moderado (outliers pueden influir en componentes latentes)",
  "‚úÖ Dise√±ado para alta colinealidad entre predictores",
  "‚ö†Ô∏è Media (componentes latentes son interpretables, pero relaciones pueden ser complejas)",
  "‚ö†Ô∏è Moderada (depende de n√∫mero de componentes y tama√±o del dataset)",
  "‚úÖ Usar k-fold para elegir n√∫mero de componentes √≥ptimos",
  "‚ùå No funciona bien si relaciones son muy no lineales o datos muy ruidosos sin preprocesar"
)

detalles <- c(
  "Modelo que proyecta predictores y respuesta a espacios latentes para maximizar covarianza.",
  "PLSR encuentra componentes que explican varianza en X y covarianza con Y.",
  "Todas las variables num√©ricas deben escalarse; convertir categ√≥ricas en indicadores.",
  "Combina reducci√≥n de dimensi√≥n (PCA-like) con regresi√≥n en componentes latentes.",
  "No impone supuestos estrictos, pero residuos normales facilitan inferencia estad√≠stica.",
  "Mejor si muestras son independientes; RLSR en datos correlacionados requiere cuidado.",
  "Escalar y homogeneizar predictores e incluso respuesta mejora la estabilidad.",
  "Outliers extremos pueden distorsionar c√°lculo de componentes; usar robust PLSR para mitigarlo.",
  "PLSR maneja colinealidad al construir pocas componentes que representan grupos de variables correlacionadas.",
  "Componentes latentes tienen pesos interpretables, pero interpretar combinaciones puede ser complejo.",
  "El m√©todo usa descomposici√≥n de matrices; eficiente con BLAS/LAPACK optimizado.",
  "Validaci√≥n cruzada ayuda a determinar el n√∫mero √≥ptimo de componentes latentes a usar.",
  "No es adecuado para relaciones puramente no lineales; en ese caso usar Kernel PLSR o m√©todos no lineales."
)

tabla_plsr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_plsr %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PLSR",
             subtitle = "Partial Least Squares Regression (PLSR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
``` 


## Partial Least Squares Discriminant Analysis (PLSDA)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/PLSDA.png"))
```

El **An√°lisis Discriminante de M√≠nimos Cuadrados Parciales (PLSDA)** es una extensi√≥n del algoritmo de **Regresi√≥n por M√≠nimos Cuadrados Parciales (PLSR)**, adaptada para problemas de **clasificaci√≥n**. Al igual que PLSR, PLSDA es particularmente √∫til cuando se tienen muchas **variables predictoras (X)** y estas est√°n **altamente correlacionadas (multicolinealidad)**, situaciones comunes en campos como la metabol√≥mica, la prote√≥mica o la espectroscopia.

En esencia, PLSDA transforma un problema de clasificaci√≥n en un problema de regresi√≥n. Esto se logra de la siguiente manera:

1.  **Codificaci√≥n de la Variable de Clase:** La variable de respuesta categ√≥rica (la clase a la que pertenece una observaci√≥n) se transforma en una o m√°s variables num√©ricas. Por ejemplo, en un problema de clasificaci√≥n binaria, una clase puede codificarse como '0' y la otra como '1'. Para m√∫ltiples clases, se puede usar una codificaci√≥n "one-hot encoding" (ej., [1,0,0] para Clase A, [0,1,0] para Clase B, etc.).
2.  **Extracci√≥n de Componentes Latentes:** Similar a PLSR, PLSDA construye componentes latentes (factores PLS) que son combinaciones lineales de las variables predictoras. Estos componentes se eligen para maximizar la covarianza entre las variables predictoras y las variables de respuesta codificadas. Esto asegura que los componentes capturen la varianza en X que es relevante para la separaci√≥n de clases en Y.
3.  **Clasificaci√≥n:** Una vez que se han obtenido los componentes PLS y se ha realizado la regresi√≥n sobre ellos para predecir los valores codificados de la clase, se aplica una regla de decisi√≥n (por ejemplo, un umbral o un clasificador lineal simple) a las predicciones para asignar cada observaci√≥n a una clase. Si se usa codificaci√≥n one-hot, la observaci√≥n se asigna a la clase con el valor predicho m√°s alto.

PLSDA es ventajoso porque puede manejar conjuntos de datos con muchas m√°s variables que observaciones (problemas $p \gg n$), y es robusto a la multicolinealidad.


**Aprendizaje Global vs. Local:**

El An√°lisis Discriminante de M√≠nimos Cuadrados Parciales (PLSDA) es un modelo de **aprendizaje global**.

* **Aspecto Global:** PLSDA busca una **transformaci√≥n lineal global** de las variables predictoras a componentes latentes, y luego una **relaci√≥n lineal global** entre esos componentes y la variable de respuesta codificada (clase). Los componentes PLS se derivan de la estructura de covarianza de todo el conjunto de datos, y el modelo de regresi√≥n final (que se usa para la clasificaci√≥n) se aplica de manera consistente a cualquier nueva observaci√≥n. La frontera de decisi√≥n impl√≠cita en PLSDA es t√≠picamente lineal en el espacio de los componentes PLS (y por lo tanto lineal o una combinaci√≥n lineal de las variables originales), lo que resulta en un clasificador que opera globalmente en el espacio de caracter√≠sticas.

* **Enfoque en la Relevancia Global:** Aunque reduce la dimensionalidad y selecciona componentes que son relevantes para la respuesta, la soluci√≥n final es un mapeo y una regla de decisi√≥n que son v√°lidos para todo el dominio de los datos. No ajusta modelos locales para diferentes regiones del espacio de caracter√≠sticas. Por lo tanto, PLSDA es una t√©cnica eficiente para encontrar patrones globales de separaci√≥n de clases en presencia de alta dimensionalidad y multicolinealidad, pero si las relaciones entre las variables y las clases son inherentemente no lineales o tienen estructuras muy complejas que no pueden ser capturadas por una transformaci√≥n lineal, su capacidad puede ser limitada.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (binaria o multicategor√≠a)",
  "‚úÖ Num√©ricas (se proyectan a componentes)",
  "‚úÖ Captura relaciones lineales y no lineales a trav√©s de proyecciones",
  "‚ùå No aplica directamente (modelo de clasificaci√≥n)",
  "‚ùå No aplica como en regresi√≥n cl√°sica",
  "‚ùå No se eval√∫a como en modelos de regresi√≥n",
  "‚ö†Ô∏è Algo sensible a outliers (pueden influir en componentes)",
  "‚úÖ Muy √∫til si hay multicolinealidad",
  "‚ö†Ô∏è Menos interpretable que modelos cl√°sicos; depende de componentes",
  "‚úÖ Eficiente, especialmente con datos de alta dimensi√≥n",
  "‚úÖ Se recomienda usar validaci√≥n cruzada para elegir el n√∫mero de componentes",
  "‚ùå Si las proyecciones no separan bien las clases o hay mucho ruido"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n basado en PLS (Partial Least Squares) que proyecta los datos para maximizar la separaci√≥n entre clases.",
  "Se requiere que la variable dependiente sea categ√≥rica. PLS-DA funciona bien con 2 o m√°s clases.",
  "Las variables predictoras deben ser num√©ricas para que el modelo pueda proyectarlas en componentes latentes.",
  "El modelo encuentra combinaciones de predictores que mejor separan las clases en el espacio proyectado.",
  "No se eval√∫a normalidad de residuos como en modelos de regresi√≥n; la salida es de clasificaci√≥n.",
  "Tampoco aplica la independencia cl√°sica de errores ya que se clasifican observaciones.",
  "El supuesto de homoscedasticidad no es relevante aqu√≠.",
  "Outliers pueden afectar la construcci√≥n de componentes, distorsionando la separaci√≥n de clases.",
  "PLS-DA es √∫til cuando los predictores est√°n altamente correlacionados, ya que crea componentes ortogonales.",
  "Los componentes no son directamente interpretables como las variables originales, aunque se pueden analizar los pesos de carga.",
  "Es un algoritmo relativamente eficiente, especialmente para conjuntos con muchas variables.",
  "La validaci√≥n cruzada es cr√≠tica para seleccionar el n√∫mero √≥ptimo de componentes y evitar overfitting.",
  "No funciona bien si las clases no est√°n bien separadas en el espacio proyectado o si hay demasiado ruido en los datos."
)

tabla_plsda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_plsda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PLSDA",
             subtitle = "Partial Least Squares Discriminant Analysis (PLSDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Principal Component Analysis (PCA)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/PCA.png"))
```

El **An√°lisis de Componentes Principales (PCA)** es una t√©cnica fundamental de **reducci√≥n de dimensionalidad** no supervisada. Su objetivo principal es simplificar conjuntos de datos complejos con muchas variables, transform√°ndolos en un conjunto m√°s peque√±o de nuevas variables, llamadas **componentes principales**, sin perder demasiada informaci√≥n. Estos componentes principales son combinaciones lineales de las variables originales y son **ortogonales (no correlacionados)** entre s√≠.

PCA funciona identificando las direcciones en el espacio de datos donde la **varianza es m√°xima**. La primera componente principal (PC1) captura la mayor cantidad de varianza posible en los datos. La segunda componente principal (PC2) captura la mayor varianza restante, sujeta a ser ortogonal a la primera, y as√≠ sucesivamente. De esta manera, PCA organiza la varianza en los datos en un conjunto jer√°rquico de componentes.

Los usos comunes de PCA incluyen:
* **Reducci√≥n de dimensionalidad:** Disminuir el n√∫mero de variables en un dataset, lo que puede acelerar los algoritmos de Machine Learning y reducir el riesgo de sobreajuste.
* **Visualizaci√≥n de datos:** Proyectar datos de alta dimensi√≥n en 2D o 3D para facilitar su visualizaci√≥n y la identificaci√≥n de patrones, clusters o outliers.
* **Denoising:** Eliminar el ruido de los datos al retener solo los componentes principales que capturan la se√±al real.


**Aprendizaje Global vs. Local:**

El An√°lisis de Componentes Principales (PCA) es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** PCA busca una **transformaci√≥n lineal global** del espacio de caracter√≠sticas. Los componentes principales se derivan de la matriz de covarianza (o correlaci√≥n) de **todo el conjunto de datos**. Esto significa que las direcciones de m√°xima varianza se determinan considerando la estructura de dispersi√≥n general de todos los puntos de datos. El conjunto de componentes principales que se obtiene es un sistema de coordenadas global al que se proyecta cualquier punto de datos. No se ajustan diferentes transformaciones para distintas regiones o vecindarios de datos; en su lugar, se aprende una √∫nica proyecci√≥n que se aplica uniformemente a todo el dominio.

Por lo tanto, si la estructura de los datos es consistentemente lineal o tiene relaciones de varianza que se extienden linealmente a lo largo del espacio, PCA funcionar√° muy bien. Sin embargo, si los datos tienen estructuras no lineales complejas (por ejemplo, datos que forman una espiral o una esfera), PCA puede tener limitaciones para capturar estas relaciones, ya que solo busca direcciones lineales de m√°xima varianza.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è No supervisado (reducci√≥n de dimensiones)",
  "‚ùå No aplica (no hay target a predecir)",
  "‚úÖ Num√©ricas (requiere escalado), categ√≥ricas procesadas como dummies",
  "‚úÖ Captura correlaciones lineales entre predictores",
  "‚ùå No requiere supuestos de distribuci√≥n en residuos",
  "‚ö†Ô∏è Ideal si las observaciones son independientes, aunque no cr√≠tico",
  "‚úÖ Deseable (datos homogenizados tras escalado)",
  "‚ö†Ô∏è Moderado (outliers pueden distorsionar componentes principales)",
  "‚úÖ Sensible a colinealidad (reduce variables correlacionadas a componentes)",
  "‚ö†Ô∏è Media (componentes lineales son interpretables, pero combinaciones pueden no serlo)",
  "‚úÖ R√°pido en datasets medianos; escalable con √°lgebra lineal optimizada",
  "‚ö†Ô∏è No se aplica CV cl√°sico; se puede usar reconstrucci√≥n de error o validaci√≥n por bloques",
  "‚ùå No funciona bien si las relaciones son no lineales o datos muy ruidosos sin preprocesar"
)

detalles <- c(
  "M√©todo no supervisado para reducir la dimensi√≥n del espacio de predictores.",
  "No predice variables, se centra en variabilidad interna de los datos.",
  "Todas las variables num√©ricas deben escalarse; las categ√≥ricas convertir a variables indicadoras.",
  "Busca direcciones (componentes) que maximizan varianza lineal entre predictores.",
  "No impone supuestos sobre errores; se basa en descomposici√≥n de la matriz de covarianza.",
  "Mejor si las muestras no est√°n correlacionadas en el tiempo o espacialmente.",
  "Escalar y homogeneizar mejora el c√°lculo de componentes principales.",
  "Outliers extremos pueden sesgar la direcci√≥n de los componentes principales.",
  "Reduce colinealidad al combinar variables correlacionadas en componentes ortogonales.",
  "Componentes iniciales pueden interpretarse mediante pesos, pero componentes posteriores son combinaciones lineales complejas.",
  "Computaci√≥n depende de descomposici√≥n de matrices (SVD), es eficiente con optimizaci√≥n BLAS.",
  "Se puede evaluar n√∫mero √≥ptimo de componentes con validaci√≥n de reconstrucci√≥n o bootstrap de SVD.",
  "No apto para relaciones no lineales complejas; en tal caso usar Kernel PCA o m√©todos no lineales."
)

tabla_pca <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pca %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PCA",
             subtitle = "Principal Component Analysis (PCA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Principal Component Regression (PCR)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/PCR.png"))
```

La **Regresi√≥n de Componentes Principales (PCR)** es un m√©todo de **regresi√≥n** que combina el **An√°lisis de Componentes Principales (PCA)** con la **Regresi√≥n por M√≠nimos Cuadrados Ordinarios (OLS)**. Su principal utilidad radica en situaciones donde se tienen muchas **variables predictoras (X)** y existe una **alta multicolinealidad** (fuerte correlaci√≥n entre ellas), lo que puede hacer que los modelos de regresi√≥n OLS sean inestables o ineficientes.

El proceso de PCR consta de dos pasos principales:

1.  **Reducci√≥n de Dimensionalidad con PCA:** Primero, se aplica PCA a las variables predictoras (X) para transformarlas en un conjunto m√°s peque√±o de **componentes principales**. Estos componentes son combinaciones lineales no correlacionadas de las variables originales y capturan la mayor parte de la varianza en las variables X. Se selecciona un subconjunto de estos componentes principales (aquellos que explican la mayor parte de la varianza total) para retener. Es importante destacar que, en este paso, PCA no tiene conocimiento de la variable de respuesta (Y); solo se enfoca en la estructura de las variables X.
2.  **Regresi√≥n OLS sobre Componentes:** Una vez que se han obtenido los componentes principales seleccionados, se realiza una regresi√≥n lineal est√°ndar (OLS) de la variable de respuesta (Y) sobre estos componentes. Como los componentes principales son ortogonales, la multicolinealidad ya no es un problema en este paso de regresi√≥n.

El beneficio de PCR es que permite construir un modelo de regresi√≥n en escenarios con multicolinealidad severa, reduciendo el n√∫mero de variables a un conjunto m√°s manejable y estable, mientras se intenta preservar la mayor cantidad de informaci√≥n de las variables predictoras.

**Aprendizaje Global vs. Local:**

La Regresi√≥n de Componentes Principales (PCR) es un modelo de **aprendizaje global**.

* **Aspecto Global:** Ambos pasos de PCR son intr√≠nsecamente globales.
    1.  **PCA (Paso Global):** Como se mencion√≥ anteriormente, PCA es una t√©cnica global que encuentra una transformaci√≥n lineal de los datos que se aplica de manera uniforme a todo el espacio de caracter√≠sticas. Los componentes principales se derivan de la estructura de varianza global de las variables predictoras.
    2.  **OLS (Paso Global):** La regresi√≥n realizada sobre los componentes principales es un modelo OLS est√°ndar, que tambi√©n es una t√©cnica global. Busca una √∫nica relaci√≥n lineal que se aplica a todos los datos transformados.

En conjunto, PCR construye una **funci√≥n de regresi√≥n global** que mapea el espacio de caracter√≠sticas original (transformado a componentes principales) a la variable de respuesta. La soluci√≥n resultante es una ecuaci√≥n que se aplica de manera consistente para todas las observaciones, sin ajustar modelos diferentes para subconjuntos locales de datos. Esto significa que si la relaci√≥n entre las variables predictoras y la respuesta es no lineal o cambia dr√°sticamente en diferentes regiones del espacio de caracter√≠sticas, PCR podr√≠a no ser la opci√≥n m√°s flexible, ya que se basa en transformaciones y regresiones lineales globales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (combinaci√≥n de PCA + regresi√≥n)",
  "‚úÖ Variable continua (num√©rica)",
  "‚úÖ Num√©ricas (se aplica PCA primero)",
  "‚úÖ Puede capturar relaciones lineales (con reducci√≥n de dimensionalidad)",
  "‚ö†Ô∏è Requiere verificar residuos del modelo final",
  "‚ö†Ô∏è Se deben revisar los residuos como en regresi√≥n cl√°sica",
  "‚ö†Ô∏è Requiere diagn√≥stico posterior a la regresi√≥n",
  "‚ö†Ô∏è PCA puede estar influenciada por outliers",
  "‚úÖ Reduce multicolinealidad usando componentes ortogonales",
  "‚ö†Ô∏è Menos interpretable (usa componentes, no variables originales)",
  "‚úÖ Eficiente, especialmente con datos de alta dimensi√≥n",
  "‚úÖ Puede usar validaci√≥n cruzada para elegir n√∫mero de componentes",
  "‚ùå Si las primeras componentes no explican bien la variable respuesta"
)

detalles <- c(
  "Modelo supervisado que aplica PCA a los predictores y luego ajusta una regresi√≥n lineal sobre los componentes principales seleccionados.",
  "Se requiere que la variable dependiente sea num√©rica (continua).",
  "Se espera que los predictores sean num√©ricos para aplicar PCA adecuadamente.",
  "PCR puede detectar relaciones lineales al reducir la dimensionalidad primero y luego ajustar la regresi√≥n.",
  "Aunque el PCA es no supervisado, los residuos de la regresi√≥n deben ser normales para cumplir los supuestos de OLS.",
  "Es necesario revisar la independencia de errores como en cualquier regresi√≥n lineal.",
  "Tambi√©n deben analizarse posibles problemas de heterocedasticidad en los residuos.",
  "Outliers pueden influir en los componentes principales y, por lo tanto, en el modelo final.",
  "PCR es muy √∫til cuando los predictores est√°n altamente correlacionados.",
  "Interpretar los resultados puede ser dif√≠cil porque las componentes no corresponden a variables originales.",
  "El proceso es r√°pido incluso con muchos predictores, ya que PCA reduce la dimensi√≥n.",
  "Usualmente se usa validaci√≥n cruzada para determinar cu√°ntas componentes usar.",
  "No es efectivo si los primeros componentes (con mayor varianza) no est√°n relacionados con la variable dependiente."
)

tabla_pcr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pcr %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PCR",
             subtitle = "Principal Component Regression (PCR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Projection Pursuit (PP)  {-}     

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/PP.png"))
```

**Projection Pursuit (PP)** es una t√©cnica estad√≠stica de **reducci√≥n de dimensionalidad y an√°lisis exploratorio de datos** utilizada para encontrar las proyecciones "m√°s interesantes" de datos multivariados de alta dimensi√≥n en un espacio de menor dimensi√≥n (generalmente 1D o 2D). La clave de PP es que las proyecciones "interesantes" son aquellas que **se desv√≠an m√°s de una distribuci√≥n normal (gaussiana)**, ya que las estructuras como agrupaciones, valores at√≠picos, o formas inusuales tienden a ser m√°s evidentes en proyecciones no gaussianas.

El algoritmo de PP no busca simplemente la mayor varianza (como PCA), sino que intenta encontrar direcciones de proyecci√≥n que revelen la estructura subyacente y las caracter√≠sticas no lineales de los datos. Lo hace **maximizando un "√≠ndice de proyecci√≥n"** que mide la "interesante" o la "no-gaussianidad" de la proyecci√≥n. Diferentes √≠ndices pueden enfocarse en diferentes aspectos, como la asimetr√≠a, la curtosis, o la presencia de m√∫ltiples modos (grupos).

Existen variantes de PP para diferentes prop√≥sitos, como:
* **Exploratory Projection Pursuit (EPP):** Para visualizaci√≥n y detecci√≥n de estructuras.
* **Projection Pursuit Regression (PPR):** Para construir modelos de regresi√≥n no lineales.
* **Projection Pursuit Classification (PPC):** Para tareas de clasificaci√≥n.

**Aprendizaje Global vs. Local:**

Projection Pursuit (PP) se puede considerar como un modelo que **combina aspectos de aprendizaje global y local**, con un fuerte √©nfasis en la detecci√≥n de caracter√≠sticas locales en un contexto global.

* **Aspecto Global:** PP busca una **transformaci√≥n lineal global** (la direcci√≥n de proyecci√≥n) que se aplica a todo el conjunto de datos para encontrar las proyecciones "m√°s interesantes". La optimizaci√≥n del √≠ndice de proyecci√≥n se realiza sobre todo el espacio de caracter√≠sticas para identificar estas direcciones. Las funciones resultantes (como en PPR o PPC) son combinaciones de funciones no lineales aplicadas a estas proyecciones globales.

* **Aspecto Local (al revelar estructuras):** Donde PP exhibe un car√°cter "local" es en su capacidad para **resaltar estructuras que son intr√≠nsecamente locales** (como clusters o valores at√≠picos) que podr√≠an estar ocultas en las altas dimensiones o en proyecciones puramente globales (como PCA). Al buscar desviaciones de la normalidad, PP es capaz de "perseguir" (de ah√≠ "pursuit") las direcciones que exponen agrupaciones densas o huecos en los datos, que son fen√≥menos locales. La idea es que si los datos no se distribuyen linealmente o tienen estructuras complejas, PP puede encontrar proyecciones donde la "densidad" o "forma" local de los datos es m√°s informativa, permitiendo al usuario o a un algoritmo posterior identificar estas estructuras que son una forma de "regresi√≥n ponderada localmente" o un an√°lisis local de patrones.

En resumen, PP es una t√©cnica potente para explorar la estructura de datos de alta dimensi√≥n, especialmente cuando las relaciones son no lineales o complejas. Si bien el proceso de b√∫squeda de proyecciones es global, el "inter√©s" de estas proyecciones a menudo radica en su capacidad para revelar caracter√≠sticas locales y no gaussianas que son cruciales para entender los datos.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (requiere matriz de datos)",
  "‚úÖ Detecta proyecciones no lineales con estructura interesante",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede ser sensible a valores extremos",
  "‚ö†Ô∏è Puede verse afectado si hay alta redundancia",
  "‚ö†Ô∏è Interpretaci√≥n m√°s dif√≠cil que PCA; proyecciones no son ortogonales",
  "‚ùå Puede ser lento por b√∫squeda iterativa de proyecciones",
  "‚ö†Ô∏è Validaci√≥n subjetiva o basada en heur√≠sticas de inter√©s",
  "‚ùå No √∫til si no hay estructuras no gaussianas en los datos"
)

detalles <- c(
  "M√©todo no supervisado que busca proyecciones de los datos donde se maximice cierta 'interesantitud' (varianza no gaussiana, agrupamientos, etc.).",
  "No est√° dise√±ado para predicci√≥n, sino para exploraci√≥n visual o estructural.",
  "Se aplica a datos num√©ricos, generalmente estandarizados, buscando direcciones relevantes.",
  "A diferencia del PCA (que busca m√°xima varianza), PP busca patrones como colas pesadas, clusters, o distribuciones no normales.",
  "No es un modelo predictivo, por tanto no se calculan residuos.",
  "No se modela el error; se enfoca en la estructura interna de los datos.",
  "No tiene varianzas residuales, por lo que no aplica homoscedasticidad.",
  "Proyecciones pueden verse distorsionadas por valores extremos.",
  "Variables muy correlacionadas pueden dominar las proyecciones si no se controlan.",
  "Proyecciones son dif√≠ciles de interpretar directamente; pueden requerir an√°lisis posterior.",
  "Requiere m√©todos num√©ricos iterativos para encontrar direcciones de inter√©s, lo que lo vuelve computacionalmente intensivo.",
  "Puede usarse validaci√≥n visual (por ejemplo, si se detectan agrupamientos) o criterios como 'kurtosis'.",
  "Si los datos son gaussianos y no contienen patrones relevantes, PP no encuentra proyecciones √∫tiles."
)

tabla_pp <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pp %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PP",
             subtitle = "Projection Pursuit (PP) ")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Sammon Mapping  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/Sammon Mapping.png"))
```


**Sammon Mapping** es una t√©cnica de **reducci√≥n de dimensionalidad no lineal** que se utiliza para visualizar datos de alta dimensi√≥n en un espacio de menor dimensi√≥n (generalmente 2D o 3D). Su principal objetivo es **preservar la estructura de distancia local** de los datos originales en la representaci√≥n de menor dimensi√≥n.

A diferencia de t√©cnicas como PCA que buscan preservar la varianza global (y por lo tanto las distancias euclidianas globales), Sammon Mapping se enfoca en que las **distancias peque√±as** (entre puntos cercanos) en el espacio original sean representadas con **mayor fidelidad** en el espacio reducido que las distancias grandes. Esto lo hace particularmente bueno para revelar agrupaciones o clusters que podr√≠an estar ocultos en proyecciones lineales o en otras t√©cnicas de reducci√≥n de dimensionalidad que no priorizan las distancias locales.

El algoritmo de Sammon Mapping funciona minimizando una **funci√≥n de "error" o "estr√©s"** espec√≠fica, conocida como el **"estr√©s de Sammon"**. Esta funci√≥n penaliza m√°s fuertemente las grandes discrepancias en las distancias peque√±as que las grandes discrepancias en las distancias grandes. La minimizaci√≥n de esta funci√≥n se realiza mediante un proceso iterativo de descenso de gradiente.

**Aprendizaje Global vs. Local:**

Sammon Mapping es un modelo que exhibe un fuerte car√°cter de **aprendizaje local**, aunque la optimizaci√≥n se realiza sobre la totalidad de los datos.

* **Aspecto Local:** La caracter√≠stica distintiva de Sammon Mapping es su √©nfasis en la **preservaci√≥n de las distancias locales**. Al penalizar m√°s las distancias peque√±as que se deforman en la proyecci√≥n, el algoritmo se esfuerza por mantener a los puntos que estaban cerca en el espacio original, cerca en el espacio de menor dimensi√≥n. Esto es crucial para revelar la **estructura local y las agrupaciones** dentro de los datos. Es como si el algoritmo estuviera haciendo una serie de "regresiones ponderadas localmente" para cada vecindario de puntos, ajustando las posiciones en el mapa de baja dimensi√≥n para que las relaciones cercanas se mantengan. Esta prioridad en las relaciones de vecindad es una marca del aprendizaje local.

* **Optimizaci√≥n Global:** A pesar de su enfoque local, la funci√≥n de estr√©s de Sammon se calcula y se minimiza sobre **todos los pares de puntos** en el conjunto de datos. La soluci√≥n final es una configuraci√≥n global de puntos en el espacio de baja dimensi√≥n. Por lo tanto, el proceso de optimizaci√≥n es global, pero su criterio de "mejor ajuste" da una importancia desproporcionada a la preservaci√≥n de las relaciones locales.

En resumen, Sammon Mapping es una t√©cnica poderosa para visualizar datos de alta dimensi√≥n, especialmente cuando los clusters o las estructuras locales son importantes. Si los datos no se distribuyen linealmente y lo que se busca es entender c√≥mo se agrupan los puntos en sus vecindarios, Sammon Mapping ofrece una representaci√≥n donde las relaciones locales son el foco principal, lo que lo convierte en una excelente herramienta para la exploraci√≥n de estructuras no lineales y la detecci√≥n de agrupaciones.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (distancias euclidianas)",
  "‚úÖ No lineal, mantiene distancias entre puntos",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è S√≠, es sensible a valores at√≠picos",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Interpretaci√≥n visual en 2D o 3D; no en componentes",
  "‚ùå Lento para conjuntos grandes (algoritmo iterativo)",
  "‚ö†Ô∏è Se puede validar visualmente o con estr√©s",
  "‚ùå Mal desempe√±o en datos ruidosos o de alta dimensi√≥n sin estructura"

)

detalles <- c(
  "M√©todo no supervisado para proyectar datos de alta dimensi√≥n en espacios de menor dimensi√≥n preservando distancias.",
  "No busca predecir, sino representar relaciones de cercan√≠a entre observaciones.",
  "Usa distancias entre puntos; solo variables num√©ricas tienen sentido.",
  "A diferencia de PCA, Sammon busca preservar distancias relativas entre puntos originales y proyectados.",
  "No genera residuos como un modelo predictivo, por lo tanto no se aplica la normalidad.",
  "No hay modelo de error porque no hay predicci√≥n.",
  "No aplica el supuesto de homoscedasticidad.",
  "Valores extremos alteran las distancias y distorsionan el mapa resultante.",
  "Como es una t√©cnica de reducci√≥n, no le afecta multicolinealidad directamente.",
  "El mapa resultante puede interpretarse en t√©rminos de proximidad, no de pesos o coeficientes.",
  "Implementaci√≥n cl√°sica es iterativa y costosa computacionalmente en datasets grandes.",
  "Puede usarse estr√©s (error entre distancias originales y proyectadas) como m√©trica de calidad.",
  "Si las distancias no reflejan bien la estructura real (por ruido o dimensiones irrelevantes), el m√©todo falla en representar datos √∫tiles."
)

tabla_sammon <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_sammon %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir sammon mapping",
             subtitle = "Sammon Mapping")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Regularized Discriminant Analysis (RDA)  {-}   


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/RDA.png"))
```

El **An√°lisis Discriminante Regularizado (RDA)** es un m√©todo de **clasificaci√≥n** que act√∫a como un **intermedio flexible entre el An√°lisis Discriminante Lineal (LDA) y el An√°lisis Discriminante Cuadr√°tico (QDA)**. Fue desarrollado por Jerome Friedman para abordar las limitaciones de LDA (que asume covarianzas iguales para todas las clases, lo que resulta en fronteras lineales) y QDA (que permite covarianzas separadas pero puede ser inestable con pocos datos o muchas variables).

RDA introduce dos **par√°metros de regularizaci√≥n**, $\alpha$ y $\gamma$, que controlan la flexibilidad del modelo y su capacidad para adaptarse a los datos:

1.  **Par√°metro $\alpha$ (alpha):** Controla el grado en que la matriz de covarianza de cada clase se **contrae hacia una matriz de covarianza com√∫n** (como en LDA).
    * Si $\alpha = 0$, RDA se comporta como **QDA** (cada clase tiene su propia matriz de covarianza).
    * Si $\alpha = 1$, RDA se comporta como **LDA** (todas las clases comparten una matriz de covarianza com√∫n).
    * Para valores entre 0 y 1, RDA utiliza un promedio ponderado de la matriz de covarianza espec√≠fica de la clase y la matriz de covarianza com√∫n. Esto ayuda a estabilizar las estimaciones de covarianza en QDA, especialmente cuando los tama√±os de muestra son peque√±os o el n√∫mero de variables es grande.

2.  **Par√°metro $\gamma$ (gamma):** Controla el grado en que la matriz de covarianza (ya sea com√∫n o espec√≠fica de la clase, dependiendo de $\alpha$) se **contrae hacia una matriz diagonal**.
    * Si $\gamma = 0$, no hay contracci√≥n diagonal adicional.
    * Si $\gamma = 1$, la matriz de covarianza se contrae completamente a una matriz diagonal (lo que implica independencia entre las variables).
    * Para valores entre 0 y 1, se aplica una contracci√≥n hacia la diagonal, lo que puede ser √∫til cuando hay multicolinealidad.

Al sintonizar estos dos par√°metros (generalmente mediante validaci√≥n cruzada), RDA puede encontrar un equilibrio √≥ptimo entre la simplicidad de LDA y la flexibilidad de QDA, adapt√°ndose mejor a la estructura de covarianza real de los datos y mejorando la estabilidad del modelo.

**Aprendizaje Global vs. Local:**

El An√°lisis Discriminante Regularizado (RDA) es un modelo de **aprendizaje global** que incorpora un grado de **adaptaci√≥n local** a trav√©s de su regularizaci√≥n.

* **Aspecto Global:** Al igual que LDA y QDA, RDA construye un **clasificador global** basado en las distribuciones de probabilidad modeladas para cada clase. Las matrices de covarianza regularizadas y las medias de las clases se estiman a partir de todo el conjunto de datos de entrenamiento, y la regla de clasificaci√≥n resultante se aplica de manera consistente en todo el espacio de caracter√≠sticas. La frontera de decisi√≥n que RDA define es una funci√≥n global (que puede ser lineal o cuadr√°tica, o una combinaci√≥n de ambas, dependiendo de los par√°metros de regularizaci√≥n).

* **Adaptaci√≥n Local (a trav√©s de la regularizaci√≥n de covarianza):** La flexibilidad de RDA para ajustarse mejor a los datos que LDA o QDA proviene de su capacidad para modelar las **estructuras de covarianza de las clases de una manera m√°s matizada**. Al permitir una contracci√≥n parcial de las matrices de covarianza hacia una com√∫n (par√°metro $\alpha$) o hacia una diagonal (par√°metro $\gamma$), RDA puede adaptar las formas de las distribuciones de las clases. Esto permite que el modelo capture mejor las caracter√≠sticas de dispersi√≥n de los datos en diferentes regiones, lo que en √∫ltima instancia se traduce en fronteras de decisi√≥n m√°s adaptables que pueden manejar cierto grado de no linealidad o formas complejas de clase. No es un ajuste local en el sentido de LOESS, sino una forma de adaptar la complejidad del modelo global a la estructura de covarianza percibida de cada clase.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas",
  "‚úÖ No lineal (transici√≥n entre LDA y QDA)",
  "‚ùå No aplica (no es un modelo de regresi√≥n)",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Controla la homoscedasticidad mediante regularizaci√≥n",
  "‚ö†Ô∏è Puede ser sensible, aunque la regularizaci√≥n reduce impacto",
  "‚úÖ Reduce impacto mediante regularizaci√≥n de covarianzas",
  "‚ö†Ô∏è Menos interpretable que LDA/QDA puro, pero con mayor flexibilidad",
  "‚úÖ M√°s eficiente que QDA en conjuntos peque√±os o ruidosos",
  "‚úÖ Muy √∫til para evitar overfitting, sobre todo con validaci√≥n cruzada",
  "‚ùå Puede no mejorar sobre LDA/QDA si no hay problemas de varianza o sobreajuste"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n que combina LDA y QDA usando par√°metros de regularizaci√≥n.",
  "Clasifica observaciones en clases discretas bas√°ndose en variables num√©ricas predictoras.",
  "Requiere variables num√©ricas para calcular medias y covarianzas por clase.",
  "Introduce par√°metros de mezcla que ajustan la matriz de covarianza hacia la identidad (como ridge) y hacia la covarianza com√∫n.",
  "No genera residuos como un modelo de regresi√≥n, por lo tanto el supuesto no aplica.",
  "No se enfoca en errores independientes, sino en distribuciones de clase.",
  "La regularizaci√≥n suaviza las diferencias entre covarianzas, mitigando problemas de homoscedasticidad.",
  "Los valores at√≠picos pueden influir en la estimaci√≥n, pero se reduce con regularizaci√≥n.",
  "Mejor manejo de multicolinealidad que QDA gracias a la matriz regularizada.",
  "La interpretaci√≥n depende de los valores de regularizaci√≥n elegidos; m√°s flexible pero menos directa.",
  "Reduce complejidad computacional respecto a QDA; √∫til con pocas observaciones por clase.",
  "Es com√∫n usar validaci√≥n cruzada para seleccionar los par√°metros de regularizaci√≥n √≥ptimos.",
  "No aporta mejoras significativas si los supuestos de LDA o QDA se cumplen perfectamente sin sobreajuste."
)

tabla_rda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_rda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RDA",
             subtitle = "Regularized Discriminant Analysis (RDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## t-Distributed Stochastic Neighbor Embedding (t-SNE) {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/t-SNE.png"))
```

## Uniform Manifold Approximation and Projection (UMAP) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Dimensionality Reduction/UMAP.png"))
```

**Uniform Manifold Approximation and Projection (UMAP)** es una t√©cnica de **reducci√≥n de dimensionalidad no lineal** de vanguardia, utilizada principalmente para la **visualizaci√≥n de datos** de alta dimensi√≥n y para el **aprendizaje de caracter√≠sticas (feature learning)**. Fue desarrollada por Leland McInnes, John Healy y James Melville. UMAP es una alternativa m√°s reciente y a menudo m√°s r√°pida y escalable a t-SNE (t-Distributed Stochastic Neighbor Embedding), manteniendo su capacidad para preservar la estructura local y global de los datos.

La idea central de UMAP se basa en la **teor√≠a de los conjuntos difusos (fuzzy set theory)** y la **geometr√≠a riemanniana**. Intenta construir una representaci√≥n de baja dimensi√≥n de los datos asumiendo que los datos de alta dimensi√≥n residen en una **variedad (manifold) subyacente de baja dimensi√≥n**. El algoritmo opera en dos fases:

1.  **Construcci√≥n del Grafo de Vecindad Difusa:**
    * Primero, UMAP construye un **grafo ponderado difuso** en el espacio de alta dimensi√≥n. Los nodos del grafo son los puntos de datos y los pesos de las aristas representan la probabilidad de que dos puntos est√©n conectados (es decir, qu√© tan similares o cercanos son).
    * Para ello, UMAP estima las distancias entre los puntos en el manifold subyacente y luego convierte estas distancias en probabilidades de conectividad. Esto es crucial porque le permite adaptarse a la densidad local de los datos (puntos en regiones densas pueden estar cerca incluso con distancias euclidianas grandes, y viceversa en regiones dispersas).

2.  **Optimizaci√≥n del Dise√±o en Baja Dimensi√≥n:**
    * Luego, UMAP optimiza el dise√±o de los puntos en un espacio de baja dimensi√≥n (ej., 2D) para que la **estructura del grafo construido en alta dimensi√≥n sea lo m√°s similar posible** al grafo construido en baja dimensi√≥n.
    * Esto se logra minimizando una funci√≥n de costo que intenta hacer que las probabilidades de conectividad en el espacio de baja dimensi√≥n coincidan con las probabilidades de conectividad del grafo de alta dimensi√≥n.

UMAP es valorado por su velocidad, escalabilidad a grandes conjuntos de datos, y su capacidad para preservar simult√°neamente la **estructura local y global** de los datos, lo que lo hace ideal para visualizar agrupaciones y relaciones complejas.

**Aprendizaje Global vs. Local:**

UMAP es un excelente ejemplo de un modelo que logra un equilibrio sofisticado entre el **aprendizaje local y global**.

* **Aspecto Local:** UMAP pone un fuerte √©nfasis en la **preservaci√≥n de la estructura local**. Al construir el grafo de vecindad difusa, se enfoca en las relaciones de los vecinos m√°s cercanos de cada punto (controlado por el par√°metro `n_neighbors`). La forma en que calcula las probabilidades de conectividad se adapta a la densidad local de los datos, asegurando que los cl√∫steres y las agrupaciones cercanas se mantengan cohesivos en la representaci√≥n de baja dimensi√≥n. Las relaciones "locales" son las que definen el "manifold" en primera instancia. Esto significa que si los datos no se distribuyen linealmente y tienen estructuras complejas con vecindarios distintos (como diferentes clusters o ramas en una estructura), UMAP es capaz de capturarlas con alta fidelidad, de forma similar a como una "regresi√≥n ponderada localmente" operar√≠a en cada vecindario.

* **Aspecto Global:** A pesar de su √©nfasis local, UMAP tambi√©n hace un esfuerzo consciente por preservar la **estructura global** de los datos. Al minimizar la funci√≥n de costo para que la estructura del grafo se mantenga en el espacio de baja dimensi√≥n, UMAP no solo se asegura de que los puntos cercanos permanezcan cercanos, sino que tambi√©n intenta que los grupos de puntos que estaban globalmente separados en la alta dimensi√≥n permanezcan separados en la baja dimensi√≥n. El par√°metro `min_dist` ayuda a controlar cu√°n compactos deben ser los cl√∫steres, lo que influye en la separaci√≥n global. Esta capacidad de equilibrar ambos aspectos es una de las principales ventajas de UMAP sobre t√©cnicas que a veces sacrifican la estructura global (como t-SNE, que puede "romper" grandes cl√∫steres).


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (o categ√≥ricas codificadas)",
  "‚úÖ Captura relaciones no lineales y estructura local/global",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Algo sensible a outliers (puede distorsionar estructuras)",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Visual en 2D o 3D; dif√≠cil interpretaci√≥n formal",
  "‚úÖ Muy r√°pido incluso en grandes conjuntos de datos",
  "‚ö†Ô∏è No usa validaci√≥n cruzada cl√°sica, pero puede evaluarse la estabilidad",
  "‚ùå Datos con mucho ruido, escalas mal ajustadas o sin estructura latente clara"
)

detalles <- c(
  "T√©cnica no supervisada de reducci√≥n de dimensionalidad que preserva tanto estructura local como global de los datos.",
  "No busca predecir, sino proyectar observaciones a un espacio de menor dimensi√≥n.",
  "Funciona con datos num√©ricos; variables categ√≥ricas deben ser codificadas antes.",
  "A diferencia de PCA, puede descubrir relaciones no lineales m√°s complejas.",
  "No genera residuos; no aplica el supuesto de normalidad.",
  "No hay modelo de error residual, por lo que no aplica la independencia.",
  "No es un modelo predictivo, as√≠ que no se eval√∫a homoscedasticidad.",
  "Outliers pueden influir en el mapa de manera desproporcionada.",
  "Como es una t√©cnica de reducci√≥n, la multicolinealidad no le afecta directamente.",
  "La interpretaci√≥n se limita a la distribuci√≥n visual de puntos.",
  "UMAP es computacionalmente eficiente y escalable a grandes vol√∫menes de datos.",
  "No utiliza validaci√≥n cruzada directa, pero puede evaluarse la estabilidad de la proyecci√≥n.",
  "Cuando no existe una estructura clara en los datos, la proyecci√≥n puede ser confusa o poco √∫til."
)

tabla_umap <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_umap %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir UMAP",
             subtitle = "Uniform Manifold Approximation and Projection (UMAP)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


<!--chapter:end:05-dimensionality_reduction.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üß¨ 6. Modelos Bayesianos {-}  

**Ejemplos:** Naive Bayes, Redes Bayesianas.  
**Uso:** Ideales para **clasificaci√≥n r√°pida**, especialmente en escenarios con **supuestos simples** sobre los datos. Son muy populares en tareas de **procesamiento de texto** y **detecci√≥n de spam**.  
**Ventajas:** Son modelos **muy r√°pidos** de entrenar y predecir, y est√°n s√≥lidamente **fundamentados en la teor√≠a de probabilidad**.  
**Limitaciones:** La principal es que **asumen independencia** entre las variables predictoras, lo cual no siempre se cumple en la realidad y puede afectar su precisi√≥n en ciertos problemas.  

---

## Averaged One - Dependence Estimators (AODE)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/AODE.png"))
```

**Averaged One-Dependence Estimators (AODE)** es un algoritmo de **clasificaci√≥n supervisada** que pertenece a la familia de los clasificadores basados en **modelos bayesianos**. Es una mejora sobre el cl√°sico **Naive Bayes (NB)**, dise√±ado para superar la limitaci√≥n clave de NB: la **asunci√≥n de independencia condicional** estricta entre las variables predictoras (atributos) dado el valor de la clase. Esta suposici√≥n, aunque simplifica mucho el c√°lculo y permite a Naive Bayes ser muy eficiente, rara vez se cumple en la realidad y puede llevar a una p√©rdida de precisi√≥n.

AODE relaja parcialmente la suposici√≥n de independencia de Naive Bayes al considerar que **cada atributo es dependiente, como m√°ximo, de un solo otro atributo** (adem√°s de la variable de clase). En lugar de construir un √∫nico modelo de √°rbol de dependencia (como en el √Årbol de Dependencia de Atributos - ADTree), AODE construye una **colecci√≥n de clasificadores "One-Dependence" (ODE)** y luego **promedia sus predicciones**.

El funcionamiento de AODE se puede resumir as√≠:

1.  **Generaci√≥n de Clasificadores ODE:** Para cada atributo predictivo $A_i$ en el conjunto de datos (que cumpla ciertos criterios, como tener suficientes instancias), AODE construye un clasificador ODE. Este clasificador asume que todos los dem√°s atributos son condicionalmente independientes de $A_i$ dado la clase. En otras palabras, se estima la probabilidad condicional de cada atributo $A_j$ dado la clase $C$ y el atributo $A_i$: $P(A_j | C, A_i)$.
2.  **Ponderaci√≥n y Promedio:** Cuando se hace una predicci√≥n para una nueva instancia, AODE calcula la probabilidad de cada clase para cada uno de los clasificadores ODE generados. Luego, estas probabilidades se combinan (t√≠picamente promediando) para obtener una predicci√≥n final.

Al promediar las predicciones de m√∫ltiples modelos ODE, AODE logra mitigar el sesgo introducido por la suposici√≥n de independencia estricta de Naive Bayes, a menudo obteniendo un mejor rendimiento sin incurrir en una complejidad computacional excesiva.


**Aprendizaje Global vs. Local:**

Averaged One-Dependence Estimators (AODE) es un modelo que se clasifica como de **aprendizaje global**, aunque con una estructura que busca capturar dependencias que tienen una naturaleza m√°s "local" en el contexto de las relaciones entre atributos.

* **Aspecto Global:** AODE construye un conjunto de modelos (los ODEs) que son entrenados sobre la **totalidad del conjunto de datos** para estimar las probabilidades condicionales. La combinaci√≥n de estas probabilidades (el promedio) para llegar a una predicci√≥n final es una regla que se aplica de manera consistente a cualquier nueva observaci√≥n. Los par√°metros de cada clasificador ODE (las probabilidades condicionales) se estiman de manera global a partir de las frecuencias observadas en todo el conjunto de entrenamiento.

* **Matiz (Captura de Dependencias Locales):** Aunque el enfoque general es global, la raz√≥n por la que AODE es m√°s potente que Naive Bayes radica en su capacidad para modelar **dependencias entre atributos**. Cada clasificador ODE considera que un atributo espec√≠fico tiene una dependencia directa de otro atributo, lo que es una forma de capturar una relaci√≥n "local" entre un par de atributos dado el contexto de la clase. Al promediar sobre estos m√∫ltiples modelos que capturan diferentes dependencias de "un solo par", AODE puede adaptarse mejor a las complejidades de los datos donde las relaciones no son puramente independientes y no se distribuyen linealmente, sin la necesidad de dividir el espacio de caracter√≠sticas en regiones discretas como los √°rboles de decisi√≥n. Sin embargo, la soluci√≥n final de promediado es un clasificador global que se aplica a toda la instancia de entrada.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica",
  "‚úÖ Categ√≥ricas principalmente",
  "‚ö†Ô∏è Modela dependencias limitadas entre atributos (mejora sobre NB)",
  "‚ùå No aplica (no es regresi√≥n)",
  "‚úÖ Requiere independencia entre instancias",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede verse afectado por outliers si se usan variables num√©ricas sin tratamiento",
  "‚úÖ Menos afectado que Naive Bayes por dependencias entre atributos",
  "‚ö†Ô∏è Moderadamente interpretable (combinaci√≥n de varios modelos NB con 1 dependencia)",
  "‚ö†Ô∏è M√°s costoso que NB, pero a√∫n eficiente",
  "‚úÖ Puede validarse mediante k-fold cross-validation",
  "‚ùå Desempe√±a mal con muchos atributos irrelevantes o con pocos datos por combinaci√≥n de atributos"
)

detalles <- c(
  "Clasificador bayesiano que promedia modelos con una √∫nica dependencia entre pares de atributos para mejorar sobre Naive Bayes.",
  "Dise√±ado para problemas de clasificaci√≥n con clases categ√≥ricas.",
  "Se usa t√≠picamente con variables categ√≥ricas, aunque puede adaptarse a discretizadas.",
  "Relaja el supuesto de independencia total de Naive Bayes permitiendo una √∫nica dependencia por atributo.",
  "No es un modelo de regresi√≥n, por lo que no aplica el supuesto de normalidad de residuos.",
  "Las observaciones deben ser independientes para que las estimaciones sean v√°lidas.",
  "No supone homoscedasticidad debido a su naturaleza probabil√≠stica.",
  "Los valores at√≠picos pueden afectar la calidad de la estimaci√≥n de probabilidades.",
  "Tolera mejor la multicolinealidad al permitir dependencias limitadas entre atributos.",
  "La interpretaci√≥n es m√°s compleja que NB, pero a√∫n comprensible por su estructura promedio.",
  "Requiere m√°s tiempo de c√≥mputo que NB, pero sigue siendo razonablemente eficiente.",
  "La validaci√≥n cruzada es √∫til para evaluar el desempe√±o y generalizaci√≥n del modelo.",
  "El rendimiento cae si hay muchos atributos irrelevantes o datos escasos por combinaci√≥n de atributos."
)

tabla_aode <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_aode %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir AODE",
             subtitle = "Averaged One - Dependence Estimators (AODE)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Bayesian Network (BN)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/BN.png"))
```

Una **Red Bayesiana (BN)**, tambi√©n conocida como **Red Bayesiana Causal** o **Modelo Gr√°fico Dirigido Ac√≠clico (DAG)**, es un modelo probabil√≠stico que representa un conjunto de **variables y sus relaciones de dependencia condicional** utilizando un **grafo dirigido ac√≠clico**. En este grafo:

* **Nodos:** Representan las variables aleatorias (pueden ser discretas o continuas).
* **Arcos (flechas):** Representan las dependencias condicionales entre las variables. Una flecha de A a B significa que B depende directamente de A (A es "padre" de B). La ausencia de un arco entre dos nodos indica una independencia condicional.

La estructura del grafo de una Red Bayesiana permite visualizar y comprender las relaciones de causa y efecto (o asociaci√≥n) entre las variables. Junto con la estructura del grafo, una BN tambi√©n especifica las **distribuciones de probabilidad condicional (CPDs)** para cada nodo, dadas las combinaciones de estados de sus nodos padre. Por ejemplo, si un nodo tiene padres, se define la probabilidad de sus valores para cada combinaci√≥n de valores de sus padres.

Las Redes Bayesianas son potentes para:
* **Modelado de Conocimiento:** Codificar el conocimiento experto o aprendido de los datos sobre c√≥mo interact√∫an las variables.
* **Inferencia Probabil√≠stica:** Calcular la probabilidad de que una variable tome un valor espec√≠fico, dadas las observaciones de otras variables (evidencia). Esto puede incluir diagn√≥stico (inferir causas a partir de efectos) o predicci√≥n (inferir efectos a partir de causas).
* **Aprendizaje de Estructura y Par√°metros:** Aprender la estructura del grafo (las dependencias) y las CPDs a partir de datos.

**Aprendizaje Global vs. Local:**

Una Red Bayesiana (BN) es fundamentalmente un modelo de **aprendizaje global** en su estructura general, pero con una fuerte base en el **aprendizaje local** de las dependencias.

* **Aspecto Global:** La **estructura del grafo** y el conjunto de **tablas de probabilidad condicional (CPDs)** forman un **modelo probabil√≠stico coherente y global** de la distribuci√≥n de probabilidad conjunta de todas las variables. Este modelo global puede ser utilizado para realizar inferencias sobre cualquier combinaci√≥n de variables en cualquier parte del espacio de datos. La red define c√≥mo la informaci√≥n fluye y c√≥mo las probabilidades se propagan a trav√©s de todas las variables, dando una visi√≥n hol√≠stica de las interacciones del sistema.

* **Aspecto Local (Dependencias y Parametrizaci√≥n):** Donde la BN tiene un fuerte componente local es en la **definici√≥n de las dependencias y la parametrizaci√≥n de las CPDs**. Cada nodo solo necesita conocer las probabilidades condicionales dadas sus **padres directos**. Esto es un principio de **independencia condicional local**: una variable es independiente de sus no-descendientes dado sus padres. Esto descompone un problema complejo de modelado de la distribuci√≥n conjunta en problemas m√°s peque√±os y manejables de modelar las dependencias locales. Por ejemplo, para estimar $P(X_i | Padres(X_i))$, solo se necesita informaci√≥n local relacionada con $X_i$ y sus padres, no con todas las dem√°s variables en la red. Esta capacidad de modelar dependencias de forma localizada, y luego ensamblarlas en un modelo global, permite a las BNs manejar relaciones no lineales y complejas de una manera estructurada y probabil√≠stica. Si los datos no se distribuyen linealmente, la estructura de la BN puede adaptarse para reflejar las relaciones no lineales entre las variables a trav√©s de sus arcos y CPDs.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado o no supervisado",
  "‚úÖ Categ√≥rica o continua (depende del tipo de red)",
  "‚úÖ Categ√≥ricas y/o continuas",
  "‚úÖ Modela relaciones condicionales entre variables (estructura dirigida)",
  "‚ùå No aplica (no hay residuos t√≠picos)",
  "‚ö†Ô∏è Depende de la estructura de la red",
  "‚ùå No aplica como en regresi√≥n cl√°sica",
  "‚ö†Ô∏è Puede ser sensible si afecta las probabilidades condicionales",
  "‚ö†Ô∏è Puede causar redundancia si no se ajusta bien la estructura",
  "‚úÖ Muy interpretables si se visualiza la red y se conocen las dependencias",
  "‚ö†Ô∏è Aprendizaje de estructura puede ser computacionalmente costoso",
  "‚úÖ Puede usarse validaci√≥n cruzada para evaluar rendimiento predictivo",
  "‚ùå Cuando hay muchas variables y poca informaci√≥n para definir relaciones"
)

detalles <- c(
  "Modelo probabil√≠stico que representa relaciones condicionales entre variables mediante un grafo dirigido ac√≠clico (DAG).",
  "Puede predecir una variable (modo supervisado) o descubrir estructura entre variables (modo no supervisado).",
  "Acepta variables mixtas, aunque muchas implementaciones requieren discretizaci√≥n.",
  "Cada nodo representa una variable y las conexiones modelan dependencias condicionales.",
  "No genera residuos como los modelos de regresi√≥n, por lo que no aplica este supuesto.",
  "Algunas estructuras pueden implicar independencia condicional; otras no.",
  "No se eval√∫a homoscedasticidad, pues no hay predicci√≥n de error num√©rico.",
  "Valores at√≠picos pueden sesgar las probabilidades estimadas si no se controlan.",
  "Si hay variables redundantes o fuertemente correlacionadas, se debe ajustar la estructura de la red para evitar errores.",
  "La red permite interpretar c√≥mo influyen unas variables sobre otras, ideal para razonamiento causal.",
  "El ajuste de par√°metros es eficiente, pero aprender la estructura de la red puede ser lento.",
  "Puede evaluarse el desempe√±o con k-fold o validaci√≥n simple en tareas supervisadas.",
  "Cuando no se tiene informaci√≥n previa o datos suficientes, la red puede no capturar relaciones reales."
)

tabla_bn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir BN",
             subtitle = "Bayesian Network (BN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Bayesian Belief Network (BBN)  {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/BBN.png"))
```

Una **Red de Creencia Bayesiana (BBN)** es simplemente **otro t√©rmino para una Red Bayesiana (BN)**. No hay una diferencia fundamental entre ambos nombres; ambos se refieren al mismo tipo de modelo probabil√≠stico. La terminolog√≠a "Red de Creencia" a menudo enfatiza la capacidad del modelo para representar y actualizar "creencias" (probabilidades) sobre el estado de variables inciertas a medida que se introduce nueva evidencia.

Como ya se describi√≥, una BBN (o BN) es un **modelo gr√°fico probabil√≠stico dirigido ac√≠clico (DAG)** que representa un conjunto de **variables aleatorias** como **nodos** y sus **relaciones de dependencia condicional** como **arcos (flechas)**. La ausencia de un arco entre dos nodos indica una independencia condicional. Cada nodo est√° asociado con una **distribuci√≥n de probabilidad condicional (CPD)** que cuantifica la relaci√≥n de ese nodo con sus padres.

Las BBNs son herramientas poderosas para:
* **Modelar el conocimiento incierto:** Permiten representar c√≥mo diferentes factores interact√∫an bajo incertidumbre.
* **Inferencia probabil√≠stica:** Dada alguna evidencia (observaciones de algunas variables), la red puede calcular las probabilidades actualizadas de las otras variables. Esto es fundamental para el diagn√≥stico, la predicci√≥n y la toma de decisiones bajo incertidumbre.
* **Aprendizaje a partir de datos:** Las BBNs pueden ser aprendidas tanto en su estructura (c√≥mo se conectan los nodos) como en sus par√°metros (las CPDs) a partir de conjuntos de datos.


**Aprendizaje Global vs. Local:**

Al igual que una Red Bayesiana, una Red de Creencia Bayesiana es fundamentalmente un modelo de **aprendizaje global** en su formulaci√≥n general, pero se basa en la **especificaci√≥n local** de las dependencias probabil√≠sticas.

* **Aspecto Global:** La BBN como un todo representa la **distribuci√≥n de probabilidad conjunta global** de todas las variables en el sistema. Una vez que la estructura y las CPDs est√°n definidas, la red puede usarse para calcular cualquier probabilidad marginal o condicional de inter√©s, proporcionando una visi√≥n probabil√≠stica completa y coherente del dominio. Es una funci√≥n que mapea el espacio de todas las posibles combinaciones de variables a sus probabilidades, y se aplica de manera consistente en todo el espacio.

* **Aspecto Local (Definici√≥n de Dependencias):** La fortaleza y eficiencia de las BBNs radica en el principio de **independencia condicional local**. Cada variable (nodo) solo necesita tener su distribuci√≥n de probabilidad condicionada a sus **padres directos** en el grafo. No es necesario especificar las dependencias con todas las dem√°s variables en la red. Esta factorizaci√≥n de la distribuci√≥n conjunta en componentes locales (las CPDs) es lo que hace que las BBNs sean computacionalmente manejables y permite que el modelo capture **relaciones no lineales y complejas** entre las variables de una manera estructurada. Al modelar estas dependencias "locales" de forma expl√≠cita, la BBN puede representar con precisi√≥n c√≥mo la probabilidad de un evento cambia en funci√≥n de los eventos directamente relacionados, incluso si la relaci√≥n no es lineal.

En resumen, las Redes de Creencia Bayesianas son modelos globales que permiten modelar relaciones probabil√≠sticas complejas y no lineales al especificar dependencias de manera local entre las variables. Son herramientas poderosas para el razonamiento bajo incertidumbre y la toma de decisiones.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado y no supervisado (estructuras probabil√≠sticas)",
  "‚úÖ Categ√≥rica o continua (depende de implementaci√≥n)",
  "‚úÖ Categ√≥ricas o continuas discretizadas",
  "‚úÖ Modela relaciones condicionales entre variables (gr√°ficamente)",
  "‚ùå No aplica (no es modelo de regresi√≥n lineal)",
  "‚úÖ Requiere independencia condicional entre nodos seg√∫n la red",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede ser sensible a outliers si se estiman mal las distribuciones",
  "‚úÖ Puede manejar correlaci√≥n entre variables de forma expl√≠cita en la red",
  "‚úÖ Alta interpretabilidad visual con grafos dirigidos ac√≠clicos",
  "‚ö†Ô∏è Costoso computacionalmente en grandes redes o aprendizaje estructural",
  "‚úÖ Validaci√≥n cruzada puede aplicarse en tareas supervisadas (clasificaci√≥n)",
  "‚ùå Mal rendimiento si hay muchas variables irrelevantes o dependencias no detectadas"
)

detalles <- c(
  "Modelo probabil√≠stico gr√°fico que representa relaciones condicionales entre variables mediante una red bayesiana (DAG).",
  "Puede usarse para clasificaci√≥n, predicci√≥n o inferencia probabil√≠stica.",
  "Se adapta a datos categ√≥ricos principalmente, pero tambi√©n se puede usar con discretizaci√≥n de continuas.",
  "Captura relaciones condicionales entre variables expl√≠citamente como conexiones dirigidas.",
  "No genera residuos como los modelos de regresi√≥n, por lo que la normalidad no aplica.",
  "Las dependencias condicionales deben estar bien modeladas en la estructura de la red.",
  "No hay un modelo de varianza/residuos tradicional como para aplicar homoscedasticidad.",
  "Distribuciones err√≥neas o mal estimadas pueden afectar resultados, especialmente con valores extremos.",
  "El modelo representa expl√≠citamente la correlaci√≥n entre variables mediante arcos.",
  "La estructura de la red permite ver c√≥mo interact√∫an las variables entre s√≠.",
  "El aprendizaje estructural y de par√°metros puede ser costoso en t√©rminos computacionales.",
  "Si se usa para clasificaci√≥n, la validaci√≥n cruzada es una forma est√°ndar de evaluaci√≥n.",
  "BBN requiere una buena estructura; datos mal preparados o muy ruidosos deterioran su capacidad explicativa."
)

tabla_bbn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bbn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir BBN",
             subtitle = "Bayesian Belief Network (BBN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Bayesian Linear Regression (BLR) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/BLR.png"))
```

## Gaussian Naive Bayes (GNB) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/GNB.png"))
```

**Gaussian Naive Bayes (GNB)** es una variante del popular algoritmo **Naive Bayes (NB)**, utilizado para tareas de **clasificaci√≥n supervisada**. Es particularmente adecuado cuando las **variables predictoras (atributos) son de tipo continuo**. Al igual que todos los clasificadores Naive Bayes, GNB se basa en el **Teorema de Bayes** y, fundamentalmente, en la **suposici√≥n de independencia condicional** entre las variables predictoras, dado el valor de la clase.

La diferencia clave entre GNB y otras variantes de Naive Bayes (como Multinomial Naive Bayes o Bernoulli Naive Bayes) es la forma en que modela la **probabilidad de los atributos continuos**. Espec√≠ficamente:

1.  **Suposici√≥n de Distribuci√≥n Gaussiana:** GNB asume que los valores de cada atributo continuo, *dada una clase espec√≠fica*, siguen una **distribuci√≥n normal (Gaussiana)**. Es decir, para cada clase y cada atributo, se estima la media ($\mu$) y la desviaci√≥n est√°ndar ($\sigma$) de los valores de ese atributo dentro de esa clase.
2.  **C√°lculo de Probabilidades:** Cuando se necesita clasificar una nueva observaci√≥n, GNB utiliza las funciones de densidad de probabilidad (PDF) de estas distribuciones Gaussianas para calcular la probabilidad de observar el valor del atributo para cada clase.
3.  **Aplicaci√≥n del Teorema de Bayes:** Finalmente, utiliza el Teorema de Bayes para calcular la probabilidad posterior de cada clase, dadas las probabilidades de los atributos, y asigna la observaci√≥n a la clase con la probabilidad posterior m√°s alta.

A pesar de su suposici√≥n de independencia (que rara vez se cumple perfectamente en la pr√°ctica), GNB a menudo funciona sorprendentemente bien, especialmente en conjuntos de datos grandes o cuando las caracter√≠sticas son ruidosas. Su simplicidad y eficiencia computacional lo hacen un buen punto de partida para muchos problemas de clasificaci√≥n.

**Aprendizaje Global vs. Local:**

Gaussian Naive Bayes (GNB) es un modelo de **aprendizaje global**.

* **Aspecto Global:** GNB construye un **modelo probabil√≠stico global** de la relaci√≥n entre las caracter√≠sticas y las clases. Las medias y desviaciones est√°ndar de las distribuciones Gaussianas para cada atributo dentro de cada clase se estiman a partir de **todos los datos de entrenamiento**. La regla de clasificaci√≥n final, que asigna una nueva instancia a la clase m√°s probable, se basa en estas distribuciones param√©tricas globales y en el Teorema de Bayes, aplic√°ndose de manera uniforme en todo el espacio de caracter√≠sticas. No se ajustan modelos locales para diferentes vecindarios de datos.

* **Impacto de la Asunci√≥n de Independencia:** La suposici√≥n de independencia condicional (que los atributos son independientes entre s√≠ dado la clase) significa que GNB no intenta capturar interacciones complejas o no lineales entre las variables predictoras. Si bien esto simplifica dr√°sticamente el modelo y lo hace eficiente, tambi√©n implica que su capacidad para modelar relaciones no lineales entre *predictores* es limitada. Si los datos no se distribuyen linealmente y las interacciones entre los predictores son cruciales para la clasificaci√≥n, GNB podr√≠a no ser el modelo m√°s flexible. Sin embargo, su robustez ante la violaci√≥n de suposiciones y su velocidad lo mantienen como una opci√≥n valiosa en muchos escenarios.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas (asume distribuci√≥n normal por clase)",
  "‚ùå No modela relaciones entre predictores (independencia asumida)",
  "‚ùå No aplica (no hay residuos como en regresi√≥n)",
  "‚úÖ Asume independencia condicional entre predictores",
  "‚úÖ Cada predictor se modela con varianza homog√©nea por clase",
  "‚ö†Ô∏è Sensible a outliers porque afectan media y varianza de la normal",
  "‚ö†Ô∏è Alta multicolinealidad viola el supuesto de independencia",
  "‚úÖ Altamente interpretable: muestra probabilidades y distribuci√≥n por clase",
  "‚úÖ Muy r√°pido y eficiente, incluso con grandes datasets",
  "‚úÖ Se puede validar f√°cilmente con k-fold o hold-out",
  "‚ùå Si los predictores no son aproximadamente normales por clase, el rendimiento baja"
)

detalles <- c(
  "Clasificador probabil√≠stico que asume que cada predictor sigue una distribuci√≥n normal dentro de cada clase.",
  "Se utiliza para predecir clases categ√≥ricas a partir de predictores continuos.",
  "Cada variable num√©rica se modela con una distribuci√≥n Gaussiana separada por clase.",
  "No considera correlaciones entre predictores; cada uno contribuye de manera independiente.",
  "No genera residuos como un modelo de regresi√≥n, as√≠ que no aplica normalidad de errores.",
  "El supuesto clave es independencia condicional entre predictores dado la clase.",
  "Cada variable tiene su propia media y varianza por clase, sin heterocedasticidad.",
  "Los valores at√≠picos pueden distorsionar los par√°metros estimados de la distribuci√≥n normal.",
  "Altamente correlacionadas violan la independencia condicional asumida y afectan rendimiento.",
  "F√°cil de explicar: se basa en la probabilidad de cada clase dado cada predictor.",
  "Uno de los algoritmos m√°s r√°pidos para clasificaci√≥n supervisada.",
  "Evaluaci√≥n est√°ndar con validaci√≥n cruzada o conjunto de prueba.",
  "Si las variables no tienen forma aproximadamente normal dentro de clases, el modelo puede clasificarlas mal."
)

tabla_gnb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_gnb %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GNB",
             subtitle = "Gaussian Naive Bayes (GNB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Hidden Markov Models (HMMs) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/HMMs.png"))
```

## Kalman Filter {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/KF.png"))
```

## Multinomial Naive Bayes (MNB) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/MNB.png"))
```

**Multinomial Naive Bayes (MNB)** es una variante del algoritmo **Naive Bayes** dise√±ada espec√≠ficamente para la **clasificaci√≥n de datos discretos**, y es particularmente popular en tareas de **procesamiento de lenguaje natural (NLP)**, como la clasificaci√≥n de texto (ej., spam/no spam, clasificaci√≥n de documentos por tema). Al igual que otras formas de Naive Bayes, se basa en el **Teorema de Bayes** y la **suposici√≥n clave de independencia condicional** entre las caracter√≠sticas, dado el valor de la clase.

La diferencia fundamental de MNB radica en que asume que las caracter√≠sticas (como el recuento de palabras en un documento de texto) provienen de una **distribuci√≥n multinomial**. Esto significa que:

1.  **Caracter√≠sticas de Recuento:** MNB es ideal para caracter√≠sticas que representan **frecuencias o recuentos** (ej., el n√∫mero de veces que aparece una palabra en un documento, el n√∫mero de veces que ocurre un evento).
2.  **Modelado de Probabilidades:** Para cada clase, MNB calcula la probabilidad de observar cada caracter√≠stica (ej., cada palabra del vocabulario) dado que la instancia pertenece a esa clase. Estas probabilidades se estiman a menudo utilizando **suavizado Laplace (o aditivo)** para evitar probabilidades de cero para palabras no vistas durante el entrenamiento.
3.  **Aplicaci√≥n del Teorema de Bayes:** Luego, para clasificar una nueva instancia, multiplica las probabilidades de las caracter√≠sticas (asumiendo independencia) por la probabilidad previa de cada clase, y elige la clase que tiene la probabilidad posterior m√°s alta.

MNB es altamente eficiente, escalable para grandes conjuntos de datos y a menudo sorprendentemente efectivo a pesar de su ingenua suposici√≥n de independencia, lo que lo convierte en una l√≠nea base s√≥lida para muchos problemas de clasificaci√≥n de texto.

**Aprendizaje Global vs. Local:**

Multinomial Naive Bayes (MNB) es un modelo de **aprendizaje global**.

* **Aspecto Global:** MNB construye un **modelo probabil√≠stico global** para la relaci√≥n entre las caracter√≠sticas discretas (como recuentos de palabras) y las clases. Las probabilidades de las caracter√≠sticas dadas las clases (y las probabilidades previas de las clases) se estiman a partir de **todos los datos de entrenamiento**. La regla de clasificaci√≥n final, basada en el Teorema de Bayes, se aplica de manera uniforme a cualquier nueva instancia en el espacio de caracter√≠sticas. No se ajustan modelos locales para diferentes vecindarios de datos; en su lugar, se utilizan las mismas probabilidades estimadas globalmente para todas las predicciones.

* **Impacto de la Asunci√≥n de Independencia:** La suposici√≥n de independencia condicional entre las caracter√≠sticas (ej., que la presencia de una palabra no influye en la probabilidad de otra palabra dada la categor√≠a del documento) es una simplificaci√≥n global. Si bien esta simplicidad permite que MNB sea muy eficiente y robusto a veces, tambi√©n significa que no puede capturar interacciones complejas o dependencias no lineales entre las caracter√≠sticas en el mismo sentido que modelos m√°s avanzados. Sin embargo, en muchas aplicaciones como la clasificaci√≥n de texto, donde la frecuencia individual de las palabras es muy informativa, esta suposici√≥n es lo suficientemente robusta para un buen rendimiento. Es un modelo que asume una estructura de probabilidad global y la aplica consistentemente.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Discretas, conteos (ej. frecuencia de palabras)",
  "‚ùå No modela relaciones entre predictores (asume independencia condicional)",
  "‚ùå No aplica (no hay residuos)",
  "‚úÖ Asume independencia condicional entre predictores",
  "‚ùå No aplica (no se modela varianza)",
  "‚ö†Ô∏è Menos sensible a outliers que Gaussian NB, pero a√∫n puede verse afectado",
  "‚ö†Ô∏è Multicolinealidad viola el supuesto de independencia y puede degradar el rendimiento",
  "‚úÖ Muy interpretable: probabilidades por clase y variable",
  "‚úÖ Extremadamente eficiente en problemas de texto y alta dimensi√≥n",
  "‚úÖ Puede usarse k-fold o validaci√≥n simple",
  "‚ùå No es adecuado para variables continuas o datos que no representen conteos"
)

detalles <- c(
  "Clasificador basado en probabilidad que modela la distribuci√≥n multinomial de conteos por clase.",
  "Usado t√≠picamente para clasificaci√≥n de texto, spam detection, y otros problemas con datos categ√≥ricos o de conteo.",
  "Funciona mejor con variables que representan frecuencia (n√∫mero de veces que aparece un t√©rmino).",
  "Asume que los predictores son independientes condicionalmente dados la clase, sin correlaci√≥n entre ellos.",
  "No hay residuos como en modelos de regresi√≥n, por lo tanto no aplica este supuesto.",
  "La independencia condicional de los predictores es un supuesto fundamental del modelo.",
  "Como no se modela la varianza expl√≠citamente, el supuesto de homoscedasticidad no aplica.",
  "Outliers tienen menor efecto porque se espera que los datos est√©n en formato de conteo (discretos).",
  "Predictores altamente correlacionados pueden afectar negativamente la precisi√≥n del modelo.",
  "La probabilidad condicional de cada clase y predictor es f√°cil de interpretar.",
  "Muy r√°pido incluso en datasets grandes con miles de caracter√≠sticas (como texto).",
  "La precisi√≥n puede evaluarse con validaci√≥n cruzada como en otros clasificadores.",
  "Si las variables son continuas o no reflejan bien los conteos, el modelo puede fallar."
)

tabla_mnb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mnb %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MNB",
             subtitle = "Multinomial Naive Bayes (MNB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Naive Bayes (NB) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/NB.png"))
```

**Naive Bayes (NB)** es un algoritmo de **clasificaci√≥n supervisada** popular y computacionalmente eficiente, basado en el **Teorema de Bayes** y una **fuerte (o "ingenua") suposici√≥n de independencia condicional** entre las caracter√≠sticas (variables predictoras) dado el valor de la clase. Esta suposici√≥n significa que el modelo asume que la presencia o ausencia de una caracter√≠stica particular no afecta la presencia o ausencia de otra caracter√≠stica, una vez que se conoce la clase.

A pesar de que esta suposici√≥n rara vez se cumple perfectamente en problemas del mundo real, Naive Bayes a menudo ofrece un **rendimiento sorprendentemente bueno**, especialmente en tareas de clasificaci√≥n de texto y con grandes conjuntos de datos. Su simplicidad y velocidad lo convierten en un excelente algoritmo de l√≠nea base.

El funcionamiento b√°sico de Naive Bayes es el siguiente:

1.  **C√°lculo de Probabilidades Previas:** Estima la probabilidad de cada clase en el conjunto de entrenamiento (ej., P(Clase A), P(Clase B)).
2.  **C√°lculo de Probabilidades de Verosimilitud:** Para cada caracter√≠stica y cada clase, calcula la probabilidad de que la caracter√≠stica tome un valor espec√≠fico, dado que la instancia pertenece a esa clase (ej., P(Caracter√≠stica X | Clase A)). Aqu√≠ es donde entran las diferentes variantes de Naive Bayes (Gaussian para caracter√≠sticas continuas, Multinomial para recuentos, Bernoulli para caracter√≠sticas binarias).
3.  **Aplicaci√≥n del Teorema de Bayes:** Para clasificar una nueva instancia, utiliza el Teorema de Bayes para combinar estas probabilidades y calcular la probabilidad posterior de cada clase, dadas las caracter√≠sticas de la nueva instancia. Finalmente, asigna la instancia a la clase con la probabilidad posterior m√°s alta.


**Aprendizaje Global vs. Local:**

Naive Bayes (NB) es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** Naive Bayes construye un **modelo probabil√≠stico global** que describe la relaci√≥n entre las caracter√≠sticas y las clases para todo el conjunto de datos. Las probabilidades previas de las clases y las probabilidades condicionales de las caracter√≠sticas dadas las clases se estiman a partir de **todos los datos de entrenamiento**. La regla de clasificaci√≥n resultante se aplica de manera uniforme a cualquier nueva instancia en el espacio de caracter√≠sticas, sin ajustar modelos locales para diferentes vecindarios de datos. El modelo aprende una distribuci√≥n de probabilidad que se asume v√°lida para todo el dominio.

* **Impacto de la Suposici√≥n de Independencia:** La "ingenuidad" del modelo, es decir, la suposici√≥n de independencia entre las caracter√≠sticas, es una simplificaci√≥n global. No intenta capturar interacciones complejas o no lineales entre las caracter√≠sticas en s√≠. Si bien esto puede ser una limitaci√≥n cuando las relaciones entre las caracter√≠sticas son muy intrincadas y no lineales, es precisamente esta suposici√≥n la que le otorga su eficiencia y robustez en muchos escenarios pr√°cticos. Es un modelo que asume una estructura de probabilidad global y la aplica de manera consistente.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Categ√≥ricas o num√©ricas (seg√∫n variante del modelo)",
  "‚ùå Asume independencia condicional entre predictores",
  "‚ùå No aplica (no es regresi√≥n)",
  "‚úÖ Observaciones deben ser independientes",
  "‚ùå No se requiere homocedasticidad",
  "‚ö†Ô∏è Puede ser sensible si se usa con predictores num√©ricos y hay valores extremos",
  "‚úÖ No afectado por multicolinealidad debido a suponer independencia",
  "‚úÖ Alta, se pueden interpretar probabilidades y efectos de cada variable",
  "‚úÖ Muy r√°pido incluso con grandes conjuntos de datos",
  "‚úÖ Puede usarse para afinar y evaluar desempe√±o del modelo",
  "‚ùå Bajo rendimiento si los predictores no son realmente independientes o las distribuciones asumidas no se cumplen"
)

detalles <- c(
  "Clasificador probabil√≠stico basado en el teorema de Bayes con asunci√≥n de independencia condicional entre predictores.",
  "Funciona para clasificaci√≥n en m√∫ltiples clases categ√≥ricas.",
  "Puede trabajar con variables categ√≥ricas (Multinomial NB) o num√©ricas (Gaussian NB).",
  "Supone que las variables predictoras son independientes entre s√≠ dentro de cada clase.",
  "No genera residuos en el sentido cl√°sico porque no es un modelo de regresi√≥n.",
  "Las observaciones deben ser independientes para que las probabilidades se combinen correctamente.",
  "No requiere igualdad de varianzas; Gaussian NB asume varianza igual por clase pero se puede ajustar.",
  "Los valores at√≠picos pueden distorsionar la estimaci√≥n de probabilidades si hay predictores num√©ricos.",
  "La independencia entre predictores hace que la multicolinealidad no sea problema.",
  "Los resultados pueden interpretarse en t√©rminos de probabilidades a posteriori por clase.",
  "Muy eficiente para entrenamiento y predicci√≥n, incluso con muchos atributos.",
  "Puede validarse usando k-fold o leave-one-out para asegurar estabilidad del modelo.",
  "El supuesto fuerte de independencia condicional rara vez se cumple completamente, lo que puede afectar la precisi√≥n."
)

tabla_nb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_nb %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir NB",
             subtitle = "Naive Bayes (NB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Particle Filter {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/Particle Filter.png"))
```

<!--chapter:end:06-bayesian.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üßÆ 7. Regularizaci√≥n {-}  

**Ejemplos:** L1 (Lasso), L2 (Ridge), Elastic Net.  
**Uso:** Esencial para **prevenir el sobreajuste** en modelos, especialmente los lineales y las redes neuronales. Muy √∫til cuando trabajas con **muchas variables** (alta dimensionalidad).  
**Ventajas:** Su principal beneficio es que **penaliza la complejidad del modelo**, forz√°ndolo a ser m√°s simple y generalizable.  
**Limitaciones:** Si se aplica en exceso, la regularizaci√≥n puede **eliminar variables √∫tiles** y, por lo tanto, afectar el rendimiento del modelo.  

---

## Elastic Net  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/ENet.png"))
```

**Elastic Net** es un m√©todo de **regresi√≥n lineal regularizada** que combina las penalizaciones de **Ridge Regression (regresi√≥n L2)** y **Lasso Regression (regresi√≥n L1)**. Fue desarrollado para superar las limitaciones de Lasso, que puede tener problemas cuando hay un gran n√∫mero de variables predictoras o cuando estas variables est√°n altamente correlacionadas (multicolinealidad). Elastic Net es una herramienta muy vers√°til para la **selecci√≥n de caracter√≠sticas**, la **reducci√≥n de sobreajuste** y el manejo de **datos de alta dimensi√≥n**.

La funci√≥n de costo de Elastic Net a√±ade dos t√©rminos de penalizaci√≥n a la suma de los errores cuadrados de los residuos (como en la regresi√≥n OLS):

1.  **Penalizaci√≥n L1 (Lasso):** La suma del valor absoluto de los coeficientes. Esta penalizaci√≥n tiende a **reducir los coeficientes de las variables menos importantes a cero**, realizando as√≠ una **selecci√≥n autom√°tica de caracter√≠sticas**.
2.  **Penalizaci√≥n L2 (Ridge):** La suma del cuadrado de los coeficientes. Esta penalizaci√≥n **encoge los coeficientes** hacia cero, pero no los fuerza a ser exactamente cero. Es particularmente √∫til para manejar la **multicolinealidad**, ya que tiende a distribuir la influencia de las variables correlacionadas de manera m√°s equitativa.

Elastic Net utiliza dos hiperpar√°metros de sintonizaci√≥n:

* **$\alpha$ (alpha):** Controla el **balance entre las penalizaciones L1 y L2**.
    * Si $\alpha = 0$, Elastic Net se convierte en **Ridge Regression**.
    * Si $\alpha = 1$, Elastic Net se convierte en **Lasso Regression**.
    * Para valores entre 0 y 1, es una mezcla de ambas.
* **$\lambda$ (lambda):** Controla la **fuerza general de la regularizaci√≥n**. Un $\lambda$ m√°s grande implica una mayor penalizaci√≥n y, por lo tanto, coeficientes m√°s peque√±os.

Al combinar L1 y L2, Elastic Net logra lo mejor de ambos mundos: realiza selecci√≥n de caracter√≠sticas como Lasso y maneja la multicolinealidad y la estabilidad de los coeficientes como Ridge. Esto lo hace muy robusto en escenarios donde hay muchas variables correlacionadas.


**Aprendizaje Global vs. Local:**

Elastic Net es un modelo de **aprendizaje global**.

* **Aspecto Global:** Elastic Net construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes de la regresi√≥n se estiman optimizando una funci√≥n de costo que considera todos los puntos de datos simult√°neamente. La penalizaci√≥n se aplica a todos los coeficientes de manera uniforme, lo que busca una soluci√≥n que minimice el error de predicci√≥n y controle la complejidad del modelo a nivel global. La ecuaci√≥n de regresi√≥n final es una funci√≥n que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n en el espacio de caracter√≠sticas.

* **Influencia de la Regularizaci√≥n:** Aunque la regresi√≥n en s√≠ es global, las penalizaciones de regularizaci√≥n pueden tener un efecto que podr√≠amos considerar "adaptativo" en el sentido de que ajustan la influencia de las variables en funci√≥n de su relaci√≥n con otras variables y la respuesta. Por ejemplo, la penalizaci√≥n L1 puede "localizar" las variables m√°s importantes al poner otras a cero, y la L2 puede distribuir la importancia entre variables correlacionadas. Sin embargo, estas son propiedades de la optimizaci√≥n global del modelo, no de ajustar modelos separados para diferentes subregiones del espacio de datos. La Elastic Net, al igual que OLS, Ridge y Lasso, busca una √∫nica relaci√≥n lineal que describa la tendencia general de los datos.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal (como OLS)",
  "‚ö†Ô∏è Deseable, pero no esencial",
  "‚úÖ Supuesto importante",
  "‚úÖ Requiere homoscedasticidad",
  "‚ö†Ô∏è Afectado por outliers (no tan robusto)",
  "‚úÖ Ideal para multicolinealidad alta (mejor que LASSO)",
  "‚ö†Ô∏è Puede ser menos interpretable que LASSO si hay muchas variables seleccionadas",
  "‚úÖ R√°pido incluso con datos grandes",
  "‚úÖ Requiere validar los hiperpar√°metros `lambda` y `alpha`",
  "‚ùå Relaci√≥n no lineal, o pocos datos con muchas variables no relevantes"
)

detalles <- c(
  "Modelo de regresi√≥n penalizada que combina LASSO (L1) y Ridge (L2) en un solo modelo.",
  "Predice una variable continua a partir de variables independientes num√©ricas.",
  "Las variables deben estar estandarizadas para evitar que la penalizaci√≥n sesgue los coeficientes.",
  "Asume que la relaci√≥n entre variables es lineal.",
  "La normalidad ayuda para inferencia, pero no es cr√≠tica para predicci√≥n.",
  "Errores deben ser independientes para que los coeficientes sean v√°lidos.",
  "Varianza constante de los errores es un supuesto clave.",
  "Aunque regulariza, no es inmune a valores at√≠picos.",
  "Funciona bien cuando hay muchas variables correlacionadas entre s√≠.",
  "Mezcla selecci√≥n de variables (L1) y regularizaci√≥n (L2), lo cual puede dificultar la interpretaci√≥n directa.",
  "A pesar de usar dos penalizaciones, sigue siendo eficiente con librer√≠as como `glmnet`.",
  "Validaci√≥n cruzada se usa para seleccionar los mejores valores de `lambda` y `alpha`.",
  "Puede tener bajo rendimiento si no hay una relaci√≥n lineal o si las variables relevantes no est√°n presentes en el conjunto."
)

tabla_elastic_net <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_elastic_net %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir elastic net",
             subtitle = "Elastic Net")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Ridge Regression  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/RR.png"))
```

**Ridge Regression (Regresi√≥n Ridge)** es un m√©todo de **regresi√≥n lineal regularizada** que se utiliza para mejorar la estimaci√≥n de los coeficientes en modelos lineales, especialmente cuando existe **multicolinealidad** (alta correlaci√≥n entre las variables predictoras) o cuando el n√∫mero de predictores es grande en relaci√≥n con el n√∫mero de observaciones. Ridge Regression fue una de las primeras t√©cnicas de regularizaci√≥n y es fundamental para comprender m√©todos m√°s avanzados como Lasso o Elastic Net.

La Regresi√≥n Ridge aborda los problemas de la regresi√≥n por m√≠nimos cuadrados ordinarios (OLS) al a√±adir un **t√©rmino de penalizaci√≥n L2** a la funci√≥n de costo de los m√≠nimos cuadrados. La funci√≥n de costo que minimiza Ridge Regression es:

$$\text{RSS} + \lambda \sum_{j=1}^{p} \beta_j^2$$

Donde:
* $\text{RSS}$ es la suma de los errores cuadrados de los residuos (Residual Sum of Squares), que es lo que minimiza OLS.
* $\lambda$ (lambda) es un **par√°metro de sintonizaci√≥n (hiperpar√°metro)** no negativo. Este par√°metro controla la **fuerza de la penalizaci√≥n**.
* $\sum_{j=1}^{p} \beta_j^2$ es la **penalizaci√≥n L2**, que es la suma de los cuadrados de los coeficientes de regresi√≥n (excluyendo el intercepto).

**Efecto de la Penalizaci√≥n L2:**
* **Encogimiento de Coeficientes:** La penalizaci√≥n L2 **encoge los coeficientes** hacia cero. Cuanto mayor sea el valor de $\lambda$, mayor ser√° el encogimiento y m√°s peque√±os ser√°n los coeficientes.
* **Reducci√≥n de Varianza:** Este encogimiento reduce la varianza de las estimaciones de los coeficientes, haci√©ndolos m√°s estables y menos sensibles a peque√±as variaciones en los datos de entrenamiento. Esto ayuda a **reducir el sobreajuste**.
* **Manejo de Multicolinealidad:** En presencia de multicolinealidad, OLS puede asignar grandes valores a los coeficientes de variables correlacionadas. Ridge Regression distribuye la influencia entre las variables correlacionadas de manera m√°s uniforme y reduce la magnitud de estos coeficientes, lo que resulta en un modelo m√°s robusto.
* **No realiza selecci√≥n de caracter√≠sticas:** A diferencia de Lasso, Ridge Regression encoge los coeficientes, pero **rara vez los fuerza a ser exactamente cero**. Esto significa que todas las variables predictoras (o casi todas) seguir√°n en el modelo.

El valor √≥ptimo de $\lambda$ se selecciona t√≠picamente mediante t√©cnicas de validaci√≥n cruzada.

**Aprendizaje Global vs. Local:**

Ridge Regression es un modelo de **aprendizaje global**.

* **Aspecto Global:** Ridge Regression construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes se estiman optimizando una funci√≥n de costo que considera todos los puntos de datos simult√°neamente. La penalizaci√≥n L2 se aplica a todos los coeficientes para controlar la complejidad y la estabilidad del modelo a nivel global. La ecuaci√≥n de regresi√≥n resultante es una funci√≥n √∫nica que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n espec√≠fica en el espacio de caracter√≠sticas.

* **Estabilizaci√≥n Global:** Aunque la regularizaci√≥n L2 mejora la estabilidad de las estimaciones de los coeficientes y ayuda a manejar la multicolinealidad, lo hace como parte de una optimizaci√≥n global. No implica la creaci√≥n de m√∫ltiples modelos locales o la adaptaci√≥n a subregiones espec√≠ficas de los datos. La Regresi√≥n Ridge busca una relaci√≥n lineal subyacente que sea la mejor aproximaci√≥n para el conjunto de datos completo, penalizando la complejidad para mejorar la generalizaci√≥n global.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal (como OLS)",
  "‚ö†Ô∏è Supuesto deseable pero no estricto",
  "‚úÖ Supuesto necesario",
  "‚úÖ Supuesto necesario",
  "‚ö†Ô∏è Puede verse afectado, pero menos que OLS",
  "‚úÖ Dise√±ado para mitigarla mediante penalizaci√≥n",
  "‚ö†Ô∏è Menos interpretable que OLS (coeficientes sesgados)",
  "‚úÖ Eficiente incluso con muchas variables",
  "‚úÖ Requiere validaci√≥n para ajustar par√°metro lambda",
  "‚ùå Si la relaci√≥n no es lineal o hay muchas variables irrelevantes"
)

detalles <- c(
  "Extensi√≥n de la regresi√≥n lineal que agrega penalizaci√≥n L2 para reducir sobreajuste y manejar multicolinealidad.",
  "Se utiliza cuando se desea predecir una variable num√©rica continua.",
  "Las variables deben ser num√©ricas y estar estandarizadas para que la penalizaci√≥n tenga sentido.",
  "Asume relaci√≥n lineal entre predictores y variable respuesta, como la regresi√≥n lineal.",
  "La normalidad es deseable para inferencia, pero no indispensable para predicci√≥n.",
  "Se espera independencia entre observaciones para que el modelo sea v√°lido.",
  "Es importante que los errores tengan varianza constante para predicciones fiables.",
  "Reduce varianza, pero valores extremos a√∫n pueden afectar los resultados.",
  "La penalizaci√≥n reduce varianza al achicar coeficientes, √∫til con predictores correlacionados.",
  "Coeficientes penalizados dificultan la interpretaci√≥n directa, pero mejoran estabilidad.",
  "R√°pido y adecuado para problemas con muchas variables; incluso p > n.",
  "Se usa validaci√≥n cruzada para elegir el mejor valor de lambda (par√°metro de regularizaci√≥n).",
  "No se recomienda cuando la relaci√≥n entre variables es no lineal o se requiere interpretaci√≥n clara."
)

tabla_ridge <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_ridge %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir ridge",
             subtitle = "Ridge Regression")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Least Absolute Shrinkage and Selection Operator (LASSO)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/LASSO.png"))
```



**LASSO (Least Absolute Shrinkage and Selection Operator)** es un m√©todo de **regresi√≥n lineal regularizada** que, al igual que Ridge Regression, se utiliza para mejorar la estimaci√≥n de los coeficientes en modelos lineales y para abordar el **sobreajuste**, especialmente en escenarios con un gran n√∫mero de variables predictoras o cuando algunas de ellas son irrelevantes. LASSO es particularmente famoso por su capacidad para realizar **selecci√≥n autom√°tica de caracter√≠sticas**.

LASSO logra esto a√±adiendo un **t√©rmino de penalizaci√≥n L1** a la funci√≥n de costo de los m√≠nimos cuadrados. La funci√≥n de costo que minimiza LASSO es:

$$\text{RSS} + \lambda \sum_{j=1}^{p} |\beta_j|$$

Donde:
* $\text{RSS}$ es la suma de los errores cuadrados de los residuos.
* $\lambda$ (lambda) es un **par√°metro de sintonizaci√≥n (hiperpar√°metro)** no negativo que controla la **fuerza de la penalizaci√≥n**.
* $\sum_{j=1}^{p} |\beta_j|$ es la **penalizaci√≥n L1**, que es la suma del valor absoluto de los coeficientes de regresi√≥n (excluyendo el intercepto).

**Efecto de la Penalizaci√≥n L1:**
* **Encogimiento de Coeficientes:** Similar a Ridge, la penalizaci√≥n L1 encoge los coeficientes hacia cero.
* **Selecci√≥n de Caracter√≠sticas:** La caracter√≠stica distintiva de LASSO es que, debido a la naturaleza de la penalizaci√≥n L1 (la suma de los valores absolutos), **puede forzar los coeficientes de las variables menos importantes a ser exactamente cero**. Esto significa que LASSO no solo encoge los coeficientes, sino que tambi√©n **realiza una selecci√≥n autom√°tica de caracter√≠sticas**, eliminando efectivamente las variables irrelevantes del modelo. Esto resulta en modelos m√°s simples y f√°ciles de interpretar.
* **Manejo de Multicolinealidad (con cuidado):** Aunque LASSO puede manejar la multicolinealidad, tiende a seleccionar arbitrariamente una de las variables correlacionadas y poner a cero las dem√°s, lo que puede ser una desventaja en comparaci√≥n con Ridge (que distribuye la influencia). Elastic Net surgi√≥ para abordar esto.

El valor √≥ptimo de $\lambda$ se selecciona t√≠picamente mediante t√©cnicas de validaci√≥n cruzada.


**Aprendizaje Global vs. Local:**

LASSO es un modelo de **aprendizaje global**.

* **Aspecto Global:** LASSO construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes se estiman optimizando una funci√≥n de costo que considera todos los puntos de datos simult√°neamente. La penalizaci√≥n L1 se aplica a todos los coeficientes para controlar la complejidad y realizar la selecci√≥n de caracter√≠sticas a nivel global. La ecuaci√≥n de regresi√≥n final es una funci√≥n √∫nica que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n en el espacio de caracter√≠sticas.

* **Selecci√≥n Global de Caracter√≠sticas:** Aunque LASSO puede "localizar" qu√© variables son importantes al reducir sus coeficientes a cero, esto se hace como parte de un proceso de optimizaci√≥n global que eval√∫a la contribuci√≥n de cada variable a la predicci√≥n general del modelo. No implica la creaci√≥n de m√∫ltiples modelos locales o la adaptaci√≥n a subregiones espec√≠ficas de los datos. LASSO busca la relaci√≥n lineal m√°s parsimoniosa que mejor se ajuste al conjunto de datos completo.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal (como OLS)",
  "‚ö†Ô∏è Deseable pero no estrictamente necesaria",
  "‚úÖ Requiere independencia de errores",
  "‚úÖ Requiere homoscedasticidad",
  "‚ö†Ô∏è Puede verse afectado por outliers extremos",
  "‚úÖ Maneja multicolinealidad mediante regularizaci√≥n",
  "‚úÖ Realiza selecci√≥n de variables (coeficientes pueden ser 0)",
  "‚úÖ Eficiente en alta dimensi√≥n; mejor que OLS",
  "‚úÖ Validaci√≥n cruzada necesaria para lambda",
  "‚ùå No es adecuado si la relaci√≥n es no lineal o hay muchas variables correlacionadas con igual relevancia"
)

detalles <- c(
  "Modelo de regresi√≥n penalizada que agrega penalizaci√≥n L1, capaz de forzar coeficientes a cero (selecci√≥n de variables).",
  "Se usa cuando se desea predecir una variable continua.",
  "Las variables predictoras deben estandarizarse para que la penalizaci√≥n sea justa entre coeficientes.",
  "Asume una relaci√≥n lineal entre los predictores y la respuesta.",
  "La normalidad ayuda para inferencia, pero no es cr√≠tica para predicci√≥n.",
  "Los errores deben ser independientes para que las estimaciones sean v√°lidas.",
  "Es deseable que la varianza de los errores sea constante a lo largo de los valores ajustados.",
  "Puede verse afectado por valores at√≠picos, aunque penaliza el modelo.",
  "Disminuye la varianza de los coeficientes y ayuda a estabilizar el modelo frente a multicolinealidad.",
  "Permite eliminar autom√°ticamente variables irrelevantes, facilitando modelos m√°s simples y explicables.",
  "Es computacionalmente eficiente, incluso cuando hay m√°s variables que observaciones.",
  "Lambda (par√°metro de penalizaci√≥n) se selecciona generalmente v√≠a validaci√≥n cruzada.",
  "Si hay muchas variables correlacionadas, LASSO tiende a seleccionar solo una de ellas, lo que puede ser inadecuado en algunos contextos."
)

tabla_lasso <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lasso %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LASSO",
             subtitle = "Least Absolute Shrinkage and Selection Operator (LASSO)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```








<!--chapter:end:07-regularization.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üîç 8. Modelos Basados en Instancias {-}  

**Ejemplos:** K-Nearest Neighbors (KNN).  
**Uso:** Son ideales cuando tienes una **cantidad limitada de datos** y esperas que los patrones relevantes se encuentren en la **similitud local** entre casos. Se utilizan mucho cuando la **similitud directa** entre las observaciones es un factor clave.   
**Ventajas:** Su implementaci√≥n es **simple** y son bastante **eficaces** en problemas con pocas dimensiones.   
**Limitaciones:** **Escalan mal** con grandes vol√∫menes de datos debido a que necesitan almacenar y comparar cada instancia. Adem√°s, son **sensibles al ruido** en los datos.   

---

## Case-Based Reasoning (CBR) {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/CBR.png"))
```


## k - Nearest Neighbour (kNN)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/kNN.png"))
```


**k-Nearest Neighbour (kNN)** es un algoritmo de **Machine Learning no param√©trico** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**. Es considerado uno de los algoritmos m√°s simples y se basa en la idea de que los puntos de datos que est√°n cerca entre s√≠ en el espacio de caracter√≠sticas suelen tener propiedades similares. No es un algoritmo que "aprende" un modelo expl√≠cito durante la fase de entrenamiento, sino que es un **algoritmo perezoso (lazy learner)**.

**Funcionamiento de kNN:**

1.  **Entrenamiento:** En la fase de entrenamiento, kNN simplemente **almacena todo el conjunto de datos de entrenamiento**. No hay un proceso de "aprendizaje" de par√°metros o construcci√≥n de un modelo, como en la regresi√≥n lineal o las redes neuronales.
2.  **Predicci√≥n (para una nueva instancia):**
    * **Identificar Vecinos:** Para clasificar o predecir el valor de una nueva instancia, kNN calcula la **distancia** entre esta nueva instancia y *todas* las instancias en el conjunto de entrenamiento. La m√©trica de distancia m√°s com√∫n es la **distancia euclidiana**, pero se pueden usar otras (Manhattan, Minkowski, etc.).
    * **Seleccionar 'k' Vecinos M√°s Cercanos:** Se identifican los 'k' puntos de datos del entrenamiento que son m√°s cercanos a la nueva instancia. El valor de 'k' es un **hiperpar√°metro** que debe ser seleccionado por el usuario.
    * **Clasificaci√≥n:** Para tareas de clasificaci√≥n, la nueva instancia se asigna a la clase que es la **mayor√≠a entre sus 'k' vecinos m√°s cercanos** (votaci√≥n mayoritaria).
    * **Regresi√≥n:** Para tareas de regresi√≥n, el valor predicho para la nueva instancia es el **promedio (o mediana) de los valores de la variable de respuesta de sus 'k' vecinos m√°s cercanos**.

La elecci√≥n del valor de 'k' es crucial: un 'k' peque√±o puede hacer el modelo sensible al ruido (sobreajuste), mientras que un 'k' grande puede suavizar demasiado la predicci√≥n (subajuste) y las fronteras de decisi√≥n.

**Aprendizaje Global vs. Local:**

k-Nearest Neighbour (kNN) es el ejemplo por excelencia de un modelo de **aprendizaje puramente local**.

* **Aspecto Local:** La predicci√≥n para una nueva instancia depende **exclusivamente de los 'k' puntos de datos m√°s cercanos a ella en el espacio de caracter√≠sticas**. No se construye un modelo global que abarque todo el conjunto de datos. En cambio, para cada nueva consulta, el algoritmo "re-calcula" el vecindario relevante y realiza una predicci√≥n basada solo en la informaci√≥n de esa peque√±a regi√≥n local. Esto significa que la frontera de decisi√≥n (en clasificaci√≥n) o la funci√≥n de regresi√≥n (en regresi√≥n) se ajusta localmente a las caracter√≠sticas del vecindario del punto de consulta. Si los datos no se distribuyen linealmente y tienen estructuras complejas con patrones que var√≠an en diferentes regiones, kNN es muy efectivo porque puede adaptarse a estas variaciones locales al funcionar como una **"regresi√≥n (o clasificaci√≥n) ponderada localmente"**.

* **Sin Modelo Expl√≠cito Global:** Debido a su naturaleza de "aprendizaje perezoso", kNN no genera una funci√≥n matem√°tica expl√≠cita o un conjunto de coeficientes que describan la relaci√≥n global entre las variables. Todo el conocimiento del modelo est√° impl√≠cito en la base de datos de entrenamiento.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n y regresi√≥n)",
  "‚úÖ Num√©rica (regresi√≥n) o categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas (preferible), aunque puede adaptarse para categ√≥ricas",
  "‚ùå No asume ninguna forma funcional",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Muy sensible a valores at√≠picos",
  "‚ö†Ô∏è Problemas si hay predictores muy correlacionados",
  "‚ö†Ô∏è Dif√≠cil de interpretar (modelo basado en instancias)",
  "‚ùå Lento con grandes vol√∫menes (requiere calcular distancias)",
  "‚úÖ Crucial para elegir el mejor valor de *k*",
  "‚ùå No escala bien con datos grandes o con ruido"
)

detalles <- c(
  "Modelo no param√©trico que predice en funci√≥n de la cercan√≠a a ejemplos del conjunto de entrenamiento.",
  "Se usa tanto para clasificaci√≥n como para regresi√≥n seg√∫n el tipo de variable objetivo.",
  "Las variables deben estar en la misma escala; se recomienda estandarizar.",
  "No asume una relaci√≥n espec√≠fica entre variables; se basa en similitud.",
  "No se ajusta una funci√≥n, por lo tanto no hay residuos como tal.",
  "No se estiman errores independientes, ya que no hay funci√≥n de error expl√≠cita.",
  "No hay regresi√≥n residual, por lo tanto este supuesto no aplica.",
  "Outliers pueden alterar los vecinos m√°s cercanos y afectar la predicci√≥n.",
  "No requiere modelo expl√≠cito, pero predictores correlacionados pueden afectar el peso relativo en la distancia.",
  "Predicci√≥n se basa en instancias cercanas, dif√≠cil de resumir en una f√≥rmula.",
  "Requiere calcular distancia para cada predicci√≥n ‚Üí lento con grandes bases.",
  "Se suele usar validaci√≥n cruzada para encontrar el n√∫mero √≥ptimo de vecinos (*k*).",
  "Alta dimensi√≥n, ruido o escalas distintas entre variables afectan el rendimiento del modelo."
)

tabla_knn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_knn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir kNN",
             subtitle = "k - Nearest Neighbour (kNN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Kernel Regression / Nadaraya-Watson Estimator   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/Kernel Regression.png"))
```

## Learning Vector Quantization (LVQ)  {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/LVQ.png"))
```

**Learning Vector Quantization (LVQ)** es un algoritmo de **clasificaci√≥n supervisada basado en prototipos**, desarrollado por Teuvo Kohonen. Puede ser visto como un tipo de **red neuronal artificial** que utiliza un enfoque de "ganador se lleva todo" (winner-take-all) para aprender a clasificar datos. LVQ es una alternativa al algoritmo k-Nearest Neighbour (kNN) que busca reducir la cantidad de informaci√≥n necesaria para almacenar los datos de entrenamiento, aprendiendo un conjunto m√°s peque√±o de **prototipos** que representan las clases.

La idea central de LVQ es la siguiente:

1.  **Representaci√≥n por Prototipos:** En lugar de memorizar todos los puntos de datos de entrenamiento (como kNN), LVQ aprende un conjunto de **vectores prototipo (o "codebook vectors")**. Cada prototipo est√° asociado a una clase espec√≠fica y representa una "regi√≥n" en el espacio de caracter√≠sticas que pertenece a esa clase.
2.  **Proceso de Aprendizaje (Entrenamiento Supervisado):**
    * Se inicializan los prototipos (a menudo aleatoriamente o con puntos de datos de entrenamiento).
    * Para cada instancia de entrenamiento:
        * Se encuentra el prototipo m√°s cercano (el "ganador") a esa instancia utilizando una m√©trica de distancia (com√∫nmente la distancia euclidiana).
        * Se ajusta la posici√≥n de este prototipo ganador:
            * Si el prototipo ganador tiene la **misma clase** que la instancia de entrenamiento, el prototipo se **mueve ligeramente m√°s cerca** de la instancia (recompensa).
            * Si el prototipo ganador tiene una **clase diferente** a la instancia de entrenamiento, el prototipo se **mueve ligeramente m√°s lejos** de la instancia (penalizaci√≥n).
    * Este proceso iterativo contin√∫a hasta que los prototipos convergen o se alcanza un n√∫mero m√°ximo de √©pocas. Las diferentes variantes de LVQ (LVQ1, LVQ2.1, LVQ3) tienen reglas de actualizaci√≥n ligeramente distintas.
3.  **Clasificaci√≥n (Predicci√≥n):** Para clasificar una nueva instancia, simplemente se encuentra el prototipo m√°s cercano a esa instancia en el espacio de caracter√≠sticas. La nueva instancia se asigna a la clase asociada con ese prototipo m√°s cercano. Es similar a un clasificador 1-NN que opera sobre los prototipos aprendidos.

LVQ es valorado por la interpretabilidad de sus prototipos (ya que son puntos en el espacio de caracter√≠sticas que representan una clase) y por su eficiencia una vez que los prototipos han sido aprendidos, ya que la predicci√≥n es mucho m√°s r√°pida que kNN en grandes conjuntos de datos.


**Aprendizaje Global vs. Local:**

Learning Vector Quantization (LVQ) es un modelo que exhibe caracter√≠sticas de **aprendizaje tanto global como local**.

* **Aspecto Local:** El coraz√≥n del aprendizaje en LVQ es la **adaptaci√≥n local de los prototipos**. En cada paso de entrenamiento, solo el prototipo m√°s cercano (o los dos prototipos m√°s cercanos en algunas variantes como LVQ2.1 y LVQ3) a una instancia de entrenamiento se ajusta. Esto significa que las reglas de aprendizaje operan en un **vecindario localizado** alrededor de la instancia de entrada. Los prototipos se mueven en el espacio de caracter√≠sticas para delimitar mejor las fronteras de clase, lo que refleja la estructura local de los datos. De esta manera, LVQ puede modelar **relaciones no lineales** y estructuras de clase complejas al ajustar las posiciones de estos "representantes" locales de las clases.

* **Aspecto Global:** Aunque el ajuste es local, el conjunto de todos los prototipos de LVQ, una vez entrenados, forma una **representaci√≥n global del espacio de caracter√≠sticas** que se utiliza para la clasificaci√≥n. Estos prototipos definen un mapa de clasificaci√≥n en todo el espacio de entrada, donde cada regi√≥n (celda de Voronoi) se asocia con una clase. Por lo tanto, el modelo final, que es la colecci√≥n de prototipos, se aplica de manera global para clasificar cualquier nueva observaci√≥n. El proceso de optimizaci√≥n para encontrar las posiciones de los prototipos, aunque iterativo y basado en actualizaciones locales, busca una configuraci√≥n global √≥ptima que minimice el error de clasificaci√≥n en todo el conjunto de entrenamiento.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica",
  "‚úÖ Num√©ricas (requiere c√°lculo de distancias)",
  "‚ùå No asume relaci√≥n funcional directa",
  "‚ùå No aplica (modelo de clasificaci√≥n, no regresi√≥n)",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Sensible a valores extremos (afectan los prototipos)",
  "‚ö†Ô∏è Variables correlacionadas pueden distorsionar distancias",
  "‚ö†Ô∏è Intermedio: prototipos ayudan pero no son tan interpretables como reglas",
  "‚úÖ R√°pido despu√©s del entrenamiento (dependiendo del n√∫mero de prototipos)",
  "‚úÖ √ötil para ajustar n√∫mero y posici√≥n de prototipos",
  "‚ùå Problemas si los datos no est√°n bien escalados o si hay clases muy desbalanceadas"
)

detalles <- c(
  "T√©cnica supervisada basada en instancias que usa prototipos para representar clases.",
  "Se usa para tareas de clasificaci√≥n en donde las clases est√°n etiquetadas.",
  "Requiere variables num√©ricas porque se basa en distancias euclidianas para asignaci√≥n de clases.",
  "No asume una relaci√≥n funcional, simplemente asigna una clase basada en el prototipo m√°s cercano.",
  "No se generan residuos, por tanto la normalidad no se eval√∫a.",
  "No hay error estructurado como en modelos de regresi√≥n, por lo tanto este supuesto no aplica.",
  "No se eval√∫a la varianza de errores ya que no es un modelo de regresi√≥n.",
  "Outliers pueden alterar la posici√≥n de los prototipos y generar errores de clasificaci√≥n.",
  "Variables altamente correlacionadas pueden sesgar las distancias, lo que afecta la clasificaci√≥n.",
  "Aunque los prototipos pueden ofrecer intuici√≥n sobre la clase, no son completamente transparentes.",
  "Despu√©s del ajuste de los prototipos, la clasificaci√≥n es eficiente.",
  "Es com√∫n usar validaci√≥n cruzada para seleccionar el n√∫mero y la distribuci√≥n de los prototipos.",
  "No es adecuado si las variables est√°n en escalas distintas o si no hay separaci√≥n clara entre clases."
)

tabla_lvq <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lvq %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LVQ",
             subtitle = "Learning Vector Quantization (LVQ)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Locally Weighted Learning (LWL)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/LWL.png"))
```

**Locally Weighted Learning (LWL)** es una clase de algoritmos de **aprendizaje supervisado no param√©trico** que se distingue por su enfoque en la **construcci√≥n de modelos locales** para cada nueva instancia de consulta, en lugar de aprender un √∫nico modelo global para todo el conjunto de datos. Es un tipo de **"aprendizaje perezoso" (lazy learning)**, lo que significa que la mayor parte del "trabajo" (c√°lculos) se realiza en el momento de la predicci√≥n, no durante una fase de entrenamiento expl√≠cita.

La idea central de LWL es que, para predecir la salida de una nueva instancia de consulta, se construye un modelo simple (a menudo lineal o polin√≥mico) utilizando solo las **instancias de entrenamiento que son "cercanas"** a la instancia de consulta. Adem√°s, a las instancias de entrenamiento m√°s cercanas se les asigna un **peso mayor** en la construcci√≥n de este modelo local.

El proceso de LWL (especialmente para regresi√≥n, conocida como **Regresi√≥n Lineal Ponderada Localmente - LWLR** o LOESS/LOWESS) implica:

1.  **Sin Fase de Entrenamiento expl√≠cita:** El algoritmo simplemente almacena todo el conjunto de datos de entrenamiento.
2.  **Para cada Instancia de Consulta (Predicci√≥n):**
    * **C√°lculo de Distancias:** Se calcula la distancia entre la instancia de consulta y todas las instancias de entrenamiento.
    * **Asignaci√≥n de Pesos:** Se aplica una **funci√≥n de kernel (funci√≥n de ponderaci√≥n)** a estas distancias para asignar un peso a cada instancia de entrenamiento. Las instancias m√°s cercanas a la consulta reciben un peso mayor, y los pesos disminuyen a medida que la distancia aumenta. Un hiperpar√°metro llamado **ancho de banda (bandwidth)** controla qu√© tan r√°pido disminuyen los pesos con la distancia (determina el "tama√±o del vecindario" influyente).
    * **Construcci√≥n del Modelo Local:** Se ajusta un modelo simple (ej., una regresi√≥n lineal) a las instancias de entrenamiento, pero esta vez, cada instancia se pondera seg√∫n el peso calculado. Esto es, se minimiza una suma de errores cuadrados ponderada.
    * **Predicci√≥n:** El valor predicho para la instancia de consulta se obtiene utilizando este modelo local reci√©n construido. El modelo local se descarta despu√©s de hacer la predicci√≥n para esa instancia.

LWL es muy efectivo para modelar relaciones **no lineales y complejas** en los datos porque puede adaptar la forma de la funci√≥n de predicci√≥n a las variaciones locales. Es una generalizaci√≥n de k-Nearest Neighbors (kNN) donde en lugar de solo promediar o votar, se ajusta un modelo ponderado.


**Aprendizaje Global vs. Local:**

Locally Weighted Learning (LWL) es el ep√≠tome del **aprendizaje puramente local**.

* **Aspecto Local:** LWL es intr√≠nsecamente local en su funcionamiento. Para **cada nueva predicci√≥n**, se construye un **modelo espec√≠fico y √∫nico** que solo es v√°lido en el **vecindario local** de la instancia de consulta. Los pesos asignados a las instancias de entrenamiento enfatizan las que est√°n m√°s cerca del punto de consulta, lo que significa que el modelo se "adapta" a la estructura de los datos en esa regi√≥n particular del espacio de caracter√≠sticas. Esto le permite manejar eficientemente relaciones no lineales y heterog√©neas, ya que la relaci√≥n puede ser diferente en distintas partes del dominio de los datos.

* **Sin Modelo Expl√≠cito Global:** No hay un conjunto fijo de par√°metros o una funci√≥n matem√°tica √∫nica que describa la relaci√≥n entre las entradas y las salidas para todo el conjunto de datos. En cambio, el "modelo" se genera din√°micamente para cada punto de consulta, utilizando solo la informaci√≥n relevante de su vecindario. La complejidad computacional de LWL aumenta con el n√∫mero de predicciones, ya que cada una requiere la construcci√≥n de un nuevo modelo local.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n o clasificaci√≥n)",
  "‚úÖ Num√©rica o categ√≥rica, seg√∫n tarea",
  "‚úÖ Num√©ricas o categ√≥ricas (requiere distancias)",
  "‚úÖ No lineal (ajustes locales en cada predicci√≥n)",
  "‚ö†Ô∏è Depende del modelo local usado (e.g., regresi√≥n lineal)",
  "‚ö†Ô∏è Puede no cumplir si hay dependencia local",
  "‚ö†Ô∏è Evaluada localmente, var√≠a seg√∫n vecindario",
  "‚úÖ S√≠, muy sensible a outliers en vecindarios locales",
  "‚ö†Ô∏è Puede causar inestabilidad en predicciones locales",
  "‚ö†Ô∏è Dif√≠cil de interpretar globalmente, clara localmente",
  "‚ùå Lento, necesita recalcular modelo para cada punto",
  "‚úÖ S√≠, especialmente leave-one-out o k-fold por zonas",
  "‚ùå Ineficiente con muchos datos o alta dimensi√≥n"
)

detalles <- c(
  "Modelo supervisado que ajusta un modelo distinto en cada punto de predicci√≥n usando los vecinos m√°s cercanos.",
  "Puede ser regresi√≥n (respuesta num√©rica) o clasificaci√≥n (respuesta categ√≥rica).",
  "Utiliza variables para calcular distancias a partir de un punto de consulta.",
  "Ajusta modelos simples en regiones locales, permitiendo capturar relaciones no lineales.",
  "En regresi√≥n local puede requerirse que los residuos sean normales si se desea inferencia.",
  "Los errores pueden no ser independientes si hay estructuras repetitivas locales.",
  "La varianza puede cambiar entre zonas del espacio, por lo que se revisa localmente.",
  "Los valores at√≠picos pueden sesgar el modelo local si caen cerca del punto de predicci√≥n.",
  "La multicolinealidad puede afectar si el modelo local es lineal, aunque su efecto se restringe localmente.",
  "La interpretaci√≥n es clara en zonas locales, pero no se generaliza a toda la muestra.",
  "Cada predicci√≥n entrena un nuevo modelo, lo que es computacionalmente costoso.",
  "La validaci√≥n cruzada ayuda a elegir par√°metros como el ancho del vecindario (kernel).",
  "El rendimiento cae en grandes vol√∫menes de datos o si los datos no presentan estructura local clara."
)

tabla_lwl <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lwl %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LWL",
             subtitle = "Locally Weighted Learning (LWL)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Self - Organizing Map (SOM)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/SOM.png"))
```

Una **Self-Organizing Map (SOM)**, tambi√©n conocida como **Mapa Autoorganizado de Kohonen** o **Mapa de Caracter√≠sticas Autoorganizado (SOFM)**, es un tipo de **red neuronal artificial no supervisada** utilizada principalmente para **reducci√≥n de dimensionalidad y visualizaci√≥n de datos**. Su objetivo es producir una representaci√≥n de baja dimensi√≥n (t√≠picamente bidimensional) de un conjunto de datos de alta dimensi√≥n, mientras **preserva la estructura topol√≥gica** de los datos originales. Esto significa que los puntos de datos que son similares en el espacio de alta dimensi√≥n se mapean a neuronas cercanas en el mapa de baja dimensi√≥n.

A diferencia de otras redes neuronales que utilizan el aprendizaje por retropropagaci√≥n y descenso de gradiente (aprendizaje basado en el error), las SOM utilizan un proceso de **aprendizaje competitivo**.

El funcionamiento de un SOM implica los siguientes pasos iterativos:

1.  **Inicializaci√≥n:** Se crea una cuadr√≠cula de "neuronas" (tambi√©n llamadas unidades o nodos) en el espacio de baja dimensi√≥n (ej., una cuadr√≠cula 2D). A cada neurona se le asigna un **vector de pesos** con la misma dimensionalidad que los datos de entrada. Estos vectores de pesos se inicializan aleatoriamente o de forma lineal.
2.  **Competencia:** Para cada vector de entrada (punto de datos) del conjunto de entrenamiento:
    * Se calcula la distancia (com√∫nmente euclidiana) entre el vector de entrada y el vector de pesos de cada neurona en la cuadr√≠cula.
    * La neurona con el vector de pesos m√°s cercano al vector de entrada se denomina **Unidad de Mejor Coincidencia (BMU - Best Matching Unit)**.
3.  **Cooperaci√≥n (Vecindad):** La BMU y sus neuronas **vecinas** (dentro de un radio definido en la cuadr√≠cula) son identificadas. El tama√±o de este radio de vecindad disminuye con el tiempo a medida que avanza el entrenamiento. La influencia del ajuste de los pesos disminuye con la distancia de la BMU dentro de esta vecindad (definido por una **funci√≥n de vecindad**, como una Gaussiana).
4.  **Adaptaci√≥n:** Los vectores de pesos de la BMU y sus neuronas vecinas se **ajustan ligeramente** para que se acerquen al vector de entrada original. La magnitud del ajuste est√° determinada por una **tasa de aprendizaje**, que tambi√©n disminuye con el tiempo. El ajuste es mayor para la BMU y menor para las neuronas m√°s alejadas dentro del radio de vecindad.
5.  **Iteraci√≥n:** Los pasos 2-4 se repiten para un gran n√∫mero de √©pocas (iteraciones) y para todos los vectores de entrada, hasta que los pesos de las neuronas convergen y la red se "autoorganiza".

Al final del entrenamiento, las neuronas en el mapa se han organizado de tal manera que las neuronas cercanas representan datos de entrada similares, creando un "mapa" donde las regiones con densidades de datos similares forman grupos o clusters.

**Aprendizaje Global vs. Local:**

Una Self-Organizing Map (SOM) es un modelo que **combina aspectos de aprendizaje global y local** de una manera muy particular, que evoluciona a lo largo del proceso de entrenamiento.

* **Aspecto Global (Fases Iniciales del Entrenamiento):** Al principio del entrenamiento, el radio de vecindad y la tasa de aprendizaje son grandes. Esto significa que cuando una BMU se ajusta, un **gran n√∫mero de neuronas circundantes en el mapa tambi√©n se ajustan**, incluso aquellas que est√°n relativamente lejos de la BMU. Este amplio ajuste permite que el mapa se "organice globalmente" para capturar la estructura general de los datos. La topolog√≠a general de la proyecci√≥n se establece en esta fase inicial. El mapa se estira y se contrae para abarcar la dispersi√≥n global de los datos, como si una "regresi√≥n ponderada localmente" de gran escala estuviera adaptando el mapa entero.

* **Aspecto Local (Fases Posteriores del Entrenamiento):** A medida que el entrenamiento avanza, el radio de vecindad y la tasa de aprendizaje **disminuyen gradualmente**. Esto hace que los ajustes a los pesos sean cada vez m√°s localizados. En las etapas finales, solo la BMU y sus vecinos m√°s cercanos (o incluso solo la BMU) se ajustan significativamente. Esta fase de "afinamiento" permite que el mapa capture los detalles m√°s finos y las **estructuras locales** dentro de los datos, refinando las fronteras entre los grupos y asegurando que los puntos similares se agrupen con alta precisi√≥n.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad y clustering)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (basado en distancias)",
  "‚úÖ No lineal (mapea datos de alta dimensi√≥n a una grilla)",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede ser sensible a outliers (afectan la topolog√≠a de la grilla)",
  "‚ö†Ô∏è No impacta directamente pero puede distorsionar distancias",
  "‚ö†Ô∏è Dif√≠cil de interpretar (requiere visualizaciones espec√≠ficas)",
  "‚ö†Ô∏è Entrenamiento puede ser lento en datasets grandes, luego eficiente",
  "‚ö†Ô∏è No es tradicional, pero se puede evaluar topolog√≠a y distorsi√≥n",
  "‚ùå Mal desempe√±o si los datos est√°n mal escalados o no hay estructura"
)

detalles <- c(
  "T√©cnica no supervisada que proyecta datos de alta dimensi√≥n a una grilla 2D preservando la topolog√≠a.",
  "No predice una variable, sino agrupa y organiza datos similares espacialmente.",
  "Basado en distancias euclidianas entre vectores de caracter√≠sticas; requiere variables num√©ricas.",
  "Preserva relaciones de vecindad: observaciones similares se ubican cerca en la grilla.",
  "No se generan residuos como en modelos de regresi√≥n o clasificaci√≥n.",
  "No hay modelo de error; no aplica este supuesto.",
  "No hay varianza de errores al no haber predicci√≥n.",
  "Outliers pueden alterar las posiciones en la grilla y afectar la interpretaci√≥n.",
  "La correlaci√≥n entre variables puede afectar las distancias y la formaci√≥n de grupos.",
  "Interpretaci√≥n se basa en visualizaci√≥n de mapas de componentes y distancias.",
  "Entrenamiento iterativo, m√°s lento que PCA pero √∫til para exploraci√≥n visual.",
  "No se usa validaci√≥n cruzada directa, pero se puede evaluar la topolog√≠a y mapas de distancia.",
  "Datos ruidosos, mal escalados o con muchas variables irrelevantes dificultan resultados √∫tiles."
)

tabla_som <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_som %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir SOM",
             subtitle = "Self - Organizing Map (SOM)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Prototype-Based Learning (General Concept) {-}     

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/PBL.png"))
```

<!--chapter:end:08-instance_based.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üìè 9. Clustering (Aprendizaje No Supervisado) {-}  

**Ejemplos:** K-Means, DBSCAN, Agrupamiento Jer√°rquico.   
**Uso:** Excelente para **agrupar datos sin etiquetas previas**, permiti√©ndote descubrir **estructuras ocultas** o identificar **segmentos de mercado** dentro de tus conjuntos de datos. Es una herramienta clave en la exploraci√≥n de datos.   
**Ventajas:** Es incre√≠blemente √∫til para la **exploraci√≥n de datos** y para **reducir la complejidad** al encontrar patrones inherentes.  
**Limitaciones:** Generalmente, necesitas **elegir el n√∫mero de grupos** de antemano (excepto en DBSCAN), lo cual puede ser un desaf√≠o. Adem√°s, algunos algoritmos pueden ser **sensibles a la escala** de las caracter√≠sticas de tus datos.

---

## Affinity Propagation {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/AF.png"))
```


## Agglomerative Clustering {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/Agglomerative Clustering.png"))
```

## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/DBSCAN.png"))
```

**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** es un algoritmo de **agrupamiento (clustering) no supervisado** que se distingue de los algoritmos basados en centroides (como k-Means) por su capacidad para encontrar **clusters de formas arbitrarias** y para identificar **puntos de ruido (outliers)**. Su idea central es que los clusters son regiones densas de puntos en el espacio de caracter√≠sticas, separadas por regiones de baja densidad.

DBSCAN define tres tipos de puntos:

1.  **Punto N√∫cleo (Core Point):** Un punto es un punto n√∫cleo si, dentro de un radio especificado ($\epsilon$ o `eps`), contiene un n√∫mero m√≠nimo de otros puntos ( `MinPts`).
2.  **Punto Frontera (Border Point):** Un punto es un punto frontera si est√° dentro del radio $\epsilon$ de un punto n√∫cleo, pero no es un punto n√∫cleo en s√≠ mismo (no tiene `MinPts` vecinos dentro de su propio radio $\epsilon$).
3.  **Punto de Ruido (Noise Point):** Cualquier punto que no es un punto n√∫cleo ni un punto frontera. Estos puntos son considerados outliers.

El algoritmo de DBSCAN opera de la siguiente manera:

1.  **Inicializaci√≥n:** Selecciona un punto arbitrario del conjunto de datos que a√∫n no ha sido visitado.
2.  **Expansi√≥n de Cluster:**
    * Si el punto seleccionado es un **punto n√∫cleo**, se inicia un nuevo cluster. Todos sus vecinos dentro del radio $\epsilon$ se a√±aden al cluster.
    * Recursivamente, se visitan y a√±aden los vecinos de esos nuevos puntos. Si un vecino es tambi√©n un punto n√∫cleo, sus propios vecinos tambi√©n se a√±aden al cluster. Este proceso contin√∫a hasta que no se puedan a√±adir m√°s puntos al cluster (es decir, todos los puntos alcanzables por densidad han sido encontrados).
    * Si el punto seleccionado **no es un punto n√∫cleo**, se marca como ruido (o se deja para ser procesado m√°s tarde si es un punto frontera de otro cluster ya formado).
3.  **Iteraci√≥n:** El proceso se repite con otro punto no visitado hasta que todos los puntos han sido procesados.

DBSCAN es particularmente √∫til para encontrar clusters complejos en conjuntos de datos ruidosos y no requiere que el usuario especifique el n√∫mero de clusters de antemano. Sus dos hiperpar√°metros clave son `eps` (el radio de b√∫squeda de vecindad) y `MinPts` (el n√∫mero m√≠nimo de puntos para formar un n√∫cleo).

**Aprendizaje Global vs. Local:**

DBSCAN es un algoritmo de **agrupamiento inherentemente local**, aunque el resultado final es una partici√≥n global de los datos en clusters y ruido.

* **Aspecto Local:** El coraz√≥n de DBSCAN reside en la definici√≥n de densidad local y la conectividad. Las decisiones sobre si un punto es un n√∫cleo, un frontera o ruido, y si dos puntos pertenecen al mismo cl√∫ster, se basan **exclusivamente en la densidad de puntos en un vecindario muy localizado** definido por el radio $\epsilon$ y el `MinPts`. El algoritmo "expande" los cl√∫steres al moverse de un punto n√∫cleo a sus vecinos, y de estos a sus vecinos, y as√≠ sucesivamente. Esta capacidad de crecer y formar cl√∫steres org√°nicamente a partir de las densidades locales es lo que permite a DBSCAN descubrir formas arbitrarias y adaptarse a la estructura local de los datos. No hay una funci√≥n global o centroides predefinidos que gu√≠en la agrupaci√≥n; todo se deriva de las propiedades de densidad local.

* **Resultado Global (Partici√≥n):** Aunque el proceso es local, el resultado final es una **partici√≥n global del conjunto de datos** en varios cl√∫steres y un conjunto de puntos de ruido. Una vez que todos los puntos han sido procesados y los cl√∫steres expandidos, se obtiene una vista global de la estructura de agrupamiento.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (distancias euclidianas u otras m√©tricas)",
  "‚úÖ Detecta clusters basados en densidad, no forma lineal",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚úÖ Robusto a outliers (los detecta como ruido)",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Clusters pueden ser arbitrarios, pero es intuitivo identificar ruido",
  "‚úÖ Razonablemente r√°pido para conjuntos medianos",
  "‚ùå No usa validaci√≥n cruzada cl√°sica; se eval√∫a con m√©tricas de clustering",
  "‚ùå No funciona bien con clusters de densidades muy diferentes o alta dimensionalidad"
)

detalles <- c(
  "Algoritmo de clustering basado en densidad que agrupa puntos cercanos y marca puntos aislados como ruido.",
  "No busca predecir, sino agrupar observaciones.",
  "Se basa en distancias; variables num√©ricas adecuadas; variables categ√≥ricas necesitan transformaci√≥n.",
  "No asume formas de clusters lineales ni convexas; puede detectar clusters arbitrarios.",
  "No genera residuos; no aplica normalidad.",
  "No hay modelo de error residual, no aplica independencia.",
  "No es un modelo predictivo, no aplica homoscedasticidad.",
  "Detecta outliers etiquet√°ndolos como ruido, siendo robusto frente a ellos.",
  "No hay predictores en sentido tradicional, por lo que multicolinealidad no afecta.",
  "Interpretaci√≥n basada en grupos densos y puntos aislados (ruido).",
  "Es eficiente, aunque su rendimiento puede disminuir en alta dimensionalidad.",
  "No utiliza validaci√≥n cruzada est√°ndar; evaluaci√≥n se basa en √≠ndices de clustering como Silhouette.",
  "Dificultades con clusters con diferentes densidades y cuando la dimensionalidad es muy alta."
)

tabla_dbscan <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_dbscan %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir DBSCAN",
             subtitle = "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Expectation Maximization (EM) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/EM.png"))
```

El algoritmo **Expectation-Maximization (EM)** es un m√©todo iterativo utilizado en estad√≠stica para encontrar las **estimaciones de m√°xima verosimilitud (MLE)** o las estimaciones de m√°xima a posteriori (MAP) de los par√°metros en modelos estad√≠sticos, especialmente cuando el modelo depende de **variables latentes (no observadas o "ocultas")** o cuando los datos est√°n "incompletos".

EM es particularmente √∫til para modelos de mezcla, donde se asume que los datos observados son una mezcla de varias distribuciones subyacentes, y la pertenencia de cada punto de datos a una distribuci√≥n espec√≠fica es la variable latente. El algoritmo consta de dos pasos principales que se alternan hasta la convergencia:

1.  **Paso E (Expectation Step - Paso de Expectativa):**
    * En este paso, dadas las estimaciones actuales de los par√°metros del modelo, se calculan las **probabilidades esperadas (o "responsabilidades")** de que cada punto de datos observado pertenezca a cada una de las componentes latentes (o de que las variables latentes tomen ciertos valores).
    * Esencialmente, se est√° haciendo una "suposici√≥n" sobre los valores de las variables latentes bas√°ndose en los par√°metros actuales del modelo y los datos observados.

2.  **Paso M (Maximization Step - Paso de Maximizaci√≥n):**
    * En este paso, utilizando las "responsabilidades" calculadas en el Paso E (trat√°ndolas como si fueran observaciones completas), se **re-estiman los par√°metros del modelo** para maximizar la verosimilitud esperada.
    * Esto es t√≠picamente un problema de optimizaci√≥n m√°s simple que el problema original de m√°xima verosimilitud con datos incompletos. Se ajustan los par√°metros (ej., medias, varianzas, pesos de mezcla) para que el modelo se ajuste mejor a los datos, considerando las asignaciones "blandas" a las variables latentes.

Los Pasos E y M se repiten iterativamente. La verosimilitud del modelo est√° garantizada para no disminuir en cada iteraci√≥n, y el algoritmo converge a un **m√°ximo local** de la funci√≥n de verosimilitud.

**Aplicaciones comunes:**
* **Modelos de Mezcla Gaussiana (GMMs):** Un uso protot√≠pico del EM para el clustering no supervisado.
* **Modelos Ocultos de Markov (HMMs):** Para problemas de reconocimiento de voz y bioinform√°tica.
* **Imputaci√≥n de datos faltantes:** Para estimar valores faltantes en un conjunto de datos.
* **An√°lisis de componentes latentes.**


**Aprendizaje Global vs. Local:**

El algoritmo Expectation-Maximization (EM) es un m√©todo de **aprendizaje global**, pero es importante entender un matiz sobre su convergencia.

* **Aspecto Global:** EM tiene como objetivo encontrar los **par√°metros de un modelo probabil√≠stico global** (como un GMM completo que describe la distribuci√≥n de todo el conjunto de datos) que maximicen la verosimilitud de los datos observados. Los par√°metros que se estiman (medias, covarianzas, pesos de mezcla en un GMM) son v√°lidos para todo el espacio de caracter√≠sticas. El algoritmo itera sobre todo el conjunto de datos en cada paso E y M para actualizar estos par√°metros globales. La soluci√≥n que busca EM es una representaci√≥n unificada y global de las distribuciones subyacentes de los datos.

* **Convergencia a M√°ximos Locales:** Aunque EM busca una soluci√≥n global, una limitaci√≥n cr√≠tica es que **solo est√° garantizado para converger a un m√°ximo local** de la funci√≥n de verosimilitud, no necesariamente al m√°ximo global. Esto significa que el resultado final puede depender de la **inicializaci√≥n** de los par√°metros del modelo. Si la funci√≥n de verosimilitud tiene m√∫ltiples "picos" (m√°ximos locales), EM puede quedar "atrapado" en uno de ellos. Para mitigar esto, es una pr√°ctica com√∫n ejecutar EM varias veces con diferentes inicializaciones aleatorias y seleccionar el resultado con la verosimilitud m√°s alta.

Por lo tanto, mientras que el objetivo de EM es aprender un modelo global que abarque todo el espacio de datos, su m√©todo iterativo de optimizaci√≥n lo hace susceptible a encontrar √≥ptimos locales en la funci√≥n de verosimilitud. La forma en que un modelo probabil√≠stico como un GMM puede modelar **relaciones no lineales** en los datos es que, al combinar m√∫ltiples distribuciones gaussianas (lineales), el modelo resultante puede capturar formas y densidades complejas y no lineales en el espacio de caracter√≠sticas. EM es el algoritmo que permite aprender estos componentes subyacentes.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (estimaci√≥n de par√°metros en modelos con datos incompletos o mixtos)",
  "‚ùå No aplica directamente (modelo probabil√≠stico)",
  "‚úÖ Variables num√©ricas o categ√≥ricas seg√∫n modelo",
  "‚úÖ Estima par√°metros m√°ximos de verosimilitud, puede manejar modelos complejos",
  "‚ö†Ô∏è Depende del modelo espec√≠fico usado con EM",
  "‚ö†Ô∏è Depende del modelo; errores independientes si asume modelo estad√≠stico cl√°sico",
  "‚ö†Ô∏è Depende del modelo estad√≠stico subyacente",
  "‚ö†Ô∏è Puede ser sensible a outliers dependiendo del modelo y datos",
  "‚ö†Ô∏è Depende del modelo y las variables involucradas",
  "‚ö†Ô∏è La interpretaci√≥n depende del modelo y par√°metros estimados",
  "‚ùå Puede ser lento si el modelo es complejo o datos muy grandes",
  "‚ùå Validaci√≥n cruzada depende del modelo, no es intr√≠nseco a EM",
  "‚ùå Puede converger a m√°ximos locales; requiere buen punto inicial y modelo adecuado"
)

detalles <- c(
  "Algoritmo iterativo para estimar par√°metros de modelos estad√≠sticos con datos faltantes o variables latentes.",
  "No genera predicciones directas, sino estima par√°metros para modelos probabil√≠sticos.",
  "Aplicable a datos num√©ricos o categ√≥ricos dependiendo del modelo (mezcla de Gaussianas, por ejemplo).",
  "Maximiza la funci√≥n de verosimilitud de manera iterativa, estimando variables latentes y par√°metros.",
  "La normalidad depende del modelo (por ejemplo, mezcla de Gaussianas asume normalidad).",
  "Si el modelo asume errores independientes, entonces s√≠; depende del modelo estad√≠stico usado.",
  "Homoscedasticidad depende del modelo estad√≠stico subyacente.",
  "Sensibilidad a outliers var√≠a seg√∫n la robustez del modelo y datos.",
  "Multicolinealidad afecta seg√∫n la estructura del modelo y variables involucradas.",
  "Interpretaci√≥n es sobre par√°metros estimados y variables latentes, no sobre coeficientes directos.",
  "Puede requerir muchas iteraciones, afectando velocidad en modelos complejos.",
  "La validaci√≥n cruzada depende del modelo aplicado tras la estimaci√≥n por EM.",
  "Puede quedarse atrapado en soluciones sub√≥ptimas; se recomienda m√∫ltiples inicios."
)

tabla_em <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_em %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir EM",
             subtitle = "Expectation Maximization (EM)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Gaussian Mixture Models (GMMs) {-}   


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/GMMs.png"))
```

## Hierarchical Clustering (hclust) {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/hclust.png"))
```

El **Agrupamiento Jer√°rquico (Hierarchical Clustering)**, a menudo abreviado como **hclust**, es un m√©todo de **agrupamiento (clustering) no supervisado** que construye una **jerarqu√≠a de clusters** en lugar de una partici√≥n plana de los datos (como k-Means). El resultado de un agrupamiento jer√°rquico se visualiza com√∫nmente como un **dendrograma**, un diagrama en forma de √°rbol que muestra la secuencia de fusiones o divisiones de los clusters.

Existen dos tipos principales de agrupamiento jer√°rquico:

1.  **Agrupamiento Aglomerativo ("Bottom-Up"):** Es el tipo m√°s com√∫n.
    * Comienza tratando **cada punto de datos como un cluster individual**.
    * En cada paso, **fusiona los dos clusters m√°s cercanos** en un nuevo cluster.
    * Este proceso contin√∫a hasta que todos los puntos de datos pertenecen a un √∫nico cluster grande.
    * La "cercan√≠a" entre clusters se define por una **m√©trica de enlace (linkage)**. Las m√©tricas de enlace comunes incluyen:
        * **Enlace √önico (Single Linkage):** Distancia m√≠nima entre dos puntos en diferentes clusters. Tiende a formar clusters "largos" y "delgados".
        * **Enlace Completo (Complete Linkage):** Distancia m√°xima entre dos puntos en diferentes clusters. Tiende a formar clusters compactos.
        * **Enlace Promedio (Average Linkage):** Distancia promedio entre todos los pares de puntos en diferentes clusters.
        * **M√©todo de Ward:** Minimiza la varianza total dentro de los clusters despu√©s de la fusi√≥n. Tiende a formar clusters compactos de tama√±o similar.

2.  **Agrupamiento Divisivo ("Top-Down"):**
    * Comienza con **todos los puntos en un solo cluster grande**.
    * En cada paso, **divide el cluster actual en dos sub-clusters** m√°s peque√±os.
    * Este proceso contin√∫a hasta que cada punto de datos est√° en su propio cluster individual.
    * Es menos com√∫n en la pr√°ctica debido a su mayor complejidad computacional.

La principal ventaja de hclust es que no requiere especificar el n√∫mero de clusters de antemano; en cambio, el n√∫mero de clusters se puede determinar inspeccionando el dendrograma y "cort√°ndolo" a una altura apropiada. Tambi√©n es muy bueno para revelar la estructura anidada de los datos.


**Aprendizaje Global vs. Local:**

El Agrupamiento Jer√°rquico (hclust) es un algoritmo que se puede clasificar como de **aprendizaje local** en su construcci√≥n incremental, pero que al final revela una **estructura global** de los datos.

* **Aspecto Local (Proceso de Fusi√≥n/Divisi√≥n):** En cada paso del agrupamiento aglomerativo, la decisi√≥n de qu√© clusters fusionar se basa **exclusivamente en la distancia (o similitud) entre los clusters m√°s cercanos en ese momento**. Esta es una decisi√≥n puramente local, ya que solo se consideran los pares de clusters m√°s pr√≥ximos. El algoritmo construye la jerarqu√≠a fusionando iterativamente los vecinos m√°s cercanos, lo que le permite adaptarse a la forma y densidad local de los datos. Las fronteras de los cl√∫steres no est√°n predefinidas por un modelo global; en cambio, emergen de las relaciones de proximidad locales. Esto permite a hclust descubrir clusters de **formas arbitrarias** y **relaciones no lineales** que podr√≠an no ser detectadas por m√©todos que asumen formas espec√≠ficas de clusters (como k-Means con suposiciones esf√©ricas).

* **Aspecto Global (Dendrograma):** Aunque las decisiones de fusi√≥n son locales, el resultado final (el dendrograma) es una **representaci√≥n jer√°rquica global** de las relaciones de todos los puntos de datos. Proporciona una visi√≥n completa de c√≥mo todos los puntos se agrupan en diferentes niveles de granularidad, desde clusters individuales hasta un solo cluster grande. Esta estructura global revela patrones de anidamiento y relaciones a diferentes escalas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering jer√°rquico)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Variables num√©ricas o categ√≥ricas (seg√∫n medida de distancia)",
  "‚úÖ Agrupa observaciones en base a similitud o distancia",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Sensible a valores at√≠picos que pueden distorsionar distancias",
  "‚ö†Ô∏è No afecta directamente (no hay predictores ni multicolinealidad)",
  "‚úÖ Dendrograma facilita interpretaci√≥n visual de grupos",
  "‚ö†Ô∏è Puede ser lento en datasets muy grandes",
  "‚ùå No se suele usar validaci√≥n cruzada, pero s√≠ m√©todos de evaluaci√≥n interna",
  "‚ùå Resultados muy sensibles a elecci√≥n de distancia y m√©todo de enlace"
)

detalles <- c(
  "M√©todo no supervisado para agrupar observaciones en una jerarqu√≠a basada en distancias.",
  "No busca predecir, sino identificar grupos o clusters.",
  "Puede trabajar con variables num√©ricas y categ√≥ricas si se define distancia adecuada.",
  "Construye dendrograma que muestra agrupamientos sucesivos desde observaciones individuales hasta un solo cluster.",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia de errores.",
  "No requiere homoscedasticidad.",
  "Valores at√≠picos pueden alterar significativamente la estructura del dendrograma.",
  "Como es una t√©cnica de agrupamiento, no existe multicolinealidad entre variables predictoras.",
  "Dendrograma permite interpretar las relaciones y agrupamientos entre observaciones.",
  "La complejidad aumenta r√°pido con el n√∫mero de observaciones (O(n^3)).",
  "Se eval√∫an √≠ndices de validaci√≥n de clusters (silhouette, Dunn, etc.) en lugar de CV.",
  "La elecci√≥n de m√©trica de distancia (Euclidiana, Manhattan) y m√©todo de enlace (completo, promedio, single) afecta mucho los resultados."
)

tabla_hclust <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_hclust %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir HClust",
             subtitle = "Hierarchical Clustering (hclust)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## k-Means  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/k-means.png"))
```

**k-Means** es uno de los algoritmos de **agrupamiento (clustering) no supervisado** m√°s populares y ampliamente utilizados. Su objetivo es particionar un conjunto de $n$ observaciones en $k$ grupos o "clusters", donde cada observaci√≥n pertenece al cluster cuyo centroide (media) es el m√°s cercano.

El algoritmo k-Means opera de la siguiente manera:

1.  **Inicializaci√≥n:**
    * Se elige un n√∫mero predefinido de clusters, $k$. Este es un **hiperpar√°metro** que debe ser especificado por el usuario.
    * Se inicializan $k$ **centroides** (puntos centrales de los clusters). Esto se puede hacer de forma aleatoria seleccionando $k$ puntos de datos al azar como centroides iniciales, o utilizando m√©todos m√°s sofisticados como k-Means++.

2.  **Paso de Asignaci√≥n (Expectation / E-step):**
    * Para cada punto de datos en el conjunto, se calcula su distancia (com√∫nmente euclidiana) a cada uno de los $k$ centroides.
    * Cada punto de datos se **asigna al cluster cuyo centroide es el m√°s cercano**.

3.  **Paso de Actualizaci√≥n (Maximization / M-step):**
    * Para cada uno de los $k$ clusters, se **recalcula la posici√≥n del centroide** como la media (promedio) de todos los puntos de datos que han sido asignados a ese cluster.

4.  **Iteraci√≥n:**
    * Los pasos de Asignaci√≥n y Actualizaci√≥n se repiten iterativamente.
    * El algoritmo converge cuando las asignaciones de los puntos a los clusters ya no cambian, o cuando las posiciones de los centroides no cambian significativamente entre iteraciones.

El objetivo del algoritmo es minimizar la **suma de los cuadrados de las distancias** de cada punto a su centroide asignado (tambi√©n conocida como la inercia del cluster o la suma de cuadrados dentro del cluster - WCSS).

**Ventajas:** Es simple de implementar, computacionalmente eficiente y escalable para grandes conjuntos de datos.
**Limitaciones:** Requiere que el n√∫mero de clusters $k$ sea especificado de antemano, es sensible a la inicializaci√≥n de los centroides, y tiende a formar clusters esf√©ricos de tama√±o similar, lo que puede ser una desventaja si los clusters tienen formas arbitrarias o densidades muy diferentes. Tambi√©n es sensible a los outliers.


**Aprendizaje Global vs. Local:**

k-Means es un modelo de **aprendizaje global**.

* **Aspecto Global:** k-Means busca una **partici√≥n global de todo el conjunto de datos** en $k$ clusters. El objetivo de la optimizaci√≥n (minimizar la suma de los cuadrados de las distancias a los centroides) se calcula sobre **todos los puntos de datos** y todos los clusters simult√°neamente. Los centroides, una vez convergidos, representan los "centros" de los clusters en el espacio de caracter√≠sticas, y estos se utilizan para asignar cualquier nuevo punto a su cluster correspondiente. La soluci√≥n final es una asignaci√≥n de cada punto a un cluster que se aplica a nivel global.

* **Asignaciones Locales dentro de una Optimizaci√≥n Global:** Aunque en cada iteraci√≥n los puntos se asignan a su centroide "local" m√°s cercano, esta asignaci√≥n es parte de un proceso iterativo que busca optimizar un criterio global (la inercia total del cluster). Los centroides mismos son influenciados por todos los puntos asignados a su cluster, y la reubicaci√≥n de los centroides afecta las asignaciones de todos los puntos en la siguiente iteraci√≥n. El resultado son **fronteras de decisi√≥n lineales (hiperplanos)** entre los clusters (cuyas combinaciones pueden formar pol√≠gonos de Voronoi), que son una caracter√≠stica de un modelo global que divide el espacio. Si los datos no se distribuyen linealmente y los clusters tienen formas no esf√©ricas o densidades muy diferentes, k-Means puede tener dificultades para descubrirlos, precisamente por su naturaleza global de optimizaci√≥n de la distancia euclidiana a un centroide.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering por partici√≥n)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Variables num√©ricas (recomendado estandarizar)",
  "‚úÖ Agrupa observaciones seg√∫n distancia a centroides",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Sensible a valores at√≠picos y centroides iniciales",
  "‚ö†Ô∏è No afecta directamente (no hay predictores ni multicolinealidad)",
  "‚úÖ F√°cil interpretaci√≥n de clusters y centroides",
  "‚úÖ R√°pido y eficiente para datasets grandes",
  "‚ùå No se usa validaci√≥n cruzada, pero s√≠ √≠ndices de cluster (silhouette, etc.)",
  "‚ùå No funciona bien con clusters no esf√©ricos o tama√±os muy dispares"
)

detalles <- c(
  "M√©todo no supervisado que particiona datos en k clusters minimizando suma de cuadrados dentro de clusters.",
  "No busca predecir, sino encontrar grupos o clusters.",
  "Requiere variables num√©ricas; es com√∫n estandarizarlas para evitar sesgos por escala.",
  "Cada observaci√≥n se asigna al cluster con el centroide m√°s cercano (distancia Euclidiana).",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia.",
  "No requiere homoscedasticidad.",
  "Los outliers pueden mover centroides y distorsionar clusters.",
  "Como t√©cnica de agrupamiento, no hay multicolinealidad entre variables.",
  "Centroides y clusters son f√°ciles de interpretar y visualizar.",
  "Algoritmo r√°pido, converge r√°pido en general.",
  "Se usan √≠ndices externos e internos para evaluar calidad del clustering, no validaci√≥n cruzada.",
  "No funciona bien si los clusters tienen formas complejas, tama√±os muy distintos o solapamientos."
)

tabla_kmeans <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_kmeans %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir k-means",
             subtitle = "K - Means")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## k-Medians  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/k-medians.png"))
```

**k-Medians** es un algoritmo de **agrupamiento (clustering) no supervisado** que es una variante de **k-Means**. Al igual que k-Means, su objetivo es particionar un conjunto de $n$ observaciones en $k$ grupos o "clusters". La principal diferencia radica en c√≥mo se calcula el "centro" de cada cluster y la m√©trica de distancia utilizada en su funci√≥n de costo.

Mientras que k-Means utiliza la **media (mean)** de los puntos de un cluster como su centroide y minimiza la suma de los **cuadrados de las distancias euclidianas** (norma L2), k-Medians utiliza la **mediana (median)** de los puntos de un cluster como su "centro" y minimiza la **suma de las distancias absolutas** (norma L1 o distancia de Manhattan).

El algoritmo k-Medians opera de manera muy similar a k-Means:

1.  **Inicializaci√≥n:**
    * Se elige un n√∫mero predefinido de clusters, $k$.
    * Se inicializan $k$ **medianas** (puntos centrales de los clusters), a menudo de forma aleatoria.

2.  **Paso de Asignaci√≥n:**
    * Para cada punto de datos en el conjunto, se calcula su **distancia de Manhattan (L1)** a cada una de las $k$ medianas.
    * Cada punto de datos se **asigna al cluster cuya mediana es la m√°s cercana**.

3.  **Paso de Actualizaci√≥n:**
    * Para cada uno de los $k$ clusters, se **recalcula la posici√≥n de la mediana** como la mediana multivariada (componente por componente) de todos los puntos de datos que han sido asignados a ese cluster.

4.  **Iteraci√≥n:**
    * Los pasos de Asignaci√≥n y Actualizaci√≥n se repiten iterativamente.
    * El algoritmo converge cuando las asignaciones de los puntos a los clusters ya no cambian, o cuando las posiciones de las medianas no cambian significativamente.

**Ventajas clave de k-Medians sobre k-Means:**

* **Robustez a Outliers:** Al usar la mediana en lugar de la media, k-Medians es significativamente **m√°s robusto a los valores at√≠picos (outliers)**. Los outliers influyen fuertemente en la media (tirando de ella), pero tienen un impacto mucho menor en la mediana.
* **M√©trica de Distancia:** La distancia L1 es a veces m√°s apropiada que la L2 cuando las diferencias entre las caracter√≠sticas son m√°s importantes que sus valores al cuadrado, o cuando los datos no son necesariamente continuos o gaussianos.

**Limitaciones:**
* Requiere que el n√∫mero de clusters $k$ sea especificado de antemano.
* La mediana multivariada puede ser m√°s compleja de calcular que la media.
* Puede ser m√°s lento que k-Means en algunos escenarios.

**Aprendizaje Global vs. Local:**

Al igual que k-Means, **k-Medians es un modelo de aprendizaje global**.

* **Aspecto Global:** k-Medians busca una **partici√≥n global de todo el conjunto de datos** en $k$ clusters. El objetivo de la optimizaci√≥n (minimizar la suma de las distancias L1 a las medianas) se calcula sobre **todos los puntos de datos** y todos los clusters simult√°neamente. Las medianas, una vez convergidas, representan los "centros" robustos de los clusters en el espacio de caracter√≠sticas, y estos se utilizan para asignar cualquier nuevo punto a su cluster correspondiente. La soluci√≥n final es una asignaci√≥n de cada punto a un cluster que se aplica a nivel global.

* **Asignaciones Locales dentro de una Optimizaci√≥n Global:** Si bien en cada iteraci√≥n los puntos se asignan a su mediana "local" m√°s cercana, esta asignaci√≥n es parte de un proceso iterativo que busca optimizar un criterio global (la suma total de distancias L1). Las medianas mismas son influenciadas por todos los puntos asignados a su cluster, y la reubicaci√≥n de las medianas afecta las asignaciones de todos los puntos en la siguiente iteraci√≥n. El resultado son **fronteras de decisi√≥n lineales** (debido al uso de la distancia L1, similar a las fronteras de Voronoi), que son una caracter√≠stica de un modelo global que divide el espacio. Aunque es m√°s robusto a outliers, k-Medians todav√≠a tiende a encontrar clusters que son m√°s o menos "esf√©ricos" o convexos en la m√©trica L1, y puede tener dificultades con clusters de formas muy arbitrarias o no lineales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering por partici√≥n)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Variables num√©ricas (recomendado estandarizar)",
  "‚úÖ Agrupa observaciones seg√∫n distancia a la mediana (Manhattan)",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚úÖ M√°s robusto a valores at√≠picos que k-Means",
  "‚ö†Ô∏è No afecta directamente (no hay predictores ni multicolinealidad)",
  "‚úÖ Interpretaci√≥n clara de clusters y medianas",
  "‚ö†Ô∏è Algo m√°s lento que k-Means por c√°lculo de medianas",
  "‚ùå No se usa validaci√≥n cruzada, pero s√≠ √≠ndices de cluster (silhouette, etc.)",
  "‚ùå No funciona bien con clusters no esf√©ricos o tama√±os muy dispares"
)

detalles <- c(
  "M√©todo no supervisado que particiona datos en k clusters minimizando suma de distancias absolutas dentro de clusters.",
  "No busca predecir, sino encontrar grupos o clusters.",
  "Requiere variables num√©ricas; se recomienda estandarizaci√≥n para evitar sesgo por escala.",
  "Cada observaci√≥n se asigna al cluster con la mediana m√°s cercana usando distancia Manhattan.",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia.",
  "No requiere homoscedasticidad.",
  "M√°s robusto frente a outliers porque usa medianas en lugar de medias.",
  "Como t√©cnica de agrupamiento, no hay multicolinealidad entre variables.",
  "Medianas y clusters son f√°ciles de interpretar y visualizar.",
  "Computacionalmente puede ser un poco m√°s lento que k-Means.",
  "Se usan √≠ndices externos e internos para evaluar calidad del clustering, no validaci√≥n cruzada.",
  "No funciona bien si los clusters tienen formas complejas, tama√±os muy distintos o solapamientos."
)

tabla_kmedians <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_kmedians %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir k-medians",
             subtitle = "K - Medians")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Mean-Shift {-}   


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/Mean-Shift.png"))
```

## Ordering Points To Identify the Clustering Structure (OPTICS) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/OPTICS.png"))
```

## Spectral Clustering {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/Spectral Clustering.png"))
```


<!--chapter:end:09-clustering.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üìê 10. Sistemas Basados en Reglas {-}  

**Ejemplos:** RuleFit, √Årboles de Decisi√≥n con reglas, l√≥gica difusa.  
**Uso:** Estos sistemas son ideales cuando la **interpretabilidad es absolutamente crucial**, como en decisiones legales o m√©dicas donde entender el "porqu√©" es tan importante como el "qu√©". Tambi√©n son perfectos para **incorporar conocimiento experto** humano directamente en el modelo.  
**Ventajas:** Son notablemente **f√°ciles de entender y auditar**, lo que los hace transparentes y confiables en entornos regulados.  
**Limitaciones:** Su principal desventaja es que no suelen ser tan **precisos** como otros m√©todos m√°s complejos cuando se enfrentan a datos muy intrincados o patrones no lineales.  

---


## Associative Classification (e.g., CBA, CMAR, FP-Growth based methods) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Associative Classification.png"))
```

## CN2 Algorithm {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/CN2.png"))
```

## Decision List/Decision Tree to Rules {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Decision Tree to Rules.png"))
```

## Decision Rules  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Decision Rules.png"))
```

Las **Reglas de Decisi√≥n** no son un algoritmo de Machine Learning en s√≠ mismas, sino una **forma de representar un modelo predictivo** que es altamente **interpretable y f√°cil de entender**. Son una de las formas m√°s intuitivas de expresar el conocimiento extra√≠do de los datos. Una regla de decisi√≥n t√≠picamente toma la forma de una declaraci√≥n **"SI-ENTONCES" (IF-THEN)**, donde la parte "SI" (antecedente) describe las condiciones que deben cumplirse y la parte "ENTONCES" (consecuente) especifica la predicci√≥n o la acci√≥n a tomar.

**Estructura de una Regla de Decisi√≥n:**

Una regla de decisi√≥n b√°sica tiene la siguiente forma:

**SI** *condici√≥n 1* **Y** *condici√≥n 2* **Y** ... **Y** *condici√≥n N* **ENTONCES** *predicci√≥n / clase / valor*

**Ejemplos:**

* **Para Clasificaci√≥n:**
    * **SI** `Edad` > 30 **Y** `Ingreso` < 50,000 **ENTONCES** `Clase: Riesgo Bajo`
    * **SI** `Clima` = Soleado **Y** `Humedad` > 70% **ENTONCES** `Acci√≥n: No Jugar Tenis`

* **Para Regresi√≥n:**
    * **SI** `Tama√±o_Casa` > 150 m¬≤ **Y** `Num_Habitaciones` = 3 **ENTONCES** `Precio_Estimado: $300,000`

**Caracter√≠sticas Clave:**   

* **Interpretabilidad:** Son inherentemente f√°ciles de entender por los humanos, incluso por aquellos sin conocimientos t√©cnicos profundos.
* **Modularidad:** Un modelo completo puede estar compuesto por un conjunto de reglas. Cada regla es independiente y f√°cil de examinar.
* **No Linealidad:** Aunque cada regla es una declaraci√≥n l√≥gica simple, un conjunto de reglas puede modelar relaciones no lineales complejas en los datos, ya que cada regla cubre una regi√≥n diferente del espacio de caracter√≠sticas.
* **Selecci√≥n de Caracter√≠sticas Impl√≠cita:** Al construir reglas, solo las caracter√≠sticas relevantes para la condici√≥n se incluyen, lo que realiza una selecci√≥n impl√≠cita de caracter√≠sticas.
* **Robustez:** A menudo son bastante robustas a los outliers y a los datos ruidosos si las reglas se derivan y podan correctamente.

**Algoritmos que Generan Reglas de Decisi√≥n:** 

Si bien las reglas de decisi√≥n son la forma de representaci√≥n, varios algoritmos de Machine Learning se especializan en aprender estas reglas a partir de los datos:

* **√Årboles de Decisi√≥n:** Cada ruta desde la ra√≠z hasta una hoja en un √°rbol de decisi√≥n se puede traducir directamente en una regla de decisi√≥n.
* **Algoritmos Basados en Reglas:**
    * **OneR:** Genera una √∫nica regla basada en el atributo m√°s predictivo.
    * **RIPPER:** Produce conjuntos de reglas optimizados para la clasificaci√≥n, con √©nfasis en la poda y la simplicidad.
    * **RuleFit:** Combina reglas extra√≠das de ensamblajes de √°rboles con caracter√≠sticas originales en un modelo lineal regularizado.
* **Sistemas de L√≥gica Difusa:** Utilizan reglas difusas para manejar la incertidumbre y la imprecisi√≥n.


**Aprendizaje Global vs. Local:**

Los modelos basados en **Reglas de Decisi√≥n** combinan de manera muy efectiva **aspectos de aprendizaje global y local**.

* **Aspecto Local (Cada Regla Individual):**
    * Cada regla de decisi√≥n es intr√≠nsecamente **local**. Define una **regi√≥n espec√≠fica** (un subespacio, a menudo un hiperrect√°ngulo) en el espacio de caracter√≠sticas. Dentro de esta regi√≥n, la regla hace una predicci√≥n particular. Por ejemplo, la regla "SI `Edad` > 30 Y `Ingreso` < 50,000" solo se aplica a un subconjunto espec√≠fico de instancias.
    * La naturaleza "local" de cada regla permite al modelo adaptarse a **relaciones no lineales y complejas**. Diferentes reglas pueden activarse en distintas partes del espacio de datos, capturando patrones que var√≠an significativamente de una regi√≥n a otra, similar a una **"regresi√≥n (o clasificaci√≥n) ponderada localmente"** donde cada regla representa un modelo simple para su regi√≥n.

* **Aspecto Global (El Conjunto de Reglas):**
    * Aunque las reglas individuales son locales, el **conjunto completo de reglas** que conforma el modelo se aplica para cubrir **todo el espacio de caracter√≠sticas relevante**. Este conjunto de reglas forma un **modelo predictivo global** que puede clasificar o predecir un valor para cualquier nueva instancia.
    * Los algoritmos que generan estos conjuntos de reglas (como RIPPER) a menudo optimizan la colecci√≥n de reglas para un rendimiento global, buscando un equilibrio entre la precisi√≥n y la complejidad del modelo en su conjunto.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n o regresi√≥n)",
  "‚úÖ Num√©rica o categ√≥rica",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Basado en reglas (if-then) que segmentan el espacio predictor",
  "‚ùå No aplica directamente (no es un modelo param√©trico)",
  "‚ö†Ô∏è Asume independencia de observaciones",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente sensible, outliers pueden crear reglas poco √∫tiles",
  "‚ö†Ô∏è No afecta directamente, pero reglas redundantes pueden complicar modelo",
  "‚úÖ Muy interpretable, reglas claras y simples",
  "‚úÖ R√°pido para datasets peque√±os y medianos",
  "‚úÖ Compatible con validaci√≥n cruzada",
  "‚ùå Puede sobreajustar si hay mucho ruido o datos muy complejos"
)

detalles <- c(
  "Modelos que usan reglas l√≥gicas para realizar predicciones, f√°cilmente entendibles.",
  "Permite predecir tanto clases (clasificaci√≥n) como valores num√©ricos (regresi√≥n).",
  "Acepta tanto variables categ√≥ricas como num√©ricas como predictores.",
  "Segmenta el espacio en regiones mediante reglas que dividen las variables predictoras.",
  "No genera residuos ni supone distribuci√≥n de error como modelos param√©tricos.",
  "Las reglas asumen que las observaciones son independientes.",
  "No aplica homoscedasticidad porque no es un modelo estad√≠stico cl√°sico.",
  "Outliers pueden influir en la generaci√≥n de reglas, creando reglas poco generalizables.",
  "La multicolinealidad no afecta directamente, pero puede generar reglas redundantes.",
  "La fortaleza es la interpretabilidad clara y sencilla de las reglas obtenidas.",
  "Generalmente eficiente, pero depende del n√∫mero de reglas y complejidad del dataset.",
  "Se recomienda validar con m√©todos como k-fold para evitar sobreajuste.",
  "No es ideal para datasets con mucho ruido o cuando la relaci√≥n es muy compleja o sutil."
)

tabla_rules <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rules %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir decision rules",
             subtitle = "Decision Rules")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## L√≥gica Difusa (Fuzzy Logic) {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Fuzzy Logic.png"))
```

La **L√≥gica Difusa (Fuzzy Logic)** no es un algoritmo de Machine Learning en s√≠ mismo, sino un **paradigma de computaci√≥n basado en la noci√≥n de "grado de verdad"** en lugar de los valores binarios "verdadero o falso" (1 o 0) de la l√≥gica booleana cl√°sica. Permite modelar el razonamiento humano, que a menudo involucra informaci√≥n imprecisa, ambigua o vaga. Es un marco para representar y manipular el conocimiento que es inherentemente incierto o subjetivo.

La idea central de la l√≥gica difusa es que un elemento puede **pertenecer a un conjunto en un cierto grado** (entre 0 y 1), en lugar de pertenecer completamente o no pertenecer en absoluto. Por ejemplo, una persona puede ser "alta" en un grado de 0.8 y "mediana" en un grado de 0.2, en lugar de ser estrictamente alta o estrictamente mediana.

Los componentes clave de un sistema de l√≥gica difusa suelen incluir:

1.  **Conjuntos Difusos (Fuzzy Sets):** Definen el grado de pertenencia de un elemento a una categor√≠a. Por ejemplo, un conjunto difuso para "temperatura alta" podr√≠a tener una funci√≥n de pertenencia que asigne un valor de 0 a 10 grados, 0.5 a 20 grados, y 1 a 30 grados.
2.  **Variables Ling√º√≠sticas:** Son variables cuyos valores son palabras o oraciones del lenguaje natural (ej., "temperatura", cuyos valores pueden ser "fr√≠o", "tibio", "caliente").
3.  **Funciones de Pertenencia (Membership Functions):** Gr√°ficos que definen matem√°ticamente el grado de pertenencia de un elemento a un conjunto difuso.
4.  **Reglas Difusas (Fuzzy Rules):** Reglas de tipo "SI-ENTONCES" que utilizan variables ling√º√≠sticas y conjuntos difusos. Por ejemplo: "SI la temperatura es *caliente* Y la humedad es *alta* ENTONCES la velocidad del ventilador es *r√°pida*".
5.  **Fuzificaci√≥n:** Proceso de convertir valores de entrada n√≠tidos (crisp inputs) en grados de pertenencia a conjuntos difusos.
6.  **Motor de Inferencia:** Aplica las reglas difusas para producir una salida difusa.
7.  **Defuzificaci√≥n:** Proceso de convertir la salida difusa en un valor de salida n√≠tido (crisp output) que pueda ser utilizado en el mundo real.

La l√≥gica difusa es ampliamente utilizada en sistemas de control (ej., lavadoras, sistemas de frenos ABS, c√°maras de video), sistemas expertos y en el procesamiento de informaci√≥n imprecisa.

**Aprendizaje Global vs. Local:**

La L√≥gica Difusa, como paradigma, tiene la capacidad de integrar aspectos de **modelado global y local**, dependiendo de c√≥mo se implemente y se "entrene" (o sintonice).

* **Aspecto Local (Granularidad de las Reglas y Conjuntos Difusos):**
    * Las **reglas difusas** operan sobre condiciones locales (ej., "SI la temperatura es *caliente*"). Cada regla cubre una porci√≥n espec√≠fica del espacio de entrada/salida. Los **conjuntos difusos** y sus funciones de pertenencia particionan el espacio de caracter√≠sticas en regiones "borrosas" o traslapadas, lo que permite que el modelo se adapte a las caracter√≠sticas de los datos en vecindarios espec√≠ficos. Es decir, la respuesta del sistema se construye a partir de la activaci√≥n ponderada de varias reglas locales, cada una representando un comportamiento en una regi√≥n del espacio de entrada. Esto es muy similar a una **"regresi√≥n ponderada localmente"**, donde la contribuci√≥n de cada regla (o modelo impl√≠cito) se pondera por el grado en que la entrada actual pertenece a la regi√≥n de esa regla. Esta granularidad y superposici√≥n le permiten manejar **relaciones no lineales y complejas** al aproximarlas con una combinaci√≥n de estas contribuciones locales.

* **Aspecto Global (Coherencia del Sistema y Cobertura):**
    * Aunque las reglas son locales, un sistema de l√≥gica difusa bien dise√±ado cubre todo el espacio de entrada relevante y proporciona una **respuesta global coherente**. El conjunto de todas las reglas y funciones de pertenencia, junto con el motor de inferencia, forma un **sistema global** que puede mapear cualquier entrada a una salida. La defuzificaci√≥n final produce un valor n√≠tido que es el resultado de la combinaci√≥n de todas las activaciones de las reglas.

* **Aprendizaje y Sintonizaci√≥n:** Cuando se combinan con t√©cnicas de Machine Learning (como redes neuronales o algoritmos gen√©ticos), los sistemas difusos pueden "aprender" o "sintonizar" sus funciones de pertenencia y reglas. Este proceso de aprendizaje puede optimizar el rendimiento del sistema a nivel global (minimizando un error general), pero los ajustes siguen afectando las propiedades locales de las reglas y conjuntos difusos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado o no supervisado (depende del sistema)",
  "‚úÖ Variables de salida pueden ser continuas o categ√≥ricas (fuzzy sets)",
  "‚úÖ Variables num√©ricas o categ√≥ricas, modeladas como grados de pertenencia",
  "‚úÖ Modela relaciones imprecisas, no binarias, usando l√≥gica difusa",
  "‚ùå No aplica (no modelo estad√≠stico tradicional)",
  "‚ö†Ô∏è Puede asumir independencia pero depende del dise√±o del sistema",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente sensible, depende de la funci√≥n de membres√≠a",
  "‚ö†Ô∏è No afecta directamente pero variables correlacionadas pueden complicar reglas",
  "‚úÖ Interpretaci√≥n basada en reglas ling√º√≠sticas y grados de verdad",
  "‚ö†Ô∏è Puede ser lento con muchos conjuntos difusos y reglas complejas",
  "‚ö†Ô∏è Validaci√≥n cruzada posible, pero no est√°ndar en l√≥gica difusa",
  "‚ùå No es adecuado si los datos son muy exactos y no presentan incertidumbre"
)

detalles <- c(
  "Sistemas que usan l√≥gica difusa para manejar incertidumbre y aproximaci√≥n en los datos.",
  "Las variables respuesta pueden ser valores continuos o categor√≠as definidas mediante conjuntos difusos.",
  "Los predictores se transforman en grados de pertenencia a conjuntos difusos para evaluar reglas.",
  "Permite modelar relaciones vagamente definidas, imprecisas o con fronteras difusas.",
  "No se basa en supuestos estad√≠sticos cl√°sicos, por eso no aplica normalidad.",
  "La independencia depende de c√≥mo se dise√±en las reglas y sistemas difusos.",
  "No es un modelo param√©trico cl√°sico, por eso homoscedasticidad no aplica.",
  "Outliers pueden afectar las funciones de pertenencia y la l√≥gica aplicada.",
  "Variables altamente correlacionadas pueden hacer que las reglas sean redundantes o complejas.",
  "Los resultados se interpretan mediante reglas tipo 'Si... entonces...' con grados de verdad.",
  "La complejidad crece con n√∫mero de variables y reglas, afectando velocidad.",
  "Validar es menos est√°ndar, se usan m√©todos espec√≠ficos seg√∫n la aplicaci√≥n.",
  "No es √∫til para datos precisos donde no existe incertidumbre o imprecisi√≥n."
)

tabla_fuzzy <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_fuzzy %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir decision fuzzy rules",
             subtitle = "Fuzzy Rules")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Minsky's Perceptron {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Minsky's Perceptron.png"))
```

## One Rule (OneR)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/OneR.png"))
```


**One Rule (OneR)** es un algoritmo de **clasificaci√≥n supervisada** notable por su **simplicidad y alta interpretabilidad**. Fue propuesto por Robert Holte en 1993 y, a pesar de su sencillez, a menudo logra una precisi√≥n sorprendentemente buena en comparaci√≥n con algoritmos mucho m√°s complejos, sirviendo como una excelente l√≠nea base (benchmark) para el rendimiento del modelo.

La idea central de OneR es construir un clasificador que se base en **una √∫nica regla** para tomar decisiones. Esta regla se deriva de **un solo atributo (caracter√≠stica)** del conjunto de datos que es el m√°s predictivo de la clase de salida.

El funcionamiento del algoritmo OneR es el siguiente:

1.  **Iterar a Trav√©s de Cada Atributo:** Para cada atributo en el conjunto de datos de entrenamiento (se asume que los atributos son categ√≥ricos; si son continuos, primero deben discretizarse):
    * **Crear una Regla para Cada Valor:** Para cada valor √∫nico que puede tomar ese atributo, se construye una regla.
    * **Encontrar la Clase M√°s Frecuente:** Para cada una de estas reglas, se cuenta cu√°ntas veces aparece cada clase de destino cuando el atributo toma ese valor. La clase que ocurre con mayor frecuencia se convierte en la predicci√≥n para esa regla.
    * **Calcular el Error de la Regla:** Se calcula el n√∫mero de errores que comete esta regla (es decir, el n√∫mero de instancias para las cuales la clase predicha no coincide con la clase real).
2.  **Seleccionar el Mejor Atributo:** Una vez que se han generado reglas y calculado los errores para *todos* los atributos, OneR selecciona el atributo (y su conjunto de reglas asociadas) que tiene el **menor error total**. Si hay un empate entre varios atributos, se puede elegir el primero o usar un criterio secundario (como el test de chi-cuadrado).
3.  **El Modelo Final:** El conjunto de reglas derivado de este atributo seleccionado se convierte en el modelo final de clasificaci√≥n.

Por ejemplo, si tenemos un atributo "Clima" con valores "Soleado", "Nublado", "Lluvioso" y una clase "Jugar al Golf" (S√≠/No):
* Si Clima = Soleado: La mayor√≠a juega golf (S√≠). Error: 2 (de 5)
* Si Clima = Nublado: La mayor√≠a juega golf (S√≠). Error: 0 (de 4)
* Si Clima = Lluvioso: La mayor√≠a NO juega golf (No). Error: 1 (de 5)
* Error total para "Clima" = 2 + 0 + 1 = 3.

Este proceso se repetir√≠a para otros atributos como "Temperatura", "Humedad", etc., y el atributo con el menor error total ser√≠a el elegido.

**Aprendizaje Global vs. Local:**

One Rule (OneR) es un modelo de **aprendizaje fundamentalmente global**, aunque la regla que aprende implica una partici√≥n del espacio de caracter√≠sticas.

* **Aspecto Global:** OneR eval√∫a **todos los atributos en su totalidad** y selecciona el **√∫nico atributo que es globalmente el m√°s predictivo** para la tarea de clasificaci√≥n sobre todo el conjunto de datos. La regla elegida y sus condiciones se aplican uniformemente a cualquier nueva instancia en el espacio de caracter√≠sticas. No se construyen modelos separados o locales para diferentes regiones del espacio de caracter√≠sticas. El algoritmo busca la mejor regla √∫nica que resuma el patr√≥n m√°s fuerte en todos los datos.

* **Partici√≥n del Espacio (Reglas):** Aunque el modelo es global, la "regla" que genera s√≠ que particiona el espacio de caracter√≠sticas. Por ejemplo, si el atributo seleccionado es "Color" y tiene valores "Rojo", "Azul", "Verde", el modelo crea una regla para cada uno de estos valores. Esto crea "regiones" en el espacio de datos (instancias donde Color=Rojo, donde Color=Azul, etc.). Sin embargo, la predicci√≥n dentro de cada una de estas regiones es simplemente la clase mayoritaria observada en esa regi√≥n, y el modelo en su conjunto es una √∫nica estructura de decisi√≥n global basada en ese √∫nico atributo. No es un ajuste din√°mico o ponderado localmente de par√°metros como en otros modelos de aprendizaje local.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Variable respuesta categ√≥rica (clases)",
  "‚úÖ Variables predictoras num√©ricas o categ√≥ricas",
  "‚úÖ Relaci√≥n simple basada en una sola regla (una variable predictora)",
  "‚ùå No aplica (no es modelo estad√≠stico param√©trico)",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede ser sensible a outliers si afectan la regla",
  "‚ö†Ô∏è No afecta directamente (usa solo una variable para la regla)",
  "‚úÖ Muy interpretable: una regla sencilla basada en un solo predictor",
  "‚úÖ Muy r√°pido y eficiente, especialmente en datasets peque√±os o medianos",
  "‚úÖ Se puede usar validaci√≥n cruzada para evaluar rendimiento",
  "‚ùå No funciona bien si las relaciones son complejas y requieren m√∫ltiples variables"
)

detalles <- c(
  "Modelo supervisado para clasificaci√≥n basado en la regla m√°s simple y efectiva de un solo predictor.",
  "Predice la clase de salida bas√°ndose en el valor de una √∫nica variable predictora.",
  "Acepta variables categ√≥ricas o num√©ricas (estas se discretizan para generar reglas).",
  "Genera una regla simple: 'Si predictor X tiene valor Y, entonces clase Z'.",
  "No es un modelo param√©trico ni estad√≠stico tradicional, no eval√∫a residuos.",
  "No considera errores ni supuestos de independencia.",
  "No aplica el supuesto de homoscedasticidad.",
  "Outliers pueden influir si cambian la regla seleccionada.",
  "Como usa solo un predictor, la multicolinealidad no es un problema.",
  "F√°cil de entender y explicar, ideal para explicar decisiones simples.",
  "Muy r√°pido de entrenar y evaluar, √∫til para benchmarks o como baseline.",
  "Se recomienda evaluar mediante validaci√≥n cruzada para evitar sobreajuste.",
  "No es √∫til para problemas que requieren modelar interacciones complejas entre variables."
)

tabla_oner <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_oner %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir decision OneR",
             subtitle = "One Rule (OneR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Repeated Incremental Pruning to Produce Error Reduction (RIPPER)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/RIPPER.png"))
```

**RIPPER (Repeated Incremental Pruning to Produce Error Reduction)** es un algoritmo de **clasificaci√≥n supervisada** muy conocido, desarrollado por William W. Cohen. Es una extensi√≥n del algoritmo de reglas `IREP` y es especialmente valorado por su capacidad para generar **conjuntos de reglas de clasificaci√≥n precisos y de alta calidad** que son a menudo m√°s simples e interpretables que los modelos de √°rbol de decisi√≥n complejos, al tiempo que es muy eficiente en t√©rminos computacionales, incluso con grandes conjuntos de datos.

RIPPER construye un conjunto de reglas `IF-THEN` para cada clase de forma secuencial. Opera con un enfoque de **"divide y vencer√°s"**, pero con un fuerte √©nfasis en la **poda (pruning)** para evitar el sobreajuste.

El proceso general de RIPPER para construir reglas para una clase espec√≠fica es el siguiente:

1.  **Generaci√≥n de Reglas (Growing):**
    * Comienza con una regla vac√≠a.
    * A√±ade t√©rminos (condiciones) a la regla que maximicen alguna m√©trica de calidad (por ejemplo, el ratio de ganancia de informaci√≥n) hasta que la regla cubre un cierto n√∫mero de ejemplos positivos (ejemplos de la clase actual) y pocos ejemplos negativos.
2.  **Poda de Reglas (Pruning):**
    * Una vez que una regla ha sido generada, se somete a un proceso de poda incremental. Se eliminan t√©rminos de la regla que no mejoran significativamente el error de la regla en un conjunto de validaci√≥n separado (el "conjunto de poda"). Esto ayuda a generalizar la regla y evitar el sobreajuste.
3.  **Adici√≥n de Reglas:**
    * Despu√©s de podar una regla, se a√±ade al conjunto de reglas para la clase actual.
    * Los ejemplos cubiertos por esta nueva regla se eliminan del conjunto de entrenamiento.
    * Se repiten los pasos 1-3 para construir nuevas reglas para la misma clase hasta que no queden suficientes ejemplos de la clase o no se puedan generar m√°s reglas que cumplan ciertos criterios.
4.  **Optimizaci√≥n Global y Re-poda:**
    * Una vez que se ha generado un conjunto inicial de reglas para una clase, RIPPER realiza varias pasadas de optimizaci√≥n.
    * En cada pasada, se intenta reemplazar o modificar reglas para reducir a√∫n m√°s el error total del conjunto de reglas. Esto incluye estrategias como la sustituci√≥n de reglas (reemplazar una regla existente por una mejor), la reversi√≥n de reglas (eliminar una regla), y la combinaci√≥n de reglas.
    * Se utiliza una m√©trica como la **descripci√≥n m√≠nima de la longitud (MDL - Minimum Description Length)** para penalizar la complejidad del modelo.

El proceso se repite para todas las clases, y las reglas se organizan en un orden de prioridad.


**Aprendizaje Global vs. Local:**

RIPPER es un algoritmo que combina de manera muy efectiva aspectos de **aprendizaje global y local**, pero con un fuerte √©nfasis en la generaci√≥n de **reglas locales** que se combinan en un modelo global.

* **Aspecto Local (Generaci√≥n de Reglas Individuales):** Cada regla que RIPPER aprende es esencialmente un **modelo local** que cubre una regi√≥n espec√≠fica del espacio de caracter√≠sticas. Las condiciones de la regla (`IF A AND B THEN Class X`) definen un hiperrect√°ngulo (o una regi√≥n m√°s compleja) en el espacio de caracter√≠sticas. La regla se aprende para predecir correctamente los puntos dentro de esa regi√≥n. La poda de reglas, en particular, se enfoca en optimizar el rendimiento de la regla en su "vecindario" de datos cubiertos, evitando el sobreajuste a puntos de entrenamiento individuales. Las reglas se ajustan a patrones y relaciones **locales** dentro de los datos.

* **Aspecto Global (Conjunto de Reglas y Priorizaci√≥n):** Aunque las reglas individuales son locales, el **conjunto final de reglas** para todas las clases, y su orden de prioridad, forman un **modelo de clasificaci√≥n global** que cubre todo el espacio de caracter√≠sticas. Cuando una nueva instancia necesita ser clasificada, se eval√∫a contra todas las reglas en orden hasta que una se activa, y esa regla dicta la predicci√≥n. La optimizaci√≥n global del conjunto de reglas, mediante la repoda y las fases de reemplazo de reglas, asegura que el modelo final sea coherente y preciso a nivel de todo el conjunto de datos.    


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Variable respuesta categ√≥rica",
  "‚úÖ Variables predictoras num√©ricas o categ√≥ricas",
  "‚úÖ Relaci√≥n no lineal basada en reglas de decisi√≥n",
  "‚ùå No aplica (no es modelo param√©trico)",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente sensible a outliers, depende de la calidad de datos",
  "‚ö†Ô∏è No afecta directamente, pero multicolinealidad puede confundir reglas",
  "‚ö†Ô∏è Interpretaci√≥n mediante reglas, m√°s complejas que OneR pero a√∫n legibles",
  "‚ö†Ô∏è Moderada, puede ser lento en datasets muy grandes",
  "‚úÖ Se puede validar con m√©todos de validaci√≥n cruzada",
  "‚ùå No funciona bien si las reglas no separan claramente las clases o en datos muy ruidosos"
)

detalles <- c(
  "Algoritmo supervisado para clasificaci√≥n que genera reglas de decisi√≥n con poda incremental para reducir error.",
  "Predice clases usando conjuntos de reglas de decisi√≥n extra√≠das y podadas iterativamente.",
  "Soporta variables num√©ricas y categ√≥ricas, con discretizaci√≥n interna si es necesario.",
  "Captura relaciones no lineales y complejas mediante reglas combinadas.",
  "No utiliza modelos estad√≠sticos cl√°sicos, por lo que no aplica an√°lisis de residuos.",
  "No asume independencia entre observaciones.",
  "No requiere homoscedasticidad ni otros supuestos de regresi√≥n.",
  "Outliers pueden afectar la calidad y la complejidad de las reglas generadas.",
  "Multicolinealidad no afecta directamente, pero puede inducir reglas redundantes.",
  "Las reglas generadas son m√°s interpretables que √°rboles complejos, pero menos simples que OneR.",
  "El proceso iterativo y poda pueden ser computacionalmente costosos en grandes datasets.",
  "Validaci√≥n cruzada es recomendada para evaluar generalizaci√≥n y evitar sobreajuste.",
  "No es adecuado para datos con ruido elevado o donde no existen reglas claras para separar clases."
)

tabla_ripper <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_ripper %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir decision RIPPER",
             subtitle = "Repeated Incremental Pruning to Produce Error Reduction (RIPPER)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Rule Fit  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Rule Fit.png"))
```

**RuleFit** es un potente y, al mismo tiempo, **interpretable algoritmo de aprendizaje autom√°tico** desarrollado por Jerome H. Friedman y Bogdan Popescu. Est√° dise√±ado para combinar la **precisi√≥n de los m√©todos de ensamblaje (como el *gradient boosting* o los *random forests*)** con la **interpretabilidad de los modelos lineales y las reglas de decisi√≥n**. RuleFit es particularmente √∫til cuando necesita un modelo que funcione bien *y* que le permita comprender el "porqu√©" detr√°s de sus predicciones.

La idea central de RuleFit es aprender un **modelo lineal disperso** que utiliza tanto las **caracter√≠sticas de entrada originales** como un conjunto de **"caracter√≠sticas de regla" reci√©n generadas** como predictores. Estas caracter√≠sticas de regla se derivan de un ensamblaje de √°rboles de decisi√≥n.

As√≠ es como funciona RuleFit:

1.  **Generaci√≥n del Ensamblaje de √Årboles:**
    * Primero, RuleFit entrena un **ensamblaje de √°rboles de decisi√≥n poco profundos** en el conjunto de datos. Esto se puede hacer utilizando algoritmos como *Gradient Boosting Machines* (GBM) o *Random Forests*. Los √°rboles suelen mantenerse poco profundos (por ejemplo, una profundidad m√°xima de 3-5) para producir reglas m√°s simples e interpretables.
    * Estos √°rboles se entrenan para predecir la variable objetivo, lo que significa que sus divisiones son significativas para la tarea.

2.  **Extracci√≥n de Reglas:**
    * Cada **ruta desde la ra√≠z hasta un nodo hoja** en cualquiera de los √°rboles de decisi√≥n generados se extrae y se convierte en una **regla de decisi√≥n binaria**.
    * Por ejemplo, si una ruta en un √°rbol es "si (Caracter√≠stica1 > 10) Y (Caracter√≠stica2 < 5)", esto se convierte en una regla.
    * Cada regla se trata entonces como una **nueva caracter√≠stica binaria** para cada instancia de datos: toma un valor de 1 si la instancia satisface todas las condiciones de la regla, y 0 en caso contrario.

3.  **Ajuste del Modelo Lineal con Regularizaci√≥n:**
    * Las caracter√≠sticas originales del conjunto de datos se combinan con estas caracter√≠sticas de regla binarias reci√©n creadas.
    * Luego, se ajusta un **modelo lineal disperso** (normalmente una **regresi√≥n Lasso**, que utiliza regularizaci√≥n L1) a este conjunto de caracter√≠sticas expandido.
    * La regularizaci√≥n Lasso realiza autom√°ticamente la **selecci√≥n de caracter√≠sticas**, estableciendo los coeficientes de las caracter√≠sticas originales y de las caracter√≠sticas de regla menos importantes en cero. Esto da como resultado un modelo final m√°s simple e interpretable que solo incluye los t√©rminos m√°s relevantes.

El modelo RuleFit final es una ecuaci√≥n lineal:
$\text{predicci√≥n} = \beta_0 + \sum_{j=1}^{p} \beta_j X_j + \sum_{k=1}^{R} \alpha_k r_k(X)$

Donde:
* $\beta_0$ es el intercepto.
* $\beta_j X_j$ son los t√©rminos para las caracter√≠sticas lineales originales $X_j$.
* $\alpha_k r_k(X)$ son los t√©rminos para las caracter√≠sticas de regla binarias $r_k(X)$.

Los coeficientes ($\beta_j$ y $\alpha_k$) indican la importancia y la direcci√≥n del efecto de cada caracter√≠stica original y de cada regla en la predicci√≥n.

**Aprendizaje Global vs. Local:**

RuleFit es un excelente ejemplo de un algoritmo que combina perfectamente las caracter√≠sticas de **aprendizaje global y local**.

* **Aspecto Global (Modelo Lineal Final y Estructura General):**
    * La etapa final de RuleFit consiste en ajustar un **√∫nico modelo lineal global** (con regularizaci√≥n Lasso). Este modelo lineal es un predictor global que combina las caracter√≠sticas originales y todas las caracter√≠sticas de regla generadas. Los coeficientes en este modelo lineal global definen la relaci√≥n general entre las caracter√≠sticas de entrada (originales y basadas en reglas) y la variable objetivo en todo el conjunto de datos.
    * Este modelo lineal proporciona una **comprensi√≥n global** de la importancia de las caracter√≠sticas y de c√≥mo contribuyen colectivamente a la predicci√≥n.

* **Aspecto Local (Caracter√≠sticas de Regla y Efectos de Interacci√≥n):**
    * El poder de RuleFit para manejar relaciones no lineales e interacciones proviene de su **aspecto local**: las **reglas de decisi√≥n**. Cada regla, extra√≠da de una ruta en un √°rbol de decisi√≥n, define una **subregi√≥n espec√≠fica** o "vecindario" del espacio de caracter√≠sticas. Por ejemplo, una regla "SI la edad es > 30 Y el ingreso es < 50k" captura una condici√≥n local muy espec√≠fica.
    * Al incluir estas caracter√≠sticas de regla binarias, el modelo lineal puede aprender efectivamente **diferentes relaciones lineales dentro de diferentes regiones locales** definidas por las reglas. Estas reglas capturan expl√≠citamente los **efectos de interacci√≥n** entre caracter√≠sticas que un modelo lineal est√°ndar pasar√≠a por alto.
    * Esto es similar a tener un **modelo local (impl√≠citamente, una constante o un modelo lineal simple dentro de la regi√≥n de la regla)** que se activa cuando se cumplen sus condiciones, lo que permite que el modelo lineal global adapte sus predicciones bas√°ndose en estos patrones locales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n y regresi√≥n)",
  "‚úÖ Num√©rica o categ√≥rica seg√∫n el tipo de problema",
  "‚úÖ Variables num√©ricas y categ√≥ricas",
  "‚úÖ Modela relaciones no lineales a trav√©s de reglas y combinaciones lineales",
  "‚ùå No es modelo param√©trico cl√°sico, no se eval√∫a normalidad",
  "‚ùå No aplica directamente",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede ser sensible a outliers, pero menos que modelos lineales puros",
  "‚ö†Ô∏è Puede verse afectado, se recomienda an√°lisis previo",
  "‚ö†Ô∏è Interpretabilidad moderada: combina reglas f√°ciles con coeficientes lineales",
  "‚ö†Ô∏è Moderado, depende del n√∫mero de reglas generadas",
  "‚úÖ Se puede validar con validaci√≥n cruzada",
  "‚ùå No funciona bien en datasets muy peque√±os o con relaciones altamente complejas sin reglas claras"
)

detalles <- c(
  "Combina √°rboles de decisi√≥n para generar reglas y luego aplica regresi√≥n lineal para asignar pesos a estas reglas.",
  "Puede usarse para problemas de regresi√≥n o clasificaci√≥n.",
  "Admite variables categ√≥ricas y num√©ricas sin grandes restricciones.",
  "Captura tanto efectos lineales como interacciones no lineales v√≠a reglas extra√≠das.",
  "No asume distribuci√≥n normal de errores.",
  "No requiere independencia estricta entre observaciones.",
  "No requiere homoscedasticidad.",
  "Puede mitigar el impacto de outliers mediante regularizaci√≥n.",
  "Multicolinealidad puede afectar coeficientes pero el modelo regulariza para evitarlo.",
  "Las reglas extra√≠das facilitan la interpretaci√≥n frente a otros modelos complejos.",
  "La eficiencia depende del n√∫mero de reglas y tama√±o de datos.",
  "Validaci√≥n cruzada ayuda a evitar sobreajuste y seleccionar hiperpar√°metros.",
  "Puede fallar si las reglas generadas no capturan bien la relaci√≥n o en datos ruidosos."
)

tabla_rulefit <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rulefit %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir decision rule fit",
             subtitle = "Rule Fit")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Zero Rule (ZeroR)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Rule-System/Rule Fit.png"))
```

**Zero Rule (ZeroR)** es el algoritmo de **clasificaci√≥n supervisada** m√°s simple imaginable. No utiliza ninguna de las caracter√≠sticas predictoras en el conjunto de datos para hacer sus predicciones. En su lugar, simplemente **predice la clase m√°s frecuente (mayoritaria)** que se observa en el conjunto de datos de entrenamiento.

**C√≥mo funciona ZeroR:**   

1.  **Conteo de Frecuencias:** El algoritmo cuenta las ocurrencias de cada clase en el conjunto de datos de entrenamiento.
2.  **Identificaci√≥n de la Clase Mayoritaria:** Identifica la clase que tiene la mayor frecuencia (es decir, la clase que aparece m√°s veces).
3.  **Predicci√≥n Universal:** Para cualquier nueva instancia, ZeroR simplemente predice esta clase mayoritaria.

**Ejemplo:** Si en un conjunto de datos para predecir si un cliente "comprar√°" o "no comprar√°" un producto, el 70% de los clientes en el conjunto de entrenamiento "no compraron" y el 30% "compraron", ZeroR predecir√° "no comprar√°" para *todos* los nuevos clientes. Su precisi√≥n en el conjunto de entrenamiento ser√≠a del 70%.

**¬øPor qu√© es importante ZeroR?**  

Aunque ZeroR no tiene ning√∫n poder predictivo real en el sentido de aprender patrones complejos de los datos, es **fundamentalmente importante en Machine Learning como una l√≠nea base (benchmark)**.

* **Punto de Referencia:** Cualquier algoritmo de clasificaci√≥n m√°s sofisticado debe superar la precisi√≥n de ZeroR para ser considerado √∫til. Si un modelo complejo tiene una precisi√≥n inferior a la de ZeroR, significa que el modelo no est√° aprendiendo nada significativo de los datos, o incluso est√° aprendiendo patrones incorrectos.
* **Detecci√≥n de Sesgos de Clase:** En conjuntos de datos desequilibrados (donde una clase es mucho m√°s frecuente que otras), ZeroR puede lograr una precisi√≥n aparentemente alta. Esto resalta la importancia de usar m√©tricas de evaluaci√≥n m√°s all√° de la simple precisi√≥n en esos casos (como precisi√≥n, recall, F1-score, o AUC-ROC), ya que una alta precisi√≥n de ZeroR podr√≠a ser enga√±osa.
* **Simplicidad Extrema:** Sirve como el punto de partida m√°s b√°sico para entender c√≥mo los algoritmos de clasificaci√≥n intentan mejorar sobre una suposici√≥n trivial.

**Aprendizaje Global vs. Local:**

ZeroR es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** ZeroR calcula una √∫nica estad√≠stica (la clase mayoritaria) a partir de todo el conjunto de datos de entrenamiento y aplica esta predicci√≥n **uniformemente a todas las instancias**, sin importar sus caracter√≠sticas individuales o su ubicaci√≥n en el espacio de datos. No hay ninguna adaptaci√≥n local o consideraci√≥n de subregiones del espacio de caracter√≠sticas. El modelo es una √∫nica regla global e inmutable.

* **Sin Modelado de Relaciones:** Al ignorar todas las caracter√≠sticas de entrada, ZeroR no modela ninguna relaci√≥n, lineal o no lineal, entre los predictores y la variable objetivo. Su conocimiento se limita a la distribuci√≥n marginal de la clase de salida.

En resumen, ZeroR es el clasificador m√°s simple y sirve como un punto de referencia crucial para evaluar el rendimiento de modelos de Machine Learning m√°s avanzados. Su naturaleza es inherentemente global, ya que aplica una √∫nica decisi√≥n derivada del patr√≥n m√°s frecuente en todo el conjunto de datos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n y regresi√≥n)",
  "‚úÖ Categ√≥rica (modo) o num√©rica (media)",
  "‚ùå No utiliza predictores",
  "‚ùå No considera relaciones, predice constante",
  "‚ùå No aplica (no hay residuos de regresi√≥n)",
  "‚ùå No aplica (no hay modelo estructurado)",
  "‚ùå No aplica",
  "‚ùå No aplica (no hay influencia de valores at√≠picos)",
  "‚ùå No aplica (no hay predictores)",
  "‚úÖ Alta, ya que solo predice un valor fijo",
  "‚úÖ Extremadamente r√°pido",
  "‚ö†Ô∏è Se puede usar como baseline en validaci√≥n",
  "‚ùå No debe usarse como modelo final salvo como referencia base"
)

detalles <- c(
  "Modelo supervisado trivial que predice el valor m√°s frecuente (clasificaci√≥n) o promedio (regresi√≥n) de la variable respuesta.",
  "Funciona para variable respuesta categ√≥rica (modo) o continua (media).",
  "No utiliza ninguna variable predictora; solo la respuesta.",
  "No aprende relaciones, √∫til solo como l√≠nea base de comparaci√≥n.",
  "No genera residuos, pues predice una constante.",
  "No hay estructura de error o modelo que se eval√∫e.",
  "No hay dispersi√≥n de errores porque no hay ajuste.",
  "Outliers no afectan porque el modelo predice una constante global.",
  "No existe relaci√≥n entre predictores, as√≠ que no aplica multicolinealidad.",
  "F√°cil de interpretar: siempre predice lo mismo.",
  "Modelo extremadamente simple y r√°pido de calcular.",
  "Sirve como l√≠nea base para comparar modelos m√°s complejos.",
  "In√∫til para hacer predicciones significativas; solo sirve como referencia comparativa."
)

tabla_zeror <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_zeror %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir decision ZeroR",
             subtitle = "Zero Rule (ZeroR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```








<!--chapter:end:10-rule_based_systems.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# ü§ñ 4. Deep Learning {-}  

**Ejemplos:** CNN, RNN, Transformers.  
**Uso:** Ideal para **im√°genes**, **texto** y **series temporales**, especialmente con **grandes datos no estructurados**.  
**Ventajas:** Poderoso para datos complejos.   
**Limitaciones:** Exige **mucha data** y **computaci√≥n**; **poca interpretabilidad**.   

---



## Deep Boltzman Machine (DBM) {-}  


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/DBM.png"))
```

El modelo de **Deep Boltzman Machine (DBM)** es un tipo de **red neuronal profunda generativa** que pertenece a la familia de los **modelos gr√°ficos probabil√≠sticos**. Se construye apilando m√∫ltiples **M√°quinas de Boltzmann Restringidas (RBMs)**, lo que le permite aprender representaciones jer√°rquicas y abstractas de los datos de entrada. Su principal objetivo es modelar la **distribuci√≥n de probabilidad conjunta** entre un conjunto de variables observables y m√∫ltiples capas de variables latentes (ocultas).

Las DBMs son modelos **no dirigidos** (las conexiones entre las neuronas son sim√©tricas y no tienen una direcci√≥n espec√≠fica) y est√°n compuestas por capas de unidades visibles (los datos de entrada) y varias capas de unidades ocultas. A diferencia de las RBMs simples que tienen una sola capa oculta, las DBMs tienen **m√∫ltiples capas ocultas**, lo que les permite capturar dependencias m√°s complejas y caracter√≠sticas de alto nivel en los datos. El proceso de aprendizaje en una DBM busca ajustar los pesos de las conexiones de manera que la red asigne una alta probabilidad a los datos de entrenamiento y una baja probabilidad a los datos que no son de entrenamiento.

Las caracter√≠sticas clave de las DBMs incluyen:

1.  **Representaci√≥n Jer√°rquica:** Cada capa oculta aprende representaciones progresivamente m√°s abstractas de los datos. Las primeras capas pueden capturar caracter√≠sticas de bajo nivel (ej., bordes en im√°genes), mientras que las capas superiores combinan estas para formar representaciones de alto nivel (ej., partes de objetos o conceptos).
2.  **Aprendizaje No Supervisado:** Las DBMs se entrenan t√≠picamente de forma **no supervisada**, lo que significa que no requieren etiquetas para el entrenamiento. Esto las hace valiosas para el pre-entrenamiento de modelos profundos en conjuntos de datos grandes y sin etiquetar, donde pueden aprender caracter√≠sticas √∫tiles que luego pueden ser utilizadas en tareas de aprendizaje supervisado (como la clasificaci√≥n).
3.  **Inferencia y Generaci√≥n:** Una vez entrenadas, las DBMs pueden ser utilizadas tanto para **inferencia** (estimar las representaciones ocultas dadas las entradas visibles) como para **generaci√≥n** (muestrear nuevas instancias de datos a partir de la distribuci√≥n aprendida del modelo).

Debido a su complejidad computacional en el entrenamiento exacto, las DBMs a menudo se entrenan utilizando un enfoque de **aprendizaje codicioso por capas** (entrenando RBMs individuales y apil√°ndolas) seguido de un ajuste fino de todo el modelo utilizando algoritmos como el **Contraste Divergente Aproximado (ACD)**.


**Aprendizaje Global vs. Local:**

El modelo de **M√°quina de Boltzmann Profunda (DBM)** es un modelo de **aprendizaje global**.

* **Aspecto Global:** Las DBMs construyen un **modelo probabil√≠stico unificado y global** de la distribuci√≥n de los datos. Los pesos de conexi√≥n en todas las capas de la red se ajustan para representar las dependencias y correlaciones en todo el espacio de entrada. No se crean modelos espec√≠ficos para subconjuntos locales de datos; en cambio, el modelo aprende una representaci√≥n coherente y jer√°rquica que se aplica a todos los puntos de datos. La funci√≥n de energ√≠a (o funci√≥n de coste) de la DBM se define sobre el espacio completo de variables visibles y ocultas, y el entrenamiento busca minimizar esta energ√≠a globalmente para que los datos de entrenamiento tengan una energ√≠a baja.

* **Impacto de la Estructura Jer√°rquica:** Aunque la DBM aprende representaciones en diferentes niveles de abstracci√≥n (jerarqu√≠as), estas representaciones contribuyen a un entendimiento cohesivo y global de los datos. La interacci√≥n entre las capas y las unidades es parte de una estructura probabil√≠stica interconectada que busca modelar la distribuci√≥n general de los datos. El proceso de inferencia y generaci√≥n, aunque implica pasar informaci√≥n a trav√©s de las capas, se basa en los par√°metros globales de la red para producir resultados consistentes y representativos de la distribuci√≥n aprendida. Esto contrasta con modelos locales que podr√≠an segmentar el espacio de entrada y construir modelos independientes para cada segmento.  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Generativo / Discriminativo (apilado, no supervisado/supervisado)",
  "‚úÖ Num√©rica continua o categ√≥rica (binaria, multiclase)",
  "‚úÖ Num√©ricas y/o Categ√≥ricas (requiere preprocesamiento)",
  "‚úÖ No lineal y Compleja",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Asume independencia condicional",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Robusto hasta cierto punto, pero datos muy ruidosos afectan",
  "‚úÖ Maneja bien multicolinealidad",
  "‚ö†Ô∏è Baja (tipo caja negra)",
  "‚ö†Ô∏è Lento en entrenamiento, r√°pido en inferencia",
  "‚úÖ √ötil para ajuste de hiperpar√°metros",
  "‚ùå Datos insuficientes; ‚ùå alta dimensionalidad sin suficiente regularizaci√≥n; ‚ùå problemas de escalabilidad en conjuntos muy grandes."
)

detalles <- c(
  "Modelo generativo y discriminativo de aprendizaje profundo. Se entrena capa por capa de forma no supervisada (como RBMs) y luego se ajusta con una capa supervisada.",
  "Puede usarse para regresi√≥n (respuesta continua) o clasificaci√≥n (respuesta categ√≥rica).",
  "Acepta diversos tipos de variables predictoras, pero requieren normalizaci√≥n/estandarizaci√≥n y codificaci√≥n (ej., one-hot encoding para categ√≥ricas).",
  "Capaz de modelar relaciones altamente no lineales y complejas entre las variables.",
  "Los DBMs no asumen ni requieren la normalidad de los residuos, ya que no son modelos estad√≠sticos lineales tradicionales.",
  "A nivel de las capas, los DBMs asumen independencia condicional entre las variables visibles eÈöêvariables latentes dada la otra capa.",
  "Al igual que la normalidad de residuos, la homoscedasticidad no es un supuesto directo para DBMs.",
  "Puede manejar cierto nivel de ruido, pero el rendimiento disminuye con outliers extremos que distorsionan el espacio latente.",
  "Debido a su naturaleza de aprendizaje de representaciones, los DBMs son inherentemente capaces de manejar la multicolinealidad entre predictores.",
  "Generalmente, los DBMs son modelos de 'caja negra' con baja interpretabilidad de las relaciones directas entre las entradas y salidas.",
  "El entrenamiento de DBMs puede ser computacionalmente costoso y lento, especialmente con grandes datasets y muchas capas. La inferencia es m√°s r√°pida.",
  "La validaci√≥n cruzada es esencial para seleccionar hiperpar√°metros como el n√∫mero de capas, unidades por capa, tasas de aprendizaje y regularizaci√≥n.",
  "Requiere una cantidad significativa de datos para entrenar de forma efectiva. Puede tener dificultades con conjuntos de datos peque√±os. La alta dimensionalidad sin una regularizaci√≥n adecuada puede llevar a sobreajuste o problemas de escalabilidad en conjuntos de datos muy grandes."
)

tabla_dbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_dbm %>%
  gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Deep Boltzmann Machine (DBM)",
             subtitle = "Caracter√≠sticas y Consideraciones") %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia y principios de aprendizaje profundo") %>%
  tab_options(heading.title.font.size = 14,
              heading.subtitle.font.size = 12,
              table.font.names = "Century Gothic",
              table.font.size = 10,
              data_row.padding = px(1)) %>%
  tab_style(style = list(cell_text(align = "left",
                                   weight = 'bold')),
            locations = list(cells_title(groups = c("title")))) %>%
  tab_style(style = list(cell_text(align = "left")),
            locations = list(cells_title(groups = c("subtitle")))) %>%
  cols_width(starts_with("Detalles") ~ px(500),
             everything() ~ px(200)) %>%
  as_raw_html()
```


## Deep Belief Networks (DBNet) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/DBNet.png"))
```

El **Deep Belief Network (DBN)** es un modelo de **red neuronal profunda generativa** que se construye apilando m√∫ltiples **M√°quinas de Boltzmann Restringidas (RBMs)**. Fue un avance significativo en el campo del aprendizaje profundo, particularmente en la superaci√≥n de los desaf√≠os de entrenamiento de redes neuronales con muchas capas ocultas. Las DBNs son modelos probabil√≠sticos que buscan aprender una **distribuci√≥n de probabilidad conjunta** sobre los datos de entrada y sus representaciones latentes (ocultas).   

La arquitectura de una DBN es una jerarqu√≠a de capas, donde cada capa es una RBM. La capa inferior es la capa visible (o de entrada), que recibe los datos. Las capas subsiguientes son capas ocultas, y la caracter√≠stica clave es que el resultado de la capa oculta de una RBM se convierte en la capa visible para la siguiente RBM en la pila. La conexi√≥n entre la capa superior m√°s alta (que es una RBM) es no dirigida y bidireccional, mientras que las conexiones entre las capas inferiores suelen ser dirigidas (de arriba hacia abajo) en la fase generativa despu√©s del entrenamiento.  

Las DBNs se caracterizan por:  
  
1.  **Construcci√≥n por Capas:** Se construyen apilando RBMs, donde cada RBM se entrena de forma independiente y no supervisada para aprender una representaci√≥n de su entrada.   
2.  **Pre-entrenamiento Codicioso por Capas:** La innovaci√≥n clave de las DBNs fue el algoritmo de pre-entrenamiento codicioso por capas. En lugar de intentar entrenar toda la red a la vez (lo que era dif√≠cil debido a problemas como los gradientes desvanecientes/explosivos y los m√≠nimos locales), cada RBM se entrena individualmente para aprender caracter√≠sticas √∫tiles de la entrada que recibe. La salida de la capa oculta de una RBM entrenada se utiliza como entrada para la capa visible de la siguiente RBM. Este proceso contin√∫a hasta que se entrenan todas las capas.  
3.  **Aprendizaje No Supervisado para Extracci√≥n de Caracter√≠sticas:** La fase de pre-entrenamiento es completamente no supervisada. Las RBMs aprenden a reconstruir sus entradas, lo que les permite extraer caracter√≠sticas relevantes y de alto nivel de los datos sin necesidad de etiquetas. Esto es especialmente valioso para conjuntos de datos grandes y no etiquetados.  
4.  **Ajuste Fino (Fine-tuning) Supervisado:** Despu√©s del pre-entrenamiento no supervisado, la DBN puede ser "desenrollada" y tratada como una red neuronal feed-forward para tareas supervisadas como la clasificaci√≥n. Se a√±ade una capa de salida (ej., softmax) en la parte superior, y toda la red se ajusta utilizando algoritmos de aprendizaje supervisado como la retropropagaci√≥n. El pre-entrenamiento act√∫a como una buena inicializaci√≥n de los pesos, ayudando a que el entrenamiento supervisado converja m√°s r√°pido y alcance mejores m√≠nimos.  
5.  **Generaci√≥n de Datos:** Dado que son modelos generativos, las DBNs pueden aprender la distribuci√≥n subyacente de los datos y, por lo tanto, pueden generar nuevas muestras de datos similares a las de entrenamiento.   

Las DBNs fueron fundamentales para demostrar la viabilidad del entrenamiento de redes profundas y abrieron el camino para el resurgimiento del aprendizaje profundo.   


  
**Aprendizaje Global vs. Local:**
  
El modelo **Deep Belief Network (DBN)** emplea un enfoque h√≠brido, pero en su fase de aprendizaje de caracter√≠sticas, se inclina hacia un **aprendizaje global** a trav√©s de una estrategia local y progresiva.

* **Aspecto Global (Objetivo Final y Representaci√≥n):** El objetivo general de una DBN es construir un **modelo probabil√≠stico jer√°rquico global** de los datos. Aunque el entrenamiento se realiza capa por capa, la intenci√≥n es que cada capa capture caracter√≠sticas que contribuyan a una comprensi√≥n m√°s abstracta y completa de la distribuci√≥n de los datos de entrada en su conjunto. Las caracter√≠sticas de bajo nivel aprendidas por las primeras RBMs se combinan en las capas superiores para formar representaciones m√°s complejas y de alto nivel, que son intr√≠nsecas a la estructura global de los datos. El modelo final, una vez que todas las RBMs est√°n entrenadas y se aplica el ajuste fino, opera como una red unificada que mapea entradas a salidas basadas en un entendimiento global de las relaciones en los datos.

* **Aspecto Local (Estrategia de Entrenamiento):** La fase de **pre-entrenamiento codicioso por capas** de las DBNs tiene un fuerte componente local. Cada **RBM individual** se entrena de manera local, optimizando sus propios pesos para modelar la relaci√≥n entre su capa visible y su capa oculta, **sin considerar expl√≠citamente las capas m√°s all√° de s√≠ misma en ese momento**. La entrada para cada RBM superior proviene de la activaci√≥n de la capa oculta de la RBM inferior ya entrenada. Este entrenamiento local y secuencial es lo que permite que las DBNs escalen a redes profundas y eviten problemas de optimizaci√≥n de modelos globales complejos desde cero. Sin embargo, esta "localidad" es solo en la etapa de entrenamiento por partes; el efecto acumulativo de estas optimizaciones locales es la construcci√≥n de una representaci√≥n jer√°rquica que eventualmente se une en un modelo global cuando se realiza el ajuste fino de toda la red.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Generativo / Discriminativo (apilado, no supervisado/supervisado)",
  "‚úÖ Num√©rica continua o categ√≥rica (binaria, multiclase)",
  "‚úÖ Num√©ricas y/o Categ√≥ricas (requiere preprocesamiento)",
  "‚úÖ No lineal y Compleja",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Asume independencia condicional entre capas",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Robusto hasta cierto punto, pero datos muy ruidosos afectan",
  "‚úÖ Maneja bien multicolinealidad",
  "‚ö†Ô∏è Baja (tipo caja negra)",
  "‚ö†Ô∏è Lento en entrenamiento, r√°pido en inferencia",
  "‚úÖ √ötil para ajuste de hiperpar√°metros",
  "‚ùå Datos insuficientes; ‚ùå alta dimensionalidad sin suficiente regularizaci√≥n; ‚ùå problemas de escalabilidad."
)

detalles <- c(
  "Modelo generativo y discriminativo que consiste en apilar varias Restricted Boltzmann Machines (RBMs) o componentes similares. Cada RBM se entrena de forma no supervisada, y luego la red completa puede ser ajustada de forma supervisada.",
  "Puede ser utilizado tanto para tareas de regresi√≥n (variables continuas) como de clasificaci√≥n (variables categ√≥ricas, incluyendo binarias y multiclase), especialmente despu√©s de un ajuste supervisado (fine-tuning).",
  "Acepta una variedad de tipos de variables de entrada. Las variables num√©ricas generalmente requieren normalizaci√≥n o estandarizaci√≥n, y las categ√≥ricas necesitan ser codificadas (ej. one-hot encoding).",
  "Las DBNs son extremadamente capaces de aprender y modelar relaciones complejas y no lineales entre las variables de entrada y salida, gracias a su arquitectura profunda y sus capas ocultas.",
  "Al igual que con los DBMs, las DBNs no se basan en supuestos de normalidad de los residuos, ya que no son modelos estad√≠sticos lineales tradicionales.",
  "La independencia condicional es un supuesto clave en la forma en que cada RBM dentro de la DBN procesa la informaci√≥n entre sus capas visible y oculta.",
  "La homoscedasticidad no es un supuesto inherente o un requisito directo para el entrenamiento o la aplicaci√≥n de las DBNs.",
  "Aunque pueden ser algo robustas al ruido en los datos, los valores at√≠picos extremos pueden afectar negativamente el proceso de aprendizaje de las representaciones en las capas ocultas.",
  "Su capacidad para aprender representaciones jer√°rquicas y de bajo nivel de los datos ayuda a mitigar los problemas causados por la multicolinealidad entre las variables predictoras.",
  "Las DBNs, como muchos modelos de aprendizaje profundo, son consideradas 'cajas negras'. Es dif√≠cil interpretar directamente c√≥mo las caracter√≠sticas de entrada se mapean a las decisiones de salida.",
  "El entrenamiento de una DBN puede ser muy lento y costoso computacionalmente, especialmente en conjuntos de datos grandes o con muchas capas. Sin embargo, una vez entrenadas, la fase de inferencia es generalmente r√°pida.",
  "La validaci√≥n cruzada es una t√©cnica crucial para la selecci√≥n y optimizaci√≥n de hiperpar√°metros importantes, como el n√∫mero de RBMs, el n√∫mero de unidades en cada capa, las tasas de aprendizaje y los coeficientes de regularizaci√≥n.",
  "Requieren grandes vol√∫menes de datos etiquetados (para el fine-tuning supervisado) o no etiquetados (para el pre-entrenamiento no supervisado) para aprender representaciones significativas. Pueden tener problemas de escalabilidad con conjuntos de datos masivos o arquitecturas muy profundas."
)

tabla_dbn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_dbn %>%
  gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Deep Belief Network (DBN)",
             subtitle = "Caracter√≠sticas y Consideraciones") %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia y principios de aprendizaje profundo") %>%
  tab_options(heading.title.font.size = 14,
              heading.subtitle.font.size = 12,
              table.font.names = "Century Gothic",
              table.font.size = 10,
              data_row.padding = px(1)) %>%
  tab_style(style = list(cell_text(align = "left",
                                   weight = 'bold')),
            locations = list(cells_title(groups = c("title")))) %>%
  tab_style(style = list(cell_text(align = "left")),
            locations = list(cells_title(groups = c("subtitle")))) %>%
  cols_width(starts_with("Detalles") ~ px(500),
             everything() ~ px(200)) %>%
  as_raw_html()
```

## Reinforcement Learning (DL-based RL) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/Reinforcement Learning.png"))
```


## Stacked Auto-Enconders {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/SAE.png"))
```

Un **Autoencoder Apilado (Stacked Autoencoder - SAE)** es un tipo de **red neuronal profunda** que se construye apilando m√∫ltiples **autoencoders (AE)** simples. Al igual que los Autoencoders individuales, su prop√≥sito principal es el **aprendizaje de caracter√≠sticas no supervisado** y la **reducci√≥n de dimensionalidad**. La idea central es aprender representaciones compactas y de baja dimensionalidad (codificaciones) de los datos de entrada, que capturen las caracter√≠sticas m√°s importantes.

La arquitectura de un SAE se organiza en capas, donde cada capa es un autoencoder. Un autoencoder b√°sico consta de dos partes: un **codificador (encoder)** que mapea la entrada a una representaci√≥n de menor dimensi√≥n (el "c√≥digo" o "bottleneck"), y un **decodificador (decoder)** que reconstruye la entrada original a partir de esta representaci√≥n. En un autoencoder apilado:

1.  **Codificador y Decodificador:** Cada autoencoder en la pila tiene su propio codificador y decodificador.
2.  **Formaci√≥n de Capas:** La salida de la capa codificadora de un autoencoder se convierte en la entrada para la siguiente capa (el autoencoder superior). De esta manera, las capas progresivas aprenden representaciones de caracter√≠sticas cada vez m√°s abstractas y de alto nivel.

Las caracter√≠sticas clave de los Stacked Autoencoders incluyen:

1.  **Pre-entrenamiento Codicioso por Capas:** Similar a las DBNs, los SAEs se entrenan utilizando un enfoque de **pre-entrenamiento codicioso por capas**.
    * Primero, se entrena un autoencoder para aprender una representaci√≥n de la capa de entrada original.
    * Una vez entrenado, la capa del codificador de este AE se "congela" y sus salidas (las caracter√≠sticas aprendidas) se utilizan como entrada para el entrenamiento del siguiente autoencoder en la pila.
    * Este proceso se repite, entrenando un nuevo autoencoder sobre las representaciones aprendidas por el autoencoder anterior, construyendo as√≠ una jerarqu√≠a de caracter√≠sticas.
2.  **Aprendizaje No Supervisado:** Toda la fase de pre-entrenamiento es **no supervisada**, lo que significa que los SAEs pueden aprender representaciones poderosas de datos sin necesidad de etiquetas. Esto los hace muy √∫tiles en escenarios donde los datos etiquetados son escasos.
3.  **Reducci√≥n de Dimensionalidad y Extracci√≥n de Caracter√≠sticas:** El objetivo de cada autoencoder es encontrar una representaci√≥n de baja dimensionalidad que permita una buena reconstrucci√≥n de la entrada. Al apilar estos, el SAE aprende una jerarqu√≠a de caracter√≠sticas donde las capas m√°s profundas capturan abstracciones m√°s complejas y significativas de los datos.
4.  **Ajuste Fino (Fine-tuning) Supervisado:** Despu√©s del pre-entrenamiento no supervisado, el decodificador de cada autoencoder suele descartarse. Se toma la pila de codificadores como una red de extracci√≥n de caracter√≠sticas. A esta red se le a√±ade una capa de salida (ej., una capa softmax para clasificaci√≥n) y todo el modelo se ajusta finamente utilizando un algoritmo de aprendizaje supervisado (como retropropagaci√≥n con gradiente descendente) en una tarea espec√≠fica. El pre-entrenamiento act√∫a como una excelente inicializaci√≥n de los pesos, lo que ayuda a evitar m√≠nimos locales pobres y a acelerar la convergencia.

Los Stacked Autoencoders fueron un modelo popular antes del auge de las Redes Convolucionales y Recurrentes m√°s especializadas, y demostraron la efectividad del pre-entrenamiento no supervisado para inicializar redes profundas.



**Aprendizaje Global vs. Local:**

El modelo de **Autoencoder Apilado (Stacked Autoencoder - SAE)** es un modelo de **aprendizaje global** que se construye a trav√©s de una estrategia de entrenamiento local y secuencial.

* **Aspecto Global (Objetivo Final y Representaci√≥n):** El objetivo final de un SAE es aprender una **representaci√≥n global y jer√°rquica** de los datos. Cada capa codificadora en la pila extrae caracter√≠sticas de un nivel de abstracci√≥n creciente, contribuyendo a una comprensi√≥n m√°s profunda y compacta de toda la distribuci√≥n de los datos de entrada. La codificaci√≥n final producida por el SAE es una representaci√≥n de baja dimensionalidad que intenta encapsular la informaci√≥n m√°s relevante de los datos en su conjunto, permitiendo su reconstrucci√≥n. Cuando se utiliza para tareas posteriores (como clasificaci√≥n) despu√©s del ajuste fino, la red opera como un modelo unificado que aplica las caracter√≠sticas globales aprendidas a nuevas entradas.

* **Aspecto Local (Estrategia de Entrenamiento por Capas):** La fase de **pre-entrenamiento codicioso por capas** de los SAEs tiene un componente fuertemente local. Cada **autoencoder individual** en la pila se entrena de forma independiente para aprender una codificaci√≥n √≥ptima y una reconstrucci√≥n de su propia entrada. Esto significa que los pesos de cada autoencoder se optimizan localmente, en un momento dado, sin considerar directamente la optimizaci√≥n simult√°nea de toda la red. La entrada a cada autoencoder subsiguiente es la representaci√≥n codificada aprendida por el autoencoder anterior. Esta estrategia de entrenamiento "por partes" permite que las redes profundas sean entrenadas de manera m√°s eficiente y eficaz, ya que descompone un problema de optimizaci√≥n complejo en subproblemas m√°s manejables. Sin embargo, el resultado acumulado de estas optimizaciones locales es la construcci√≥n de una jerarqu√≠a de caracter√≠sticas que, en √∫ltima instancia, forma parte de un modelo global y unificado de los datos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Aprendizaje de representaci√≥n (no supervisado) / Discriminativo (supervisado)",
  "‚úÖ Num√©rica continua o categ√≥rica (binaria, multiclase)",
  "‚úÖ Num√©ricas y/o Categ√≥ricas (requiere preprocesamiento)",
  "‚úÖ No lineal y Compleja",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è No es un supuesto directo, pero la compresi√≥n busca regularidad",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Sensible, pueden afectar la calidad de las representaciones aprendidas",
  "‚úÖ Maneja bien multicolinealidad",
  "‚ö†Ô∏è Baja (tipo caja negra)",
  "‚ö†Ô∏è Lento en entrenamiento, r√°pido en inferencia",
  "‚úÖ √ötil para ajuste de hiperpar√°metros",
  "‚ùå Datos insuficientes; ‚ùå arquitectura inadecuada (cuello de botella); ‚ùå si las representaciones no son significativas para la tarea final."
)

detalles <- c(
  "Compuesto por m√∫ltiples autoencoders apilados. Cada autoencoder se entrena para aprender una representaci√≥n (codificaci√≥n) de la entrada de la capa anterior. Se usa para pre-entrenamiento no supervisado y luego fine-tuning supervisado para tareas espec√≠ficas.",
  "La capa de salida final puede adaptarse para tareas de regresi√≥n (variables continuas) o clasificaci√≥n (variables categ√≥ricas).",
  "Requiere preprocesamiento: variables num√©ricas estandarizadas/normalizadas y variables categ√≥ricas codificadas (ej. one-hot encoding).",
  "Excelente para capturar relaciones complejas, no lineales y de alta dimensionalidad en los datos a trav√©s de representaciones aprendidas.",
  "Los Stacked Autoencoders son modelos de aprendizaje autom√°tico no param√©tricos y no tienen supuestos sobre la normalidad de los residuos.",
  "No es un supuesto expl√≠cito, pero el objetivo de los autoencoders de reconstruir la entrada fomenta la captura de dependencias y regularidades en los datos, no necesariamente errores independientes.",
  "La homoscedasticidad no es una consideraci√≥n directa ni un requisito para el entrenamiento de Stacked Autoencoders.",
  "Pueden ser sensibles a valores at√≠picos, ya que estos pueden influir fuertemente en la forma en que se aprenden las representaciones latentes, potencialmente llevando a una reconstrucci√≥n deficiente o a caracter√≠sticas sesgadas.",
  "Al aprender representaciones de caracter√≠sticas de menor dimensionalidad, los SAEs son robustos frente a la multicolinealidad en las variables predictoras originales.",
  "Similar a otras redes neuronales profundas, los SAEs operan como 'cajas negras', haciendo dif√≠cil la interpretaci√≥n directa de las caracter√≠sticas latentes que aprenden.",
  "El entrenamiento capa por capa puede ser intensivo en tiempo y recursos computacionales, especialmente con grandes datasets y arquitecturas complejas. La fase de inferencia es r√°pida.",
  "La validaci√≥n cruzada es crucial para la selecci√≥n de hiperpar√°metros, como el n√∫mero de capas, el n√∫mero de unidades en cada capa latente, las tasas de aprendizaje y los t√©rminos de regularizaci√≥n.",
  "Requieren una cantidad sustancial de datos para aprender representaciones significativas y evitar el sobreajuste. Una arquitectura de cuello de botella mal dise√±ada puede limitar la capacidad de representaci√≥n. Si las caracter√≠sticas aprendidas por los autoencoders no son relevantes para la tarea final supervisada, el rendimiento puede ser pobre."
)

tabla_sae <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_sae %>%
  gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Stacked Autoencoders (SAE)",
             subtitle = "Caracter√≠sticas y Consideraciones") %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia y principios de aprendizaje profundo") %>%
  tab_options(heading.title.font.size = 14,
              heading.subtitle.font.size = 12,
              table.font.names = "Century Gothic",
              table.font.size = 10,
              data_row.padding = px(1)) %>%
  tab_style(style = list(cell_text(align = "left",
                                   weight = 'bold')),
            locations = list(cells_title(groups = c("title")))) %>%
  tab_style(style = list(cell_text(align = "left")),
            locations = list(cells_title(groups = c("subtitle")))) %>%
  cols_width(starts_with("Detalles") ~ px(500),
             everything() ~ px(200)) %>%
  as_raw_html()
```


## Variational Autoencoders (VAEs) {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/VAEs.png"))
```

<!--chapter:end:11-deep-learning.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
`r if (knitr::is_html_output()) '
# References {-}
'`

Sagi, S. (2019). ML Algorithms: One SD (œÉ). The obvious questions to ask when‚Ä¶ | by Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6 

Kuhn, M. (2019). The caret Package. https://topepo.github.io/caret/index.html

<!--chapter:end:12-references.Rmd-->

