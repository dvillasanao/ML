--- 
title: "Machine Learning (Apuntes) "
author: "Diana Villasana Ocampo"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::bs4_book,
  set in the _output.yml file.
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---

# url: your book url like https://bookdown.org/yihui/bookdown

Placeholder


## üìå Cuadro {-}

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üîç **1. Regressi√≥n** {.unnumbered}

**Ejemplos:** Linear Regression, Ridge, Lasso   

**Cu√°ndo usarlo:**  

* Predicci√≥n de valores num√©ricos continuos (e.g. precios, temperaturas).
* Relaciones lineales entre variables.  

**Ventajas:** Simple, interpretable.  
**Limitaciones:** Mal desempe√±o con relaciones no lineales complejas.  

---  


## Ordinary Least Squares Regression (`OLSR`) {-}   


La **Regresi√≥n por M√≠nimos Cuadrados Ordinarios (OLS)** es una t√©cnica fundamental en estad√≠stica y Machine Learning para modelar la **relaci√≥n lineal** entre una **variable dependiente** (a predecir) y una o m√°s **variables independientes**. Su objetivo es encontrar la **l√≠nea (o hiperplano) que mejor se ajusta** a los datos, minimizando la **suma de los cuadrados de las diferencias** entre los valores reales y los predichos por el modelo. Es decir, busca los coeficientes que hacen que la distancia (al cuadrado) de los puntos a la l√≠nea sea m√≠nima.

Los coeficientes de OLS se pueden calcular directamente con una f√≥rmula matem√°tica, sin necesidad de procesos iterativos complejos, bajo ciertos supuestos como la linealidad de la relaci√≥n y la independencia de los errores.

En el contexto del **aprendizaje global vs. local**, OLS es un ejemplo claro de un modelo de **aprendizaje global**. OLS busca una **√∫nica ecuaci√≥n** o un conjunto de coeficientes que describan la relaci√≥n entre las variables para **todo el conjunto de datos**. La l√≠nea o hiperplano que encuentra es una soluci√≥n global que se aplica de manera uniforme en todo el espacio de caracter√≠sticas. Esto la hace muy interpretable y computacionalmente eficiente, pero limitada si la relaci√≥n real entre las variables no es estrictamente lineal en todo el dominio de los datos.


```{r, echo = FALSE}
# Datos de la tabla
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas y/o categ√≥ricas",
  "‚úÖ Lineal (supuesto clave)",
  "‚òëÔ∏è Deseable",
  "‚úÖ Necesaria",
  "‚úÖ Necesaria",
  "‚ö†Ô∏è S√≠",
  "‚ö†Ô∏è Problema com√∫n",
  "‚úÖ Alta",
  "‚úÖ Muy alta",
  "‚úÖ Compatible",
  "‚ùå Relaciones no lineales, outliers severos, colinealidad"
)
detalles <- c(
  "Se entrena con datos X ‚Üí y",
  "Ej. mpg, precio, ingresos",
  "Categor√≠as convertidas a dummies",
  "Se asume una relaci√≥n lineal entre X e Y",
  "Importante para intervalos de confianza v√°lidos",
  "Errores deben ser independientes",
  "Varianza de errores debe ser constante",
  "Outliers pueden influir mucho en el modelo",
  "Usar VIF para detectar problemas",
  "Modelo f√°cil de explicar",
  "R√°pido incluso con datos grandes",
  "Ayuda a prevenir overfitting",
  "Evitar si no hay linealidad o hay muchos outliers"
)
# Crear y mostrar tabla
tabla_olsr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_olsr %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir OLSR",
             subtitle = "Regresi√≥n de M√≠nimos Cuadrados Ordinarios (OLSR)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Linear Regression {.unnumbered}

## Regresi√≥n Log√≠stica {.unnumbered}

La **Regresi√≥n Log√≠stica** es un modelo estad√≠stico usado principalmente para problemas de **clasificaci√≥n binaria**, donde el objetivo es predecir la **probabilidad** de que una instancia pertenezca a una de dos clases (por ejemplo, "s√≠" o "no", "0" o "1"). A pesar de su nombre, no predice un valor continuo, sino una probabilidad.

Este modelo utiliza una **funci√≥n sigmoide (o log√≠stica)** para transformar una combinaci√≥n lineal de las variables de entrada en un valor entre 0 y 1, que se interpreta como una probabilidad. Los coeficientes del modelo se aprenden maximizando la verosimilitud de observar los datos, generalmente a trav√©s de algoritmos como el descenso de gradiente.

En el contexto del **aprendizaje global vs. local**, la Regresi√≥n Log√≠stica es un modelo de **aprendizaje global**. Esto significa que busca un **√∫nico conjunto de coeficientes** que definen una frontera de decisi√≥n (un hiperplano) que se aplica a todo el espacio de caracter√≠sticas para separar las clases. Asume una relaci√≥n lineal entre las variables de entrada y el logaritmo de la probabilidad, y una vez entrenado, usa esta relaci√≥n global para hacer predicciones en cualquier parte del espacio de datos. Si bien es eficiente y muy interpretable, su naturaleza global puede limitar su rendimiento en casos donde las fronteras de decisi√≥n son inherentemente no lineales o muy complejas.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Lineal entre log-odds y predictores",
  "‚ùå No es requisito",
  "‚úÖ Necesaria",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠",
  "‚ö†Ô∏è Puede afectar",
  "‚úÖ Alta (coeficientes interpretables)",
  "‚úÖ Alta",
  "‚úÖ Compatible",
  "‚ùå Respuesta no binaria o multiclase sin ajuste"
)
detalles <- c(
  "Clasificaci√≥n binaria",
  "Ej. √©xito/fracaso, s√≠/no",
  "Convertir categ√≥ricas a dummies",
  "Relaci√≥n entre log(p/(1-p)) y X debe ser lineal",
  "No se exige normalidad en errores",
  "Independencia entre observaciones",
  "Idealmente varianza constante",
  "Outliers pueden alterar los coeficientes",
  "Usar VIF y regularizaci√≥n si hay problema",
  "Coeficientes en t√©rminos de odds/log-odds",
  "R√°pido y estable para datasets medianos",
  "K-fold funciona muy bien",
  "Evitar si hay multiclase sin ajuste"
)
tabla_logit <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

require(gt)
tabla_logit %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir logit",
             subtitle = "Regresi√≥n log√≠stica") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Locally Estimated Scatterplot Smoothing (`LOESS`) {.unnumbered}

**LOESS (Locally Estimated Scatterplot Smoothing)**, o LOWESS, es una t√©cnica de **regresi√≥n no param√©trica** para crear una curva suave que se ajusta a los datos en un diagrama de dispersi√≥n. Su gran ventaja es que **no asume una forma funcional global** espec√≠fica para la relaci√≥n entre las variables, lo que la hace muy flexible para identificar tendencias y patrones no lineales.

El principio de LOESS es simple: para estimar el valor suavizado en un punto, se seleccionan los **puntos de datos cercanos** (definido por un par√°metro de **"span"** o ancho de banda), se les asignan **pesos** (dando m√°s peso a los puntos m√°s cercanos), y luego se ajusta un **polinomio de bajo grado** (com√∫nmente lineal o cuadr√°tico) a esos puntos usando m√≠nimos cuadrados ponderados. Este proceso se repite para cada punto de inter√©s para construir la curva.

En el contexto del **aprendizaje global vs. local**, LOESS es un modelo de **aprendizaje puramente local**. Su flexibilidad reside en que **ajusta m√∫ltiples modelos simples y locales** (regresiones ponderadas) en diferentes vecindarios de los datos. No busca una √∫nica ecuaci√≥n global que describa la relaci√≥n en todo el conjunto de datos. Esto le permite adaptarse maravillosamente a las variaciones en las relaciones y curvaturas de los datos, lo que es especialmente √∫til cuando los datos no se distribuyen linealmente. Sin embargo, su naturaleza local implica que no produce una f√≥rmula expl√≠cita del modelo, y puede ser computacionalmente m√°s intensivo para conjuntos de datos muy grandes.  


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua",
  "‚úÖ Num√©ricas (usualmente 1 o 2 predictores)",
  "‚úÖ No lineal y suave",
  "‚ùå No necesaria",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è Muy sensible",
  "‚ùå No aplica (pocos predictores)",
  "‚úÖ Muy interpretable gr√°ficamente",
  "‚ö†Ô∏è Lento en grandes vol√∫menes de datos",
  "‚úÖ Puede usarse para elegir 'span'",
  "‚ùå Datos grandes o con ruido fuerte"
)
detalles <- c(
  "Modelo no param√©trico local",
  "Regresi√≥n para valores continuos",
  "Generalmente 1 o 2 variables num√©ricas",
  "Ajuste por vecindad, suaviza la curva",
  "No asume distribuci√≥n espec√≠fica",
  "Supuesto deseable si hay dependencias temporales",
  "Ideal si la varianza no cambia mucho localmente",
  "Altamente afectado por outliers locales",
  "No es una t√©cnica multivariable compleja",
  "La curva ajustada se interpreta visualmente",
  "Computacionalmente costoso con datos grandes",
  "Ayuda a seleccionar el mejor 'span'",
  "Poco eficaz en alta dimensi√≥n o datos muy dispersos"
)
tabla_loess <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
tabla_loess %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LOESS",
             subtitle = "Locally Estimated Scatterplot Smoothing (LOESS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Multivariate Adaptive Regression Splines (`MARS`) {.unnumbered}

**Multivariate Adaptive Regression Splines (MARS)** es un algoritmo de **regresi√≥n no param√©trica** que extiende los modelos lineales para manejar relaciones no lineales y complejas. Desarrollado por Jerome Friedman, MARS construye su modelo al **dividir el espacio de entrada en m√∫ltiples regiones y ajustar una funci√≥n lineal simple (o de orden superior) a cada regi√≥n**.

El proceso de MARS consta de dos fases: una **fase de adelante** que a√±ade iterativamente **funciones base** (pares de funciones "hinge" o bisagra) y **nudos** (puntos de corte) para capturar no linealidades e interacciones entre variables, y una **fase de atr√°s** que poda las funciones base menos significativas utilizando criterios como la **Validaci√≥n Cruzada Generalizada (GCV)** para prevenir el sobreajuste. Esto permite a MARS ser adaptable a las particularidades de los datos.

En el contexto del **aprendizaje global vs. local**, MARS se sit√∫a como un modelo de **aprendizaje adaptativo que combina aspectos globales y locales**. Es "local" en el sentido de que sus funciones base y nudos dividen el espacio de datos en regiones, y dentro de cada regi√≥n se aplica una relaci√≥n simple. Sin embargo, es "global" porque la suma de todas estas funciones base forma una √∫nica ecuaci√≥n que describe la relaci√≥n en todo el conjunto de datos y se aplica de forma consistente. Esto significa que si los datos no se distribuyen linealmente, MARS puede aprender y modelar estas relaciones complejas de forma adaptativa, encontrando autom√°ticamente d√≥nde y c√≥mo las relaciones cambian, ofreciendo una soluci√≥n que es tanto flexible como interpretable.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua o categ√≥rica (binaria con extensi√≥n)",
  "‚úÖ Num√©ricas (categ√≥ricas con dummies)",
  "‚úÖ No lineal (autom√°tico)",
  "‚ùå No requerida",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠ (aunque algo robusto)",
  "‚ö†Ô∏è Puede afectar",
  "‚ö†Ô∏è Media (modelo tipo caja negra)",
  "‚úÖ Razonable para tama√±os medianos",
  "‚úÖ Recomendado (ej. repeated k-fold)",
  "‚ùå Relaci√≥n puramente lineal o muchos factores irrelevantes"
)
detalles <- c(
  "Regresi√≥n flexible no lineal",
  "Ideal para regresi√≥n continua (tambi√©n clasificaci√≥n con `earth`)",
  "Crea autom√°ticamente 'splines' por variable",
  "Crea funciones por tramos con 'nudos'",
  "No exige distribuci√≥n espec√≠fica de errores",
  "Mejor si los datos no est√°n correlacionados temporalmente",
  "Idealmente errores con varianza constante",
  "Puede ser sensible a valores extremos",
  "Detecta interacciones, pero VIF sigue siendo √∫til",
  "Coeficientes no tan interpretables como OLS",
  "M√°s lento que OLS pero m√°s flexible",
  "CV ayuda a elegir n√∫mero √≥ptimo de t√©rminos",
  "Tiene riesgo de sobreajuste si no se controla bien"
)
tabla_mars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_mars %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MARS",
             subtitle = "Splines de Regresi√≥n Adaptativa Multivariante (MARS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Stepwise Regression {.unnumbered}

**Regresi√≥n por pasos** : a√±ade caracter√≠sticas al modelo una a una
hasta encontrar la puntuaci√≥n √≥ptima para tu conjunto de
caracter√≠sticas. La selecci√≥n por pasos alterna entre el avance y el
retroceso, incorporando y eliminando variables que cumplen los criterios
de entrada o eliminaci√≥n, hasta alcanzar un conjunto estable de
variables.

## Support Vector Machine (SVM) {.unnumbered}

**Support Vector Machine (SVM)** es un potente y vers√°til algoritmo de **Machine Learning** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**, aunque es m√°s conocido por su aplicaci√≥n en clasificaci√≥n. Su objetivo principal es encontrar el **hiperplano √≥ptimo** que separe las clases en el espacio de caracter√≠sticas con el **margen** m√°s grande posible. Los puntos de datos m√°s cercanos a este hiperplano se llaman **vectores de soporte**, y son cruciales para definir la frontera de decisi√≥n.

Para manejar datos que no son linealmente separables, SVM utiliza el **"truco del kernel"**. Este truco permite a SVM mapear impl√≠citamente los datos a un espacio de mayor dimensi√≥n donde las clases podr√≠an ser linealmente separables, sin necesidad de calcular expl√≠citamente las coordenadas. Funciones kernel comunes como el **Radial Basis Function (RBF) o Gaussiano** permiten a SVM modelar fronteras de decisi√≥n no lineales complejas en el espacio original de baja dimensi√≥n.

En el contexto del **aprendizaje global vs. local**, SVM se clasifica principalmente como un modelo de **aprendizaje global**. Esto se debe a que busca un **√∫nico hiperplano √≥ptimo** (o una frontera de decisi√≥n no lineal definida por el kernel) que se aplica a la totalidad del espacio de caracter√≠sticas. Una vez entrenado, el modelo predice evaluando la posici√≥n de un nuevo punto con respecto a esta frontera global. Sin embargo, hay un matiz "local" en su funcionamiento: la determinaci√≥n de este hiperplano depende **cr√≠ticamente solo de los vectores de soporte**, que son los puntos de datos "m√°s dif√≠ciles" cercanos a la frontera. Los puntos que est√°n lejos del margen no influyen en la definici√≥n del modelo. Aunque la frontera de decisi√≥n es una funci√≥n global que se aplica en todas partes, su construcci√≥n est√° influenciada por estos puntos localmente relevantes, permitiendo a SVM adaptar su aproximaci√≥n incluso cuando las relaciones en los datos no se distribuyen linealmente, al encontrar la separaci√≥n √≥ptima en un espacio transformado.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas (categor√≠as deben codificarse)",
  "‚úÖ Capta relaciones no lineales (kernel)",
  "‚ùå No requiere",
  "‚úÖ Idealmente s√≠",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠, especialmente sin margen amplio",
  "‚úÖ Puede manejarla bien",
  "‚ùå Baja (modelo es una caja negra)",
  "‚ö†Ô∏è Lento con muchos datos o predictores",
  "‚úÖ Esencial para elegir kernel y par√°metros",
  "‚ùå Datos con mucho ruido o solapamiento entre clases"
)
detalles <- c(
  "Modelo supervisado que maximiza el margen entre clases",
  "Clasificaci√≥n binaria, multiclase o regresi√≥n (SVR)",
  "Requiere escalar o estandarizar las variables num√©ricas",
  "Puede usar kernel para resolver problemas no lineales",
  "No requiere supuestos cl√°sicos como normalidad",
  "Mejor si los datos son independientes",
  "Puede usarse aunque haya heterocedasticidad",
  "Los outliers cercanos al margen afectan el modelo",
  "Los kernels pueden reducir el efecto de multicolinealidad",
  "Dif√≠cil de explicar; es un modelo de tipo caja negra",
  "Puede ser costoso computacionalmente con datos grandes",
  "Par√°metros como C y gamma se ajustan v√≠a validaci√≥n cruzada",
  "No es ideal si hay ruido o datos mal etiquetados"
)
tabla_svm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_svm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir SVM",
             subtitle = "Support Vector Machine (SVM) ") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

<!--chapter:end:01-regression.Rmd-->


# üå≤ **2. √Årboles de Decisi√≥n y Derivados** {-}  

Placeholder


## C4.5  {-}   
## C5.0  {-}  
## Classification and Regression Tree (CART)  {-} 
## Chi-squared Automatic Interaction Detection (CHAID)  {-}    
## Conditional Decision Trees (Conditional Inference Trees - CITs)  {-}   
## Decision Stump  {-}  
## Iterative Dichotomiser 3 (ID3)  {-}    
## M5 (Model Tree) {-}  

<!--chapter:end:02-decision_tree.Rmd-->


# üåü **3. Ensambles (Ensemble Methods)** {-}

Placeholder


## Adaptive Boosting (AdaBoost)  {-}  
## Boosting  {-}   
## Bootstrapped Aggregation (Bagging)  {-}    
## Extreme Gradient Boosting (XGBoost)  {-}    
## Gradient Boosting Machines (GBM)  {-}   
## Gradient Boosted Regression Trees (GBRT)  {-}   
## Light Gradient Boosting Machine (LightGBM)  {-}   
## Random Forest  {-}   
## Stacked Generlization (Blending) {-}   

<!--chapter:end:03-ensemble.Rmd-->


# üß† **4. Redes Neuronales y Deep Learning** {-}  

Placeholder


## Autoenconder  {-}  
## Back - Propagation  {-}  
## Convolutional Neural Network (CNN)  {-}   
## Hopfield Network  {-}   
## Multilayer Perceptron (MP)  {-}     
## Perceptron  {-}     
## Radial Basis Function Network (RBFN)  {-}      
## Recurrent Neural Networks (RNNs) {-}   
## Transformers  {-}  

<!--chapter:end:04-neural-networks.Rmd-->


# üß© **5. Reducci√≥n de Dimensionalidad** {-}   

Placeholder


## Flexible Discriminant Analysis (FDA)  {-}   
## Linear Discriminant Analysis (LDA)  {-}    
## Mixture Discriminant Analysis (MDA)  {-}   
## Multidimensional Scaling (MDS)  {-}    
## Quadratic Discriminant Analysis (QDA) {-}  
## Partial Least Squares Regression (PLSR)  {-}    
## Partial Least Squares Discriminant Analysis (PLSDA)  {-}   
## Principal Component Analysis (PCA)  {-}   
## Principal Component Regression (PCR)  {-}    
## Projection Pursuit (PP)  {-}     
## Sammon Mapping  {-}   
## Regularized Discriminant Analysis (RDA)  {-}    
## Uniform Manifold Approximation and Projection (UMAP) {-}   

<!--chapter:end:05-dimensionality_reduction.Rmd-->


# üß¨ **6. Bayesianos** {-}  

Placeholder


## Averaged One - Dependence Estimators (AODE)  {-}    
## Bayesian Network (BN)  {-}    
## Bayesian Belief Network (BBN)  {-}  
## Gaussian Naive Bayes (GNB) {-}     
## Multinomial Naive Bayes (MNB) {-}   
## Naive Bayes (NB) {-}   

<!--chapter:end:06-bayesian.Rmd-->


# üßÆ **7. Regularizaci√≥n** {-}  

Placeholder


## Elastic Net  {-}   
## Ridge Regression  {-}    
## Least Absolute Shrinkage and Selection Operator (LASSO)  {-}  
## Least Angle Regression (LARS)  {-}   

<!--chapter:end:07-regularization.Rmd-->


# üîç **8. Instance-Based (Basados en Instancias)** {-}  

Placeholder


## k - Nearest Neighbour (kNN)  {-}    
## Learning Vector Quantization (LVQ)  {-}    
## Locally Weighted Learning (LWL)  {-}   
## Self - Organizing Map (SOM)  {-}   

<!--chapter:end:08-instance_based.Rmd-->


# üìè **9. Clustering (No Supervisado)** {-}  

Placeholder


## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)  {-}    
## Expectation Maximization (EM) {-}  
## Hierarchical Clustering (hclust) {-}  
## k-Means  {-}  
## k-Medians  {-}   

<!--chapter:end:09-clustering.Rmd-->


# üìê **10. Sistemas Basados en Reglas (Rule-Based Systems)** {-}

Placeholder


## Cubist  {-}  
## Decision Rules  {-}  
## Fuzzy Logic {-}
## One Rule (OneR)  {-}   
## Repeated Incremental Pruning to Produce Error Reduction (RIPPER)  {-} 
## Rule Fit  {-}   
## Zero Rule (ZeroR)  {-}    

<!--chapter:end:10-rule_based_systems.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
`r if (knitr::is_html_output()) '
# References {-}
'`

Sagi, S. (2019). ML Algorithms: One SD (œÉ). The obvious questions to ask when‚Ä¶ | by Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6 

Kuhn, M. (2019). The caret Package. https://topepo.github.io/caret/index.html

<!--chapter:end:11-references.Rmd-->

