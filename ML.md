--- 
title: "Machine Learning (Apuntes) "
author: "Diana Villasana Ocampo"
date: "2025-06-02"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::bs4_book,
  set in the _output.yml file.
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---

# url: your book url like https://bookdown.org/yihui/bookdown

Placeholder


## üìå Cuadro

<!--chapter:end:index.Rmd-->


# üîç **1. Regressi√≥n** {-}

Placeholder


## Ordinary Least Squares Regression (`OLSR`) {-} 
## Linear Regression {-} 
## Logistic Regression {-} 
## Stepwise Regression {-} 
## Multivariate Adaptive Regression Splines (`MARS`) {-} 
## Locally Estimated Scatterplot Smoothing (`LOESS`) {-} 
## Regression Ridge {-} 
## Least Absolute Shrinkage and Selection Operator (`LASSO`) {-}

<!--chapter:end:01-regression.Rmd-->

# üå≤ **2. √Årboles de Decisi√≥n y Derivados** {-}  

**Ejemplos:** Decision Tree, Random Forest, Gradient Boosting  
**Cu√°ndo usarlo:**  

* Problemas tabulares con relaciones no lineales y variables categ√≥ricas o num√©ricas.
* Cuando interpretabilidad es importante.

**Ventajas:** Manejan datos heterog√©neos, f√°ciles de interpretar (√°rboles simples).   
**Limitaciones:** Sobreajuste en √°rboles simples; menor desempe√±o en datos muy ruidosos sin ensambles.

---

<!--chapter:end:02-decision_tree.Rmd-->

# üåü **3. Ensambles (Ensemble Methods)** {-}

**Ejemplos:** Random Forest, AdaBoost, XGBoost, LightGBM   
**Cu√°ndo usarlo:**   

* Cuando buscas alto rendimiento en clasificaci√≥n o regresi√≥n tabular.
* Competencias de datos (como Kaggle).

**Ventajas:** Alta precisi√≥n, robustez.   
**Limitaciones:** Dif√≠cil de interpretar; m√°s costosos computacionalmente.

---

<!--chapter:end:03-ensemble.Rmd-->

# üß† **4. Redes Neuronales y Deep Learning** {-}  

**Ejemplos:** MLP, CNN, RNN, Transformers   
**Cu√°ndo usarlo:**   

* Im√°genes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).
* Grandes vol√∫menes de datos no estructurados.

**Ventajas:** Muy poderosos para datos complejos y no estructurados.   
**Limitaciones:** Requieren mucha data y poder computacional. Menor interpretabilidad.

---

<!--chapter:end:04-neural-networks.Rmd-->

# üß© **5. Reducci√≥n de Dimensionalidad** {-}   

**Ejemplos:** PCA, t-SNE, UMAP   
**Cu√°ndo usarlo:**   

* Visualizaci√≥n de datos de alta dimensi√≥n.
* Preprocesamiento para eliminar ruido o multicolinealidad.

**Ventajas:** Mejora desempe√±o y velocidad de otros modelos.    
**Limitaciones:** Puede perder interpretabilidad; no siempre mejora modelos.

---

<!--chapter:end:05-dimensionality_reduction.Rmd-->

# üß¨ **6. Bayesianos** {-}  

**Ejemplos:** Naive Bayes, Bayesian Networks  
**Cu√°ndo usarlo:**   

* Clasificaci√≥n r√°pida con supuestos simples.
* Problemas de texto o spam detection.

**Ventajas:** Muy r√°pidos, bien fundamentados.   
**Limitaciones:** Supone independencia de variables (no siempre cierto).

---

<!--chapter:end:06-bayesian.Rmd-->

# üßÆ **7. Regularizaci√≥n** {-}  

**Ejemplos:** L1 (Lasso), L2 (Ridge), Elastic Net   
**Cu√°ndo usarlo:**   

* Para evitar sobreajuste en modelos lineales o redes neuronales.
* Cuando tienes muchas variables (alta dimensionalidad).

**Ventajas:** Penaliza modelos complejos.   
**Limitaciones:** Puede eliminar variables √∫tiles si se usa en exceso.

---

<!--chapter:end:07-regularization.Rmd-->

# üîç **8. Instance-Based (Basados en Instancias)** {-}  

**Ejemplos:** K-Nearest Neighbors (KNN)   
**Cu√°ndo usarlo:**   

* Pocos datos, con patrones locales claros.  
* Cuando la similitud entre casos es importante.

**Ventajas:** Simple y eficaz en problemas de baja dimensi√≥n.   
**Limitaciones:** Escala mal con muchos datos; sensible al ruido.

---

<!--chapter:end:08-instance_based.Rmd-->

# üìè **9. Clustering (No Supervisado)** {-}  

**Ejemplos:** K-Means, DBSCAN, Hierarchical Clustering  
**Cu√°ndo usarlo:**   

* Agrupar datos sin etiquetas previas.
* Descubrir estructuras ocultas o segmentos de mercado.

**Ventajas:** √ötil en exploraci√≥n y reducci√≥n de complejidad.   
**Limitaciones:** Requiere elegir n√∫mero de grupos (excepto DBSCAN); puede ser sensible a escala.

---

<!--chapter:end:09-clustering.Rmd-->

# üìê **10. Sistemas Basados en Reglas (Rule-Based Systems)** {-}

**Ejemplos:** RuleFit, Decision Rules, l√≥gica difusa
**Cu√°ndo usarlo:**

* Interpretabilidad es clave (por ejemplo, decisiones legales o m√©dicas).
* Incorporar conocimiento experto.

**Ventajas:** F√°cil de entender y auditar.   
**Limitaciones:** No tan precisos como otros m√©todos en datos complejos.

---

<!--chapter:end:10-rule_based_systems.Rmd-->


# References {-}


Sagi, S. (2019). ML Algorithms: One SD (œÉ). The obvious questions to ask when‚Ä¶ | by Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6 

Kuhn, M. (2019). The caret Package. https://topepo.github.io/caret/index.html

<!--chapter:end:11-references.Rmd-->

