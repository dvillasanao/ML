# üåü **3. Ensambles (Ensemble Methods)** {-}

**Ejemplos:** Random Forest, AdaBoost, XGBoost, LightGBM   
**Cu√°ndo usarlo:**   

* Cuando buscas alto rendimiento en clasificaci√≥n o regresi√≥n tabular.
* Competencias de datos (como Kaggle).

**Ventajas:** Alta precisi√≥n, robustez.   
**Limitaciones:** Dif√≠cil de interpretar; m√°s costosos computacionalmente.

---

## Adaptive Boosting (AdaBoost)  {-}  

**AdaBoost (Adaptive Boosting)** es uno de los algoritmos de **boosting** m√°s influyentes y el primero en ser propuesto con √©xito, desarrollado por Yoav Freund y Robert Schapire en 1995. Es una t√©cnica de **aprendizaje conjunto (ensemble learning)** utilizada principalmente para **clasificaci√≥n**, aunque sus principios pueden extenderse a la regresi√≥n. La idea fundamental de AdaBoost es construir un modelo fuerte combinando secuencialmente las predicciones de m√∫ltiples **clasificadores "d√©biles" o "base"**, y lo hace prestando m√°s atenci√≥n a los ejemplos que los modelos anteriores clasificaron incorrectamente.

El funcionamiento de AdaBoost se basa en un sistema de **re-ponderaci√≥n de datos** en cada iteraci√≥n:

1.  **Inicializaci√≥n de Pesos:** Se asigna un peso inicial igual a cada ejemplo de entrenamiento.
2.  **Entrenamiento del Clasificador D√©bil:** En cada iteraci√≥n, se entrena un clasificador d√©bil (a menudo un **Decision Stump**, que es un √°rbol de decisi√≥n de un solo nivel) en el conjunto de datos actual. Este clasificador se enfoca en minimizar el error ponderado.
3.  **C√°lculo del Error Ponderado:** Se calcula el error del clasificador d√©bil, teniendo en cuenta los pesos de los ejemplos. Los ejemplos mal clasificados tienen un mayor impacto en este error.
4.  **Actualizaci√≥n de Pesos de Datos:** Los pesos de los ejemplos mal clasificados por el clasificador actual son **aumentados**, mientras que los pesos de los ejemplos correctamente clasificados son **disminuidos**. Esto asegura que el siguiente clasificador d√©bil se enfoque m√°s en los ejemplos que son dif√≠ciles de clasificar.
5.  **C√°lculo del Peso del Clasificador:** Se asigna un peso (o "contribuci√≥n") al clasificador d√©bil actual en funci√≥n de su precisi√≥n. Los clasificadores m√°s precisos reciben un peso mayor en la predicci√≥n final del conjunto.
6.  **Combinaci√≥n de Predicciones:** Las predicciones finales del modelo AdaBoost se obtienen mediante una **suma ponderada** de las predicciones de todos los clasificadores d√©biles.

En el contexto del **aprendizaje global vs. local**, AdaBoost es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada clasificador d√©bil que se entrena en una iteraci√≥n puede verse como una forma de **regresi√≥n ponderada localmente** (o, m√°s precisamente, clasificaci√≥n ponderada localmente), ya que ajusta su enfoque bas√°ndose en los ejemplos que el modelo combinado anterior no pudo clasificar bien. Al iterar y ajustar los pesos de los datos, AdaBoost se enfoca progresivamente en las regiones del espacio de caracter√≠sticas donde el modelo actual tiene un rendimiento deficiente. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de clasificaci√≥n (y por extensi√≥n, las ideas de regresi√≥n) de manera altamente adaptativa. La capacidad de AdaBoost para concentrarse en los "errores" m√°s dif√≠ciles aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. El resultado es un clasificador global muy preciso y robusto, capaz de modelar relaciones complejas y no lineales, que es una combinaci√≥n ponderada de muchas decisiones locales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n adaptada)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para algunas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones mediante reponderaci√≥n iterativa",
  "‚ùå No requiere supuestos de normalidad en los residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers pueden obtener demasiado peso durante iteraciones)",
  "‚úÖ Robusto (reduce colinealidad al iterar sobre subconjuntos ponderados)",
  "‚ö†Ô∏è Baja (modelo resultante es especie de ‚Äôcaja negra‚Äô)",
  "‚ö†Ô∏è Lento con muchas iteraciones o datos grandes",
  "‚úÖ Recomendable para ajustar tasa de aprendizaje y n√∫mero de iteraciones",
  "‚ùå No es ideal con datos muy ruidosos o clases extremadamente desbalanceadas sin t√©cnicas adicionales"
)

detalles <- c(
  "Ensamble supervisado que combina varios modelos d√©biles (ej. √°rboles simples) ajustando pesos seg√∫n errores anteriores.",
  "En clasificaci√≥n ajusta pesos para mal clasificados; en regresi√≥n, adapta predicci√≥n por minimizaci√≥n de p√©rdida.",
  "Puede trabajar con datos mixtos; para variables categ√≥ricas suele usar codificaci√≥n de dummies.",
  "Cada iteraci√≥n repondera observaciones dif√≠ciles, enfoc√°ndose en patrones que previos modelos no capturaron.",
  "No impone distribuci√≥n de errores; se basa en funci√≥n de p√©rdida, no en supuestos param√©tricos.",
  "Funciona mejor si cada observaci√≥n es independiente; sensible a dependencias temporales si no se corrige.",
  "No requiere varianza constante, ya que funciona sobre el error iterativo en lugar de residuos tradicionales.",
  "Outliers dif√≠ciles de clasificar tienden a recibir mayor peso, lo que puede sesgar el ensamble si no se controla el learning rate.",
  "La reponderaci√≥n de muestras aten√∫a el efecto de predictores correlacionados, pues cada iteraci√≥n puede focalizarse en subconjuntos distintos.",
  "Es complejo desentra√±ar la contribuci√≥n de cada modelo d√©bil; se pueden usar m√©tricas de importancia o SHAP para interpretaci√≥n.",
  "Cada iteraci√≥n entrena un modelo d√©bil; muchas iteraciones o modelos complejos pueden ralentizar el entrenamiento.",
  "K-fold o repeated CV ayudan a elegir tasa de aprendizaje (learning rate) y n√∫mero de iteraciones (trials).",
  "No conviene con instancias altamente ruidosas: se puede sobreajustar r√°pidamente si la tasa de aprendizaje es alta o no se regula iteraciones."
)

tabla_adaboost <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_adaboost %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir adaboost",
             subtitle = "AdaBoost") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Boosting  {-}   

**Boosting** es una t√©cnica de **aprendizaje conjunto (ensemble learning)** que busca transformar un conjunto de **modelos "d√©biles" o "base"** en un **modelo "fuerte" o "preciso"**. La idea fundamental es construir modelos de forma **secuencial** e **iterativa**, donde cada nuevo modelo se centra en corregir los errores o deficiencias de los modelos construidos en las iteraciones anteriores. A diferencia del *bagging* (como en Random Forest), donde los modelos se entrenan de forma independiente, el *boosting* es intr√≠nsecamente secuencial y adaptativo.

El concepto clave de Boosting radica en la asignaci√≥n de **pesos** o en el enfoque en los **errores residuales**:

1.  **Iteraciones Secuenciales:** El proceso comienza con un modelo base inicial (a menudo simple, como un *decision stump*).
2.  **Enfoque en los Errores:** En cada iteraci√≥n subsiguiente, el algoritmo presta m√°s atenci√≥n a los ejemplos que fueron clasificados (o predichos) incorrectamente por los modelos anteriores, o a los errores residuales no explicados. Esto se logra ya sea **re-ponderando** los datos (dando m√°s peso a los ejemplos mal clasificados) o **ajustando** el nuevo modelo para que prediga los residuos de los modelos anteriores.
3.  **Combinaci√≥n Ponderada:** Las predicciones de todos los modelos d√©biles se combinan, generalmente a trav√©s de una suma ponderada, donde los modelos m√°s precisos reciben un mayor peso en la predicci√≥n final.

La fuerza del boosting radica en su capacidad para reducir el **sesgo** y la **varianza** del modelo final, al construir un modelo complejo a partir de componentes simples que se complementan entre s√≠.

En el contexto del **aprendizaje global vs. local**, Boosting es una estrategia de **aprendizaje global** que opera construyendo una serie de aproximaciones **locales**. Cada modelo "d√©bil" que se entrena en una iteraci√≥n puede verse como una forma de **regresi√≥n ponderada localmente** (o clasificaci√≥n ponderada localmente), ya que se enfoca en una parte espec√≠fica del espacio de las caracter√≠sticas o en los datos con mayor error. El proceso iterativo de Boosting busca corregir estos errores localizados. Si los datos no se distribuyen linealmente, el boosting permite que el concepto de regresi√≥n (o clasificaci√≥n) se aplique de manera muy flexible y potente. La capacidad de concentrarse en los "errores" residuales aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Al ensamblar muchos modelos d√©biles que se adaptan a los errores de los anteriores, Boosting construye un modelo final que es una aproximaci√≥n de funci√≥n global altamente adaptable y precisa. Algoritmos como AdaBoost y Gradient Boosting Machines (GBM) son ejemplos prominentes de esta t√©cnica.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para algunas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones complejas mediante aprendizaje secuencial",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio para muchos algoritmos",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (puede ajustar demasiado a outliers si no se controla)",
  "‚úÖ Robusto (cada iteraci√≥n utiliza un subconjunto ponderado de datos)",
  "‚ö†Ô∏è Baja (modelo en su conjunto es tipo caja negra)",
  "‚ö†Ô∏è Lento con muchos √°rboles o altas iteraciones",
  "‚úÖ Recomendable con k-fold o repeated CV para ajustar tasa de aprendizaje y n√∫mero de iteraciones",
  "‚ùå Si se tienen pocos datos, alto ruido o target muy desbalanceado sin ajuste"
)

detalles <- c(
  "Ensamble supervisado que combina varios modelos d√©biles (ej. √°rboles peque√±os) de forma secuencial",
  "En clasificaci√≥n se usan votaciones ponderadas; en regresi√≥n se suman predicciones graduadas",
  "Acepta variables mixtas; algunas bibliotecas requieren convertir categ√≥ricas en dummies",
  "Construye modelos d√©biles en cada iteraci√≥n, enfoc√°ndose en muestras mal clasificadas o con alto residuo",
  "No exige distribuci√≥n de errores, ya que se basa en funci√≥n de p√©rdida sin supuestos param√©tricos",
  "Mejor si los ejemplos son independientes; puede usar t√©cnicas especiales para datos correlacionados",
  "No asume varianza constante, usa funci√≥n de p√©rdida directa para optimizar",
  "Los outliers pueden recibir peso excesivo en iteraciones posteriores, por lo que es necesario regularizar o usar robust loss",
  "La selecci√≥n de variables se hace impl√≠citamente, reduciendo el impacto de colinealidad",
  "Dif√≠cil de interpretar directamente; se pueden usar m√©tricas de importancia, SHAP o partial dependence para explicaci√≥n",
  "Cada iteraci√≥n entrena un modelo d√©bil, por lo que puede ser costoso si el n√∫mero de iteraciones es alto",
  "CV ayuda a determinar la tasa de aprendizaje (learning rate), n√∫mero de iteraciones y complejidad de base learners",
  "No es adecuado si hay muy pocos ejemplos, alta dimensionalidad con poco se√±al o target extremadamente desequilibrado"
)

tabla_boosting <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_boosting %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Boosting",
             subtitle = "Boosting") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Bootstrapped Aggregation (Bagging)  {-}    

**Bootstrapped Aggregation (Bagging)** es una t√©cnica de **aprendizaje conjunto (ensemble learning)** dise√±ada para mejorar la estabilidad y precisi√≥n de los algoritmos de aprendizaje autom√°tico, particularmente para reducir la **varianza** en los modelos. Fue introducida por Leo Breiman en 1996 y es la base de algoritmos muy populares como **Random Forest**. La idea fundamental de Bagging es entrenar m√∫ltiples versiones de un mismo modelo base en diferentes subconjuntos del conjunto de datos original y luego combinar sus predicciones.

El proceso central de Bagging implica dos pasos clave:

1.  **Muestreo Bootstrap:** En lugar de entrenar un √∫nico modelo en todo el conjunto de datos de entrenamiento, Bagging crea **m√∫ltiples conjuntos de datos de arranque (bootstrap samples)**. Cada muestra de arranque se crea seleccionando aleatoriamente, **con reemplazo**, un n√∫mero de observaciones igual al tama√±o del conjunto de datos original. Esto significa que algunos puntos de datos pueden aparecer varias veces en una muestra de arranque, mientras que otros pueden no aparecer en absoluto. Este muestreo aleatorio introduce diversidad entre los conjuntos de entrenamiento para cada modelo.

2.  **Agregaci√≥n (Aggregation):** Una vez que se han entrenado **m√∫ltiples modelos base independientes** (por ejemplo, √°rboles de decisi√≥n) en cada una de estas muestras de arranque, sus predicciones se combinan. Para tareas de **clasificaci√≥n**, la combinaci√≥n se realiza mediante **votaci√≥n por mayor√≠a** (la clase m√°s votada). Para tareas de **regresi√≥n**, las predicciones se promedian. Esta agregaci√≥n de predicciones de modelos diversos reduce la varianza y, por lo tanto, hace que el modelo final sea m√°s robusto y menos propenso al sobreajuste que un solo modelo entrenado en todo el conjunto de datos.

En el contexto del **aprendizaje global vs. local**, Bagging es una estrategia que combina las ventajas de los modelos de **aprendizaje local** para construir una **aproximaci√≥n de funci√≥n global** m√°s estable. Cada modelo base (ej. un √°rbol de decisi√≥n) que se entrena en una muestra de arranque puede considerarse un sistema de aprendizaje local, ya que toma decisiones basadas en el subconjunto de datos que le ha sido asignado. Sin embargo, al entrenar estos m√∫ltiples modelos en paralelo y luego agregarlos, Bagging construye un modelo final que es una aproximaci√≥n de funci√≥n global altamente adaptable. La ventaja principal es que, si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se puede aplicar eficazmente mediante esta forma de **regresi√≥n ponderada localmente** (donde los "pesos" son impl√≠citos a trav√©s de la agregaci√≥n de predicciones de modelos entrenados en subconjuntos aleatorios de datos). Bagging aborda el problema de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo al promediar o votar las predicciones de m√∫ltiples modelos, lo que reduce la varianza y mejora la generalizaci√≥n.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Captura relaciones no lineales al promediar m√∫ltiples modelos",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚úÖ Robusto (cada bootstrap reduce el impacto de outliers)",
  "‚úÖ Robusto (la agregaci√≥n mitiga colinealidad)",
  "‚ö†Ô∏è Moderada (dif√≠cil interpretar conjunto de modelos)",
  "‚ö†Ô∏è Moderado (depende del n√∫mero de √°rboles y tama√±o del dataset)",
  "‚úÖ Recomendable usar k-fold",
  "‚ùå No es ideal con muy pocos datos o si los base learners son demasiado simples"
)

detalles <- c(
  "Ensamble supervisado que ajusta varios modelos (usualmente √°rboles) sobre muestras bootstrap y promedia predicciones.",
  "En clasificaci√≥n predice la clase m√°s votada; en regresi√≥n, promedia los valores predichos.",
  "Acepta todo tipo de variables; las categ√≥ricas deben codificarse adecuadamente.",
  "Al promediar m√∫ltiples modelos, reduce varianza y captura no linealidades impl√≠citamente.",
  "No impone supuestos sobre la distribuci√≥n de errores.",
  "Los datos deben ser independientes; funciona peor en datos con fuerte autocorrelaci√≥n sin ajuste.",
  "No requiere varianza constante puesto que se basa en agregaci√≥n de m√∫ltiples predicciones.",
  "Cada muestra bootstrap y √°rbol es menos sensible a valores extremos; la agregaci√≥n aumenta robustez.",
  "La selecci√≥n aleatoria de subconjuntos y bootstrap reduce el efecto de predictores correlacionados.",
  "El modelo final es un conjunto de muchos √°rboles, lo que dificulta su explicaci√≥n directa.",
  "Entrenar cientos de √°rboles toma tiempo, pero es paralelizable; la predicci√≥n es relativamente r√°pida.",
  "CV ayuda a ajustar par√°metros como n√∫mero de √°rboles y profundidad m√°xima de cada √°rbol.",
  "Con pocos ejemplos, los bootstrap no aportan diversidad suficiente; si los base learners son muy simples, no capturan bien patrones complejos."
)

tabla_bagging <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bagging %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir bagging",
             subtitle = "Bootstrapped Aggregation (Bagging) ") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Extreme Gradient Boosting (XGBoost)  {-}    

**XGBoost (Extreme Gradient Boosting)** es una implementaci√≥n optimizada y altamente eficiente del algoritmo de **Gradient Boosting Machines (GBM)**, ampliamente reconocida por su **velocidad**, **rendimiento** y **escalabilidad** en problemas de **clasificaci√≥n** y **regresi√≥n**. Gan√≥ una inmensa popularidad debido a su √©xito en numerosas competiciones de *machine learning* (como Kaggle). Aunque se basa en los principios de GBM, XGBoost introduce varias mejoras clave que lo hacen superior en muchos escenarios.

La idea fundamental de XGBoost, al igual que GBM, es construir un modelo aditivo de forma **secuencial**, donde cada nuevo √°rbol intenta corregir los errores residuales del conjunto de √°rboles previos. Sin embargo, XGBoost optimiza este proceso con las siguientes caracter√≠sticas:

1.  **Paralelizaci√≥n:** Aunque el *boosting* es inherentemente secuencial, XGBoost permite la paralelizaci√≥n de la construcci√≥n de los √°rboles individuales. Por ejemplo, en el paso de b√∫squeda de la mejor divisi√≥n, puede evaluar las posibles divisiones en paralelo a trav√©s de m√∫ltiples n√∫cleos de CPU.
2.  **Regularizaci√≥n:** Incorpora t√©rminos de **regularizaci√≥n L1 (Lasso)** y **L2 (Ridge)** en la funci√≥n de costo para controlar la complejidad del modelo y evitar el sobreajuste. Esto es crucial para la generalizaci√≥n.
3.  **Manejo de Valores Faltantes:** Tiene una capacidad incorporada para manejar valores faltantes en los datos, permitiendo al algoritmo aprender la mejor direcci√≥n para los valores ausentes.
4.  **Poda por Profundidad (Depth-First Search):** A diferencia de muchos algoritmos de √°rboles que crecen nivel por nivel, XGBoost puede usar un enfoque de poda por profundidad, lo que a menudo resulta en √°rboles m√°s eficientes.
5.  **Cach√©-Aware Computing:** Optimiza el acceso a la memoria para manejar grandes conjuntos de datos de manera eficiente.
6.  **Flexibilidad de Funci√≥n de P√©rdida:** Permite el uso de funciones de p√©rdida personalizadas, lo que lo hace adaptable a una amplia gama de problemas.

En el contexto del **aprendizaje global vs. local**, XGBoost es una poderosa estrategia de **aprendizaje global** que se construye iterativamente a partir de componentes de **aprendizaje local**. Cada √°rbol de regresi√≥n (o clasificaci√≥n) individual es un "aprendiz d√©bil" que se enfoca en las deficiencias del modelo acumulado. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) de manera altamente sofisticada mediante esta **regresi√≥n ponderada localmente**. Al centrarse en los errores residuales y optimizar el proceso de manera rigurosa, XGBoost aborda de manera excepcional la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su combinaci√≥n de precisi√≥n, velocidad y capacidad para manejar grandes conjuntos de datos lo ha convertido en uno de los algoritmos m√°s populares y efectivos en la pr√°ctica del *machine learning*.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para dummies o label encoding)",
  "‚úÖ Captura no linealidades e interacciones complejas v√≠a √°rboles en boosting",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (puede sobreajustar a outliers si no regula)",
  "‚úÖ Robusto (reduce efecto de colinealidad al usar √°rboles secuenciales)",
  "‚ö†Ô∏è Baja (modelo complejo y tipo ‚Äôcaja negra‚Äô)",
  "‚úÖ Muy r√°pido y escalable (implementaci√≥n optimizada, paralelizable)",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajustar hiperpar√°metros",
  "‚ùå No es ideal con datos muy peque√±os, ruido alto o target extremadamente desbalanceado sin ajuste"
)

detalles <- c(
  "Ensamble supervisado que combina m√∫ltiples √°rboles d√©biles optimizados con gradiente descendente acelerado.",
  "En regresi√≥n predice valores continuos; en clasificaci√≥n combina probabilidades o clases mediante log-loss o multiclass objectives.",
  "Acepta variables mixtas; las categ√≥ricas deben convertirse a formatos compatibles (p. ej. factor numerico, one-hot encoding).",
  "Cada iteraci√≥n ajusta un nuevo √°rbol enfoc√°ndose en los residuos del modelo anterior, capturando patrones complejos.",
  "No impone distribuci√≥n param√©trica de errores, ya que optimiza funciones de p√©rdida directamente.",
  "Funciona mejor si cada muestra es independiente; sensible a series de tiempo sin preparaci√≥n apropiada.",
  "No requiere varianza constante, porque basa la optimizaci√≥n en gradientes del loss, no en supuestos de error.",
  "Los outliers dif√≠ciles de predecir pueden recibir demasiado peso en iteraciones sucesivas; usar _learning_rate_ bajo y _max_depth_ peque√±o para regular.",
  "Los √°rboles reducen el impacto de variables altamente correlacionadas, aunque m√∫ltiples iteraciones pueden a√∫n privilegiar caracter√≠sticas correlacionadas.",
  "Dif√≠cil interpretar directamente cada √°rbol; se utilizan m√©tricas de importancia, SHAP values o partial dependence plots para explicaci√≥n.",
  "Implementaci√≥n en C++ altamente optimizada (CPU/GPU), permite entrenamiento muy r√°pido incluso con millones de filas.",
  "Validaci√≥n cruzada anidada o simple ayuda a elegir hiperpar√°metros como `eta` (learning_rate), `nrounds` (n√∫mero de √°rboles), `max_depth`, `subsample`, `colsample_bytree`.",
  "No conviene con datasets muy peque√±os, ya que puede sobreajustar; tampoco si el ruido es muy alto y no se regula bien la complejidad."
)

tabla_xgboost <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_xgboost %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir xgboost",
             subtitle = "XGBoost")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Gradient Boosting Machines (GBM)  {-}   

**Gradient Boosting Machines (GBM)** es un algoritmo de **aprendizaje conjunto (ensemble learning)** extremadamente potente y vers√°til, utilizado para **clasificaci√≥n**, **regresi√≥n** y otras tareas predictivas. A diferencia de Random Forest que construye √°rboles de forma independiente en paralelo (bagging), GBM construye los √°rboles de forma **secuencial** y aditiva. La idea central es que cada nuevo √°rbol en el conjunto intenta corregir los errores residuales (residuos) del conjunto de √°rboles construidos previamente.

El concepto fundamental detr√°s de GBM es el **impulso (boosting)**, donde los modelos "d√©biles" (generalmente √°rboles de decisi√≥n, a menudo √°rboles poco profundos o "stumps") se combinan para formar un modelo "fuerte". GBM logra esto de una manera espec√≠fica:

1.  **Modelo Inicial:** Comienza con una predicci√≥n inicial para todos los datos (por ejemplo, el valor promedio para regresi√≥n o la probabilidad logar√≠tmica para clasificaci√≥n).
2.  **C√°lculo de Residuos (Pseudo-residuos):** En cada iteraci√≥n, el algoritmo calcula los **residuos** (o m√°s precisamente, los "pseudo-residuos" o gradientes negativos de la funci√≥n de p√©rdida) entre los valores reales y las predicciones actuales del modelo. Estos residuos representan los "errores" que el modelo actual no ha podido capturar.
3.  **Entrenamiento de un Nuevo √Årbol:** Se entrena un nuevo √°rbol de decisi√≥n para **predecir estos residuos**. Este √°rbol es t√≠picamente peque√±o y d√©bil, dise√±ado para centrarse en las √°reas donde el modelo actual tiene los mayores errores.
4.  **Actualizaci√≥n del Modelo:** La predicci√≥n de este nuevo √°rbol se a√±ade a la predicci√≥n acumulada del modelo existente, multiplicada por una **tasa de aprendizaje (learning rate)**. Esta tasa de aprendizaje controla el tama√±o del paso de cada √°rbol, evitando que el modelo se sobreajuste r√°pidamente.
5.  **Iteraci√≥n:** Este proceso se repite para un n√∫mero predefinido de iteraciones, o hasta que una m√©trica de rendimiento deje de mejorar. Cada nuevo √°rbol contribuye a reducir los errores restantes.

En el contexto del **aprendizaje global vs. local**, GBM es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol individual en el proceso de boosting es un sistema de aprendizaje local (como los *decision stumps* o √°rboles poco profundos) que se enfoca en una parte espec√≠fica del error. Sin embargo, la combinaci√≥n aditiva y secuencial de estos modelos "d√©biles" produce un modelo predictivo global altamente sofisticado y preciso. Si los datos no se distribuyen linealmente, GBM aplica el concepto de regresi√≥n (o clasificaci√≥n) mediante una forma incremental y adaptativa de **regresi√≥n ponderada localmente**. Al centrarse en los errores residuales, GBM aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su capacidad para minimizar la funci√≥n de p√©rdida de forma gradual y dirigida lo hace excepcionalmente eficaz para modelar relaciones complejas y no lineales, a menudo logrando un rendimiento superior en muchos problemas del mundo real.


```{r, echo = FALSE}

criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n)",
  "‚úÖ Captura no linealidades e interacciones complejas v√≠a boosting",
  "‚ùå No requiere",
  "‚úÖ Deseable pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (los outliers pueden influir en √°rboles individuales)",
  "‚úÖ Robusto (los arboles manejan colinealidad localmente)",
  "‚ö†Ô∏è Baja (modelo de tipo caja negra con varios √°rboles secuenciales)",
  "‚ö†Ô∏è Lento con muchos √°rboles, datos grandes o par√°metros altos",
  "‚úÖ Recomendable con k-fold o repeated CV para ajuste de hiperpar√°metros",
  "‚ùå Si se tienen pocos datos, muchas categor√≠as o ruido alto"
)

detalles <- c(
  "Ensamble de √°rboles secuenciales donde cada √°rbol corrige errores del anterior.",
  "En clasificaci√≥n se combinan probabilidades; en regresi√≥n se promedian predicciones.",
  "Funciona con muchas variables y aprende la importancia autom√°ticamente.",
  "Construye √°rboles d√©biles que se enfocan en los errores residuales previos.",
  "No impone supuestos en la distribuci√≥n de los errores.",
  "Los datos deben ser observaciones independientes; sensible a dependencias temporales.",
  "No requiere varianza constante puesto que se basa en √°rboles.",
  "Los outliers pueden exagerar los gradientes y forzar ajustes extremos en √°rboles individuales.",
  "Los √°rboles reducen impacto de colinealidad, pero m√∫ltiples √°rboles pueden still complicarla.",
  "Dif√≠cil de interpretar el conjunto; se pueden usar importance plots o SHAP para explicaci√≥n.",
  "La construcci√≥n secuencial de cientos de √°rboles puede ser costosa en tiempo y memoria.",
  "La validaci√≥n cruzada ayuda a determinar tasa de aprendizaje, n√∫mero de √°rboles y profundidad.",
  "No es ideal cuando se tienen muy pocos datos o categor√≠as con pocos ejemplos."
)

tabla_gbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_gbm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GBM",
             subtitle = "Gradient Boosting Machines (GBM)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## Gradient Boosted Regression Trees (GBRT)  {-}   

**Gradient Boosted Regression Trees (GBRT)**, a menudo conocida como **Gradient Boosting Machines (GBM)** cuando los modelos base son √°rboles de decisi√≥n de regresi√≥n, es una t√©cnica de **aprendizaje conjunto (ensemble learning)** extremadamente potente y ampliamente utilizada para tareas de **regresi√≥n** (predicci√≥n de valores num√©ricos continuos) y tambi√©n puede adaptarse para **clasificaci√≥n**. Su fortaleza radica en su capacidad para construir un modelo predictivo robusto y preciso mediante la combinaci√≥n secuencial de m√∫ltiples √°rboles de decisi√≥n "d√©biles".

La idea central de GBRT se basa en el principio de **boosting**, donde cada nuevo √°rbol en el conjunto se entrena para **corregir los errores residuales** (la diferencia entre los valores reales y las predicciones acumuladas del modelo hasta ese momento) de los √°rboles construidos en las iteraciones anteriores. Este proceso es iterativo y aditivo:

1.  **Modelo Inicial:** El proceso comienza con una predicci√≥n inicial simple para todos los datos, a menudo el valor promedio de la variable objetivo.
2.  **C√°lculo de Pseudo-Residuos:** En cada iteraci√≥n, GBRT calcula los "pseudo-residuos", que son los **gradientes negativos de la funci√≥n de p√©rdida** con respecto a la predicci√≥n actual. Para la p√©rdida cuadr√°tica media (com√∫n en regresi√≥n), estos pseudo-residuos son simplemente los errores tradicionales (valor real - predicci√≥n).
3.  **Entrenamiento de un √Årbol de Regresi√≥n:** Se entrena un nuevo **√°rbol de decisi√≥n de regresi√≥n** (que es un "aprendiz d√©bil", a menudo un √°rbol poco profundo o un *decision stump*) para **predecir estos pseudo-residuos**. El √°rbol busca los mejores puntos de divisi√≥n para reducir estos errores.
4.  **Actualizaci√≥n del Modelo:** La predicci√≥n de este nuevo √°rbol de regresi√≥n se a√±ade a la predicci√≥n acumulada del modelo existente, pero se escala por una **tasa de aprendizaje (learning rate)**. Esta tasa de aprendizaje es un hiperpar√°metro crucial que controla la "contribuci√≥n" de cada nuevo √°rbol y ayuda a prevenir el sobreajuste.
5.  **Iteraci√≥n:** Los pasos 2 a 4 se repiten para un n√∫mero predefinido de iteraciones. Cada nuevo √°rbol se enfoca en las deficiencias del modelo combinado anterior, refinando gradualmente la predicci√≥n.

En el contexto del **aprendizaje global vs. local**, GBRT es un sistema de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol de regresi√≥n individual es un sistema de aprendizaje local que divide el espacio de caracter√≠sticas y aprende patrones en subregiones. Sin embargo, el proceso de boosting, al combinar estos √°rboles secuencialmente para reducir los errores residuales globales, construye una **aproximaci√≥n de funci√≥n global** altamente flexible y precisa. La clave es que, si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n de manera muy efectiva a trav√©s de esta **regresi√≥n ponderada localmente**. Al centrarse en los errores que el modelo actual no puede explicar, GBRT aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Es excepcionalmente potente para capturar relaciones complejas y no lineales, y es ampliamente utilizado en diversas aplicaciones, desde la predicci√≥n de precios hasta la optimizaci√≥n de rutas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n para ciertas implementaciones)",
  "‚úÖ Captura no linealidades e interacciones complejas mediante boosting de √°rboles",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers pueden influir en √°rboles individuales)",
  "‚úÖ Robusto (los √°rboles reducen el impacto de colinealidad localmente)",
  "‚ö†Ô∏è Baja (modelo en su conjunto es ‚Äúcaja negra‚Äù)",
  "‚ö†Ô∏è Lento con muchos √°rboles o datos extensos",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajuste de hiperpar√°metros",
  "‚ùå No es ideal si hay muy pocos datos o ruido excesivo"
)

detalles <- c(
  "Ensamble de √°rboles de regresi√≥n secuenciales donde cada √°rbol corrige errores del anterior mediante gradiente.",
  "Predice valores continuos sumando las predicciones ponderadas de m√∫ltiples √°rboles d√©biles.",
  "Funciona con variables mixtas; las categ√≥ricas suelen transformarse en dummies.",
  "Cada nuevo √°rbol se enfoca en los residuos del modelo anterior, capturando patrones complejos.",
  "No impone distribuci√≥n normal porque optimiza una funci√≥n de p√©rdida (por ejemplo, MSE) directamente.",
  "Mejor si las observaciones no est√°n correlacionadas en el tiempo; ajustar para series si es necesario.",
  "No requiere varianza constante puesto que se basa en √°rboles, no en un modelo param√©trico de errores.",
  "Los valores extremos pueden provocar ajustes excesivos en √°rboles individuales; usar tasa de aprendizaje baja ayuda a mitigar.",
  "Los √°rboles reducen el impacto de variables correlacionadas, aunque m√∫ltiples iteraciones pueden complicar interpretaciones.",
  "Dif√≠cil de interpretar directamente; se puede usar importancia de variables o herramientas como SHAP para explicaci√≥n.",
  "Cada iteraci√≥n entrena un √°rbol nuevo; muchos √°rboles o gran profundidad de √°rbol incrementan el tiempo de entrenamiento.",
  "CV ayuda a determinar par√°metros como tasa de aprendizaje (`learning_rate`), n√∫mero de √°rboles (`n.trees`) y profundidad m√°xima (`max_depth`).",
  "No es adecuado cuando el dataset es muy peque√±o o extremadamente ruidoso, ya que puede sobreajustar f√°cilmente."
)

tabla_gbrt <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_gbrt %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GBRT",
             subtitle = "Gradient Boosted Regression Trees (GBRT)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Light Gradient Boosting Machine (LightGBM)  {-}   

**LightGBM (Light Gradient Boosting Machine)** es otro algoritmo de **Gradient Boosting Machines (GBM)** de alto rendimiento, desarrollado por Microsoft. Est√° dise√±ado para ser **extremadamente r√°pido** y **eficiente** en el uso de memoria, especialmente con grandes conjuntos de datos, sin sacrificar una precisi√≥n significativa. Al igual que XGBoost, ha ganado popularidad en competiciones de *machine learning* por su velocidad y capacidad para manejar grandes vol√∫menes de datos.

La idea fundamental de LightGBM es la misma que la de otros algoritmos de boosting: construir un modelo aditivo de forma **secuencial**, donde cada nuevo √°rbol intenta corregir los errores residuales del modelo combinado anterior. Sin embargo, LightGBM introduce varias optimizaciones clave para lograr su notable eficiencia:

1.  **Gradient-based One-Side Sampling (GOSS):** A diferencia de XGBoost que usa todas las instancias para cada iteraci√≥n, GOSS se enfoca en las instancias que tienen un **mayor gradiente** (es decir, las que contribuyen m√°s al error). Descarta las instancias con gradientes peque√±os o las muestrea con menos frecuencia, lo que acelera el entrenamiento sin perder demasiada precisi√≥n.
2.  **Exclusive Feature Bundling (EFB):** EFB agrupa caracter√≠sticas mutuamente exclusivas (es decir, caracter√≠sticas que rara vez toman valores distintos de cero al mismo tiempo) en un solo "bundle". Esto reduce el n√∫mero de caracter√≠sticas y acelera el c√°lculo del histograma sin afectar la precisi√≥n.
3.  **Histogram-based Algorithm:** En lugar de construir √°rboles en una forma de pre-orden que es com√∫n en muchos algoritmos (lo que puede ser lento al enumerar todos los puntos de divisi√≥n), LightGBM utiliza un **algoritmo basado en histogramas**. Convierte los valores de las caracter√≠sticas continuas en *bins* discretos. Esto acelera significativamente el proceso de b√∫squeda del mejor punto de divisi√≥n.
4.  **Leaf-wise (Best-first) Tree Growth:** A diferencia de la mayor√≠a de los √°rboles de decisi√≥n que crecen nivel por nivel (como en XGBoost), LightGBM crece el √°rbol **"hoja por hoja" (leaf-wise)**. Esto significa que en cada paso, selecciona la hoja con la mayor reducci√≥n de p√©rdida y la divide. Este enfoque puede llevar a √°rboles m√°s profundos y asim√©tricos que pueden ser m√°s precisos para el mismo n√∫mero de nodos, aunque puede ser m√°s propenso al sobreajuste (lo cual se mitiga con la regularizaci√≥n).

En el contexto del **aprendizaje global vs. local**, LightGBM, al igual que otros algoritmos de boosting, es una estrategia de **aprendizaje global** que se construye de manera iterativa a partir de componentes de **aprendizaje local**. Cada √°rbol que se entrena es un "aprendiz d√©bil" que se enfoca en las deficiencias residuales del modelo acumulado. Si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) de manera muy eficiente mediante esta **regresi√≥n ponderada localmente**. Al centrarse en los errores y optimizar los c√°lculos, LightGBM aborda de manera sobresaliente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Su √©nfasis en la velocidad y la eficiencia lo hace ideal para conjuntos de datos muy grandes o escenarios donde el tiempo de entrenamiento es una preocupaci√≥n cr√≠tica.  



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n apropiada)",
  "‚úÖ Captura no linealidades e interacciones mediante √°rboles en boosting",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias independientes)",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (los outliers pueden influir en pesos de hojas)",
  "‚úÖ Robusto (usa histogram-based split que aten√∫a colinealidad)",
  "‚ö†Ô∏è Baja (modelo complejo tipo ‚Äòcaja negra‚Äô)",
  "‚úÖ Muy r√°pido y escalable (optimized gradient-based)",
  "‚úÖ Recomendable usar k-fold o repeated CV para ajustar hiperpar√°metros",
  "‚ùå No conviene con datos muy peque√±os o muy ruidosos sin regularizaci√≥n"
)

detalles <- c(
  "Ensamble supervisado que entrenan √°rboles de decisi√≥n usando histogram-based gradient boosting.",
  "En regresi√≥n predice valores continuos; en clasificaci√≥n maximiza log-loss u otras funciones objetivo.",
  "Acepta variables mixtas; las categ√≥ricas deben convertirse a formato num√©rico o usar encoding interno.",
  "Cada iteraci√≥n ajusta un √°rbol enfoc√°ndose en los residuos del anterior, capturando patrones complejos.",
  "No impone distribuci√≥n param√©trica de errores; optimiza la funci√≥n de p√©rdida directamente.",
  "Funciona mejor si las muestras son independientes; sensible a series de tiempo sin preparaci√≥n adecuada.",
  "No requiere varianza constante, dado que es un m√©todo basado en √°rbol, no en supuestos de error.",
  "Los valores extremos pueden afectar el c√°lculo de gradientes y splits; usar regularizaci√≥n y par√°metros de manejo de outliers.",
  "La divisi√≥n basada en histogramas reduce el impacto de predictores altamente correlacionados.",
  "Dif√≠cil interpretar cada √°rbol; se utilizan m√©tricas de importancia y herramientas como SHAP para explicaci√≥n.",
  "Implementaci√≥n en C++ altamente optimizada que permite entrenamiento muy r√°pido incluso con grandes vol√∫menes de datos.",
  "CV ayuda a elegir par√°metros como `learning_rate`, `num_leaves`, `max_depth`, `feature_fraction`, `bagging_fraction`.",
  "No es ideal si el dataset es muy peque√±o, pues el boosting puede sobreajustar; tampoco con mucho ruido sin regularizaci√≥n adecuada."
)

tabla_lightgbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lightgbm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LightGBM",
             subtitle = "LightGBM")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Random Forest  {-}   

**Random Forest** es un algoritmo de **aprendizaje conjunto (ensemble learning)** altamente popular y potente, utilizado tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**. Fue desarrollado por Leo Breiman en 2001 y se basa en la idea de combinar las predicciones de m√∫ltiples **√°rboles de decisi√≥n** para lograr una mayor precisi√≥n y robustez que un solo √°rbol. La fuerza de Random Forest reside en dos conceptos clave: **bagging (bootstrap aggregation)** y la **aleatoriedad en la selecci√≥n de caracter√≠sticas**.

La idea fundamental detr√°s de Random Forest es construir un "bosque" de √°rboles de decisi√≥n de una manera espec√≠fica:

1.  **Bagging (Bootstrap Aggregation):** En lugar de entrenar un solo √°rbol en todo el conjunto de datos, Random Forest entrena cada √°rbol en una **muestra de arranque (bootstrap sample)** diferente. Una muestra de arranque es un subconjunto del conjunto de datos original, muestreado con reemplazo. Esto significa que algunos puntos de datos pueden aparecer varias veces en una muestra, mientras que otros pueden no aparecer en absoluto. Este muestreo genera diversidad entre los √°rboles.

2.  **Aleatoriedad en la Selecci√≥n de Caracter√≠sticas:** Cuando cada √°rbol se construye, en cada paso de divisi√≥n (nodo), Random Forest no considera todas las caracter√≠sticas disponibles. En cambio, solo considera un **subconjunto aleatorio de caracter√≠sticas** para encontrar la mejor divisi√≥n. Esta aleatoriedad adicional (adem√°s del muestreo de arranque) descorrelaciona a√∫n m√°s los √°rboles, lo que es crucial para el rendimiento del algoritmo. Si los √°rboles estuvieran altamente correlacionados, el error de un √°rbol promedio no se reducir√≠a al promediar.

Una vez que se han construido numerosos √°rboles (t√≠picamente cientos o miles), las predicciones se combinan: para **clasificaci√≥n**, se utiliza la **votaci√≥n por mayor√≠a** (la clase m√°s votada por los √°rboles individuales); para **regresi√≥n**, se calcula el **promedio** de las predicciones de todos los √°rboles.

En el contexto del **aprendizaje global vs. local**, Random Forest se puede considerar como un sistema de **aprendizaje global** que se construye a partir de componentes de **aprendizaje local**. Cada √°rbol individual en el bosque es un sistema de aprendizaje local (como CART, que divide el problema en subproblemas m√°s peque√±os). Sin embargo, al combinar las predicciones de muchos de estos √°rboles, Random Forest logra una **aproximaci√≥n de funci√≥n global** muy robusta y flexible. La ventaja es que, si los datos no se distribuyen linealmente, el algoritmo aplica el concepto de regresi√≥n (o clasificaci√≥n) mediante una forma sofisticada de **regresi√≥n ponderada localmente**. La combinaci√≥n de √°rboles diversos y descorrelacionados mitiga la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Random Forest sobresale en capturar relaciones complejas y no lineales, manejar grandes conjuntos de datos con muchas caracter√≠sticas y es menos propenso al sobreajuste que un solo √°rbol de decisi√≥n grande.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y categ√≥ricas (requiere codificaci√≥n)",
  "‚úÖ Captura relaciones no lineales e interacciones complejas",
  "‚ùå No requiere",
  "‚úÖ Deseable pero no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚úÖ Robusto a outliers (por agregaci√≥n)",
  "‚úÖ Robusto (selecciona subconjuntos aleatorios)",
  "‚ö†Ô∏è Moderada (dif√≠cil interpretar cientos de √°rboles)",
  "‚ö†Ô∏è Lento con muchos √°rboles o datos grandes",
  "‚úÖ Recomendado usar k-fold",
  "‚ùå Puede sobreajustar si no se ajustan hiperpar√°metros (e.g. profundidad, n√∫mero de √°rboles)"
)

detalles <- c(
  "Ensamble de √°rboles de decisi√≥n, cada uno entrenado en una muestra bootstrap y usando un subconjunto aleatorio de predictores.",
  "En clasificaci√≥n predice la clase mayoritaria entre √°rboles; en regresi√≥n, el promedio de predicciones.",
  "Acepta muchas variables y selecciona autom√°ticamente las m√°s relevantes por importancia.",
  "Al generar m√∫ltiples √°rboles, capta interacciones no lineales sin necesidad de especificarlas.",
  "No hay supuestos sobre la distribuci√≥n de los errores.",
  "Los √°rboles individuales pueden manejar correlaci√≥n leve; el ensamble mitiga la dependencia.",
  "No necesita homogeneidad de varianza en los errores residuales.",
  "Cada √°rbol es poco sensible a outliers, y la agregaci√≥n mejora robustez.",
  "Reduce el problema de colinealidad al seleccionar subconjuntos de variables por √°rbol.",
  "Es dif√≠cil de explicar, aunque se pueden usar m√©tricas de importancia de variables.",
  "Puede volverse lento si se entrenan miles de √°rboles en datasets muy grandes.",
  "Cross-validation ayuda a evitar overfitting y evaluar generalizaci√≥n.",
  "No es ideal si el interpretabilidad es cr√≠tica o el tiempo computacional es limitado."
)

tabla_rf <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rf %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir random forest",
             subtitle = "Random Forest") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Stacked Generlization (Blending) {-}   

**Stacked Generalization**, com√∫nmente conocido como **Stacking**, y su variante **Blending**, son t√©cnicas avanzadas de **aprendizaje conjunto (ensemble learning)** que buscan combinar las predicciones de m√∫ltiples modelos de aprendizaje autom√°tico para obtener un rendimiento predictivo superior al de cualquier modelo individual. La idea fundamental es que, en lugar de simplemente promediar o votar las predicciones, se entrena un **modelo de segundo nivel (meta-modelo)** para aprender a combinar √≥ptimamente las predicciones de los modelos de primer nivel (modelos base).

El proceso de Stacking generalmente implica dos o m√°s "capas" de modelos:

1.  **Modelos Base (Nivel 0):** En la primera capa, se entrenan m√∫ltiples modelos de aprendizaje autom√°tico diversos (pueden ser de diferentes tipos, como √°rboles de decisi√≥n, m√°quinas de vectores de soporte, redes neuronales, etc.). Estos modelos base se entrenan sobre el conjunto de datos de entrenamiento original (o en particiones del mismo).

2.  **Generaci√≥n de Meta-Caracter√≠sticas:** Las predicciones generadas por estos modelos base sobre un conjunto de datos "fuera de muestra" (que no se us√≥ para entrenar los modelos base, t√≠picamente a trav√©s de validaci√≥n cruzada k-fold) se utilizan como **nuevas caracter√≠sticas** o "meta-caracter√≠sticas". Estas meta-caracter√≠sticas, junto con la variable objetivo original, forman un nuevo conjunto de datos de entrenamiento para el meta-modelo.

3.  **Meta-Modelo (Nivel 1):** En la segunda capa, se entrena un **meta-modelo** (a menudo un modelo m√°s simple, como regresi√≥n lineal, regresi√≥n log√≠stica o un √°rbol de decisi√≥n poco profundo) utilizando estas meta-caracter√≠sticas como entrada y la variable objetivo original como salida. El meta-modelo aprende la relaci√≥n entre las predicciones de los modelos base y la respuesta verdadera, y por lo tanto, c√≥mo "pesar" o "combinar" esas predicciones de la mejor manera.

**Blending** es una variaci√≥n m√°s sencilla de Stacking. La principal diferencia es c√≥mo se generan las meta-caracter√≠sticas para el meta-modelo. En Blending, se reserva una **subdivisi√≥n de validaci√≥n (holdout set)** del conjunto de entrenamiento original. Los modelos base se entrenan en la parte restante del conjunto de entrenamiento, y luego sus predicciones sobre este conjunto de validaci√≥n se utilizan directamente como meta-caracter√≠sticas para entrenar el meta-modelo. Esto simplifica el proceso de validaci√≥n cruzada, pero el meta-modelo se entrena con menos datos.

En el contexto del **aprendizaje global vs. local**, Stacking/Blending es una estrategia de **aprendizaje global** que explota el poder de m√∫ltiples **aproximaciones de funci√≥n local** (los modelos base) para construir un modelo final altamente sofisticado. Cada modelo base, dependiendo de su naturaleza, puede ser un sistema de aprendizaje local que descubre patrones en subregiones de datos. Sin embargo, el meta-modelo aprende una funci√≥n de combinaci√≥n global sobre las predicciones de estos modelos base. Si los datos no se distribuyen linealmente, Stacking/Blending aplica el concepto de regresi√≥n (o clasificaci√≥n) de una manera muy flexible. Al permitir que un modelo de segundo nivel aprenda a combinar las predicciones de diversos modelos, supera la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo. Es particularmente eficaz en competiciones de machine learning donde se busca el m√°ximo rendimiento, ya que aprovecha las fortalezas complementarias de diferentes algoritmos. Sin embargo, puede ser computacionalmente intensivo y m√°s dif√≠cil de interpretar que los modelos individuales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua (depende de los modelos base)",
  "‚úÖ Num√©ricas y/o categ√≥ricas (codificaci√≥n seg√∫n modelos base)",
  "‚úÖ Captura relaciones complejas v√≠a combinaci√≥n de modelos base",
  "‚ùå No exige supuestos de normalidad en residuos",
  "‚úÖ Deseable, pero no obligatorio (mejor si observaciones independientes)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (outliers afectan modelos base individuales)",
  "‚ö†Ô∏è Puede verse afectado (depende de base learners y correlated features)",
  "‚ö†Ô∏è Baja (modelo meta dif√≠cil de interpretar directamente)",
  "‚ö†Ô∏è Lento en entrenamiento y predicci√≥n, seg√∫n n√∫mero de base learners",
  "‚úÖ Esencial (usar CV anidada para entrenar meta-modelo)",
  "‚ùå Si datos muy escasos o muy ruidosos, riesgo de sobreajuste"
)

detalles <- c(
  "Ensamble supervisado que combina varias predicciones (base learners) mediante un modelo meta.",
  "El meta-modelo acepta la salida de modelos base; puede predecir clases o valores continuos.",
  "Usa predictores originales para los base learners; algunos requieren dummies, otros no.",
  "Aprende patrones no lineales e interacciones complejas a trav√©s de m√∫ltiples capas.",
  "No impone distribuci√≥n normal: cada base learner tiene sus propios supuestos.",
  "Ideal si cada muestra es independiente; sensibles a dependencias en validaciones cruzadas.",
  "No requiere varianza constante, ya que se basa en agregaci√≥n de predicciones.",
  "Modelos base (p. ej. ARBOTS, SVM) pueden verse influenciados por valores extremos;",
  "Modelos base diversificados reducen colinealidad, pero meta-modelo puede verse afectado.",
  "Dif√≠cil atribuir importancia directa; se pueden usar t√©cnicas como SHAP para interpretaci√≥n.",
  "Entrenamiento de m√∫ltiples base learners y meta-modelo incrementa tiempo; predicci√≥n tambi√©n m√°s lenta.",
  "Usar validaci√≥n cruzada anidada: inner folds para entrenar base learners y stacking, outer folds para evaluar.",
  "No recomendable si hay muy pocos datos (stacking requiere dividir en folds) o si los base learners no aportan diversidad."
)

tabla_stacking <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_stacking %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir blending",
             subtitle = "Stacked Generlizaation (Blending)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```






