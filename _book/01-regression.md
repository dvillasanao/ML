#  **1. Regressi贸n** {-}

**Ejemplos:** Linear Regression, Ridge, Lasso   
**Cu谩ndo usarlo:**

* Predicci贸n de valores num茅ricos continuos (e.g. precios, temperaturas).  
* Relaciones lineales entre variables. 

**Ventajas:** Simple, interpretable.   
**Limitaciones:** Mal desempe帽o con relaciones no lineales complejas.

## Ordinary Least Squares Regression (`OLSR`) {-} 

**Regresi贸n de M铆nimos Cuadrados Ordinarios (OLSR)**: un m茅todo de regresi贸n lineal para estimar los par谩metros desconocidos mediante la creaci贸n de un modelo que minimizar谩 la suma de los errores cuadrados entre los datos observados y los predichos (valores observados y valores estimados).  

## Linear Regression {-} 

**Regresi贸n lineal** : se utiliza para estimar valores reales (costo de las casas, n煤mero de visitas, ventas totales, etc.) basados en variables continuas.

## Logistic Regression {-} 

**Regresi贸n log铆stica** : se utiliza para estimar valores discretos (valores binarios como 0/1, s铆/no, verdadero/falso) basados en un conjunto dado de variables independientes.

## Stepwise Regression {-} 

**Regresi贸n por pasos** : a帽ade caracter铆sticas al modelo una a una hasta encontrar la puntuaci贸n 贸ptima para tu conjunto de caracter铆sticas. La selecci贸n por pasos alterna entre el avance y el retroceso, incorporando y eliminando variables que cumplen los criterios de entrada o eliminaci贸n, hasta alcanzar un conjunto estable de variables.  

## Multivariate Adaptive Regression Splines (`MARS`) {-} 

**Splines de Regresi贸n Adaptativa Multivariante (`MARS`)**: un m茅todo de regresi贸n flexible que busca interacciones y relaciones no lineales que ayudan a maximizar la precisi贸n predictiva. Este algoritmo es inherentemente no lineal (lo que significa que no es necesario adaptar el modelo a patrones no lineales en el datos agregando manualmente t茅rminos del modelo (squared terms, interaction effects).   

## Locally Estimated Scatterplot Smoothing (`LOESS`) {-} 

**Locally Estimated Scatterplot Smoothing (`LOESS`)**: un m茅todo para ajustar una curva suave entre dos variables o una superficie  entre un resultado y hasta cuatro variables predictoras. La idea es que, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi贸n. Se puede aplicar regresi贸n, lo que se denomina regresi贸n ponderada localmente. Se puede aplicar LOESS cuando la relaci贸n entre las variables independientes y dependientes no es lineal. Hoy en d铆a, la mayor铆a de los algoritmos (como las redes neuronales de propagaci贸n hacia adelante cl谩sicas, las m谩quinas de vectores de soporte, los algoritmos del vecino m谩s cercano, etc.) son sistemas de aprendizaje global que se utilizan para minimizar las funciones de p茅rdida globales (por ejemplo, el error cuadr谩tico medio). Por el contrario, los sistemas de aprendizaje local dividen el problema de aprendizaje global en m煤ltiples problemas de aprendizaje m谩s peque帽os y simples. Esto generalmente se logra dividiendo la funci贸n de costo en m煤ltiples funciones de costo locales independientes. Una de las desventajas de los m茅todos globales es que, a veces, ning煤n valor de par谩metro puede proporcionar una aproximaci贸n suficientemente buena. Pero entonces surge LOESS, una alternativa a la aproximaci贸n de funciones globales.   


## Regression Ridge {-} 

**Regresi贸n Ridge** es una extensi贸n de la regresi贸n lineal cl谩sica (`OLS`) que se usa cuando hay problemas de multicolinealidad o riesgo de sobreajuste. Aborda estos problemas introduciendo un t茅rmino de penalizaci贸n a la funci贸n de coste de la regresi贸n lineal ordinaria (m铆nimos cuadrados ordinarios, OLS).  

## Least Absolute Shrinkage and Selection Operator (`LASSO`) {-}

**Least Absolute Shrinkage and Selection Operator (`LASSO`)**: es otra t茅cnica de regularizaci贸n utilizada en modelos de regresi贸n lineal, similar a la Regresi贸n Ridge, pero con una diferencia clave en el tipo de penalizaci贸n que aplica en la funci贸n de coste de la regresi贸n lineal ordinaria.    


---
