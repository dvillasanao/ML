[{"path":"index.html","id":"machine-learning","chapter":"Machine Learning","heading":"Machine Learning","text":"","code":""},{"path":"index.html","id":"cuadro","chapter":"Machine Learning","heading":"0.1 ğŸ“Œ Cuadro","text":"","code":""},{"path":"regressiÃ³n.html","id":"regressiÃ³n","chapter":"ğŸ” 1. RegressiÃ³n","heading":"ğŸ” 1. RegressiÃ³n","text":"Ejemplos: Linear Regression, Ridge, LassoCuÃ¡ndo usarlo:PredicciÃ³n de valores numÃ©ricos continuos (e.g.Â precios, temperaturas).Relaciones lineales entre variables.Ventajas: Simple, interpretable.Limitaciones: Mal desempeÃ±o con relaciones lineales complejas.","code":""},{"path":"regressiÃ³n.html","id":"ordinary-least-squares-regression-olsr","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Ordinary Least Squares Regression (OLSR)","text":"RegresiÃ³n de MÃ­nimos Cuadrados Ordinarios (OLSR): un mÃ©todo de regresiÃ³n lineal para estimar los parÃ¡metros desconocidos mediante la creaciÃ³n de un modelo que minimizarÃ¡ la suma de los errores cuadrados entre los datos observados y los predichos (valores observados y valores estimados).","code":""},{"path":"regressiÃ³n.html","id":"linear-regression","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Linear Regression","text":"RegresiÃ³n lineal : se utiliza para estimar valores reales (costo de las casas, nÃºmero de visitas, ventas totales, etc.) basados en variables continuas.","code":""},{"path":"regressiÃ³n.html","id":"logistic-regression","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Logistic Regression","text":"RegresiÃ³n logÃ­stica : se utiliza para estimar valores discretos (valores binarios como 0/1, sÃ­/, verdadero/falso) basados en un conjunto dado de variables independientes.","code":""},{"path":"regressiÃ³n.html","id":"stepwise-regression","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Stepwise Regression","text":"RegresiÃ³n por pasos : aÃ±ade caracterÃ­sticas al modelo una una hasta encontrar la puntuaciÃ³n Ã³ptima para tu conjunto de caracterÃ­sticas. La selecciÃ³n por pasos alterna entre el avance y el retroceso, incorporando y eliminando variables que cumplen los criterios de entrada o eliminaciÃ³n, hasta alcanzar un conjunto estable de variables.","code":""},{"path":"regressiÃ³n.html","id":"multivariate-adaptive-regression-splines-mars","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Multivariate Adaptive Regression Splines (MARS)","text":"Splines de RegresiÃ³n Adaptativa Multivariante (MARS): un mÃ©todo de regresiÃ³n flexible que busca interacciones y relaciones lineales que ayudan maximizar la precisiÃ³n predictiva. Este algoritmo es inherentemente lineal (lo que significa que es necesario adaptar el modelo patrones lineales en el datos agregando manualmente tÃ©rminos del modelo (squared terms, interaction effects).","code":""},{"path":"regressiÃ³n.html","id":"locally-estimated-scatterplot-smoothing-loess","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Locally Estimated Scatterplot Smoothing (LOESS)","text":"Locally Estimated Scatterplot Smoothing (LOESS): un mÃ©todo para ajustar una curva suave entre dos variables o una superficie entre un resultado y hasta cuatro variables predictoras. La idea es que, si los datos se distribuyen linealmente, se puede aplicar el concepto de regresiÃ³n. Se puede aplicar regresiÃ³n, lo que se denomina regresiÃ³n ponderada localmente. Se puede aplicar LOESS cuando la relaciÃ³n entre las variables independientes y dependientes es lineal. Hoy en dÃ­a, la mayorÃ­a de los algoritmos (como las redes neuronales de propagaciÃ³n hacia adelante clÃ¡sicas, las mÃ¡quinas de vectores de soporte, los algoritmos del vecino mÃ¡s cercano, etc.) son sistemas de aprendizaje global que se utilizan para minimizar las funciones de pÃ©rdida globales (por ejemplo, el error cuadrÃ¡tico medio). Por el contrario, los sistemas de aprendizaje local dividen el problema de aprendizaje global en mÃºltiples problemas de aprendizaje mÃ¡s pequeÃ±os y simples. Esto generalmente se logra dividiendo la funciÃ³n de costo en mÃºltiples funciones de costo locales independientes. Una de las desventajas de los mÃ©todos globales es que, veces, ningÃºn valor de parÃ¡metro puede proporcionar una aproximaciÃ³n suficientemente buena. Pero entonces surge LOESS, una alternativa la aproximaciÃ³n de funciones globales.","code":""},{"path":"regressiÃ³n.html","id":"regression-ridge","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Regression Ridge","text":"RegresiÃ³n Ridge es una extensiÃ³n de la regresiÃ³n lineal clÃ¡sica (OLS) que se usa cuando hay problemas de multicolinealidad o riesgo de sobreajuste. Aborda estos problemas introduciendo un tÃ©rmino de penalizaciÃ³n la funciÃ³n de coste de la regresiÃ³n lineal ordinaria (mÃ­nimos cuadrados ordinarios, OLS).","code":""},{"path":"regressiÃ³n.html","id":"least-absolute-shrinkage-and-selection-operator-lasso","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Least Absolute Shrinkage and Selection Operator (LASSO)","text":"Least Absolute Shrinkage Selection Operator (LASSO): es otra tÃ©cnica de regularizaciÃ³n utilizada en modelos de regresiÃ³n lineal, similar la RegresiÃ³n Ridge, pero con una diferencia clave en el tipo de penalizaciÃ³n que aplica en la funciÃ³n de coste de la regresiÃ³n lineal ordinaria.","code":""},{"path":"Ã¡rboles-de-decisiÃ³n-y-derivados.html","id":"Ã¡rboles-de-decisiÃ³n-y-derivados","chapter":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","heading":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","text":"Ejemplos: Decision Tree, Random Forest, Gradient BoostingCuÃ¡ndo usarlo:Problemas tabulares con relaciones lineales y variables categÃ³ricas o numÃ©ricas.Cuando interpretabilidad es importante.Ventajas: Manejan datos heterogÃ©neos, fÃ¡ciles de interpretar (Ã¡rboles simples).Limitaciones: Sobreajuste en Ã¡rboles simples; menor desempeÃ±o en datos muy ruidosos sin ensambles.","code":""},{"path":"ensambles-ensemble-methods.html","id":"ensambles-ensemble-methods","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","text":"Ejemplos: Random Forest, AdaBoost, XGBoost, LightGBMCuÃ¡ndo usarlo:Cuando buscas alto rendimiento en clasificaciÃ³n o regresiÃ³n tabular.Competencias de datos (como Kaggle).Ventajas: Alta precisiÃ³n, robustez.Limitaciones: DifÃ­cil de interpretar; mÃ¡s costosos computacionalmente.","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"redes-neuronales-y-deep-learning","chapter":"ğŸ§  4. Redes Neuronales y Deep Learning","heading":"ğŸ§  4. Redes Neuronales y Deep Learning","text":"Ejemplos: MLP, CNN, RNN, TransformersCuÃ¡ndo usarlo:ImÃ¡genes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).Grandes volÃºmenes de datos estructurados.Ventajas: Muy poderosos para datos complejos y estructurados.Limitaciones: Requieren mucha data y poder computacional. Menor interpretabilidad.","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"reducciÃ³n-de-dimensionalidad","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","text":"Ejemplos: PCA, t-SNE, UMAPCuÃ¡ndo usarlo:VisualizaciÃ³n de datos de alta dimensiÃ³n.Preprocesamiento para eliminar ruido o multicolinealidad.Ventajas: Mejora desempeÃ±o y velocidad de otros modelos.Limitaciones: Puede perder interpretabilidad; siempre mejora modelos.","code":""},{"path":"bayesianos.html","id":"bayesianos","chapter":"ğŸ§¬ 6. Bayesianos","heading":"ğŸ§¬ 6. Bayesianos","text":"Ejemplos: Naive Bayes, Bayesian NetworksCuÃ¡ndo usarlo:ClasificaciÃ³n rÃ¡pida con supuestos simples.Problemas de texto o spam detection.Ventajas: Muy rÃ¡pidos, bien fundamentados.Limitaciones: Supone independencia de variables (siempre cierto).","code":""},{"path":"regularizaciÃ³n.html","id":"regularizaciÃ³n","chapter":"ğŸ§® 7. RegularizaciÃ³n","heading":"ğŸ§® 7. RegularizaciÃ³n","text":"Ejemplos: L1 (Lasso), L2 (Ridge), Elastic NetCuÃ¡ndo usarlo:Para evitar sobreajuste en modelos lineales o redes neuronales.Cuando tienes muchas variables (alta dimensionalidad).Ventajas: Penaliza modelos complejos.Limitaciones: Puede eliminar variables Ãºtiles si se usa en exceso.","code":""},{"path":"instance-based-basados-en-instancias.html","id":"instance-based-basados-en-instancias","chapter":"ğŸ” 8. Instance-Based (Basados en Instancias)","heading":"ğŸ” 8. Instance-Based (Basados en Instancias)","text":"Ejemplos: K-Nearest Neighbors (KNN)CuÃ¡ndo usarlo:Pocos datos, con patrones locales claros.Cuando la similitud entre casos es importante.Ventajas: Simple y eficaz en problemas de baja dimensiÃ³n.Limitaciones: Escala mal con muchos datos; sensible al ruido.","code":""},{"path":"clustering-no-supervisado.html","id":"clustering-no-supervisado","chapter":"ğŸ“ 9. Clustering (No Supervisado)","heading":"ğŸ“ 9. Clustering (No Supervisado)","text":"Ejemplos: K-Means, DBSCAN, Hierarchical ClusteringCuÃ¡ndo usarlo:Agrupar datos sin etiquetas previas.Descubrir estructuras ocultas o segmentos de mercado.Ventajas: Ãštil en exploraciÃ³n y reducciÃ³n de complejidad.Limitaciones: Requiere elegir nÃºmero de grupos (excepto DBSCAN); puede ser sensible escala.","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"sistemas-basados-en-reglas-rule-based-systems","chapter":"ğŸ“ 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"ğŸ“ 10. Sistemas Basados en Reglas (Rule-Based Systems)","text":"Ejemplos: RuleFit, Decision Rules, lÃ³gica difusa\nCuÃ¡ndo usarlo:Interpretabilidad es clave (por ejemplo, decisiones legales o mÃ©dicas).Incorporar conocimiento experto.Ventajas: FÃ¡cil de entender y auditar.Limitaciones: tan precisos como otros mÃ©todos en datos complejos.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Sagi, S. (2019). ML Algorithms: One SD (Ïƒ). obvious questions ask â€¦ | Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6Kuhn, M. (2019). caret Package. https://topepo.github.io/caret/index.html","code":""}]
