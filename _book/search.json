[{"path":"index.html","id":"machine-learning","chapter":"Machine Learning","heading":"Machine Learning","text":"","code":""},{"path":"index.html","id":"regressión","chapter":"Machine Learning","heading":"🔍 1. Regressión","text":"Ejemplos: Linear Regression, Ridge, LassoCuándo usarlo:Predicción de valores numéricos continuos (e.g. precios, temperaturas).Relaciones lineales entre variables.Ventajas: Simple, interpretable.Limitaciones: Mal desempeño con relaciones lineales complejas.","code":""},{"path":"index.html","id":"ordinary-least-squares-regression-olsr","chapter":"Machine Learning","heading":"Ordinary Least Squares Regression (OLSR)","text":"Regresión de Mínimos Cuadrados Ordinarios (OLSR): un método de regresión lineal para estimar los parámetros desconocidos mediante la creación de un modelo que minimizará la suma de los errores cuadrados entre los datos observados y los predichos (valores observados y valores estimados).","code":""},{"path":"index.html","id":"linear-regression","chapter":"Machine Learning","heading":"Linear Regression","text":"Regresión lineal : se utiliza para estimar valores reales (costo de las casas, número de visitas, ventas totales, etc.) basados en variables continuas.","code":""},{"path":"index.html","id":"logistic-regression","chapter":"Machine Learning","heading":"Logistic Regression","text":"Regresión logística : se utiliza para estimar valores discretos (valores binarios como 0/1, sí/, verdadero/falso) basados en un conjunto dado de variables independientes.","code":""},{"path":"index.html","id":"stepwise-regression","chapter":"Machine Learning","heading":"Stepwise Regression","text":"Regresión por pasos : añade características al modelo una una hasta encontrar la puntuación óptima para tu conjunto de características. La selección por pasos alterna entre el avance y el retroceso, incorporando y eliminando variables que cumplen los criterios de entrada o eliminación, hasta alcanzar un conjunto estable de variables.","code":""},{"path":"index.html","id":"multivariate-adaptive-regression-splines-mars","chapter":"Machine Learning","heading":"Multivariate Adaptive Regression Splines (MARS)","text":"Splines de Regresión Adaptativa Multivariante (MARS): un método de regresión flexible que busca interacciones y relaciones lineales que ayudan maximizar la precisión predictiva. Este algoritmo es inherentemente lineal (lo que significa que es necesario adaptar el modelo patrones lineales en el datos agregando manualmente términos del modelo (squared terms, interaction effects).","code":""},{"path":"index.html","id":"locally-estimated-scatterplot-smoothing-loess","chapter":"Machine Learning","heading":"Locally Estimated Scatterplot Smoothing (LOESS)","text":"Locally Estimated Scatterplot Smoothing (LOESS): un método para ajustar una curva suave entre dos variables o una superficie entre un resultado y hasta cuatro variables predictoras. La idea es que, si los datos se distribuyen linealmente, se puede aplicar el concepto de regresión. Se puede aplicar regresión, lo que se denomina regresión ponderada localmente. Se puede aplicar LOESS cuando la relación entre las variables independientes y dependientes es lineal. Hoy en día, la mayoría de los algoritmos (como las redes neuronales de propagación hacia adelante clásicas, las máquinas de vectores de soporte, los algoritmos del vecino más cercano, etc.) son sistemas de aprendizaje global que se utilizan para minimizar las funciones de pérdida globales (por ejemplo, el error cuadrático medio). Por el contrario, los sistemas de aprendizaje local dividen el problema de aprendizaje global en múltiples problemas de aprendizaje más pequeños y simples. Esto generalmente se logra dividiendo la función de costo en múltiples funciones de costo locales independientes. Una de las desventajas de los métodos globales es que, veces, ningún valor de parámetro puede proporcionar una aproximación suficientemente buena. Pero entonces surge LOESS, una alternativa la aproximación de funciones globales.","code":""},{"path":"index.html","id":"regression-ridge","chapter":"Machine Learning","heading":"Regression Ridge","text":"Regresión Ridge es una extensión de la regresión lineal clásica (OLS) que se usa cuando hay problemas de multicolinealidad o riesgo de sobreajuste. Aborda estos problemas introduciendo un término de penalización la función de coste de la regresión lineal ordinaria (mínimos cuadrados ordinarios, OLS).","code":""},{"path":"index.html","id":"least-absolute-shrinkage-and-selection-operator-lasso","chapter":"Machine Learning","heading":"Least Absolute Shrinkage and Selection Operator (LASSO)","text":"Least Absolute Shrinkage Selection Operator (LASSO): es otra técnica de regularización utilizada en modelos de regresión lineal, similar la Regresión Ridge, pero con una diferencia clave en el tipo de penalización que aplica en la función de coste de la regresión lineal ordinaria.","code":""},{"path":"index.html","id":"árboles-de-decisión-y-derivados","chapter":"Machine Learning","heading":"🌲 2. Árboles de Decisión y Derivados","text":"Ejemplos: Decision Tree, Random Forest, Gradient BoostingCuándo usarlo:Problemas tabulares con relaciones lineales y variables categóricas o numéricas.Cuando interpretabilidad es importante.Ventajas: Manejan datos heterogéneos, fáciles de interpretar (árboles simples).Limitaciones: Sobreajuste en árboles simples; menor desempeño en datos muy ruidosos sin ensambles.","code":""},{"path":"index.html","id":"ensambles-ensemble-methods","chapter":"Machine Learning","heading":"🌟 3. Ensambles (Ensemble Methods)","text":"Ejemplos: Random Forest, AdaBoost, XGBoost, LightGBMCuándo usarlo:Cuando buscas alto rendimiento en clasificación o regresión tabular.Competencias de datos (como Kaggle).Ventajas: Alta precisión, robustez.Limitaciones: Difícil de interpretar; más costosos computacionalmente.","code":""},{"path":"index.html","id":"redes-neuronales-y-deep-learning","chapter":"Machine Learning","heading":"🧠 4. Redes Neuronales y Deep Learning","text":"Ejemplos: MLP, CNN, RNN, TransformersCuándo usarlo:Imágenes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).Grandes volúmenes de datos estructurados.Ventajas: Muy poderosos para datos complejos y estructurados.Limitaciones: Requieren mucha data y poder computacional. Menor interpretabilidad.","code":""},{"path":"index.html","id":"reducción-de-dimensionalidad","chapter":"Machine Learning","heading":"🧩 5. Reducción de Dimensionalidad","text":"Ejemplos: PCA, t-SNE, UMAPCuándo usarlo:Visualización de datos de alta dimensión.Preprocesamiento para eliminar ruido o multicolinealidad.Ventajas: Mejora desempeño y velocidad de otros modelos.Limitaciones: Puede perder interpretabilidad; siempre mejora modelos.","code":""},{"path":"index.html","id":"bayesianos","chapter":"Machine Learning","heading":"🧬 6. Bayesianos","text":"Ejemplos: Naive Bayes, Bayesian NetworksCuándo usarlo:Clasificación rápida con supuestos simples.Problemas de texto o spam detection.Ventajas: Muy rápidos, bien fundamentados.Limitaciones: Supone independencia de variables (siempre cierto).","code":""},{"path":"index.html","id":"regularización","chapter":"Machine Learning","heading":"🧮 7. Regularización","text":"Ejemplos: L1 (Lasso), L2 (Ridge), Elastic NetCuándo usarlo:Para evitar sobreajuste en modelos lineales o redes neuronales.Cuando tienes muchas variables (alta dimensionalidad).Ventajas: Penaliza modelos complejos.Limitaciones: Puede eliminar variables útiles si se usa en exceso.","code":""},{"path":"index.html","id":"instance-based-basados-en-instancias","chapter":"Machine Learning","heading":"🔍 8. Instance-Based (Basados en Instancias)","text":"Ejemplos: K-Nearest Neighbors (KNN)Cuándo usarlo:Pocos datos, con patrones locales claros.Cuando la similitud entre casos es importante.Ventajas: Simple y eficaz en problemas de baja dimensión.Limitaciones: Escala mal con muchos datos; sensible al ruido.","code":""},{"path":"index.html","id":"clustering-no-supervisado","chapter":"Machine Learning","heading":"📏 9. Clustering (No Supervisado)","text":"Ejemplos: K-Means, DBSCAN, Hierarchical ClusteringCuándo usarlo:Agrupar datos sin etiquetas previas.Descubrir estructuras ocultas o segmentos de mercado.Ventajas: Útil en exploración y reducción de complejidad.Limitaciones: Requiere elegir número de grupos (excepto DBSCAN); puede ser sensible escala.","code":""},{"path":"index.html","id":"sistemas-basados-en-reglas-rule-based-systems","chapter":"Machine Learning","heading":"📐 10. Sistemas Basados en Reglas (Rule-Based Systems)","text":"Ejemplos: RuleFit, Decision Rules, lógica difusa\nCuándo usarlo:Interpretabilidad es clave (por ejemplo, decisiones legales o médicas).Incorporar conocimiento experto.Ventajas: Fácil de entender y auditar.Limitaciones: tan precisos como otros métodos en datos complejos.","code":""},{"path":"index.html","id":"cuadro","chapter":"Machine Learning","heading":"0.1 📌 Cuadro","text":"","code":""},{"path":"hello-bookdown.html","id":"hello-bookdown","chapter":"1 Hello bookdown","heading":"1 Hello bookdown","text":"chapters start first-level heading followed chapter title, like line . one first-level heading (#) per .Rmd file.","code":""},{"path":"hello-bookdown.html","id":"a-section","chapter":"1 Hello bookdown","heading":"1.1 A section","text":"chapter sections start second-level (##) higher heading followed section title, like sections . can many want within chapter.","code":""},{"path":"hello-bookdown.html","id":"an-unnumbered-section","chapter":"1 Hello bookdown","heading":"An unnumbered section","text":"Chapters sections numbered default. un-number heading, add {.unnumbered} shorter {-} end heading, like section.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Sagi, S. (2019). ML Algorithms: One SD (σ). obvious questions ask … | Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6Kuhn, M. (2019). caret Package. https://topepo.github.io/caret/index.html","code":""}]
