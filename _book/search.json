[{"path":"index.html","id":"machine-learning","chapter":"Machine Learning","heading":"Machine Learning","text":"","code":""},{"path":"index.html","id":"regressi√≥n","chapter":"Machine Learning","heading":"üîç 1. Regressi√≥n","text":"Ejemplos: Linear Regression, Ridge, LassoCu√°ndo usarlo:Predicci√≥n de valores num√©ricos continuos (e.g.¬†precios, temperaturas).Relaciones lineales entre variables.Ventajas: Simple, interpretable.Limitaciones: Mal desempe√±o con relaciones lineales complejas.","code":""},{"path":"index.html","id":"ordinary-least-squares-regression-olsr","chapter":"Machine Learning","heading":"Ordinary Least Squares Regression (OLSR)","text":"Regresi√≥n de M√≠nimos Cuadrados Ordinarios (OLSR): un m√©todo de regresi√≥n lineal para estimar los par√°metros desconocidos mediante la creaci√≥n de un modelo que minimizar√° la suma de los errores cuadrados entre los datos observados y los predichos (valores observados y valores estimados).","code":""},{"path":"index.html","id":"linear-regression","chapter":"Machine Learning","heading":"Linear Regression","text":"Regresi√≥n lineal : se utiliza para estimar valores reales (costo de las casas, n√∫mero de visitas, ventas totales, etc.) basados en variables continuas.","code":""},{"path":"index.html","id":"logistic-regression","chapter":"Machine Learning","heading":"Logistic Regression","text":"Regresi√≥n log√≠stica : se utiliza para estimar valores discretos (valores binarios como 0/1, s√≠/, verdadero/falso) basados en un conjunto dado de variables independientes.","code":""},{"path":"index.html","id":"stepwise-regression","chapter":"Machine Learning","heading":"Stepwise Regression","text":"Regresi√≥n por pasos : a√±ade caracter√≠sticas al modelo una una hasta encontrar la puntuaci√≥n √≥ptima para tu conjunto de caracter√≠sticas. La selecci√≥n por pasos alterna entre el avance y el retroceso, incorporando y eliminando variables que cumplen los criterios de entrada o eliminaci√≥n, hasta alcanzar un conjunto estable de variables.","code":""},{"path":"index.html","id":"multivariate-adaptive-regression-splines-mars","chapter":"Machine Learning","heading":"Multivariate Adaptive Regression Splines (MARS)","text":"Splines de Regresi√≥n Adaptativa Multivariante (MARS): un m√©todo de regresi√≥n flexible que busca interacciones y relaciones lineales que ayudan maximizar la precisi√≥n predictiva. Este algoritmo es inherentemente lineal (lo que significa que es necesario adaptar el modelo patrones lineales en el datos agregando manualmente t√©rminos del modelo (squared terms, interaction effects).","code":""},{"path":"index.html","id":"locally-estimated-scatterplot-smoothing-loess","chapter":"Machine Learning","heading":"Locally Estimated Scatterplot Smoothing (LOESS)","text":"Locally Estimated Scatterplot Smoothing (LOESS): un m√©todo para ajustar una curva suave entre dos variables o una superficie entre un resultado y hasta cuatro variables predictoras. La idea es que, si los datos se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n. Se puede aplicar regresi√≥n, lo que se denomina regresi√≥n ponderada localmente. Se puede aplicar LOESS cuando la relaci√≥n entre las variables independientes y dependientes es lineal. Hoy en d√≠a, la mayor√≠a de los algoritmos (como las redes neuronales de propagaci√≥n hacia adelante cl√°sicas, las m√°quinas de vectores de soporte, los algoritmos del vecino m√°s cercano, etc.) son sistemas de aprendizaje global que se utilizan para minimizar las funciones de p√©rdida globales (por ejemplo, el error cuadr√°tico medio). Por el contrario, los sistemas de aprendizaje local dividen el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Esto generalmente se logra dividiendo la funci√≥n de costo en m√∫ltiples funciones de costo locales independientes. Una de las desventajas de los m√©todos globales es que, veces, ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena. Pero entonces surge LOESS, una alternativa la aproximaci√≥n de funciones globales.","code":""},{"path":"index.html","id":"regression-ridge","chapter":"Machine Learning","heading":"Regression Ridge","text":"Regresi√≥n Ridge es una extensi√≥n de la regresi√≥n lineal cl√°sica (OLS) que se usa cuando hay problemas de multicolinealidad o riesgo de sobreajuste. Aborda estos problemas introduciendo un t√©rmino de penalizaci√≥n la funci√≥n de coste de la regresi√≥n lineal ordinaria (m√≠nimos cuadrados ordinarios, OLS).","code":""},{"path":"index.html","id":"least-absolute-shrinkage-and-selection-operator-lasso","chapter":"Machine Learning","heading":"Least Absolute Shrinkage and Selection Operator (LASSO)","text":"Least Absolute Shrinkage Selection Operator (LASSO): es otra t√©cnica de regularizaci√≥n utilizada en modelos de regresi√≥n lineal, similar la Regresi√≥n Ridge, pero con una diferencia clave en el tipo de penalizaci√≥n que aplica en la funci√≥n de coste de la regresi√≥n lineal ordinaria.","code":""},{"path":"index.html","id":"√°rboles-de-decisi√≥n-y-derivados","chapter":"Machine Learning","heading":"üå≤ 2. √Årboles de Decisi√≥n y Derivados","text":"Ejemplos: Decision Tree, Random Forest, Gradient BoostingCu√°ndo usarlo:Problemas tabulares con relaciones lineales y variables categ√≥ricas o num√©ricas.Cuando interpretabilidad es importante.Ventajas: Manejan datos heterog√©neos, f√°ciles de interpretar (√°rboles simples).Limitaciones: Sobreajuste en √°rboles simples; menor desempe√±o en datos muy ruidosos sin ensambles.","code":""},{"path":"index.html","id":"ensambles-ensemble-methods","chapter":"Machine Learning","heading":"üåü 3. Ensambles (Ensemble Methods)","text":"Ejemplos: Random Forest, AdaBoost, XGBoost, LightGBMCu√°ndo usarlo:Cuando buscas alto rendimiento en clasificaci√≥n o regresi√≥n tabular.Competencias de datos (como Kaggle).Ventajas: Alta precisi√≥n, robustez.Limitaciones: Dif√≠cil de interpretar; m√°s costosos computacionalmente.","code":""},{"path":"index.html","id":"redes-neuronales-y-deep-learning","chapter":"Machine Learning","heading":"üß† 4. Redes Neuronales y Deep Learning","text":"Ejemplos: MLP, CNN, RNN, TransformersCu√°ndo usarlo:Im√°genes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).Grandes vol√∫menes de datos estructurados.Ventajas: Muy poderosos para datos complejos y estructurados.Limitaciones: Requieren mucha data y poder computacional. Menor interpretabilidad.","code":""},{"path":"index.html","id":"reducci√≥n-de-dimensionalidad","chapter":"Machine Learning","heading":"üß© 5. Reducci√≥n de Dimensionalidad","text":"Ejemplos: PCA, t-SNE, UMAPCu√°ndo usarlo:Visualizaci√≥n de datos de alta dimensi√≥n.Preprocesamiento para eliminar ruido o multicolinealidad.Ventajas: Mejora desempe√±o y velocidad de otros modelos.Limitaciones: Puede perder interpretabilidad; siempre mejora modelos.","code":""},{"path":"index.html","id":"bayesianos","chapter":"Machine Learning","heading":"üß¨ 6. Bayesianos","text":"Ejemplos: Naive Bayes, Bayesian NetworksCu√°ndo usarlo:Clasificaci√≥n r√°pida con supuestos simples.Problemas de texto o spam detection.Ventajas: Muy r√°pidos, bien fundamentados.Limitaciones: Supone independencia de variables (siempre cierto).","code":""},{"path":"index.html","id":"regularizaci√≥n","chapter":"Machine Learning","heading":"üßÆ 7. Regularizaci√≥n","text":"Ejemplos: L1 (Lasso), L2 (Ridge), Elastic NetCu√°ndo usarlo:Para evitar sobreajuste en modelos lineales o redes neuronales.Cuando tienes muchas variables (alta dimensionalidad).Ventajas: Penaliza modelos complejos.Limitaciones: Puede eliminar variables √∫tiles si se usa en exceso.","code":""},{"path":"index.html","id":"instance-based-basados-en-instancias","chapter":"Machine Learning","heading":"üîç 8. Instance-Based (Basados en Instancias)","text":"Ejemplos: K-Nearest Neighbors (KNN)Cu√°ndo usarlo:Pocos datos, con patrones locales claros.Cuando la similitud entre casos es importante.Ventajas: Simple y eficaz en problemas de baja dimensi√≥n.Limitaciones: Escala mal con muchos datos; sensible al ruido.","code":""},{"path":"index.html","id":"clustering-no-supervisado","chapter":"Machine Learning","heading":"üìè 9. Clustering (No Supervisado)","text":"Ejemplos: K-Means, DBSCAN, Hierarchical ClusteringCu√°ndo usarlo:Agrupar datos sin etiquetas previas.Descubrir estructuras ocultas o segmentos de mercado.Ventajas: √ötil en exploraci√≥n y reducci√≥n de complejidad.Limitaciones: Requiere elegir n√∫mero de grupos (excepto DBSCAN); puede ser sensible escala.","code":""},{"path":"index.html","id":"sistemas-basados-en-reglas-rule-based-systems","chapter":"Machine Learning","heading":"üìê 10. Sistemas Basados en Reglas (Rule-Based Systems)","text":"Ejemplos: RuleFit, Decision Rules, l√≥gica difusa\nCu√°ndo usarlo:Interpretabilidad es clave (por ejemplo, decisiones legales o m√©dicas).Incorporar conocimiento experto.Ventajas: F√°cil de entender y auditar.Limitaciones: tan precisos como otros m√©todos en datos complejos.","code":""},{"path":"index.html","id":"cuadro","chapter":"Machine Learning","heading":"0.1 üìå Cuadro","text":"","code":""},{"path":"hello-bookdown.html","id":"hello-bookdown","chapter":"1 Hello bookdown","heading":"1 Hello bookdown","text":"chapters start first-level heading followed chapter title, like line . one first-level heading (#) per .Rmd file.","code":""},{"path":"hello-bookdown.html","id":"a-section","chapter":"1 Hello bookdown","heading":"1.1 A section","text":"chapter sections start second-level (##) higher heading followed section title, like sections . can many want within chapter.","code":""},{"path":"hello-bookdown.html","id":"an-unnumbered-section","chapter":"1 Hello bookdown","heading":"An unnumbered section","text":"Chapters sections numbered default. un-number heading, add {.unnumbered} shorter {-} end heading, like section.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Sagi, S. (2019). ML Algorithms: One SD (œÉ). obvious questions ask ‚Ä¶ | Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6Kuhn, M. (2019). caret Package. https://topepo.github.io/caret/index.html","code":""}]
