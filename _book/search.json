[{"path":"index.html","id":"machine-learning","chapter":"Machine Learning","heading":"Machine Learning","text":"","code":""},{"path":"index.html","id":"cuadro","chapter":"Machine Learning","heading":"📌 Cuadro","text":"","code":""},{"path":"regressión.html","id":"regressión","chapter":"🔍 1. Regressión","heading":"🔍 1. Regressión","text":"Ejemplos: Linear Regression, Ridge, LassoCuándo usarlo:Predicción de valores numéricos continuos (e.g. precios, temperaturas).Relaciones lineales entre variables.Ventajas: Simple, interpretable.Limitaciones: Mal desempeño con relaciones lineales complejas.","code":""},{"path":"regressión.html","id":"ordinary-least-squares-regression-olsr","chapter":"🔍 1. Regressión","heading":"Ordinary Least Squares Regression (OLSR)","text":"Regresión de Mínimos Cuadrados Ordinarios (OLSR): un método de regresión lineal para estimar los parámetros desconocidos mediante la creación de un modelo que minimizará la suma de los errores cuadrados entre los datos observados y los predichos (valores observados y valores estimados).","code":""},{"path":"regressión.html","id":"linear-regression","chapter":"🔍 1. Regressión","heading":"Linear Regression","text":"Regresión lineal : se utiliza para estimar valores reales (costo de las casas, número de visitas, ventas totales, etc.) basados en variables continuas.","code":""},{"path":"regressión.html","id":"logistic-regression","chapter":"🔍 1. Regressión","heading":"Logistic Regression","text":"Regresión logística : se utiliza para estimar valores discretos (valores binarios como 0/1, sí/, verdadero/falso) basados en un conjunto dado de variables independientes.","code":""},{"path":"regressión.html","id":"stepwise-regression","chapter":"🔍 1. Regressión","heading":"Stepwise Regression","text":"Regresión por pasos : añade características al modelo una una hasta encontrar la puntuación óptima para tu conjunto de características. La selección por pasos alterna entre el avance y el retroceso, incorporando y eliminando variables que cumplen los criterios de entrada o eliminación, hasta alcanzar un conjunto estable de variables.","code":""},{"path":"regressión.html","id":"multivariate-adaptive-regression-splines-mars","chapter":"🔍 1. Regressión","heading":"Multivariate Adaptive Regression Splines (MARS)","text":"Splines de Regresión Adaptativa Multivariante (MARS): un método de regresión flexible que busca interacciones y relaciones lineales que ayudan maximizar la precisión predictiva. Este algoritmo es inherentemente lineal (lo que significa que es necesario adaptar el modelo patrones lineales en el datos agregando manualmente términos del modelo (squared terms, interaction effects).","code":""},{"path":"regressión.html","id":"locally-estimated-scatterplot-smoothing-loess","chapter":"🔍 1. Regressión","heading":"Locally Estimated Scatterplot Smoothing (LOESS)","text":"Locally Estimated Scatterplot Smoothing (LOESS): un método para ajustar una curva suave entre dos variables o una superficie entre un resultado y hasta cuatro variables predictoras. La idea es que, si los datos se distribuyen linealmente, se puede aplicar el concepto de regresión. Se puede aplicar regresión, lo que se denomina regresión ponderada localmente. Se puede aplicar LOESS cuando la relación entre las variables independientes y dependientes es lineal. Hoy en día, la mayoría de los algoritmos (como las redes neuronales de propagación hacia adelante clásicas, las máquinas de vectores de soporte, los algoritmos del vecino más cercano, etc.) son sistemas de aprendizaje global que se utilizan para minimizar las funciones de pérdida globales (por ejemplo, el error cuadrático medio). Por el contrario, los sistemas de aprendizaje local dividen el problema de aprendizaje global en múltiples problemas de aprendizaje más pequeños y simples. Esto generalmente se logra dividiendo la función de costo en múltiples funciones de costo locales independientes. Una de las desventajas de los métodos globales es que, veces, ningún valor de parámetro puede proporcionar una aproximación suficientemente buena. Pero entonces surge LOESS, una alternativa la aproximación de funciones globales.","code":""},{"path":"árboles-de-decisión-y-derivados.html","id":"árboles-de-decisión-y-derivados","chapter":"🌲 2. Árboles de Decisión y Derivados","heading":"🌲 2. Árboles de Decisión y Derivados","text":"Ejemplos: Decision Tree, Random Forest, Gradient BoostingCuándo usarlo:Problemas tabulares con relaciones lineales y variables categóricas o numéricas.Cuando interpretabilidad es importante.Ventajas: Manejan datos heterogéneos, fáciles de interpretar (árboles simples).Limitaciones: Sobreajuste en árboles simples; menor desempeño en datos muy ruidosos sin ensambles.","code":""},{"path":"árboles-de-decisión-y-derivados.html","id":"classification-and-regression-tree-cart","chapter":"🌲 2. Árboles de Decisión y Derivados","heading":"Classification and Regression Tree (CART)","text":"","code":""},{"path":"árboles-de-decisión-y-derivados.html","id":"support-vector-machine-svm","chapter":"🌲 2. Árboles de Decisión y Derivados","heading":"Support Vector Machine (SVM)","text":"","code":""},{"path":"árboles-de-decisión-y-derivados.html","id":"iterative-dichotomiser-3-id3","chapter":"🌲 2. Árboles de Decisión y Derivados","heading":"Iterative Dichotomiser 3 (ID3)","text":"","code":""},{"path":"árboles-de-decisión-y-derivados.html","id":"c4.5","chapter":"🌲 2. Árboles de Decisión y Derivados","heading":"C4.5","text":"","code":""},{"path":"árboles-de-decisión-y-derivados.html","id":"c5.0","chapter":"🌲 2. Árboles de Decisión y Derivados","heading":"C5.0","text":"","code":""},{"path":"árboles-de-decisión-y-derivados.html","id":"chi-squared-automatic-interaction-detection-chaid","chapter":"🌲 2. Árboles de Decisión y Derivados","heading":"Chi-squared Automatic Interaction Detection (CHAID)","text":"","code":""},{"path":"árboles-de-decisión-y-derivados.html","id":"decision-stump","chapter":"🌲 2. Árboles de Decisión y Derivados","heading":"Decision Stump","text":"","code":""},{"path":"árboles-de-decisión-y-derivados.html","id":"conditional-decision-trees","chapter":"🌲 2. Árboles de Decisión y Derivados","heading":"Conditional Decision Trees","text":"","code":""},{"path":"árboles-de-decisión-y-derivados.html","id":"m5","chapter":"🌲 2. Árboles de Decisión y Derivados","heading":"M5","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"ensambles-ensemble-methods","chapter":"🌟 3. Ensambles (Ensemble Methods)","heading":"🌟 3. Ensambles (Ensemble Methods)","text":"Ejemplos: Random Forest, AdaBoost, XGBoost, LightGBMCuándo usarlo:Cuando buscas alto rendimiento en clasificación o regresión tabular.Competencias de datos (como Kaggle).Ventajas: Alta precisión, robustez.Limitaciones: Difícil de interpretar; más costosos computacionalmente.","code":""},{"path":"ensambles-ensemble-methods.html","id":"random-forest","chapter":"🌟 3. Ensambles (Ensemble Methods)","heading":"Random Forest","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"gradient-boosting-machines-gbm","chapter":"🌟 3. Ensambles (Ensemble Methods)","heading":"Gradient Boosting Machines (GBM)","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"boosting","chapter":"🌟 3. Ensambles (Ensemble Methods)","heading":"Boosting","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"bootstrapped-aggregation-bagging","chapter":"🌟 3. Ensambles (Ensemble Methods)","heading":"Bootstrapped Aggregation (Bagging)","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"adaboost","chapter":"🌟 3. Ensambles (Ensemble Methods)","heading":"AdaBoost","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"stacked-generalization-blending","chapter":"🌟 3. Ensambles (Ensemble Methods)","heading":"Stacked Generalization (Blending)","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"gradient-boosted-regression-trees-gbrt","chapter":"🌟 3. Ensambles (Ensemble Methods)","heading":"Gradient Boosted Regression Trees (GBRT)","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"xgboost","chapter":"🌟 3. Ensambles (Ensemble Methods)","heading":"XGBoost","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"lightgbm","chapter":"🌟 3. Ensambles (Ensemble Methods)","heading":"LightGBM","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"redes-neuronales-y-deep-learning","chapter":"🧠 4. Redes Neuronales y Deep Learning","heading":"🧠 4. Redes Neuronales y Deep Learning","text":"Ejemplos: MLP, CNN, RNN, TransformersCuándo usarlo:Imágenes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).Grandes volúmenes de datos estructurados.Ventajas: Muy poderosos para datos complejos y estructurados.Limitaciones: Requieren mucha data y poder computacional. Menor interpretabilidad.","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"radial-basis-function-network-rbfn","chapter":"🧠 4. Redes Neuronales y Deep Learning","heading":"Radial Basis Function Network (RBFN)","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"perceptron","chapter":"🧠 4. Redes Neuronales y Deep Learning","heading":"Perceptron","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"back---propagation","chapter":"🧠 4. Redes Neuronales y Deep Learning","heading":"Back - Propagation","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"hopfield-network","chapter":"🧠 4. Redes Neuronales y Deep Learning","heading":"Hopfield Network","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"mlp","chapter":"🧠 4. Redes Neuronales y Deep Learning","heading":"MLP","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"cnn","chapter":"🧠 4. Redes Neuronales y Deep Learning","heading":"CNN","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"rnn","chapter":"🧠 4. Redes Neuronales y Deep Learning","heading":"RNN","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"transformers","chapter":"🧠 4. Redes Neuronales y Deep Learning","heading":"Transformers","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"reducción-de-dimensionalidad","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"🧩 5. Reducción de Dimensionalidad","text":"Ejemplos: PCA, t-SNE, UMAPCuándo usarlo:Visualización de datos de alta dimensión.Preprocesamiento para eliminar ruido o multicolinealidad.Ventajas: Mejora desempeño y velocidad de otros modelos.Limitaciones: Puede perder interpretabilidad; siempre mejora modelos.","code":""},{"path":"reducción-de-dimensionalidad.html","id":"principal-component-analysis-pca","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Principal Component Analysis (PCA)","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"partial-least-squares-regression-plsr","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Partial Least Squares Regression (PLSR)","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"sammon-mapping","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Sammon Mapping","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"multidimensional-scaling-mds","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Multidimensional Scaling (MDS)","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"projection-pursuit","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Projection Pursuit","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"principal-component-regression","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Principal Component Regression","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"partial-least-squares-discriminant-analysis","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Partial Least Squares Discriminant Analysis","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"mixture-discriminant-analysis-mda","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Mixture Discriminant Analysis (MDA)","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"quadratic-discriminant-analysis","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Quadratic Discriminant Analysis","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"regularized-discriminant-analysis-rda","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Regularized Discriminant Analysis (RDA)","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"flexible-discriminant-analysis-fda","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Flexible Discriminant Analysis (FDA)","text":"","code":""},{"path":"reducción-de-dimensionalidad.html","id":"linear-discriminant-analysis-lda","chapter":"🧩 5. Reducción de Dimensionalidad","heading":"Linear Discriminant Analysis (LDA)","text":"","code":""},{"path":"bayesianos.html","id":"bayesianos","chapter":"🧬 6. Bayesianos","heading":"🧬 6. Bayesianos","text":"Ejemplos: Naive Bayes, Bayesian NetworksCuándo usarlo:Clasificación rápida con supuestos simples.Problemas de texto o spam detection.Ventajas: Muy rápidos, bien fundamentados.Limitaciones: Supone independencia de variables (siempre cierto).","code":""},{"path":"bayesianos.html","id":"naive-bayes","chapter":"🧬 6. Bayesianos","heading":"Naive Bayes","text":"","code":""},{"path":"bayesianos.html","id":"averaged-one---dependence-estimators-aode","chapter":"🧬 6. Bayesianos","heading":"Averaged One - Dependence Estimators (AODE)","text":"","code":""},{"path":"bayesianos.html","id":"bayesian-belief-network-bbn","chapter":"🧬 6. Bayesianos","heading":"Bayesian Belief Network (BBN)","text":"","code":""},{"path":"bayesianos.html","id":"gaussian-naive-bayes","chapter":"🧬 6. Bayesianos","heading":"Gaussian Naive Bayes","text":"","code":""},{"path":"bayesianos.html","id":"multinomial-naive-bayes","chapter":"🧬 6. Bayesianos","heading":"Multinomial Naive Bayes","text":"","code":""},{"path":"bayesianos.html","id":"bayesian-network-bn","chapter":"🧬 6. Bayesianos","heading":"Bayesian Network (BN)","text":"","code":""},{"path":"regularización.html","id":"regularización","chapter":"🧮 7. Regularización","heading":"🧮 7. Regularización","text":"Ejemplos: L1 (Lasso), L2 (Ridge), Elastic NetCuándo usarlo:Para evitar sobreajuste en modelos lineales o redes neuronales.Cuando tienes muchas variables (alta dimensionalidad).Ventajas: Penaliza modelos complejos.Limitaciones: Puede eliminar variables útiles si se usa en exceso.","code":""},{"path":"regularización.html","id":"ridge-regression","chapter":"🧮 7. Regularización","heading":"Ridge Regression","text":"","code":""},{"path":"regularización.html","id":"least-absolute-shrinkage-and-selection-operator-lasso","chapter":"🧮 7. Regularización","heading":"Least Absolute Shrinkage and Selection Operator (LASSO)","text":"","code":""},{"path":"regularización.html","id":"elastic-net","chapter":"🧮 7. Regularización","heading":"Elastic Net","text":"","code":""},{"path":"regularización.html","id":"least-angle-regression-lars","chapter":"🧮 7. Regularización","heading":"Least Angle Regression (LARS)","text":"","code":""},{"path":"instance-based-basados-en-instancias.html","id":"instance-based-basados-en-instancias","chapter":"🔍 8. Instance-Based (Basados en Instancias)","heading":"🔍 8. Instance-Based (Basados en Instancias)","text":"Ejemplos: K-Nearest Neighbors (KNN)Cuándo usarlo:Pocos datos, con patrones locales claros.Cuando la similitud entre casos es importante.Ventajas: Simple y eficaz en problemas de baja dimensión.Limitaciones: Escala mal con muchos datos; sensible al ruido.","code":""},{"path":"instance-based-basados-en-instancias.html","id":"k---nearest-neighbour-knn","chapter":"🔍 8. Instance-Based (Basados en Instancias)","heading":"k - Nearest Neighbour (kNN)","text":"","code":""},{"path":"instance-based-basados-en-instancias.html","id":"learning-vector-quantization-lvq","chapter":"🔍 8. Instance-Based (Basados en Instancias)","heading":"Learning Vector Quantization (LVQ)","text":"","code":""},{"path":"instance-based-basados-en-instancias.html","id":"self---organizing-map-som","chapter":"🔍 8. Instance-Based (Basados en Instancias)","heading":"Self - Organizing Map (SOM)","text":"","code":""},{"path":"instance-based-basados-en-instancias.html","id":"locally-weigted-learning-lwl","chapter":"🔍 8. Instance-Based (Basados en Instancias)","heading":"Locally Weigted Learning (LWL)","text":"","code":""},{"path":"clustering-no-supervisado.html","id":"clustering-no-supervisado","chapter":"📏 9. Clustering (No Supervisado)","heading":"📏 9. Clustering (No Supervisado)","text":"Ejemplos: K-Means, DBSCAN, Hierarchical ClusteringCuándo usarlo:Agrupar datos sin etiquetas previas.Descubrir estructuras ocultas o segmentos de mercado.Ventajas: Útil en exploración y reducción de complejidad.Limitaciones: Requiere elegir número de grupos (excepto DBSCAN); puede ser sensible escala.","code":""},{"path":"clustering-no-supervisado.html","id":"k-means","chapter":"📏 9. Clustering (No Supervisado)","heading":"k-Means","text":"","code":""},{"path":"clustering-no-supervisado.html","id":"k-medians","chapter":"📏 9. Clustering (No Supervisado)","heading":"k-Medians","text":"","code":""},{"path":"clustering-no-supervisado.html","id":"expectation-maximization","chapter":"📏 9. Clustering (No Supervisado)","heading":"Expectation Maximization","text":"","code":""},{"path":"clustering-no-supervisado.html","id":"hierarchical-clustering","chapter":"📏 9. Clustering (No Supervisado)","heading":"Hierarchical Clustering","text":"","code":""},{"path":"clustering-no-supervisado.html","id":"dbscan","chapter":"📏 9. Clustering (No Supervisado)","heading":"DBSCAN","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"sistemas-basados-en-reglas-rule-based-systems","chapter":"📐 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"📐 10. Sistemas Basados en Reglas (Rule-Based Systems)","text":"Ejemplos: RuleFit, Decision Rules, lógica difusa\nCuándo usarlo:Interpretabilidad es clave (por ejemplo, decisiones legales o médicas).Incorporar conocimiento experto.Ventajas: Fácil de entender y auditar.Limitaciones: tan precisos como otros métodos en datos complejos.","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"cubist","chapter":"📐 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"Cubist","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"one-rule-oner","chapter":"📐 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"One Rule (OneR)","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"zero-rule-zeror","chapter":"📐 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"Zero Rule (ZeroR)","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"repeated-incremental-pruning-to-produce-error-reduction-ripper","chapter":"📐 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"Repeated Incremental Pruning to Produce Error Reduction (RIPPER)","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"rule-fit","chapter":"📐 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"Rule Fit","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"decision-rules","chapter":"📐 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"Decision Rules","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Sagi, S. (2019). ML Algorithms: One SD (σ). obvious questions ask … | Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6Kuhn, M. (2019). caret Package. https://topepo.github.io/caret/index.html","code":""}]
