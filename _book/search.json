[{"path":"index.html","id":"machine-learning-apuntes","chapter":"Machine Learning (Apuntes)","heading":"Machine Learning (Apuntes)","text":"","code":""},{"path":"index.html","id":"regressi칩n","chapter":"Machine Learning (Apuntes)","heading":"游댌 1. Regressi칩n","text":"Ejemplos: Linear Regression, Ridge, LassoCu치ndo usarlo:Predicci칩n de valores num칠ricos continuos (e.g.맗recios, temperaturas).Relaciones lineales entre variables.Ventajas: Simple, interpretable.Limitaciones: Mal desempe침o con relaciones lineales complejas.","code":""},{"path":"index.html","id":"치rboles-de-decisi칩n-y-derivados","chapter":"Machine Learning (Apuntes)","heading":"游 2. 츼rboles de Decisi칩n y Derivados","text":"Ejemplos: Decision Tree, Random Forest, Gradient BoostingCu치ndo usarlo:Problemas tabulares con relaciones lineales y variables categ칩ricas o num칠ricas.Cuando interpretabilidad es importante.Ventajas: Manejan datos heterog칠neos, f치ciles de interpretar (치rboles simples).Limitaciones: Sobreajuste en 치rboles simples; menor desempe침o en datos muy ruidosos sin ensambles.","code":""},{"path":"index.html","id":"ensambles-ensemble-methods","chapter":"Machine Learning (Apuntes)","heading":"游 3. Ensambles (Ensemble Methods)","text":"Ejemplos: Random Forest, AdaBoost, XGBoost, LightGBMCu치ndo usarlo:Cuando buscas alto rendimiento en clasificaci칩n o regresi칩n tabular.Competencias de datos (como Kaggle).Ventajas: Alta precisi칩n, robustez.Limitaciones: Dif칤cil de interpretar; m치s costosos computacionalmente.","code":""},{"path":"index.html","id":"redes-neuronales-y-deep-learning","chapter":"Machine Learning (Apuntes)","heading":"游 4. Redes Neuronales y Deep Learning","text":"Ejemplos: MLP, CNN, RNN, TransformersCu치ndo usarlo:Im치genes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).Grandes vol칰menes de datos estructurados.Ventajas: Muy poderosos para datos complejos y estructurados.Limitaciones: Requieren mucha data y poder computacional. Menor interpretabilidad.","code":""},{"path":"index.html","id":"reducci칩n-de-dimensionalidad","chapter":"Machine Learning (Apuntes)","heading":"游빌 5. Reducci칩n de Dimensionalidad","text":"Ejemplos: PCA, t-SNE, UMAPCu치ndo usarlo:Visualizaci칩n de datos de alta dimensi칩n.Preprocesamiento para eliminar ruido o multicolinealidad.Ventajas: Mejora desempe침o y velocidad de otros modelos.Limitaciones: Puede perder interpretabilidad; siempre mejora modelos.","code":""},{"path":"index.html","id":"bayesianos","chapter":"Machine Learning (Apuntes)","heading":"游빏 6. Bayesianos","text":"Ejemplos: Naive Bayes, Bayesian NetworksCu치ndo usarlo:Clasificaci칩n r치pida con supuestos simples.Problemas de texto o spam detection.Ventajas: Muy r치pidos, bien fundamentados.Limitaciones: Supone independencia de variables (siempre cierto).","code":""},{"path":"index.html","id":"regularizaci칩n","chapter":"Machine Learning (Apuntes)","heading":"游빑 7. Regularizaci칩n","text":"Ejemplos: L1 (Lasso), L2 (Ridge), Elastic NetCu치ndo usarlo:Para evitar sobreajuste en modelos lineales o redes neuronales.Cuando tienes muchas variables (alta dimensionalidad).Ventajas: Penaliza modelos complejos.Limitaciones: Puede eliminar variables 칰tiles si se usa en exceso.","code":""},{"path":"index.html","id":"instance-based-basados-en-instancias","chapter":"Machine Learning (Apuntes)","heading":"游댌 8. Instance-Based (Basados en Instancias)","text":"Ejemplos: K-Nearest Neighbors (KNN)Cu치ndo usarlo:Pocos datos, con patrones locales claros.Cuando la similitud entre casos es importante.Ventajas: Simple y eficaz en problemas de baja dimensi칩n.Limitaciones: Escala mal con muchos datos; sensible al ruido.","code":""},{"path":"index.html","id":"clustering-no-supervisado","chapter":"Machine Learning (Apuntes)","heading":"游늺 9. Clustering (No Supervisado)","text":"Ejemplos: K-Means, DBSCAN, Hierarchical ClusteringCu치ndo usarlo:Agrupar datos sin etiquetas previas.Descubrir estructuras ocultas o segmentos de mercado.Ventajas: 칔til en exploraci칩n y reducci칩n de complejidad.Limitaciones: Requiere elegir n칰mero de grupos (excepto DBSCAN); puede ser sensible escala.","code":""},{"path":"index.html","id":"sistemas-basados-en-reglas-rule-based-systems","chapter":"Machine Learning (Apuntes)","heading":"游늻 10. Sistemas Basados en Reglas (Rule-Based Systems)","text":"Ejemplos: RuleFit, Decision Rules, l칩gica difusa\nCu치ndo usarlo:Interpretabilidad es clave (por ejemplo, decisiones legales o m칠dicas).Incorporar conocimiento experto.Ventajas: F치cil de entender y auditar.Limitaciones: tan precisos como otros m칠todos en datos complejos.","code":""},{"path":"index.html","id":"cuadro","chapter":"Machine Learning (Apuntes)","heading":"0.1 游늷 Cuadro","text":"","code":"\nbookdown::render_book()"},{"path":"hello-bookdown.html","id":"hello-bookdown","chapter":"1 Hello bookdown","heading":"1 Hello bookdown","text":"chapters start first-level heading followed chapter title, like line . one first-level heading (#) per .Rmd file.","code":""},{"path":"hello-bookdown.html","id":"a-section","chapter":"1 Hello bookdown","heading":"1.1 A section","text":"chapter sections start second-level (##) higher heading followed section title, like sections . can many want within chapter.","code":""},{"path":"hello-bookdown.html","id":"an-unnumbered-section","chapter":"1 Hello bookdown","heading":"An unnumbered section","text":"Chapters sections numbered default. un-number heading, add {.unnumbered} shorter {-} end heading, like section.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
