[{"path":"index.html","id":"machine-learning-apuntes","chapter":"Machine Learning (Apuntes)","heading":"Machine Learning (Apuntes)","text":"","code":""},{"path":"index.html","id":"regressión","chapter":"Machine Learning (Apuntes)","heading":"🔍 1. Regressión","text":"Ejemplos: Linear Regression, Ridge, LassoCuándo usarlo:Predicción de valores numéricos continuos (e.g. precios, temperaturas).Relaciones lineales entre variables.Ventajas: Simple, interpretable.Limitaciones: Mal desempeño con relaciones lineales complejas.","code":""},{"path":"index.html","id":"árboles-de-decisión-y-derivados","chapter":"Machine Learning (Apuntes)","heading":"🌲 2. Árboles de Decisión y Derivados","text":"Ejemplos: Decision Tree, Random Forest, Gradient BoostingCuándo usarlo:Problemas tabulares con relaciones lineales y variables categóricas o numéricas.Cuando interpretabilidad es importante.Ventajas: Manejan datos heterogéneos, fáciles de interpretar (árboles simples).Limitaciones: Sobreajuste en árboles simples; menor desempeño en datos muy ruidosos sin ensambles.","code":""},{"path":"index.html","id":"ensambles-ensemble-methods","chapter":"Machine Learning (Apuntes)","heading":"🌟 3. Ensambles (Ensemble Methods)","text":"Ejemplos: Random Forest, AdaBoost, XGBoost, LightGBMCuándo usarlo:Cuando buscas alto rendimiento en clasificación o regresión tabular.Competencias de datos (como Kaggle).Ventajas: Alta precisión, robustez.Limitaciones: Difícil de interpretar; más costosos computacionalmente.","code":""},{"path":"index.html","id":"redes-neuronales-y-deep-learning","chapter":"Machine Learning (Apuntes)","heading":"🧠 4. Redes Neuronales y Deep Learning","text":"Ejemplos: MLP, CNN, RNN, TransformersCuándo usarlo:Imágenes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).Grandes volúmenes de datos estructurados.Ventajas: Muy poderosos para datos complejos y estructurados.Limitaciones: Requieren mucha data y poder computacional. Menor interpretabilidad.","code":""},{"path":"index.html","id":"reducción-de-dimensionalidad","chapter":"Machine Learning (Apuntes)","heading":"🧩 5. Reducción de Dimensionalidad","text":"Ejemplos: PCA, t-SNE, UMAPCuándo usarlo:Visualización de datos de alta dimensión.Preprocesamiento para eliminar ruido o multicolinealidad.Ventajas: Mejora desempeño y velocidad de otros modelos.Limitaciones: Puede perder interpretabilidad; siempre mejora modelos.","code":""},{"path":"index.html","id":"bayesianos","chapter":"Machine Learning (Apuntes)","heading":"🧬 6. Bayesianos","text":"Ejemplos: Naive Bayes, Bayesian NetworksCuándo usarlo:Clasificación rápida con supuestos simples.Problemas de texto o spam detection.Ventajas: Muy rápidos, bien fundamentados.Limitaciones: Supone independencia de variables (siempre cierto).","code":""},{"path":"index.html","id":"regularización","chapter":"Machine Learning (Apuntes)","heading":"🧮 7. Regularización","text":"Ejemplos: L1 (Lasso), L2 (Ridge), Elastic NetCuándo usarlo:Para evitar sobreajuste en modelos lineales o redes neuronales.Cuando tienes muchas variables (alta dimensionalidad).Ventajas: Penaliza modelos complejos.Limitaciones: Puede eliminar variables útiles si se usa en exceso.","code":""},{"path":"index.html","id":"instance-based-basados-en-instancias","chapter":"Machine Learning (Apuntes)","heading":"🔍 8. Instance-Based (Basados en Instancias)","text":"Ejemplos: K-Nearest Neighbors (KNN)Cuándo usarlo:Pocos datos, con patrones locales claros.Cuando la similitud entre casos es importante.Ventajas: Simple y eficaz en problemas de baja dimensión.Limitaciones: Escala mal con muchos datos; sensible al ruido.","code":""},{"path":"index.html","id":"clustering-no-supervisado","chapter":"Machine Learning (Apuntes)","heading":"📏 9. Clustering (No Supervisado)","text":"Ejemplos: K-Means, DBSCAN, Hierarchical ClusteringCuándo usarlo:Agrupar datos sin etiquetas previas.Descubrir estructuras ocultas o segmentos de mercado.Ventajas: Útil en exploración y reducción de complejidad.Limitaciones: Requiere elegir número de grupos (excepto DBSCAN); puede ser sensible escala.","code":""},{"path":"index.html","id":"sistemas-basados-en-reglas-rule-based-systems","chapter":"Machine Learning (Apuntes)","heading":"📐 10. Sistemas Basados en Reglas (Rule-Based Systems)","text":"Ejemplos: RuleFit, Decision Rules, lógica difusa\nCuándo usarlo:Interpretabilidad es clave (por ejemplo, decisiones legales o médicas).Incorporar conocimiento experto.Ventajas: Fácil de entender y auditar.Limitaciones: tan precisos como otros métodos en datos complejos.","code":""},{"path":"index.html","id":"cuadro","chapter":"Machine Learning (Apuntes)","heading":"0.1 📌 Cuadro","text":"","code":"\nbookdown::render_book()"},{"path":"hello-bookdown.html","id":"hello-bookdown","chapter":"1 Hello bookdown","heading":"1 Hello bookdown","text":"chapters start first-level heading followed chapter title, like line . one first-level heading (#) per .Rmd file.","code":""},{"path":"hello-bookdown.html","id":"a-section","chapter":"1 Hello bookdown","heading":"1.1 A section","text":"chapter sections start second-level (##) higher heading followed section title, like sections . can many want within chapter.","code":""},{"path":"hello-bookdown.html","id":"an-unnumbered-section","chapter":"1 Hello bookdown","heading":"An unnumbered section","text":"Chapters sections numbered default. un-number heading, add {.unnumbered} shorter {-} end heading, like section.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
