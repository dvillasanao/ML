[{"path":"index.html","id":"machine-learning","chapter":"Machine Learning","heading":"Machine Learning","text":"","code":""},{"path":"index.html","id":"cuadro","chapter":"Machine Learning","heading":"ğŸ“Œ Cuadro","text":"","code":""},{"path":"regressiÃ³n.html","id":"regressiÃ³n","chapter":"ğŸ” 1. RegressiÃ³n","heading":"ğŸ” 1. RegressiÃ³n","text":"Ejemplos: Linear Regression, Ridge, LassoCuÃ¡ndo usarlo:PredicciÃ³n de valores numÃ©ricos continuos (e.g.Â precios, temperaturas).Relaciones lineales entre variables.Ventajas: Simple, interpretable.Limitaciones: Mal desempeÃ±o con relaciones lineales complejas.","code":""},{"path":"regressiÃ³n.html","id":"ordinary-least-squares-regression-olsr","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Ordinary Least Squares Regression (OLSR)","text":"RegresiÃ³n de MÃ­nimos Cuadrados Ordinarios (OLSR): un mÃ©todo de regresiÃ³n lineal para estimar los parÃ¡metros desconocidos mediante la creaciÃ³n de un modelo que minimizarÃ¡ la suma de los errores cuadrados entre los datos observados y los predichos (valores observados y valores estimados).","code":""},{"path":"regressiÃ³n.html","id":"linear-regression","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Linear Regression","text":"RegresiÃ³n lineal : se utiliza para estimar valores reales (costo de las casas, nÃºmero de visitas, ventas totales, etc.) basados en variables continuas.","code":""},{"path":"regressiÃ³n.html","id":"logistic-regression","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Logistic Regression","text":"RegresiÃ³n logÃ­stica : se utiliza para estimar valores discretos (valores binarios como 0/1, sÃ­/, verdadero/falso) basados en un conjunto dado de variables independientes.","code":""},{"path":"regressiÃ³n.html","id":"stepwise-regression","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Stepwise Regression","text":"RegresiÃ³n por pasos : aÃ±ade caracterÃ­sticas al modelo una una hasta encontrar la puntuaciÃ³n Ã³ptima para tu conjunto de caracterÃ­sticas. La selecciÃ³n por pasos alterna entre el avance y el retroceso, incorporando y eliminando variables que cumplen los criterios de entrada o eliminaciÃ³n, hasta alcanzar un conjunto estable de variables.","code":""},{"path":"regressiÃ³n.html","id":"multivariate-adaptive-regression-splines-mars","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Multivariate Adaptive Regression Splines (MARS)","text":"Splines de RegresiÃ³n Adaptativa Multivariante (MARS): un mÃ©todo de regresiÃ³n flexible que busca interacciones y relaciones lineales que ayudan maximizar la precisiÃ³n predictiva. Este algoritmo es inherentemente lineal (lo que significa que es necesario adaptar el modelo patrones lineales en el datos agregando manualmente tÃ©rminos del modelo (squared terms, interaction effects).","code":""},{"path":"regressiÃ³n.html","id":"locally-estimated-scatterplot-smoothing-loess","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Locally Estimated Scatterplot Smoothing (LOESS)","text":"Locally Estimated Scatterplot Smoothing (LOESS): un mÃ©todo para ajustar una curva suave entre dos variables o una superficie entre un resultado y hasta cuatro variables predictoras. La idea es que, si los datos se distribuyen linealmente, se puede aplicar el concepto de regresiÃ³n. Se puede aplicar regresiÃ³n, lo que se denomina regresiÃ³n ponderada localmente. Se puede aplicar LOESS cuando la relaciÃ³n entre las variables independientes y dependientes es lineal. Hoy en dÃ­a, la mayorÃ­a de los algoritmos (como las redes neuronales de propagaciÃ³n hacia adelante clÃ¡sicas, las mÃ¡quinas de vectores de soporte, los algoritmos del vecino mÃ¡s cercano, etc.) son sistemas de aprendizaje global que se utilizan para minimizar las funciones de pÃ©rdida globales (por ejemplo, el error cuadrÃ¡tico medio). Por el contrario, los sistemas de aprendizaje local dividen el problema de aprendizaje global en mÃºltiples problemas de aprendizaje mÃ¡s pequeÃ±os y simples. Esto generalmente se logra dividiendo la funciÃ³n de costo en mÃºltiples funciones de costo locales independientes. Una de las desventajas de los mÃ©todos globales es que, veces, ningÃºn valor de parÃ¡metro puede proporcionar una aproximaciÃ³n suficientemente buena. Pero entonces surge LOESS, una alternativa la aproximaciÃ³n de funciones globales.","code":""},{"path":"regressiÃ³n.html","id":"regression-ridge","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Regression Ridge","text":"RegresiÃ³n Ridge es una extensiÃ³n de la regresiÃ³n lineal clÃ¡sica (OLS) que se usa cuando hay problemas de multicolinealidad o riesgo de sobreajuste. Aborda estos problemas introduciendo un tÃ©rmino de penalizaciÃ³n la funciÃ³n de coste de la regresiÃ³n lineal ordinaria (mÃ­nimos cuadrados ordinarios, OLS).","code":""},{"path":"regressiÃ³n.html","id":"least-absolute-shrinkage-and-selection-operator-lasso","chapter":"ğŸ” 1. RegressiÃ³n","heading":"Least Absolute Shrinkage and Selection Operator (LASSO)","text":"Least Absolute Shrinkage Selection Operator (LASSO): es otra tÃ©cnica de regularizaciÃ³n utilizada en modelos de regresiÃ³n lineal, similar la RegresiÃ³n Ridge, pero con una diferencia clave en el tipo de penalizaciÃ³n que aplica en la funciÃ³n de coste de la regresiÃ³n lineal ordinaria.","code":""},{"path":"Ã¡rboles-de-decisiÃ³n-y-derivados.html","id":"Ã¡rboles-de-decisiÃ³n-y-derivados","chapter":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","heading":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","text":"Ejemplos: Decision Tree, Random Forest, Gradient BoostingCuÃ¡ndo usarlo:Problemas tabulares con relaciones lineales y variables categÃ³ricas o numÃ©ricas.Cuando interpretabilidad es importante.Ventajas: Manejan datos heterogÃ©neos, fÃ¡ciles de interpretar (Ã¡rboles simples).Limitaciones: Sobreajuste en Ã¡rboles simples; menor desempeÃ±o en datos muy ruidosos sin ensambles.","code":""},{"path":"Ã¡rboles-de-decisiÃ³n-y-derivados.html","id":"classification-and-regression-tree-cart","chapter":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","heading":"0.1 Classification and Regression Tree (CART)","text":"","code":""},{"path":"Ã¡rboles-de-decisiÃ³n-y-derivados.html","id":"iterative-dichotomiser-3-id3","chapter":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","heading":"0.2 Iterative Dichotomiser 3 (ID3)","text":"","code":""},{"path":"Ã¡rboles-de-decisiÃ³n-y-derivados.html","id":"c4.5","chapter":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","heading":"0.3 C4.5","text":"","code":""},{"path":"Ã¡rboles-de-decisiÃ³n-y-derivados.html","id":"c5.0","chapter":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","heading":"0.4 C5.0","text":"","code":""},{"path":"Ã¡rboles-de-decisiÃ³n-y-derivados.html","id":"chi-squared-automatic-interaction-detection-chaid","chapter":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","heading":"0.5 Chi-squared Automatic Interaction Detection (CHAID)","text":"","code":""},{"path":"Ã¡rboles-de-decisiÃ³n-y-derivados.html","id":"decision-stump","chapter":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","heading":"0.6 Decision Stump","text":"","code":""},{"path":"Ã¡rboles-de-decisiÃ³n-y-derivados.html","id":"conditional-decision-trees","chapter":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","heading":"0.7 Conditional Decision Trees","text":"","code":""},{"path":"Ã¡rboles-de-decisiÃ³n-y-derivados.html","id":"m5","chapter":"ğŸŒ² 2. Ãrboles de DecisiÃ³n y Derivados","heading":"0.8 M5","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"ensambles-ensemble-methods","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","text":"Ejemplos: Random Forest, AdaBoost, XGBoost, LightGBMCuÃ¡ndo usarlo:Cuando buscas alto rendimiento en clasificaciÃ³n o regresiÃ³n tabular.Competencias de datos (como Kaggle).Ventajas: Alta precisiÃ³n, robustez.Limitaciones: DifÃ­cil de interpretar; mÃ¡s costosos computacionalmente.","code":""},{"path":"ensambles-ensemble-methods.html","id":"random-forest","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"0.9 Random Forest","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"gradient-boosting-machines-gbm","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"0.10 Gradient Boosting Machines (GBM)","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"boosting","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"0.11 Boosting","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"bootstrapped-aggregation-bagging","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"0.12 Bootstrapped Aggregation (Bagging)","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"adaboost","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"0.13 AdaBoost","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"stacked-generalization-blending","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"0.14 Stacked Generalization (Blending)","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"gradient-boosted-regression-trees-gbrt","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"0.15 Gradient Boosted Regression Trees (GBRT)","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"xgboost","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"0.16 XGBoost","text":"","code":""},{"path":"ensambles-ensemble-methods.html","id":"lightgbm","chapter":"ğŸŒŸ 3. Ensambles (Ensemble Methods)","heading":"0.17 LightGBM","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"redes-neuronales-y-deep-learning","chapter":"ğŸ§  4. Redes Neuronales y Deep Learning","heading":"ğŸ§  4. Redes Neuronales y Deep Learning","text":"Ejemplos: MLP, CNN, RNN, TransformersCuÃ¡ndo usarlo:ImÃ¡genes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).Grandes volÃºmenes de datos estructurados.Ventajas: Muy poderosos para datos complejos y estructurados.Limitaciones: Requieren mucha data y poder computacional. Menor interpretabilidad.","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"radial-basis-function-network-rbfn","chapter":"ğŸ§  4. Redes Neuronales y Deep Learning","heading":"0.18 Radial Basis Function Network (RBFN)","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"perceptron","chapter":"ğŸ§  4. Redes Neuronales y Deep Learning","heading":"0.19 Perceptron","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"back---propagation","chapter":"ğŸ§  4. Redes Neuronales y Deep Learning","heading":"0.20 Back - Propagation","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"hopfield-network","chapter":"ğŸ§  4. Redes Neuronales y Deep Learning","heading":"0.21 Hopfield Network","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"mlp","chapter":"ğŸ§  4. Redes Neuronales y Deep Learning","heading":"0.22 MLP","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"cnn","chapter":"ğŸ§  4. Redes Neuronales y Deep Learning","heading":"0.23 CNN","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"rnn","chapter":"ğŸ§  4. Redes Neuronales y Deep Learning","heading":"0.24 RNN","text":"","code":""},{"path":"redes-neuronales-y-deep-learning.html","id":"transformers","chapter":"ğŸ§  4. Redes Neuronales y Deep Learning","heading":"0.25 Transformers","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"reducciÃ³n-de-dimensionalidad","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","text":"Ejemplos: PCA, t-SNE, UMAPCuÃ¡ndo usarlo:VisualizaciÃ³n de datos de alta dimensiÃ³n.Preprocesamiento para eliminar ruido o multicolinealidad.Ventajas: Mejora desempeÃ±o y velocidad de otros modelos.Limitaciones: Puede perder interpretabilidad; siempre mejora modelos.","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"principal-component-analysis-pca","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.26 Principal Component Analysis (PCA)","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"partial-least-squares-regression-plsr","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.27 Partial Least Squares Regression (PLSR)","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"sammon-mapping","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.28 Sammon Mapping","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"multidimensional-scaling-mds","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.29 Multidimensional Scaling (MDS)","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"projection-pursuit","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.30 Projection Pursuit","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"principal-component-regression","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.31 Principal Component Regression","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"partial-least-squares-discriminant-analysis","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.32 Partial Least Squares Discriminant Analysis","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"mixture-discriminant-analysis-mda","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.33 Mixture Discriminant Analysis (MDA)","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"quadratic-discriminant-analysis","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.34 Quadratic Discriminant Analysis","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"regularized-discriminant-analysis-rda","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.35 Regularized Discriminant Analysis (RDA)","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"flexible-discriminant-analysis-fda","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.36 Flexible Discriminant Analysis (FDA)","text":"","code":""},{"path":"reducciÃ³n-de-dimensionalidad.html","id":"linear-discriminant-analysis-lda","chapter":"ğŸ§© 5. ReducciÃ³n de Dimensionalidad","heading":"0.37 Linear Discriminant Analysis (LDA)","text":"","code":""},{"path":"bayesianos.html","id":"bayesianos","chapter":"ğŸ§¬ 6. Bayesianos","heading":"ğŸ§¬ 6. Bayesianos","text":"Ejemplos: Naive Bayes, Bayesian NetworksCuÃ¡ndo usarlo:ClasificaciÃ³n rÃ¡pida con supuestos simples.Problemas de texto o spam detection.Ventajas: Muy rÃ¡pidos, bien fundamentados.Limitaciones: Supone independencia de variables (siempre cierto).","code":""},{"path":"bayesianos.html","id":"naive-bayes","chapter":"ğŸ§¬ 6. Bayesianos","heading":"0.38 Naive Bayes","text":"","code":""},{"path":"bayesianos.html","id":"averaged-one---dependence-estimators-aode","chapter":"ğŸ§¬ 6. Bayesianos","heading":"0.39 Averaged One - Dependence Estimators (AODE)","text":"","code":""},{"path":"bayesianos.html","id":"bayesian-belief-network-bbn","chapter":"ğŸ§¬ 6. Bayesianos","heading":"0.40 Bayesian Belief Network (BBN)","text":"","code":""},{"path":"bayesianos.html","id":"gaussian-naive-bayes","chapter":"ğŸ§¬ 6. Bayesianos","heading":"0.41 Gaussian Naive Bayes","text":"","code":""},{"path":"bayesianos.html","id":"multinomial-naive-bayes","chapter":"ğŸ§¬ 6. Bayesianos","heading":"0.42 Multinomial Naive Bayes","text":"","code":""},{"path":"bayesianos.html","id":"bayesian-network-bn","chapter":"ğŸ§¬ 6. Bayesianos","heading":"0.43 Bayesian Network (BN)","text":"","code":""},{"path":"regularizaciÃ³n.html","id":"regularizaciÃ³n","chapter":"ğŸ§® 7. RegularizaciÃ³n","heading":"ğŸ§® 7. RegularizaciÃ³n","text":"Ejemplos: L1 (Lasso), L2 (Ridge), Elastic NetCuÃ¡ndo usarlo:Para evitar sobreajuste en modelos lineales o redes neuronales.Cuando tienes muchas variables (alta dimensionalidad).Ventajas: Penaliza modelos complejos.Limitaciones: Puede eliminar variables Ãºtiles si se usa en exceso.","code":""},{"path":"regularizaciÃ³n.html","id":"ridge-regression","chapter":"ğŸ§® 7. RegularizaciÃ³n","heading":"0.44 Ridge Regression","text":"","code":""},{"path":"regularizaciÃ³n.html","id":"least-absolute-shrinkage-and-selection-operator-lasso-1","chapter":"ğŸ§® 7. RegularizaciÃ³n","heading":"0.45 Least Absolute Shrinkage and Selection Operator (LASSO)","text":"","code":""},{"path":"regularizaciÃ³n.html","id":"elastic-net","chapter":"ğŸ§® 7. RegularizaciÃ³n","heading":"0.46 Elastic Net","text":"","code":""},{"path":"regularizaciÃ³n.html","id":"least-angle-regression-lars","chapter":"ğŸ§® 7. RegularizaciÃ³n","heading":"0.47 Least Angle Regression (LARS)","text":"","code":""},{"path":"instance-based-basados-en-instancias.html","id":"instance-based-basados-en-instancias","chapter":"ğŸ” 8. Instance-Based (Basados en Instancias)","heading":"ğŸ” 8. Instance-Based (Basados en Instancias)","text":"Ejemplos: K-Nearest Neighbors (KNN)CuÃ¡ndo usarlo:Pocos datos, con patrones locales claros.Cuando la similitud entre casos es importante.Ventajas: Simple y eficaz en problemas de baja dimensiÃ³n.Limitaciones: Escala mal con muchos datos; sensible al ruido.","code":""},{"path":"instance-based-basados-en-instancias.html","id":"k---nearest-neighbour-knn","chapter":"ğŸ” 8. Instance-Based (Basados en Instancias)","heading":"0.48 k - Nearest Neighbour (kNN)","text":"","code":""},{"path":"instance-based-basados-en-instancias.html","id":"learning-vector-quantization-lvq","chapter":"ğŸ” 8. Instance-Based (Basados en Instancias)","heading":"0.49 Learning Vector Quantization (LVQ)","text":"","code":""},{"path":"instance-based-basados-en-instancias.html","id":"self---organizing-map-som","chapter":"ğŸ” 8. Instance-Based (Basados en Instancias)","heading":"0.50 Self - Organizing Map (SOM)","text":"","code":""},{"path":"instance-based-basados-en-instancias.html","id":"locally-weigted-learning-lwl","chapter":"ğŸ” 8. Instance-Based (Basados en Instancias)","heading":"0.51 Locally Weigted Learning (LWL)","text":"","code":""},{"path":"clustering-no-supervisado.html","id":"clustering-no-supervisado","chapter":"ğŸ“ 9. Clustering (No Supervisado)","heading":"ğŸ“ 9. Clustering (No Supervisado)","text":"Ejemplos: K-Means, DBSCAN, Hierarchical ClusteringCuÃ¡ndo usarlo:Agrupar datos sin etiquetas previas.Descubrir estructuras ocultas o segmentos de mercado.Ventajas: Ãštil en exploraciÃ³n y reducciÃ³n de complejidad.Limitaciones: Requiere elegir nÃºmero de grupos (excepto DBSCAN); puede ser sensible escala.","code":""},{"path":"clustering-no-supervisado.html","id":"k-means","chapter":"ğŸ“ 9. Clustering (No Supervisado)","heading":"0.52 k-Means","text":"","code":""},{"path":"clustering-no-supervisado.html","id":"k-medians","chapter":"ğŸ“ 9. Clustering (No Supervisado)","heading":"0.53 k-Medians","text":"","code":""},{"path":"clustering-no-supervisado.html","id":"expectation-maximization","chapter":"ğŸ“ 9. Clustering (No Supervisado)","heading":"0.54 Expectation Maximization","text":"","code":""},{"path":"clustering-no-supervisado.html","id":"hierarchical-clustering","chapter":"ğŸ“ 9. Clustering (No Supervisado)","heading":"0.55 Hierarchical Clustering","text":"","code":""},{"path":"clustering-no-supervisado.html","id":"dbscan","chapter":"ğŸ“ 9. Clustering (No Supervisado)","heading":"0.56 DBSCAN","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"sistemas-basados-en-reglas-rule-based-systems","chapter":"ğŸ“ 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"ğŸ“ 10. Sistemas Basados en Reglas (Rule-Based Systems)","text":"Ejemplos: RuleFit, Decision Rules, lÃ³gica difusa\nCuÃ¡ndo usarlo:Interpretabilidad es clave (por ejemplo, decisiones legales o mÃ©dicas).Incorporar conocimiento experto.Ventajas: FÃ¡cil de entender y auditar.Limitaciones: tan precisos como otros mÃ©todos en datos complejos.","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"cubist","chapter":"ğŸ“ 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"0.57 Cubist","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"one-rule-oner","chapter":"ğŸ“ 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"0.58 One Rule (OneR)","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"zero-rule-zeror","chapter":"ğŸ“ 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"0.59 Zero Rule (ZeroR)","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"repeated-incremental-pruning-to-produce-error-reduction-ripper","chapter":"ğŸ“ 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"0.60 Repeated Incremental Pruning to Produce Error Reduction (RIPPER)","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"rule-fit","chapter":"ğŸ“ 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"0.61 Rule Fit","text":"","code":""},{"path":"sistemas-basados-en-reglas-rule-based-systems.html","id":"decision-rules","chapter":"ğŸ“ 10. Sistemas Basados en Reglas (Rule-Based Systems)","heading":"0.62 Decision Rules","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Sagi, S. (2019). ML Algorithms: One SD (Ïƒ). obvious questions ask â€¦ | Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6Kuhn, M. (2019). caret Package. https://topepo.github.io/caret/index.html","code":""}]
