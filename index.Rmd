--- 
title: "Machine Learning (Apuntes) "
author: "Diana Villasana Ocampo"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::bs4_book,
  set in the _output.yml file.
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---

# Machine Learning {-}

##  **1. Regressi贸n** {-}

**Ejemplos:** Linear Regression, Ridge, Lasso   
**Cu谩ndo usarlo:**

* Predicci贸n de valores num茅ricos continuos (e.g. precios, temperaturas).  
* Relaciones lineales entre variables. 

**Ventajas:** Simple, interpretable.   
**Limitaciones:** Mal desempe帽o con relaciones no lineales complejas.

### Ordinary Least Squares Regression (`OLSR`) {-} 

**Regresi贸n de M铆nimos Cuadrados Ordinarios (OLSR)**: un m茅todo de regresi贸n lineal para estimar los par谩metros desconocidos mediante la creaci贸n de un modelo que minimizar谩 la suma de los errores cuadrados entre los datos observados y los predichos (valores observados y valores estimados).  

### Linear Regression {-} 

**Regresi贸n lineal** : se utiliza para estimar valores reales (costo de las casas, n煤mero de visitas, ventas totales, etc.) basados en variables continuas.

### Logistic Regression {-} 

**Regresi贸n log铆stica** : se utiliza para estimar valores discretos (valores binarios como 0/1, s铆/no, verdadero/falso) basados en un conjunto dado de variables independientes.

### Stepwise Regression {-} 

**Regresi贸n por pasos** : a帽ade caracter铆sticas al modelo una a una hasta encontrar la puntuaci贸n 贸ptima para tu conjunto de caracter铆sticas. La selecci贸n por pasos alterna entre el avance y el retroceso, incorporando y eliminando variables que cumplen los criterios de entrada o eliminaci贸n, hasta alcanzar un conjunto estable de variables.  

### Multivariate Adaptive Regression Splines (`MARS`) {-} 

**Splines de Regresi贸n Adaptativa Multivariante (`MARS`)**: un m茅todo de regresi贸n flexible que busca interacciones y relaciones no lineales que ayudan a maximizar la precisi贸n predictiva. Este algoritmo es inherentemente no lineal (lo que significa que no es necesario adaptar el modelo a patrones no lineales en el datos agregando manualmente t茅rminos del modelo (squared terms, interaction effects).   

### Locally Estimated Scatterplot Smoothing (`LOESS`) {-} 

**Locally Estimated Scatterplot Smoothing (`LOESS`)**: un m茅todo para ajustar una curva suave entre dos variables o una superficie  entre un resultado y hasta cuatro variables predictoras. La idea es que, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi贸n. Se puede aplicar regresi贸n, lo que se denomina regresi贸n ponderada localmente. Se puede aplicar LOESS cuando la relaci贸n entre las variables independientes y dependientes no es lineal. Hoy en d铆a, la mayor铆a de los algoritmos (como las redes neuronales de propagaci贸n hacia adelante cl谩sicas, las m谩quinas de vectores de soporte, los algoritmos del vecino m谩s cercano, etc.) son sistemas de aprendizaje global que se utilizan para minimizar las funciones de p茅rdida globales (por ejemplo, el error cuadr谩tico medio). Por el contrario, los sistemas de aprendizaje local dividen el problema de aprendizaje global en m煤ltiples problemas de aprendizaje m谩s peque帽os y simples. Esto generalmente se logra dividiendo la funci贸n de costo en m煤ltiples funciones de costo locales independientes. Una de las desventajas de los m茅todos globales es que, a veces, ning煤n valor de par谩metro puede proporcionar una aproximaci贸n suficientemente buena. Pero entonces surge LOESS, una alternativa a la aproximaci贸n de funciones globales.   


### Regression Ridge {-} 

**Regresi贸n Ridge** es una extensi贸n de la regresi贸n lineal cl谩sica (`OLS`) que se usa cuando hay problemas de multicolinealidad o riesgo de sobreajuste. Aborda estos problemas introduciendo un t茅rmino de penalizaci贸n a la funci贸n de coste de la regresi贸n lineal ordinaria (m铆nimos cuadrados ordinarios, OLS).  

### Least Absolute Shrinkage and Selection Operator (`LASSO`) {-}

**Least Absolute Shrinkage and Selection Operator (`LASSO`)**: es otra t茅cnica de regularizaci贸n utilizada en modelos de regresi贸n lineal, similar a la Regresi贸n Ridge, pero con una diferencia clave en el tipo de penalizaci贸n que aplica en la funci贸n de coste de la regresi贸n lineal ordinaria.    


---

##  **2. rboles de Decisi贸n y Derivados** {-}  

**Ejemplos:** Decision Tree, Random Forest, Gradient Boosting  
**Cu谩ndo usarlo:**  

* Problemas tabulares con relaciones no lineales y variables categ贸ricas o num茅ricas.
* Cuando interpretabilidad es importante.

**Ventajas:** Manejan datos heterog茅neos, f谩ciles de interpretar (谩rboles simples).   
**Limitaciones:** Sobreajuste en 谩rboles simples; menor desempe帽o en datos muy ruidosos sin ensambles.

---

##  **3. Ensambles (Ensemble Methods)** {-}

**Ejemplos:** Random Forest, AdaBoost, XGBoost, LightGBM   
**Cu谩ndo usarlo:**   

* Cuando buscas alto rendimiento en clasificaci贸n o regresi贸n tabular.
* Competencias de datos (como Kaggle).

**Ventajas:** Alta precisi贸n, robustez.   
**Limitaciones:** Dif铆cil de interpretar; m谩s costosos computacionalmente.

---

##  **4. Redes Neuronales y Deep Learning** {-}  

**Ejemplos:** MLP, CNN, RNN, Transformers   
**Cu谩ndo usarlo:**   

* Im谩genes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).
* Grandes vol煤menes de datos no estructurados.

**Ventajas:** Muy poderosos para datos complejos y no estructurados.   
**Limitaciones:** Requieren mucha data y poder computacional. Menor interpretabilidad.

---

## З **5. Reducci贸n de Dimensionalidad** {-}   

**Ejemplos:** PCA, t-SNE, UMAP   
**Cu谩ndo usarlo:**   

* Visualizaci贸n de datos de alta dimensi贸n.
* Preprocesamiento para eliminar ruido o multicolinealidad.

**Ventajas:** Mejora desempe帽o y velocidad de otros modelos.    
**Limitaciones:** Puede perder interpretabilidad; no siempre mejora modelos.

---

## К **6. Bayesianos** {-}  

**Ejemplos:** Naive Bayes, Bayesian Networks  
**Cu谩ndo usarlo:**   

* Clasificaci贸n r谩pida con supuestos simples.
* Problemas de texto o spam detection.

**Ventajas:** Muy r谩pidos, bien fundamentados.   
**Limitaciones:** Supone independencia de variables (no siempre cierto).

---

## М **7. Regularizaci贸n** {-}  

**Ejemplos:** L1 (Lasso), L2 (Ridge), Elastic Net   
**Cu谩ndo usarlo:**   

* Para evitar sobreajuste en modelos lineales o redes neuronales.
* Cuando tienes muchas variables (alta dimensionalidad).

**Ventajas:** Penaliza modelos complejos.   
**Limitaciones:** Puede eliminar variables 煤tiles si se usa en exceso.

---

##  **8. Instance-Based (Basados en Instancias)** {-}  

**Ejemplos:** K-Nearest Neighbors (KNN)   
**Cu谩ndo usarlo:**   

* Pocos datos, con patrones locales claros.  
* Cuando la similitud entre casos es importante.

**Ventajas:** Simple y eficaz en problemas de baja dimensi贸n.   
**Limitaciones:** Escala mal con muchos datos; sensible al ruido.

---

##  **9. Clustering (No Supervisado)** {-}  

**Ejemplos:** K-Means, DBSCAN, Hierarchical Clustering  
**Cu谩ndo usarlo:**   

* Agrupar datos sin etiquetas previas.
* Descubrir estructuras ocultas o segmentos de mercado.

**Ventajas:** til en exploraci贸n y reducci贸n de complejidad.   
**Limitaciones:** Requiere elegir n煤mero de grupos (excepto DBSCAN); puede ser sensible a escala.

---

##  **10. Sistemas Basados en Reglas (Rule-Based Systems)** {-}

**Ejemplos:** RuleFit, Decision Rules, l贸gica difusa
**Cu谩ndo usarlo:**

* Interpretabilidad es clave (por ejemplo, decisiones legales o m茅dicas).
* Incorporar conocimiento experto.

**Ventajas:** F谩cil de entender y auditar.   
**Limitaciones:** No tan precisos como otros m茅todos en datos complejos.

---

##  Cuadro

```{r, echo = FALSE}
algoritmos_ml <- data.frame(
                            Tipo = c(
                              "Regressi贸n",
                              "rboles / Decision Tree",
                              "Ensambles",
                              "Deep Learning",
                              "Reducci贸n de Dim.",
                              "Bayesianos",
                              "Regularizaci贸n",
                              "Instance-Based",
                              "Clustering",
                              "Rule-Based"
                            ),
                            Problema_tipico = c(
                              "Valores num茅ricos",
                              "Clasificaci贸n, regresi贸n",
                              "Clasificaci贸n, regresi贸n",
                              "Im谩genes, texto, audio",
                              "Visualizaci贸n, preprocesamiento",
                              "Clasificaci贸n r谩pida",
                              "Evitar overfitting",
                              "Clasificaci贸n",
                              "Agrupamiento no supervisado",
                              "Interpretabilidad"
                            ),
                            Ventajas = c(
                              "Simplicidad",
                              "Interpretabilidad",
                              "Precisi贸n",
                              "Modelos complejos",
                              "Mejora eficiencia",
                              "Velocidad",
                              "Generalizaci贸n",
                              "Simple, no requiere entrenamiento",
                              "Descubrir estructuras ocultas",
                              "L贸gica clara"
                            ),
                            Cuando_usarlo = c(
                              "Relaciones lineales",
                              "Datos tabulares",
                              "Alto rendimiento, Kaggle",
                              "Datos grandes y no estructurados",
                              "Datos con muchas variables",
                              "Texto, spam detection",
                              "Modelos lineales con muchas variables",
                              "Pocos datos y relaciones claras",
                              "Segmentaci贸n sin etiquetas",
                              "Reglas conocidas, decisiones explicables"
                            ),
                            stringsAsFactors = FALSE
                          )

require(gt)

algoritmos_ml %>% 
 gt() %>%
  tab_header(title = "Modelos y cuando usarlos") %>%
   tab_footnote(footnote = "Fuente: Elaboraci贸n propia") %>%
     fmt_integer(columns = names(data)[4:22], 
                sep_mark = " ") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Cuando_") ~ px(300),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```



```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```
