--- 
title: "Machine Learning (Apuntes) "
author: "Diana Villasana Ocampo"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::bs4_book,
  set in the _output.yml file.
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---

# Machine Learning {-}

## 🔍 **1. Regressión** {-}

**Ejemplos:** Linear Regression, Ridge, Lasso   
**Cuándo usarlo:**

* Predicción de valores numéricos continuos (e.g. precios, temperaturas).  
* Relaciones lineales entre variables. 

**Ventajas:** Simple, interpretable.   
**Limitaciones:** Mal desempeño con relaciones no lineales complejas.

### Ordinary Least Squares Regression (`OLSR`) {-} 

**Regresión de Mínimos Cuadrados Ordinarios (OLSR)**: un método de regresión lineal para estimar los parámetros desconocidos mediante la creación de un modelo que minimizará la suma de los errores cuadrados entre los datos observados y los predichos (valores observados y valores estimados).  

### Linear Regression {-} 

**Regresión lineal** : se utiliza para estimar valores reales (costo de las casas, número de visitas, ventas totales, etc.) basados en variables continuas.

### Logistic Regression {-} 

**Regresión logística** : se utiliza para estimar valores discretos (valores binarios como 0/1, sí/no, verdadero/falso) basados en un conjunto dado de variables independientes.

### Stepwise Regression {-} 

**Regresión por pasos** : añade características al modelo una a una hasta encontrar la puntuación óptima para tu conjunto de características. La selección por pasos alterna entre el avance y el retroceso, incorporando y eliminando variables que cumplen los criterios de entrada o eliminación, hasta alcanzar un conjunto estable de variables.  

### Multivariate Adaptive Regression Splines (`MARS`) {-} 

**Splines de Regresión Adaptativa Multivariante (`MARS`)**: un método de regresión flexible que busca interacciones y relaciones no lineales que ayudan a maximizar la precisión predictiva. Este algoritmo es inherentemente no lineal (lo que significa que no es necesario adaptar el modelo a patrones no lineales en el datos agregando manualmente términos del modelo (squared terms, interaction effects).   

### Locally Estimated Scatterplot Smoothing (`LOESS`) {-} 

**Locally Estimated Scatterplot Smoothing (`LOESS`)**: un método para ajustar una curva suave entre dos variables o una superficie  entre un resultado y hasta cuatro variables predictoras. La idea es que, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresión. Se puede aplicar regresión, lo que se denomina regresión ponderada localmente. Se puede aplicar LOESS cuando la relación entre las variables independientes y dependientes no es lineal. Hoy en día, la mayoría de los algoritmos (como las redes neuronales de propagación hacia adelante clásicas, las máquinas de vectores de soporte, los algoritmos del vecino más cercano, etc.) son sistemas de aprendizaje global que se utilizan para minimizar las funciones de pérdida globales (por ejemplo, el error cuadrático medio). Por el contrario, los sistemas de aprendizaje local dividen el problema de aprendizaje global en múltiples problemas de aprendizaje más pequeños y simples. Esto generalmente se logra dividiendo la función de costo en múltiples funciones de costo locales independientes. Una de las desventajas de los métodos globales es que, a veces, ningún valor de parámetro puede proporcionar una aproximación suficientemente buena. Pero entonces surge LOESS, una alternativa a la aproximación de funciones globales.   


### Regression Ridge {-} 

**Regresión Ridge** es una extensión de la regresión lineal clásica (`OLS`) que se usa cuando hay problemas de multicolinealidad o riesgo de sobreajuste. Aborda estos problemas introduciendo un término de penalización a la función de coste de la regresión lineal ordinaria (mínimos cuadrados ordinarios, OLS).  

### Least Absolute Shrinkage and Selection Operator (`LASSO`) {-}

**Least Absolute Shrinkage and Selection Operator (`LASSO`)**: es otra técnica de regularización utilizada en modelos de regresión lineal, similar a la Regresión Ridge, pero con una diferencia clave en el tipo de penalización que aplica en la función de coste de la regresión lineal ordinaria.    


---

## 🌲 **2. Árboles de Decisión y Derivados** {-}  

**Ejemplos:** Decision Tree, Random Forest, Gradient Boosting  
**Cuándo usarlo:**  

* Problemas tabulares con relaciones no lineales y variables categóricas o numéricas.
* Cuando interpretabilidad es importante.

**Ventajas:** Manejan datos heterogéneos, fáciles de interpretar (árboles simples).   
**Limitaciones:** Sobreajuste en árboles simples; menor desempeño en datos muy ruidosos sin ensambles.

---

## 🌟 **3. Ensambles (Ensemble Methods)** {-}

**Ejemplos:** Random Forest, AdaBoost, XGBoost, LightGBM   
**Cuándo usarlo:**   

* Cuando buscas alto rendimiento en clasificación o regresión tabular.
* Competencias de datos (como Kaggle).

**Ventajas:** Alta precisión, robustez.   
**Limitaciones:** Difícil de interpretar; más costosos computacionalmente.

---

## 🧠 **4. Redes Neuronales y Deep Learning** {-}  

**Ejemplos:** MLP, CNN, RNN, Transformers   
**Cuándo usarlo:**   

* Imágenes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).
* Grandes volúmenes de datos no estructurados.

**Ventajas:** Muy poderosos para datos complejos y no estructurados.   
**Limitaciones:** Requieren mucha data y poder computacional. Menor interpretabilidad.

---

## 🧩 **5. Reducción de Dimensionalidad** {-}   

**Ejemplos:** PCA, t-SNE, UMAP   
**Cuándo usarlo:**   

* Visualización de datos de alta dimensión.
* Preprocesamiento para eliminar ruido o multicolinealidad.

**Ventajas:** Mejora desempeño y velocidad de otros modelos.    
**Limitaciones:** Puede perder interpretabilidad; no siempre mejora modelos.

---

## 🧬 **6. Bayesianos** {-}  

**Ejemplos:** Naive Bayes, Bayesian Networks  
**Cuándo usarlo:**   

* Clasificación rápida con supuestos simples.
* Problemas de texto o spam detection.

**Ventajas:** Muy rápidos, bien fundamentados.   
**Limitaciones:** Supone independencia de variables (no siempre cierto).

---

## 🧮 **7. Regularización** {-}  

**Ejemplos:** L1 (Lasso), L2 (Ridge), Elastic Net   
**Cuándo usarlo:**   

* Para evitar sobreajuste en modelos lineales o redes neuronales.
* Cuando tienes muchas variables (alta dimensionalidad).

**Ventajas:** Penaliza modelos complejos.   
**Limitaciones:** Puede eliminar variables útiles si se usa en exceso.

---

## 🔍 **8. Instance-Based (Basados en Instancias)** {-}  

**Ejemplos:** K-Nearest Neighbors (KNN)   
**Cuándo usarlo:**   

* Pocos datos, con patrones locales claros.  
* Cuando la similitud entre casos es importante.

**Ventajas:** Simple y eficaz en problemas de baja dimensión.   
**Limitaciones:** Escala mal con muchos datos; sensible al ruido.

---

## 📏 **9. Clustering (No Supervisado)** {-}  

**Ejemplos:** K-Means, DBSCAN, Hierarchical Clustering  
**Cuándo usarlo:**   

* Agrupar datos sin etiquetas previas.
* Descubrir estructuras ocultas o segmentos de mercado.

**Ventajas:** Útil en exploración y reducción de complejidad.   
**Limitaciones:** Requiere elegir número de grupos (excepto DBSCAN); puede ser sensible a escala.

---

## 📐 **10. Sistemas Basados en Reglas (Rule-Based Systems)** {-}

**Ejemplos:** RuleFit, Decision Rules, lógica difusa
**Cuándo usarlo:**

* Interpretabilidad es clave (por ejemplo, decisiones legales o médicas).
* Incorporar conocimiento experto.

**Ventajas:** Fácil de entender y auditar.   
**Limitaciones:** No tan precisos como otros métodos en datos complejos.

---

## 📌 Cuadro

```{r, echo = FALSE}
algoritmos_ml <- data.frame(
                            Tipo = c(
                              "Regressión",
                              "Árboles / Decision Tree",
                              "Ensambles",
                              "Deep Learning",
                              "Reducción de Dim.",
                              "Bayesianos",
                              "Regularización",
                              "Instance-Based",
                              "Clustering",
                              "Rule-Based"
                            ),
                            Problema_tipico = c(
                              "Valores numéricos",
                              "Clasificación, regresión",
                              "Clasificación, regresión",
                              "Imágenes, texto, audio",
                              "Visualización, preprocesamiento",
                              "Clasificación rápida",
                              "Evitar overfitting",
                              "Clasificación",
                              "Agrupamiento no supervisado",
                              "Interpretabilidad"
                            ),
                            Ventajas = c(
                              "Simplicidad",
                              "Interpretabilidad",
                              "Precisión",
                              "Modelos complejos",
                              "Mejora eficiencia",
                              "Velocidad",
                              "Generalización",
                              "Simple, no requiere entrenamiento",
                              "Descubrir estructuras ocultas",
                              "Lógica clara"
                            ),
                            Cuando_usarlo = c(
                              "Relaciones lineales",
                              "Datos tabulares",
                              "Alto rendimiento, Kaggle",
                              "Datos grandes y no estructurados",
                              "Datos con muchas variables",
                              "Texto, spam detection",
                              "Modelos lineales con muchas variables",
                              "Pocos datos y relaciones claras",
                              "Segmentación sin etiquetas",
                              "Reglas conocidas, decisiones explicables"
                            ),
                            stringsAsFactors = FALSE
                          )

require(gt)

algoritmos_ml %>% 
 gt() %>%
  tab_header(title = "Modelos y cuando usarlos") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     fmt_integer(columns = names(data)[4:22], 
                sep_mark = " ") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Cuando_") ~ px(300),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```



```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```
