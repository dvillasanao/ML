# üß¨ 6. Modelos Bayesianos {-}  

**Ejemplos:** Naive Bayes, Redes Bayesianas.  
**Uso:** Ideales para **clasificaci√≥n r√°pida**, especialmente en escenarios con **supuestos simples** sobre los datos. Son muy populares en tareas de **procesamiento de texto** y **detecci√≥n de spam**.  
**Ventajas:** Son modelos **muy r√°pidos** de entrenar y predecir, y est√°n s√≥lidamente **fundamentados en la teor√≠a de probabilidad**.  
**Limitaciones:** La principal es que **asumen independencia** entre las variables predictoras, lo cual no siempre se cumple en la realidad y puede afectar su precisi√≥n en ciertos problemas.  

---

## Averaged One - Dependence Estimators (AODE)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/AODE.png"))
```

**Averaged One-Dependence Estimators (AODE)** es un algoritmo de **clasificaci√≥n supervisada** que pertenece a la familia de los clasificadores basados en **modelos bayesianos**. Es una mejora sobre el cl√°sico **Naive Bayes (NB)**, dise√±ado para superar la limitaci√≥n clave de NB: la **asunci√≥n de independencia condicional** estricta entre las variables predictoras (atributos) dado el valor de la clase. Esta suposici√≥n, aunque simplifica mucho el c√°lculo y permite a Naive Bayes ser muy eficiente, rara vez se cumple en la realidad y puede llevar a una p√©rdida de precisi√≥n.

AODE relaja parcialmente la suposici√≥n de independencia de Naive Bayes al considerar que **cada atributo es dependiente, como m√°ximo, de un solo otro atributo** (adem√°s de la variable de clase). En lugar de construir un √∫nico modelo de √°rbol de dependencia (como en el √Årbol de Dependencia de Atributos - ADTree), AODE construye una **colecci√≥n de clasificadores "One-Dependence" (ODE)** y luego **promedia sus predicciones**.

El funcionamiento de AODE se puede resumir as√≠:

1.  **Generaci√≥n de Clasificadores ODE:** Para cada atributo predictivo $A_i$ en el conjunto de datos (que cumpla ciertos criterios, como tener suficientes instancias), AODE construye un clasificador ODE. Este clasificador asume que todos los dem√°s atributos son condicionalmente independientes de $A_i$ dado la clase. En otras palabras, se estima la probabilidad condicional de cada atributo $A_j$ dado la clase $C$ y el atributo $A_i$: $P(A_j | C, A_i)$.
2.  **Ponderaci√≥n y Promedio:** Cuando se hace una predicci√≥n para una nueva instancia, AODE calcula la probabilidad de cada clase para cada uno de los clasificadores ODE generados. Luego, estas probabilidades se combinan (t√≠picamente promediando) para obtener una predicci√≥n final.

Al promediar las predicciones de m√∫ltiples modelos ODE, AODE logra mitigar el sesgo introducido por la suposici√≥n de independencia estricta de Naive Bayes, a menudo obteniendo un mejor rendimiento sin incurrir en una complejidad computacional excesiva.


**Aprendizaje Global vs. Local:**

Averaged One-Dependence Estimators (AODE) es un modelo que se clasifica como de **aprendizaje global**, aunque con una estructura que busca capturar dependencias que tienen una naturaleza m√°s "local" en el contexto de las relaciones entre atributos.

* **Aspecto Global:** AODE construye un conjunto de modelos (los ODEs) que son entrenados sobre la **totalidad del conjunto de datos** para estimar las probabilidades condicionales. La combinaci√≥n de estas probabilidades (el promedio) para llegar a una predicci√≥n final es una regla que se aplica de manera consistente a cualquier nueva observaci√≥n. Los par√°metros de cada clasificador ODE (las probabilidades condicionales) se estiman de manera global a partir de las frecuencias observadas en todo el conjunto de entrenamiento.

* **Matiz (Captura de Dependencias Locales):** Aunque el enfoque general es global, la raz√≥n por la que AODE es m√°s potente que Naive Bayes radica en su capacidad para modelar **dependencias entre atributos**. Cada clasificador ODE considera que un atributo espec√≠fico tiene una dependencia directa de otro atributo, lo que es una forma de capturar una relaci√≥n "local" entre un par de atributos dado el contexto de la clase. Al promediar sobre estos m√∫ltiples modelos que capturan diferentes dependencias de "un solo par", AODE puede adaptarse mejor a las complejidades de los datos donde las relaciones no son puramente independientes y no se distribuyen linealmente, sin la necesidad de dividir el espacio de caracter√≠sticas en regiones discretas como los √°rboles de decisi√≥n. Sin embargo, la soluci√≥n final de promediado es un clasificador global que se aplica a toda la instancia de entrada.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica",
  "‚úÖ Categ√≥ricas principalmente",
  "‚ö†Ô∏è Modela dependencias limitadas entre atributos (mejora sobre NB)",
  "‚ùå No aplica (no es regresi√≥n)",
  "‚úÖ Requiere independencia entre instancias",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede verse afectado por outliers si se usan variables num√©ricas sin tratamiento",
  "‚úÖ Menos afectado que Naive Bayes por dependencias entre atributos",
  "‚ö†Ô∏è Moderadamente interpretable (combinaci√≥n de varios modelos NB con 1 dependencia)",
  "‚ö†Ô∏è M√°s costoso que NB, pero a√∫n eficiente",
  "‚úÖ Puede validarse mediante k-fold cross-validation",
  "‚ùå Desempe√±a mal con muchos atributos irrelevantes o con pocos datos por combinaci√≥n de atributos"
)

detalles <- c(
  "Clasificador bayesiano que promedia modelos con una √∫nica dependencia entre pares de atributos para mejorar sobre Naive Bayes.",
  "Dise√±ado para problemas de clasificaci√≥n con clases categ√≥ricas.",
  "Se usa t√≠picamente con variables categ√≥ricas, aunque puede adaptarse a discretizadas.",
  "Relaja el supuesto de independencia total de Naive Bayes permitiendo una √∫nica dependencia por atributo.",
  "No es un modelo de regresi√≥n, por lo que no aplica el supuesto de normalidad de residuos.",
  "Las observaciones deben ser independientes para que las estimaciones sean v√°lidas.",
  "No supone homoscedasticidad debido a su naturaleza probabil√≠stica.",
  "Los valores at√≠picos pueden afectar la calidad de la estimaci√≥n de probabilidades.",
  "Tolera mejor la multicolinealidad al permitir dependencias limitadas entre atributos.",
  "La interpretaci√≥n es m√°s compleja que NB, pero a√∫n comprensible por su estructura promedio.",
  "Requiere m√°s tiempo de c√≥mputo que NB, pero sigue siendo razonablemente eficiente.",
  "La validaci√≥n cruzada es √∫til para evaluar el desempe√±o y generalizaci√≥n del modelo.",
  "El rendimiento cae si hay muchos atributos irrelevantes o datos escasos por combinaci√≥n de atributos."
)

tabla_aode <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_aode %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir AODE",
             subtitle = "Averaged One - Dependence Estimators (AODE)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Bayesian Network (BN)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/BN.png"))
```

Una **Red Bayesiana (BN)**, tambi√©n conocida como **Red Bayesiana Causal** o **Modelo Gr√°fico Dirigido Ac√≠clico (DAG)**, es un modelo probabil√≠stico que representa un conjunto de **variables y sus relaciones de dependencia condicional** utilizando un **grafo dirigido ac√≠clico**. En este grafo:

* **Nodos:** Representan las variables aleatorias (pueden ser discretas o continuas).
* **Arcos (flechas):** Representan las dependencias condicionales entre las variables. Una flecha de A a B significa que B depende directamente de A (A es "padre" de B). La ausencia de un arco entre dos nodos indica una independencia condicional.

La estructura del grafo de una Red Bayesiana permite visualizar y comprender las relaciones de causa y efecto (o asociaci√≥n) entre las variables. Junto con la estructura del grafo, una BN tambi√©n especifica las **distribuciones de probabilidad condicional (CPDs)** para cada nodo, dadas las combinaciones de estados de sus nodos padre. Por ejemplo, si un nodo tiene padres, se define la probabilidad de sus valores para cada combinaci√≥n de valores de sus padres.

Las Redes Bayesianas son potentes para:
* **Modelado de Conocimiento:** Codificar el conocimiento experto o aprendido de los datos sobre c√≥mo interact√∫an las variables.
* **Inferencia Probabil√≠stica:** Calcular la probabilidad de que una variable tome un valor espec√≠fico, dadas las observaciones de otras variables (evidencia). Esto puede incluir diagn√≥stico (inferir causas a partir de efectos) o predicci√≥n (inferir efectos a partir de causas).
* **Aprendizaje de Estructura y Par√°metros:** Aprender la estructura del grafo (las dependencias) y las CPDs a partir de datos.

**Aprendizaje Global vs. Local:**

Una Red Bayesiana (BN) es fundamentalmente un modelo de **aprendizaje global** en su estructura general, pero con una fuerte base en el **aprendizaje local** de las dependencias.

* **Aspecto Global:** La **estructura del grafo** y el conjunto de **tablas de probabilidad condicional (CPDs)** forman un **modelo probabil√≠stico coherente y global** de la distribuci√≥n de probabilidad conjunta de todas las variables. Este modelo global puede ser utilizado para realizar inferencias sobre cualquier combinaci√≥n de variables en cualquier parte del espacio de datos. La red define c√≥mo la informaci√≥n fluye y c√≥mo las probabilidades se propagan a trav√©s de todas las variables, dando una visi√≥n hol√≠stica de las interacciones del sistema.

* **Aspecto Local (Dependencias y Parametrizaci√≥n):** Donde la BN tiene un fuerte componente local es en la **definici√≥n de las dependencias y la parametrizaci√≥n de las CPDs**. Cada nodo solo necesita conocer las probabilidades condicionales dadas sus **padres directos**. Esto es un principio de **independencia condicional local**: una variable es independiente de sus no-descendientes dado sus padres. Esto descompone un problema complejo de modelado de la distribuci√≥n conjunta en problemas m√°s peque√±os y manejables de modelar las dependencias locales. Por ejemplo, para estimar $P(X_i | Padres(X_i))$, solo se necesita informaci√≥n local relacionada con $X_i$ y sus padres, no con todas las dem√°s variables en la red. Esta capacidad de modelar dependencias de forma localizada, y luego ensamblarlas en un modelo global, permite a las BNs manejar relaciones no lineales y complejas de una manera estructurada y probabil√≠stica. Si los datos no se distribuyen linealmente, la estructura de la BN puede adaptarse para reflejar las relaciones no lineales entre las variables a trav√©s de sus arcos y CPDs.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado o no supervisado",
  "‚úÖ Categ√≥rica o continua (depende del tipo de red)",
  "‚úÖ Categ√≥ricas y/o continuas",
  "‚úÖ Modela relaciones condicionales entre variables (estructura dirigida)",
  "‚ùå No aplica (no hay residuos t√≠picos)",
  "‚ö†Ô∏è Depende de la estructura de la red",
  "‚ùå No aplica como en regresi√≥n cl√°sica",
  "‚ö†Ô∏è Puede ser sensible si afecta las probabilidades condicionales",
  "‚ö†Ô∏è Puede causar redundancia si no se ajusta bien la estructura",
  "‚úÖ Muy interpretables si se visualiza la red y se conocen las dependencias",
  "‚ö†Ô∏è Aprendizaje de estructura puede ser computacionalmente costoso",
  "‚úÖ Puede usarse validaci√≥n cruzada para evaluar rendimiento predictivo",
  "‚ùå Cuando hay muchas variables y poca informaci√≥n para definir relaciones"
)

detalles <- c(
  "Modelo probabil√≠stico que representa relaciones condicionales entre variables mediante un grafo dirigido ac√≠clico (DAG).",
  "Puede predecir una variable (modo supervisado) o descubrir estructura entre variables (modo no supervisado).",
  "Acepta variables mixtas, aunque muchas implementaciones requieren discretizaci√≥n.",
  "Cada nodo representa una variable y las conexiones modelan dependencias condicionales.",
  "No genera residuos como los modelos de regresi√≥n, por lo que no aplica este supuesto.",
  "Algunas estructuras pueden implicar independencia condicional; otras no.",
  "No se eval√∫a homoscedasticidad, pues no hay predicci√≥n de error num√©rico.",
  "Valores at√≠picos pueden sesgar las probabilidades estimadas si no se controlan.",
  "Si hay variables redundantes o fuertemente correlacionadas, se debe ajustar la estructura de la red para evitar errores.",
  "La red permite interpretar c√≥mo influyen unas variables sobre otras, ideal para razonamiento causal.",
  "El ajuste de par√°metros es eficiente, pero aprender la estructura de la red puede ser lento.",
  "Puede evaluarse el desempe√±o con k-fold o validaci√≥n simple en tareas supervisadas.",
  "Cuando no se tiene informaci√≥n previa o datos suficientes, la red puede no capturar relaciones reales."
)

tabla_bn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir BN",
             subtitle = "Bayesian Network (BN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Bayesian Belief Network (BBN)  {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/BBN.png"))
```

Una **Red de Creencia Bayesiana (BBN)** es simplemente **otro t√©rmino para una Red Bayesiana (BN)**. No hay una diferencia fundamental entre ambos nombres; ambos se refieren al mismo tipo de modelo probabil√≠stico. La terminolog√≠a "Red de Creencia" a menudo enfatiza la capacidad del modelo para representar y actualizar "creencias" (probabilidades) sobre el estado de variables inciertas a medida que se introduce nueva evidencia.

Como ya se describi√≥, una BBN (o BN) es un **modelo gr√°fico probabil√≠stico dirigido ac√≠clico (DAG)** que representa un conjunto de **variables aleatorias** como **nodos** y sus **relaciones de dependencia condicional** como **arcos (flechas)**. La ausencia de un arco entre dos nodos indica una independencia condicional. Cada nodo est√° asociado con una **distribuci√≥n de probabilidad condicional (CPD)** que cuantifica la relaci√≥n de ese nodo con sus padres.

Las BBNs son herramientas poderosas para:
* **Modelar el conocimiento incierto:** Permiten representar c√≥mo diferentes factores interact√∫an bajo incertidumbre.
* **Inferencia probabil√≠stica:** Dada alguna evidencia (observaciones de algunas variables), la red puede calcular las probabilidades actualizadas de las otras variables. Esto es fundamental para el diagn√≥stico, la predicci√≥n y la toma de decisiones bajo incertidumbre.
* **Aprendizaje a partir de datos:** Las BBNs pueden ser aprendidas tanto en su estructura (c√≥mo se conectan los nodos) como en sus par√°metros (las CPDs) a partir de conjuntos de datos.


**Aprendizaje Global vs. Local:**

Al igual que una Red Bayesiana, una Red de Creencia Bayesiana es fundamentalmente un modelo de **aprendizaje global** en su formulaci√≥n general, pero se basa en la **especificaci√≥n local** de las dependencias probabil√≠sticas.

* **Aspecto Global:** La BBN como un todo representa la **distribuci√≥n de probabilidad conjunta global** de todas las variables en el sistema. Una vez que la estructura y las CPDs est√°n definidas, la red puede usarse para calcular cualquier probabilidad marginal o condicional de inter√©s, proporcionando una visi√≥n probabil√≠stica completa y coherente del dominio. Es una funci√≥n que mapea el espacio de todas las posibles combinaciones de variables a sus probabilidades, y se aplica de manera consistente en todo el espacio.

* **Aspecto Local (Definici√≥n de Dependencias):** La fortaleza y eficiencia de las BBNs radica en el principio de **independencia condicional local**. Cada variable (nodo) solo necesita tener su distribuci√≥n de probabilidad condicionada a sus **padres directos** en el grafo. No es necesario especificar las dependencias con todas las dem√°s variables en la red. Esta factorizaci√≥n de la distribuci√≥n conjunta en componentes locales (las CPDs) es lo que hace que las BBNs sean computacionalmente manejables y permite que el modelo capture **relaciones no lineales y complejas** entre las variables de una manera estructurada. Al modelar estas dependencias "locales" de forma expl√≠cita, la BBN puede representar con precisi√≥n c√≥mo la probabilidad de un evento cambia en funci√≥n de los eventos directamente relacionados, incluso si la relaci√≥n no es lineal.

En resumen, las Redes de Creencia Bayesianas son modelos globales que permiten modelar relaciones probabil√≠sticas complejas y no lineales al especificar dependencias de manera local entre las variables. Son herramientas poderosas para el razonamiento bajo incertidumbre y la toma de decisiones.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado y no supervisado (estructuras probabil√≠sticas)",
  "‚úÖ Categ√≥rica o continua (depende de implementaci√≥n)",
  "‚úÖ Categ√≥ricas o continuas discretizadas",
  "‚úÖ Modela relaciones condicionales entre variables (gr√°ficamente)",
  "‚ùå No aplica (no es modelo de regresi√≥n lineal)",
  "‚úÖ Requiere independencia condicional entre nodos seg√∫n la red",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede ser sensible a outliers si se estiman mal las distribuciones",
  "‚úÖ Puede manejar correlaci√≥n entre variables de forma expl√≠cita en la red",
  "‚úÖ Alta interpretabilidad visual con grafos dirigidos ac√≠clicos",
  "‚ö†Ô∏è Costoso computacionalmente en grandes redes o aprendizaje estructural",
  "‚úÖ Validaci√≥n cruzada puede aplicarse en tareas supervisadas (clasificaci√≥n)",
  "‚ùå Mal rendimiento si hay muchas variables irrelevantes o dependencias no detectadas"
)

detalles <- c(
  "Modelo probabil√≠stico gr√°fico que representa relaciones condicionales entre variables mediante una red bayesiana (DAG).",
  "Puede usarse para clasificaci√≥n, predicci√≥n o inferencia probabil√≠stica.",
  "Se adapta a datos categ√≥ricos principalmente, pero tambi√©n se puede usar con discretizaci√≥n de continuas.",
  "Captura relaciones condicionales entre variables expl√≠citamente como conexiones dirigidas.",
  "No genera residuos como los modelos de regresi√≥n, por lo que la normalidad no aplica.",
  "Las dependencias condicionales deben estar bien modeladas en la estructura de la red.",
  "No hay un modelo de varianza/residuos tradicional como para aplicar homoscedasticidad.",
  "Distribuciones err√≥neas o mal estimadas pueden afectar resultados, especialmente con valores extremos.",
  "El modelo representa expl√≠citamente la correlaci√≥n entre variables mediante arcos.",
  "La estructura de la red permite ver c√≥mo interact√∫an las variables entre s√≠.",
  "El aprendizaje estructural y de par√°metros puede ser costoso en t√©rminos computacionales.",
  "Si se usa para clasificaci√≥n, la validaci√≥n cruzada es una forma est√°ndar de evaluaci√≥n.",
  "BBN requiere una buena estructura; datos mal preparados o muy ruidosos deterioran su capacidad explicativa."
)

tabla_bbn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_bbn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir BBN",
             subtitle = "Bayesian Belief Network (BBN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Bayesian Linear Regression (BLR) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/BLR.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_blr <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_blr <- c(
  "‚úÖ Supervisado",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Lineal",
  "‚úÖ Necesaria (asunci√≥n para el error)",
  "‚úÖ Necesaria",
  "‚úÖ Necesaria",
  "‚ö†Ô∏è S√≠ (puede mitigar con priors robustos)",
  "‚úÖ Maneja bien (a trav√©s de priors)",
  "‚úÖ Alta (distribuciones de coeficientes)",
  "‚ö†Ô∏è Moderada a Baja (muestreo, computacionalmente intensivo)",
  "‚úÖ Compatible y muy √∫til",
  "‚ùå Relaciones no lineales o datos no normales"
)

detalles_blr <- c(
  "Modelo de regresi√≥n supervisado que aborda la regresi√≥n lineal desde una perspectiva Bayesiana. En lugar de estimar un √∫nico valor para los coeficientes, estima una distribuci√≥n de probabilidad para ellos, incorporando conocimiento previo (priors).",
  "La variable dependiente debe ser num√©rica y continua, al igual que en la regresi√≥n lineal cl√°sica.",
  "Las variables predictoras pueden ser num√©ricas o categ√≥ricas (estas √∫ltimas deben ser codificadas, por ejemplo, con one-hot encoding).",
  "Asume una relaci√≥n lineal entre las variables predictoras y la media de la variable respuesta. La linealidad es en los par√°metros, no necesariamente en las caracter√≠sticas (se pueden incluir t√©rminos polin√≥micos).",
  "Asume que los errores (residuos) se distribuyen normalmente (o con otra distribuci√≥n especificada) con media cero, similar a la regresi√≥n lineal cl√°sica. Sin embargo, la elecci√≥n de *priors* robustos puede mitigar la sensibilidad a la normalidad.",
  "Asume que los errores asociados a las observaciones son independientes entre s√≠. Esto es crucial para la validez de la inferencia.",
  "Asume que la varianza de los errores es constante en todos los niveles de las variables predictoras (homoscedasticidad). Al igual que la normalidad, *priors* sobre la varianza pueden influir.",
  "S√≠, la BLR puede ser sensible a outliers si se usan priors no informativos o poco robustos. Sin embargo, la flexibilidad Bayesiana permite el uso de distribuciones *prior* m√°s robustas (ej., t-Student) para los errores o los par√°metros, lo que puede mitigar este problema.",
  "Maneja la multicolinealidad de manera m√°s robusta que la regresi√≥n lineal OLS mediante la incorporaci√≥n de *priors* informativos en los coeficientes. Estos *priors* pueden 'encoger' los coeficientes y estabilizar la estimaci√≥n, actuando como una forma de regularizaci√≥n.",
  "La interpretabilidad es alta. En lugar de un solo valor, se obtienen distribuciones de probabilidad para cada coeficiente, lo que proporciona una medida de la incertidumbre. Esto permite afirmaciones probabil√≠sticas sobre el efecto de cada predictor.",
  "El entrenamiento puede ser computacionalmente m√°s intensivo y lento que la regresi√≥n lineal OLS, especialmente para grandes datasets o modelos complejos, ya que a menudo requiere m√©todos de muestreo como Monte Carlo de Cadena de Markov (MCMC). La predicci√≥n es r√°pida una vez entrenado.",
  "Es compatible y muy √∫til. Permite evaluar la robustez del modelo y la sensibilidad a la elecci√≥n de *priors*. La validaci√≥n cruzada Bayesiana (ej., Leave-One-Out Cross-Validation LOO) es una forma robusta de evaluar el rendimiento predictivo.",
  "No funciona bien si la relaci√≥n subyacente entre las variables no es lineal y no se incorporan transformaciones adecuadas (ej., t√©rminos polin√≥micos). Tampoco es ideal si los datos violan gravemente los supuestos de distribuci√≥n de errores y no se usan *priors* o modelos de error robustos."
)

tabla_blr <- data.frame(Criterio = criterios_blr, Aplica = aplica_blr, Detalles = detalles_blr)

tabla_blr %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Bayesian Linear Regression (BLR)",
    subtitle = "Regresi√≥n Lineal desde una Perspectiva Bayesiana"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Gaussian Naive Bayes (GNB) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/GNB.png"))
```

**Gaussian Naive Bayes (GNB)** es una variante del popular algoritmo **Naive Bayes (NB)**, utilizado para tareas de **clasificaci√≥n supervisada**. Es particularmente adecuado cuando las **variables predictoras (atributos) son de tipo continuo**. Al igual que todos los clasificadores Naive Bayes, GNB se basa en el **Teorema de Bayes** y, fundamentalmente, en la **suposici√≥n de independencia condicional** entre las variables predictoras, dado el valor de la clase.

La diferencia clave entre GNB y otras variantes de Naive Bayes (como Multinomial Naive Bayes o Bernoulli Naive Bayes) es la forma en que modela la **probabilidad de los atributos continuos**. Espec√≠ficamente:

1.  **Suposici√≥n de Distribuci√≥n Gaussiana:** GNB asume que los valores de cada atributo continuo, *dada una clase espec√≠fica*, siguen una **distribuci√≥n normal (Gaussiana)**. Es decir, para cada clase y cada atributo, se estima la media ($\mu$) y la desviaci√≥n est√°ndar ($\sigma$) de los valores de ese atributo dentro de esa clase.
2.  **C√°lculo de Probabilidades:** Cuando se necesita clasificar una nueva observaci√≥n, GNB utiliza las funciones de densidad de probabilidad (PDF) de estas distribuciones Gaussianas para calcular la probabilidad de observar el valor del atributo para cada clase.
3.  **Aplicaci√≥n del Teorema de Bayes:** Finalmente, utiliza el Teorema de Bayes para calcular la probabilidad posterior de cada clase, dadas las probabilidades de los atributos, y asigna la observaci√≥n a la clase con la probabilidad posterior m√°s alta.

A pesar de su suposici√≥n de independencia (que rara vez se cumple perfectamente en la pr√°ctica), GNB a menudo funciona sorprendentemente bien, especialmente en conjuntos de datos grandes o cuando las caracter√≠sticas son ruidosas. Su simplicidad y eficiencia computacional lo hacen un buen punto de partida para muchos problemas de clasificaci√≥n.

**Aprendizaje Global vs. Local:**

Gaussian Naive Bayes (GNB) es un modelo de **aprendizaje global**.

* **Aspecto Global:** GNB construye un **modelo probabil√≠stico global** de la relaci√≥n entre las caracter√≠sticas y las clases. Las medias y desviaciones est√°ndar de las distribuciones Gaussianas para cada atributo dentro de cada clase se estiman a partir de **todos los datos de entrenamiento**. La regla de clasificaci√≥n final, que asigna una nueva instancia a la clase m√°s probable, se basa en estas distribuciones param√©tricas globales y en el Teorema de Bayes, aplic√°ndose de manera uniforme en todo el espacio de caracter√≠sticas. No se ajustan modelos locales para diferentes vecindarios de datos.

* **Impacto de la Asunci√≥n de Independencia:** La suposici√≥n de independencia condicional (que los atributos son independientes entre s√≠ dado la clase) significa que GNB no intenta capturar interacciones complejas o no lineales entre las variables predictoras. Si bien esto simplifica dr√°sticamente el modelo y lo hace eficiente, tambi√©n implica que su capacidad para modelar relaciones no lineales entre *predictores* es limitada. Si los datos no se distribuyen linealmente y las interacciones entre los predictores son cruciales para la clasificaci√≥n, GNB podr√≠a no ser el modelo m√°s flexible. Sin embargo, su robustez ante la violaci√≥n de suposiciones y su velocidad lo mantienen como una opci√≥n valiosa en muchos escenarios.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas (asume distribuci√≥n normal por clase)",
  "‚ùå No modela relaciones entre predictores (independencia asumida)",
  "‚ùå No aplica (no hay residuos como en regresi√≥n)",
  "‚úÖ Asume independencia condicional entre predictores",
  "‚úÖ Cada predictor se modela con varianza homog√©nea por clase",
  "‚ö†Ô∏è Sensible a outliers porque afectan media y varianza de la normal",
  "‚ö†Ô∏è Alta multicolinealidad viola el supuesto de independencia",
  "‚úÖ Altamente interpretable: muestra probabilidades y distribuci√≥n por clase",
  "‚úÖ Muy r√°pido y eficiente, incluso con grandes datasets",
  "‚úÖ Se puede validar f√°cilmente con k-fold o hold-out",
  "‚ùå Si los predictores no son aproximadamente normales por clase, el rendimiento baja"
)

detalles <- c(
  "Clasificador probabil√≠stico que asume que cada predictor sigue una distribuci√≥n normal dentro de cada clase.",
  "Se utiliza para predecir clases categ√≥ricas a partir de predictores continuos.",
  "Cada variable num√©rica se modela con una distribuci√≥n Gaussiana separada por clase.",
  "No considera correlaciones entre predictores; cada uno contribuye de manera independiente.",
  "No genera residuos como un modelo de regresi√≥n, as√≠ que no aplica normalidad de errores.",
  "El supuesto clave es independencia condicional entre predictores dado la clase.",
  "Cada variable tiene su propia media y varianza por clase, sin heterocedasticidad.",
  "Los valores at√≠picos pueden distorsionar los par√°metros estimados de la distribuci√≥n normal.",
  "Altamente correlacionadas violan la independencia condicional asumida y afectan rendimiento.",
  "F√°cil de explicar: se basa en la probabilidad de cada clase dado cada predictor.",
  "Uno de los algoritmos m√°s r√°pidos para clasificaci√≥n supervisada.",
  "Evaluaci√≥n est√°ndar con validaci√≥n cruzada o conjunto de prueba.",
  "Si las variables no tienen forma aproximadamente normal dentro de clases, el modelo puede clasificarlas mal."
)

tabla_gnb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_gnb %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir GNB",
             subtitle = "Gaussian Naive Bayes (GNB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Hidden Markov Models (HMMs) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/HMMs.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_hmm <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_hmm <- c(
  "‚úÖ Supervisado/No Supervisado (secuencial, inferencia de estados)",
  "‚úÖ Observaciones secuenciales (discretas/continuas)",
  "‚ùå No aplica directamente (el modelo es la secuencia observada)",
  "‚úÖ Compleja (dependencias temporales y estados ocultos)",
  "‚ùå No es requisito directo (depende de la distribuci√≥n de emisi√≥n)",
  "‚ùå No aplica (asume dependencia entre estados)",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Moderada (si los estados ocultos son interpretables)",
  "‚ö†Ô∏è Moderada a Baja (entrenamiento, inferencia)",
  "‚úÖ Compatible (para evaluaci√≥n predictiva)",
  "‚ùå No hay dependencia temporal o los estados ocultos no son discretos"
)

detalles_hmm <- c(
  "Modelos probabil√≠sticos que describen un sistema que se cree que es un proceso de M√°rkov con **estados ocultos (no observables)**. Las observaciones son una funci√≥n de esos estados ocultos. Se usan para reconocimiento de patrones, modelado de series de tiempo y secuenciales.",
  "Las HMMs no tienen una 'variable respuesta' predictora en el sentido tradicional. En su lugar, tienen una secuencia de **observaciones** (que pueden ser discretas o continuas) generadas por los estados ocultos. Se utilizan para inferir la secuencia de estados ocultos m√°s probable o la probabilidad de una secuencia observada.",
  "No se utilizan 'variables predictoras' externas de la misma manera que en la regresi√≥n. El modelo se entrena sobre secuencias de observaciones para aprender las probabilidades de transici√≥n entre estados ocultos y las probabilidades de emisi√≥n de observaciones desde esos estados.",
  "La relaci√≥n es compleja: los estados ocultos forman una cadena de M√°rkov (un estado solo depende del anterior), y las observaciones dependen √∫nicamente del estado oculto actual. Esto permite modelar dependencias temporales y estructuras secuenciales.",
  "No asume normalidad de los residuos como en la regresi√≥n. La distribuci√≥n de emisi√≥n de las observaciones desde cada estado oculto puede ser gaussiana, categ√≥rica, etc., dependiendo del tipo de datos.",
  "Por su naturaleza, las HMMs **no asumen independencia de errores**. Asumen una dependencia de primer orden entre los estados ocultos (propiedad de M√°rkov) y una independencia condicional de las observaciones dado el estado actual.",
  "No asume homoscedasticidad. La varianza de las observaciones puede diferir seg√∫n el estado oculto (si se usan distribuciones de emisi√≥n con varianza).",
  "S√≠, las HMMs pueden ser sensibles a los outliers en las secuencias de observaci√≥n, ya que estos puntos an√≥malos pueden influir en las estimaciones de las probabilidades de emisi√≥n y transici√≥n, sesgando el modelo.",
  "El concepto de multicolinealidad entre 'predictores' no aplica directamente, ya que el modelo trabaja con secuencias de observaciones, no con m√∫ltiples caracter√≠sticas independientes en el mismo punto temporal.",
  "La interpretabilidad es moderada. Si los estados ocultos pueden ser conceptualmente asociados con eventos o fases reales (ej., 'hablando', 'silencio' en el habla), el modelo es interpretable. Sin embargo, los par√°metros de probabilidad suelen ser dif√≠ciles de interpretar directamente.",
  "La velocidad y eficiencia var√≠an. El entrenamiento (algoritmo de Baum-Welch) y la inferencia (algoritmo de Viterbi, Forward-Backward) pueden ser computacionalmente intensivos, especialmente para secuencias largas o un gran n√∫mero de estados ocultos.",
  "Es compatible para evaluar la capacidad predictiva del modelo sobre nuevas secuencias. Se puede usar para medir la probabilidad de secuencias no vistas o la precisi√≥n en tareas de clasificaci√≥n/segmentaci√≥n si se tienen etiquetas verdaderas.",
  "No funciona bien si: 1) no hay una dependencia temporal subyacente en los datos, 2) los estados ocultos no son discretos o no se ajustan a la suposici√≥n de M√°rkov de primer orden, 3) las secuencias son demasiado cortas para aprender patrones robustos, o 4) hay un ruido excesivo que enmascara las transiciones de estado."
)

tabla_hmm <- data.frame(Criterio = criterios_hmm, Aplica = aplica_hmm, Detalles = detalles_hmm)

tabla_hmm %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Hidden Markov Models (HMMs)",
    subtitle = "Modelos para Secuencias y Estados Ocultos"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Kalman Filter {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/KF.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_kalman <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_kalman <- c(
  "‚úÖ Supervisado (Estimaci√≥n de estados en series de tiempo)",
  "‚úÖ Estados de un sistema (internos y/o observables)",
  "‚úÖ Observaciones secuenciales (con ruido), Entradas de control",
  "‚úÖ Lineal (o linealizable, con extensiones como EKF/UKF)",
  "‚úÖ Necesaria (asunci√≥n de ruido Gaussiano)",
  "‚úÖ Necesaria (ruido del proceso y de medici√≥n)",
  "‚úÖ Necesaria (matrices de covarianza de ruido)",
  "‚ö†Ô∏è S√≠, muy",
  "‚ùå No aplica directamente (se modelan covarianza)",
  "‚úÖ Alta (estados del sistema, covarianza)",
  "‚úÖ Muy alta (computacionalmente eficiente)",
  "‚ùå No aplica directamente (evaluaci√≥n de la estimaci√≥n)",
  "‚ùå No linealidad fuerte o ruido no Gaussiano"
)

detalles_kalman <- c(
  "Algoritmo recursivo que estima el estado de un sistema din√°mico a partir de una serie de mediciones ruidosas. Es √≥ptimo para sistemas lineales con ruido Gaussiano. Consiste en una fase de predicci√≥n y una de actualizaci√≥n (correcci√≥n).",
  "El Filtro de Kalman estima las 'variables de estado' internas de un sistema que no pueden ser observadas directamente o que son medidas con ruido (ej., posici√≥n y velocidad de un objeto).",
  "Recibe como 'predictores' una secuencia de **observaciones** (mediciones ruidosas) y opcionalmente **entradas de control** (acciones externas que afectan el sistema).",
  "Asume un modelo lineal que describe la transici√≥n del estado del sistema de un momento a otro y la relaci√≥n lineal entre el estado y las observaciones. Para sistemas no lineales, se utilizan extensiones como el Filtro de Kalman Extendido (EKF) o el Filtro de Kalman No Perfumado (UKF).",
  "Una suposici√≥n **clave** es que tanto el ruido del proceso (que afecta la transici√≥n del estado) como el ruido de la medici√≥n (que afecta las observaciones) son procesos **Gaussianos** con media cero y covarianza conocida.",
  "Asume que el ruido del proceso es independiente del ruido de la medici√≥n, y que los ruidos en diferentes instantes de tiempo son independientes entre s√≠. La independencia entre las observaciones (condicionadas al estado) tambi√©n es fundamental.",
  "Asume que las matrices de covarianza del ruido del proceso y del ruido de la medici√≥n son constantes o conocidas. Si la varianza del ruido cambia, el filtro debe ser reajustado.",
  "S√≠, el Filtro de Kalman es muy sensible a los outliers. Un outlier en las mediciones puede sesgar gravemente la estimaci√≥n del estado, ya que se asume que las mediciones siguen una distribuci√≥n Gaussiana con respecto al estado verdadero.",
  "El concepto de multicolinealidad entre predictores no aplica directamente como en la regresi√≥n. En su lugar, el filtro modela las covarianza entre las variables de estado y entre el ruido.",
  "La interpretabilidad es alta. El filtro proporciona estimaciones del estado y, crucialmente, las matrices de covarianza de la estimaci√≥n, lo que permite cuantificar la incertidumbre de la estimaci√≥n en cada paso.",
  "Es extremadamente eficiente computacionalmente. El filtro es recursivo y opera en tiempo real, lo que lo hace ideal para aplicaciones en l√≠nea donde las estimaciones deben actualizarse r√°pidamente a medida que llegan nuevas mediciones.",
  "La validaci√≥n cruzada no se aplica de la misma manera que en modelos de ML predictivos. La evaluaci√≥n se centra en la precisi√≥n de la estimaci√≥n del estado (ej., error RMS) y la consistencia de las covarianza de error, a menudo compar√°ndolo con la 'verdad' de referencia si est√° disponible.",
  "No funciona bien si: 1) el sistema es **altamente no lineal** y las linealizaciones (EKF) o aproximaciones (UKF) no son suficientes, 2) el **ruido no es Gaussiano**, 3) las **matrices de covarianza del ruido no se conocen** con precisi√≥n (lo que requiere m√©todos como el filtro de Kalman adaptativo), o 4) hay una alta frecuencia de **outliers** que no son manejados por pre-procesamiento o extensiones robustas."
)

tabla_kalman <- data.frame(Criterio = criterios_kalman, Aplica = aplica_kalman, Detalles = detalles_kalman)

tabla_kalman %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir el Filtro de Kalman",
    subtitle = "Estimaci√≥n √ìptima de Estados en Sistemas Din√°micos"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Multinomial Naive Bayes (MNB) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/MNB.png"))
```

**Multinomial Naive Bayes (MNB)** es una variante del algoritmo **Naive Bayes** dise√±ada espec√≠ficamente para la **clasificaci√≥n de datos discretos**, y es particularmente popular en tareas de **procesamiento de lenguaje natural (NLP)**, como la clasificaci√≥n de texto (ej., spam/no spam, clasificaci√≥n de documentos por tema). Al igual que otras formas de Naive Bayes, se basa en el **Teorema de Bayes** y la **suposici√≥n clave de independencia condicional** entre las caracter√≠sticas, dado el valor de la clase.

La diferencia fundamental de MNB radica en que asume que las caracter√≠sticas (como el recuento de palabras en un documento de texto) provienen de una **distribuci√≥n multinomial**. Esto significa que:

1.  **Caracter√≠sticas de Recuento:** MNB es ideal para caracter√≠sticas que representan **frecuencias o recuentos** (ej., el n√∫mero de veces que aparece una palabra en un documento, el n√∫mero de veces que ocurre un evento).
2.  **Modelado de Probabilidades:** Para cada clase, MNB calcula la probabilidad de observar cada caracter√≠stica (ej., cada palabra del vocabulario) dado que la instancia pertenece a esa clase. Estas probabilidades se estiman a menudo utilizando **suavizado Laplace (o aditivo)** para evitar probabilidades de cero para palabras no vistas durante el entrenamiento.
3.  **Aplicaci√≥n del Teorema de Bayes:** Luego, para clasificar una nueva instancia, multiplica las probabilidades de las caracter√≠sticas (asumiendo independencia) por la probabilidad previa de cada clase, y elige la clase que tiene la probabilidad posterior m√°s alta.

MNB es altamente eficiente, escalable para grandes conjuntos de datos y a menudo sorprendentemente efectivo a pesar de su ingenua suposici√≥n de independencia, lo que lo convierte en una l√≠nea base s√≥lida para muchos problemas de clasificaci√≥n de texto.

**Aprendizaje Global vs. Local:**

Multinomial Naive Bayes (MNB) es un modelo de **aprendizaje global**.

* **Aspecto Global:** MNB construye un **modelo probabil√≠stico global** para la relaci√≥n entre las caracter√≠sticas discretas (como recuentos de palabras) y las clases. Las probabilidades de las caracter√≠sticas dadas las clases (y las probabilidades previas de las clases) se estiman a partir de **todos los datos de entrenamiento**. La regla de clasificaci√≥n final, basada en el Teorema de Bayes, se aplica de manera uniforme a cualquier nueva instancia en el espacio de caracter√≠sticas. No se ajustan modelos locales para diferentes vecindarios de datos; en su lugar, se utilizan las mismas probabilidades estimadas globalmente para todas las predicciones.

* **Impacto de la Asunci√≥n de Independencia:** La suposici√≥n de independencia condicional entre las caracter√≠sticas (ej., que la presencia de una palabra no influye en la probabilidad de otra palabra dada la categor√≠a del documento) es una simplificaci√≥n global. Si bien esta simplicidad permite que MNB sea muy eficiente y robusto a veces, tambi√©n significa que no puede capturar interacciones complejas o dependencias no lineales entre las caracter√≠sticas en el mismo sentido que modelos m√°s avanzados. Sin embargo, en muchas aplicaciones como la clasificaci√≥n de texto, donde la frecuencia individual de las palabras es muy informativa, esta suposici√≥n es lo suficientemente robusta para un buen rendimiento. Es un modelo que asume una estructura de probabilidad global y la aplica consistentemente.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Discretas, conteos (ej. frecuencia de palabras)",
  "‚ùå No modela relaciones entre predictores (asume independencia condicional)",
  "‚ùå No aplica (no hay residuos)",
  "‚úÖ Asume independencia condicional entre predictores",
  "‚ùå No aplica (no se modela varianza)",
  "‚ö†Ô∏è Menos sensible a outliers que Gaussian NB, pero a√∫n puede verse afectado",
  "‚ö†Ô∏è Multicolinealidad viola el supuesto de independencia y puede degradar el rendimiento",
  "‚úÖ Muy interpretable: probabilidades por clase y variable",
  "‚úÖ Extremadamente eficiente en problemas de texto y alta dimensi√≥n",
  "‚úÖ Puede usarse k-fold o validaci√≥n simple",
  "‚ùå No es adecuado para variables continuas o datos que no representen conteos"
)

detalles <- c(
  "Clasificador basado en probabilidad que modela la distribuci√≥n multinomial de conteos por clase.",
  "Usado t√≠picamente para clasificaci√≥n de texto, spam detection, y otros problemas con datos categ√≥ricos o de conteo.",
  "Funciona mejor con variables que representan frecuencia (n√∫mero de veces que aparece un t√©rmino).",
  "Asume que los predictores son independientes condicionalmente dados la clase, sin correlaci√≥n entre ellos.",
  "No hay residuos como en modelos de regresi√≥n, por lo tanto no aplica este supuesto.",
  "La independencia condicional de los predictores es un supuesto fundamental del modelo.",
  "Como no se modela la varianza expl√≠citamente, el supuesto de homoscedasticidad no aplica.",
  "Outliers tienen menor efecto porque se espera que los datos est√©n en formato de conteo (discretos).",
  "Predictores altamente correlacionados pueden afectar negativamente la precisi√≥n del modelo.",
  "La probabilidad condicional de cada clase y predictor es f√°cil de interpretar.",
  "Muy r√°pido incluso en datasets grandes con miles de caracter√≠sticas (como texto).",
  "La precisi√≥n puede evaluarse con validaci√≥n cruzada como en otros clasificadores.",
  "Si las variables son continuas o no reflejan bien los conteos, el modelo puede fallar."
)

tabla_mnb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mnb %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MNB",
             subtitle = "Multinomial Naive Bayes (MNB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Naive Bayes (NB) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/NB.png"))
```

**Naive Bayes (NB)** es un algoritmo de **clasificaci√≥n supervisada** popular y computacionalmente eficiente, basado en el **Teorema de Bayes** y una **fuerte (o "ingenua") suposici√≥n de independencia condicional** entre las caracter√≠sticas (variables predictoras) dado el valor de la clase. Esta suposici√≥n significa que el modelo asume que la presencia o ausencia de una caracter√≠stica particular no afecta la presencia o ausencia de otra caracter√≠stica, una vez que se conoce la clase.

A pesar de que esta suposici√≥n rara vez se cumple perfectamente en problemas del mundo real, Naive Bayes a menudo ofrece un **rendimiento sorprendentemente bueno**, especialmente en tareas de clasificaci√≥n de texto y con grandes conjuntos de datos. Su simplicidad y velocidad lo convierten en un excelente algoritmo de l√≠nea base.

El funcionamiento b√°sico de Naive Bayes es el siguiente:

1.  **C√°lculo de Probabilidades Previas:** Estima la probabilidad de cada clase en el conjunto de entrenamiento (ej., P(Clase A), P(Clase B)).
2.  **C√°lculo de Probabilidades de Verosimilitud:** Para cada caracter√≠stica y cada clase, calcula la probabilidad de que la caracter√≠stica tome un valor espec√≠fico, dado que la instancia pertenece a esa clase (ej., P(Caracter√≠stica X | Clase A)). Aqu√≠ es donde entran las diferentes variantes de Naive Bayes (Gaussian para caracter√≠sticas continuas, Multinomial para recuentos, Bernoulli para caracter√≠sticas binarias).
3.  **Aplicaci√≥n del Teorema de Bayes:** Para clasificar una nueva instancia, utiliza el Teorema de Bayes para combinar estas probabilidades y calcular la probabilidad posterior de cada clase, dadas las caracter√≠sticas de la nueva instancia. Finalmente, asigna la instancia a la clase con la probabilidad posterior m√°s alta.


**Aprendizaje Global vs. Local:**

Naive Bayes (NB) es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** Naive Bayes construye un **modelo probabil√≠stico global** que describe la relaci√≥n entre las caracter√≠sticas y las clases para todo el conjunto de datos. Las probabilidades previas de las clases y las probabilidades condicionales de las caracter√≠sticas dadas las clases se estiman a partir de **todos los datos de entrenamiento**. La regla de clasificaci√≥n resultante se aplica de manera uniforme a cualquier nueva instancia en el espacio de caracter√≠sticas, sin ajustar modelos locales para diferentes vecindarios de datos. El modelo aprende una distribuci√≥n de probabilidad que se asume v√°lida para todo el dominio.

* **Impacto de la Suposici√≥n de Independencia:** La "ingenuidad" del modelo, es decir, la suposici√≥n de independencia entre las caracter√≠sticas, es una simplificaci√≥n global. No intenta capturar interacciones complejas o no lineales entre las caracter√≠sticas en s√≠. Si bien esto puede ser una limitaci√≥n cuando las relaciones entre las caracter√≠sticas son muy intrincadas y no lineales, es precisamente esta suposici√≥n la que le otorga su eficiencia y robustez en muchos escenarios pr√°cticos. Es un modelo que asume una estructura de probabilidad global y la aplica de manera consistente.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Categ√≥ricas o num√©ricas (seg√∫n variante del modelo)",
  "‚ùå Asume independencia condicional entre predictores",
  "‚ùå No aplica (no es regresi√≥n)",
  "‚úÖ Observaciones deben ser independientes",
  "‚ùå No se requiere homocedasticidad",
  "‚ö†Ô∏è Puede ser sensible si se usa con predictores num√©ricos y hay valores extremos",
  "‚úÖ No afectado por multicolinealidad debido a suponer independencia",
  "‚úÖ Alta, se pueden interpretar probabilidades y efectos de cada variable",
  "‚úÖ Muy r√°pido incluso con grandes conjuntos de datos",
  "‚úÖ Puede usarse para afinar y evaluar desempe√±o del modelo",
  "‚ùå Bajo rendimiento si los predictores no son realmente independientes o las distribuciones asumidas no se cumplen"
)

detalles <- c(
  "Clasificador probabil√≠stico basado en el teorema de Bayes con asunci√≥n de independencia condicional entre predictores.",
  "Funciona para clasificaci√≥n en m√∫ltiples clases categ√≥ricas.",
  "Puede trabajar con variables categ√≥ricas (Multinomial NB) o num√©ricas (Gaussian NB).",
  "Supone que las variables predictoras son independientes entre s√≠ dentro de cada clase.",
  "No genera residuos en el sentido cl√°sico porque no es un modelo de regresi√≥n.",
  "Las observaciones deben ser independientes para que las probabilidades se combinen correctamente.",
  "No requiere igualdad de varianzas; Gaussian NB asume varianza igual por clase pero se puede ajustar.",
  "Los valores at√≠picos pueden distorsionar la estimaci√≥n de probabilidades si hay predictores num√©ricos.",
  "La independencia entre predictores hace que la multicolinealidad no sea problema.",
  "Los resultados pueden interpretarse en t√©rminos de probabilidades a posteriori por clase.",
  "Muy eficiente para entrenamiento y predicci√≥n, incluso con muchos atributos.",
  "Puede validarse usando k-fold o leave-one-out para asegurar estabilidad del modelo.",
  "El supuesto fuerte de independencia condicional rara vez se cumple completamente, lo que puede afectar la precisi√≥n."
)

tabla_nb <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_nb %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir NB",
             subtitle = "Naive Bayes (NB)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Particle Filter {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Bayesian/Particle Filter.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_particle_filter <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_particle_filter <- c(
  "‚úÖ Supervisado (Estimaci√≥n de estados en series de tiempo)",
  "‚úÖ Estados de un sistema (internos y/o observables)",
  "‚úÖ Observaciones secuenciales (con ruido), Entradas de control",
  "‚úÖ Muy compleja (no lineal, multimodal)",
  "‚ùå No es requisito",
  "‚úÖ Necesaria (ruido del proceso y de medici√≥n)",
  "‚ùå No es requisito",
  "‚úÖ S√≠, pero puede mitigar (con resampling robusto)",
  "‚ùå No aplica directamente (se modelan distribuciones)",
  "‚ö†Ô∏è Moderada (se estiman distribuciones, no valores √∫nicos)",
  "‚ö†Ô∏è Baja (computacionalmente intensivo, especialmente con muchas part√≠culas)",
  "‚ùå No aplica directamente (evaluaci√≥n de la estimaci√≥n)",
  "‚ùå Dimensiones muy altas o modelo de ruido muy incierto"
)

detalles_particle_filter <- c(
  "Algoritmo secuencial de Monte Carlo utilizado para estimar el estado de un sistema din√°mico. A diferencia del Filtro de Kalman, no asume linealidad ni ruido Gaussiano, siendo apto para sistemas no lineales y distribuciones no Gaussianas (multimodales).",
  "El Filtro de Part√≠culas estima la distribuci√≥n de probabilidad de las 'variables de estado' internas de un sistema, que pueden ser continuas o discretas, observables o inferidas a partir de mediciones ruidosas.",
  "Recibe como 'predictores' una secuencia de **observaciones** ruidosas y, opcionalmente, **entradas de control** (acciones externas). Estas observaciones se utilizan para ponderar y re-muestrear las part√≠culas.",
  "Puede modelar relaciones altamente complejas y no lineales entre los estados del sistema y entre los estados y las observaciones. Lo logra representando la distribuci√≥n de probabilidad del estado como un conjunto de 'part√≠culas' ponderadas.",
  "No asume ninguna distribuci√≥n espec√≠fica para el ruido del proceso o de la medici√≥n (es decir, no asume normalidad). Esta es una de sus mayores fortalezas, permitiendo manejar distribuciones multimodales o asim√©tricas.",
  "Asume que el ruido del proceso y el ruido de la medici√≥n son independientes entre s√≠ en cada paso de tiempo, y que los ruidos en diferentes instantes son independientes. Las observaciones son condicionalmente independientes dado el estado.",
  "No asume homoscedasticidad. Las distribuciones de ruido pueden tener varianzas que cambian con el tiempo o el estado, y el filtro puede adaptarse a ello.",
  "S√≠, el Filtro de Part√≠culas puede ser sensible a los outliers extremos, ya que una observaci√≥n muy at√≠pica podr√≠a hacer que todas las part√≠culas tengan un peso muy bajo, lo que lleva a la degeneraci√≥n del filtro. Sin embargo, puede ser m√°s robusto que el Filtro de Kalman en presencia de cierto tipo de outliers debido a su naturaleza no param√©trica y al re-muestreo.",
  "El concepto de multicolinealidad entre predictores no aplica directamente. El filtro trabaja con la evoluci√≥n probabil√≠stica del estado a lo largo del tiempo, no con relaciones est√°ticas entre caracter√≠sticas.",
  "La interpretabilidad es moderada. Aunque no produce una √∫nica estimaci√≥n puntual (como el Filtro de Kalman para sistemas lineales), el conjunto de part√≠culas ponderadas proporciona una representaci√≥n de la distribuci√≥n de probabilidad del estado, lo que permite ver la incertidumbre y las posibles multimodalidades.",
  "Es computacionalmente intensivo y puede ser lento, especialmente cuando se requiere un gran n√∫mero de part√≠culas para representar con precisi√≥n la distribuci√≥n de probabilidad del estado (t√≠picamente en sistemas de alta dimensi√≥n o con distribuciones complejas). Su complejidad escala con el n√∫mero de part√≠culas y la dimensi√≥n del estado.",
  "La validaci√≥n cruzada no se aplica de la misma manera que en modelos de ML predictivos. La evaluaci√≥n se centra en la precisi√≥n de la estimaci√≥n del estado (ej., error RMS respecto a la verdad fundamental) y la consistencia de las estimaciones de la distribuci√≥n de probabilidad del estado.",
  "No funciona bien si: 1) la **dimensionalidad del espacio de estados es muy alta** (lo que requerir√≠a un n√∫mero intratable de part√≠culas para una buena aproximaci√≥n), 2) el **modelo del proceso o de observaci√≥n es extremadamente ruidoso o incierto**, 3) la **distribuci√≥n de propuesta es deficiente**, o 4) hay **pocas observaciones** para actualizar y ponderar las part√≠culas de manera efectiva."
)

tabla_particle_filter <- data.frame(Criterio = criterios_particle_filter, Aplica = aplica_particle_filter, Detalles = detalles_particle_filter)

tabla_particle_filter %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir el Filtro de Part√≠culas",
    subtitle = "Estimaci√≥n de Estados en Sistemas No Lineales y No Gaussianos"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```

