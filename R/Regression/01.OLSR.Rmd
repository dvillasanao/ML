---
title: "Ordinary Least Squares Regression (OLSR)"
author: "Diana Villasana Ocampo"
knit: (function(inputFile, encoding) {
       rmarkdown::render(inputFile, encoding = encoding, output_dir = "../../Output/")
  })
output:
   html_document:
      code_folding: hide
      highlight: tango
      theme: flatly
      toc: true
      toc_depth: 3
      toc_float:
        collapsed: yes
---

```{=html}
<style type="text/css">
body {
text-align: justify;
font-style: normal;
font-family: "Montserrat";
font-size: 12px
}
h1.title {
  font-size: 40px;
  color: #000D3B;
}
h1 {
  color: #B6854D;
}
h2 {
  color: #172984;
}
h3 {
  color: #172984;
}
</style>
```

```{=html}
<style>
.nav>li>a {
    position: relative;
    display: block;
    padding: 10px 15px;
    color: #1C3BA4
}
.nav-pills>li.active>a, .nav-pills>li.active>a:hover, .nav-pills>li>a:focus {
    color: #ffffff;
    background-color: #09C2BC
}
</style>
```

```{=html}
<style>
.tile1-text {
    position: relative;
    display: block;
    padding: 10px 15px;
    color: #0A6A87;
    list-style: none;
}
.top1-tiles a:nth-of-type(1):hover, .top-tiles1 a:nth-of-type(1):focus{
    color: #ffffff;
    background: #0A6A87
}
</style>
```

```{=html}
<style>
.tile2-text {
    position: relative;
    display: block;
    padding: 10px 15px;
    color: #0A6CC8;
    list-style: none;
}
.top2-tiles a:nth-of-type(1):hover, .top2-tiles a:nth-of-type(1):focus{
    color: #ffffff;
    background: #0A6CC8
}
</style>
```

```{=html}
<style>
.math {
  font-size: 15px;
  color: #1e42ab;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, 
                      cache.lazy = FALSE, class.source = "fold-show")
knitr::opts_knit$set(root.dir = here::here())
setwd(here::here())
```

```{r,echo=FALSE, eval=FALSE, }
rm(list = ls())
```

```{r, echo = FALSE, results=FALSE}
# Se descargan las fuentes de la google fonts
require(showtext)
library(extrafont)
# activar showtext
windowsFonts()
```

```{r, echo = FALSE}
# 1. Cargar librerías necesarias
library(tidyverse)
library(caret)     # Para dividir datos y evaluación
library(broom)     # Para tidy modelos
library(Metrics)   # Para métricas como RMSE, MAE
require(tibble)
```

La Regresión por Mínimos Cuadrados Ordinarios (Ordinary Least Squares Regression, **OLSR** u **OLS**) es una técnica estadística fundamental utilizada para estimar la relación entre una **variable dependiente** (o de respuesta) y una o más **variables independientes** (o predictoras). Es uno de los métodos más comunes en el análisis de regresión lineal.

## Objetivo

El objetivo principal de OLSR es encontrar la "línea de mejor ajuste" a través de un conjunto de puntos de datos. Esta línea se determina minimizando la suma de los cuadrados de las diferencias entre los valores observados de la variable dependiente y los valores predichos por el modelo. A estas diferencias se les llama **residuos** o **errores**. Al minimizar la suma de los cuadrados, OLS asegura que los errores positivos y negativos no se cancelen entre sí, y que los errores grandes tengan un impacto proporcionalmente mayor en la minimización.

## Metodología

La metodología de OLSR se basa en los siguientes pasos y principios:

1.  **Modelo Lineal:** OLSR asume una relación lineal entre las variables. Para una regresión lineal simple (una variable independiente), la ecuación es:\
    $$Y = \beta_0 + \beta_1X + \epsilon$$

    Donde:

    -   $Y$ es la variable dependiente.

-   $X$ es la variable independiente.
-   $\beta_0$ es el intercepto (el valor de $Y$ cuando $X$ es 0).
-   $\beta_1$ es la pendiente (el cambio en $Y$ por cada unidad de cambio en $X$).
-   $\epsilon$ es el término de error o residual, que representa la parte de $Y$ que no puede ser explicada por $X$.

Para una regresión lineal múltiple (varias variables independientes), la ecuación se expande a:\
$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \epsilon$$

2.  **Minimización de la Suma de Cuadrados de Residuos (SSR):** El corazón de OLS es encontrar los valores de los coeficientes ($\beta_0, \beta_1$, etc.) que minimicen la siguiente función:\
    $$\text{Minimizar } \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$\
    Donde:
    -   $y_i$ es el valor observado de la variable dependiente para la observación $i$.

-   $\hat{y}_i$ es el valor predicho de la variable dependiente por el modelo para la observación $i$.
-   $(y_i - \hat{y}_i)$ es el residual para la observación $i$.

Para lograr esta minimización, se utilizan técnicas de cálculo (derivadas parciales) para encontrar los valores de los coeficientes que hacen que la pendiente de la función de suma de cuadrados sea cero.

3.  **Estimación de Coeficientes:** Los valores estimados de los coeficientes, denotados como $\hat{\beta}_0, \hat{\beta}_1$, etc., son aquellos que resultan de la minimización. Estos coeficientes son los que definen la "línea de mejor ajuste".

4.  **Supuestos del OLS:** Para que los estimadores de OLS sean los "mejores estimadores lineales insesgados" (según el Teorema de Gauss-Markov), se deben cumplir ciertas suposiciones:

    -   **Linealidad:** La relación entre las variables es lineal.

    -   **Independencia de los errores:** Los errores de una observación no están correlacionados con los errores de otra.

    -   **Homocedasticidad:** La varianza de los errores es constante en todos los niveles de las variables independientes.

    -   **Normalidad de los errores:** Los errores se distribuyen normalmente (aunque no es estrictamente necesario para la estimación, sí lo es para la inferencia estadística).

    -   **No multicolinealidad perfecta:** Las variables independientes no están perfectamente correlacionadas entre sí.

## **Pasos generales del Machine Learning supervisado**

1.  **Importar y explorar los datos**
2.  **Preprocesamiento**
3.  **División de los datos (train/test)**
4.  **Entrenamiento del modelo**
5.  **Evaluación del modelo**
6.  **Ajustes o validación cruzada (si aplica)**
7.  **Predicción con nuevos datos**
8.  **Interpretación de resultados**

------------------------------------------------------------------------

## Base de datos

La base de datos `mtcars` es un conjunto de datos clásico en R que contiene información sobre **32 automóviles** (modelos de 1973–74), y fue extraído de la revista *Motor Trend US*. Incluye **variables técnicas** del desempeño de los autos.

Aquí está una descripción de cada columna:

| Variable | Significado | Tipo de dato |
|------------------|-----------------------------------|------------------|
| `mpg` | Miles per gallon (millas por galón) | Numérica |
| `cyl` | Número de cilindros | Entero |
| `disp` | Desplazamiento del motor (en pulgadas cúbicas) | Numérica |
| `hp` | Caballos de fuerza | Entero |
| `drat` | Relación del eje trasero (rear axle ratio) | Numérica |
| `wt` | Peso del auto (en miles de libras) | Numérica |
| `qsec` | Tiempo en 1/4 de milla (segundos) | Numérica |
| `vs` | Tipo de motor: 0 = V-shaped, 1 = straight (en línea) | Binaria (factor) |
| `am` | Tipo de transmisión: 0 = automática, 1 = manual | Binaria (factor) |
| `gear` | Número de velocidades (marchas) adelante | Entero |
| `carb` | Número de carburadores | Entero |

```{r}
# 2. Cargar y se exploran los datos
data("mtcars")
```

```{r, echo = FALSE}
require(gt)

mtcars %>% 
 gt() %>%
  tab_header(title = "mtcars data") %>%
   tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Cuando_") ~ px(300),
                   everything() ~ px(50)) %>%
         as_raw_html() 

```

## Entrenamiento de los datos (train/test)

-   Se usa la función `createDataPartition()` del paquete `caret` para **dividir los datos**.
-   Se crea un **índice** con el 80% de las filas del `mtcars`, **estratificado** según la variable `mpg` (la variable objetivo).
-   El argumento `p = 0.8` significa que el 80% se usará para **entrenamiento** y el 20% restante para **prueba**.
-   `list = FALSE` devuelve los índices como un vector, no como una lista.

```{r}
# Se quiere predecir `mpg` (millas por galón) usando otras variables
# mpg será la variable dependiente (target)

# 3. Se dividen losdatos en entrenamiento y prueba
set.seed(123)  # Para reproducibilidad
train_index <- createDataPartition(mtcars$mpg, p = 0.8, list = FALSE)
train_data <- mtcars[train_index, ]
test_data <- mtcars[-train_index, ]
```

-   Particionar los datos, evita el **overfitting** (cuando el modelo memoriza los datos de entrenamiento).
-   Permite una **evaluación honesta** del modelo al probarlo en datos que no vio durante el entrenamiento.
-   Es una práctica estándar en cualquier pipeline de aprendizaje automático.

## Entrenamiento del modelo

-   la función **`lm()`** es la función base de R para ajustar un modelo de **regresión lineal** (OLS = *Ordinary Least Squares*).

-   **`mpg ~ .`** es una fórmula que indica:

    -   `mpg` es la **variable dependiente** (lo que queremos predecir).
    -   `.` significa “todas las demás variables” del conjunto de datos se usarán como **variables independientes** (predictoras).
    -   **`data = train_data`** especifica que el modelo debe entrenarse usando el conjunto de **entrenamiento** (80% de los datos).

```{r}
# 4. Entrenar el modelo de regresión lineal (OLS)
modelo_ols <- lm(mpg ~ ., data = train_data)

# Revisar resumen del modelo
summary(modelo_ols)
```

Interpretemos los resultados:

**Residuales**

````         
```r
Residuals:
  Min      1Q  Median      3Q     Max 
-3.2742 -1.3609 -0.2707  1.1921  4.9877 
```
````

-   Indica la distribución de los **errores de predicción** (diferencia entre valor real y valor predicho).
-   Lo ideal es que estén **centrados en 0** y sean **simétricos**, lo que parece cumplirse aquí.
-   El valor máximo del error es aproximadamente ±5 millas por galón.

**Ninguna variable es estadísticamente significativa individualmente** (todas tienen p \> 0.05).

| Métrica | Valor | Significado |
|------------------|------------------|------------------------------------|
| **R-squared** | 0.8861 | El modelo explica el **88.6%** de la variabilidad de `mpg` |
| **Adjusted R-squared** | 0.8191 | Ajusta el R² penalizando el número de predictores |
| **F-statistic** | 13.23 | El modelo como conjunto es **significativo** |
| **p-value global** | 3.719e-06 | Muy bajo → **el modelo es útil en conjunto** (aunque variables individuales no lo sean) |

### 📌 **Observaciones generales**

-   **El modelo es bueno en conjunto**, pero ninguna variable individual es altamente significativa.\
-   Esto puede ser por **colinealidad** (las variables están correlacionadas entre sí).\
-   Se podría:
    -   Usar un modelo más simple (quitar predictores no útiles).
    -   Aplicar **selección de variables** (ej. stepAIC, regsubsets).
    -   Verificar colinealidad con el **VIF** (Variance Inflation Factor).
    -   Visualizar los residuos para ver si el modelo cumple los supuestos.
    
    
```{r, class.source = "fold-hide"}
require(ggplot2)
require(reshape2)

tabla <- mtcars %>% 
          melt(., id = "mpg")

p <- tabla %>% 
      ggplot(aes(y = mpg, x = value, color = variable)) + 
       geom_point() +
        geom_smooth(method = "lm", se = TRUE, color = "black") +
         theme_bw() + 
          theme(plot.title = element_text(size = 22, hjust = 0.15, family = "Montserrat", face = "bold"),
                plot.subtitle = element_text(size = 18, hjust = 0, family = "Montserrat", face = "bold"),
                plot.caption = element_text(size = 11, hjust = 0.2, vjust = 1, family = "Montserrat"), 
                axis.text = element_text(family = "Montserrat"), 
                axis.title = element_text(family = "Montserrat", size = 15), 
                legend.position = "none"
               ) + 
           scale_color_viridis_d(option = "A") +
            facet_wrap(variable~., scales = "free") + 
             labs(title = "Diagramas de dispersión",
                  x = "",
                  y = "")
p
```
    
## Selección de variables   


### Criterio de Información de Akaike   


El Criterio de Información de Akaike (AIC, por sus siglas en inglés, Akaike Information Criterion) es una medida de la bondad de ajuste de un modelo estadístico que penaliza la complejidad del modelo. Su objetivo es seleccionar el modelo que mejor se ajusta a los datos con la menor cantidad de parámetros, evitando así el sobreajuste (overfitting).

La fórmula general para calcular el AIC es:
  
$$AIC = 2k - 2\ln(L)$$
  
Donde:
  * $k$ es el número de parámetros del modelo. En un modelo de regresión, esto incluye el intercepto y los coeficientes de las variables predictoras, más la varianza del error si se estima (en modelos de mínimos cuadrados ordinarios, esto es a menudo el caso, por lo que $k$ a veces se cuenta como el número de coeficientes + 1 para la varianza del error, o simplemente el número de coeficientes si la varianza del error se considera implícita en la función de verosimilitud).
* $\ln(L)$ es el logaritmo natural del valor máximo de la función de verosimilitud (log-likelihood) del modelo. La función de verosimilitud mide qué tan bien el modelo reproduce los datos observados.

#### Cómo se interpreta:

* **Valores más bajos de AIC indican un mejor modelo.** El AIC busca un equilibrio entre la bondad de ajuste del modelo a los datos (representada por $\ln(L)$) y la complejidad del modelo (representada por $2k$).
* El término $2k$ es una **penalización por la complejidad**. Cada parámetro adicional en el modelo aumenta el valor de AIC, lo que desincentiva la inclusión de variables innecesarias que podrían sobreajustar los datos.  

#### Para modelos de Regresión por Mínimos Cuadrados Ordinarios (OLS):

Aunque la fórmula general del AIC es la que se mencionó, para modelos de OLS con residuos normalmente distribuidos, la función de log-verosimilitud tiene una forma específica, lo que permite reescribir el AIC de una manera más práctica.

Para un modelo de regresión lineal con $n$ observaciones y $k$ parámetros (incluyendo el intercepto), y asumiendo errores normales con varianza constante, el AIC se puede calcular como:
  
  $$AIC = n \cdot \ln\left(\frac{RSS}{n}\right) + 2k$$
  
  Donde:
  * $n$ es el número de observaciones (tamaño de la muestra).
* $RSS$ es la Suma de Cuadrados de los Residuos (Residual Sum of Squares), que es la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos por el modelo.
* $k$ es el número de parámetros del modelo (coeficientes de las variables independientes + intercepto).

**Es importante recordar:**
  
  * El AIC no es una prueba de hipótesis en el sentido tradicional; no te dice si un modelo es "bueno" o "malo" en un sentido absoluto.
* Es una herramienta para la **selección de modelos entre un conjunto de modelos candidatos**. Siempre se compara el AIC de diferentes modelos ajustados a los **mismos datos**. El modelo con el AIC más bajo es el preferido.
* Cuando la muestra es pequeña, a veces se prefiere el **AIC corregido (AICc)**, que añade una penalización adicional por el tamaño de la muestra:
  $$AICc = AIC + \frac{2k(k+1)}{n-k-1}$$
  A medida que $n$ (tamaño de la muestra) es grande, el término de corrección se vuelve insignificante y el AICc converge al AIC.


Se realiza una selección automática de variables para un modelo de regresión lineal, utilizando el **criterio AIC (Criterio de Información de Akaike**)   

```{r}
require(MASS)

# Aplicar selección automática con criterio AIC
modelo_step <- stepAIC(modelo_ols, direction = "both")
```

El procedimiento va eliminando o agregando variables al modelo original para reducir el **AIC (Criterio de Información de Akaike)**. El objetivo es encontrar un modelo más **parsimonioso** (simple) con **buena capacidad predictiva**.  

* El proceso empieza con un modelo completo:
  `mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb`
y un AIC inicial, por ejemplo: `AIC = 57.77`.

* Luego evalúa qué pasa si **quita** una variable (`- vs`, `- cyl`, etc.).

* Si al eliminar una variable el AIC **baja**, se acepta ese cambio.
Ejemplo:
  `- vs    1    2.1293  102.57  56.353`
Significa que si quitas la variable `vs`, el AIC baja de 57.77 a 56.35 → ¡mejor!
  
  * Se acepta esa modificación y se repite el procedimiento, ahora con el nuevo modelo.

* También puede intentar **agregar** variables previamente eliminadas (`+ vs`, `+ hp`, etc.) si esto mejora el AIC.

* Cuando **ya no puede bajar más el AIC** quitando o agregando variables. El modelo final es el que **tiene el AIC más bajo** alcanzado durante el proceso. 

```{r}
# Ver resumen del modelo seleccionado
summary(modelo_step)
```

* **Error estándar residual**: En promedio, el error de predicción es de ±2.2 mpg.
* **R² = 0.8738**: El modelo explica el **87.4% de la variabilidad** en `mpg`.
* **R² ajustado = 0.8518**: Ajusta por el número de predictores; aún muy bueno.
* **F-statistic = 39.81**, **p-value muy bajo (≈0)**: El modelo global es **altamente significativo**.


### Forward or backward stepwise  

Los modelos "forward" (hacia adelante) y "backward" (hacia atrás) son dos enfoques comunes dentro de la **regresión stepwise (paso a paso)**, un método de selección automática de variables para construir modelos de regresión. El objetivo de la regresión stepwise es encontrar un subconjunto óptimo de variables predictoras que expliquen la mayor parte de la varianza en la variable dependiente con la menor complejidad posible, evitando el sobreajuste y mejorando la interpretabilidad del modelo.

Ambos métodos operan de manera iterativa, agregando o eliminando variables basándose en un criterio estadístico (como el p-valor, AIC, BIC, etc.).
 
#### **Forward Selection** (Selección Hacia Adelante)

La selección hacia adelante comienza con un modelo "nulo", es decir, un modelo que no contiene ninguna variable predictora (solo el intercepto). Luego, en cada paso, evalúa todas las variables predictoras no incluidas en el modelo y agrega aquella que, al ser incluida, produce la mejora más significativa en el ajuste del modelo. Este proceso continúa hasta que ninguna de las variables restantes cumple con el criterio de entrada preestablecido (por ejemplo, su p-valor es mayor que un umbral determinado).

#### **Backward Elimination** (Eliminación Hacia Atrás)

La eliminación hacia atrás comienza con un modelo "completo", es decir, un modelo que incluye todas las variables predictoras candidatas. Luego, en cada paso, evalúa la contribución de cada variable predictora en el modelo y elimina aquella que, al ser retirada, tiene el menor impacto negativo en el ajuste del modelo (o la que es menos significativa, por ejemplo, la que tiene el p-valor más alto). Este proceso continúa hasta que todas las variables restantes en el modelo cumplen con un criterio de permanencia preestablecido (por ejemplo, su p-valor es menor que un umbral determinado).


#### Consideraciones Generales sobre la Regresión Stepwise

Es importante mencionar que, aunque los métodos forward y backward son populares por su automatización, también tienen sus **críticas y desventajas**:
  
  * **Sobreajuste (Overfitting):** Los modelos resultantes pueden ajustarse muy bien a los datos de entrenamiento, pero no generalizar bien a nuevos datos. Esto se debe a que el proceso de selección de variables se basa en los datos observados, lo que puede llevar a incluir variables que parecen importantes por puro azar en esa muestra particular.
* **P-valores y Coeficientes Sesgados:** Los p-valores y los coeficientes de regresión obtenidos de un modelo stepwise pueden ser sesgados. Los p-valores tienden a ser más pequeños de lo que realmente son, lo que lleva a una falsa confianza en la significancia de las variables.
* **No considera la teoría:** El proceso es puramente estadístico y no incorpora el conocimiento del dominio o la teoría subyacente sobre las relaciones entre las variables. Un modelo estadísticamente "óptimo" podría carecer de sentido teórico o práctico.
* **Dependencia del orden:** La selección de variables puede ser sensible al orden en que se añaden o eliminan, especialmente en la selección hacia adelante.


```{r}
require(leaps)

#	Use exhaustive search, forward selection, backward selection or sequential replacement to search.
#method=c("exhaustive", "backward", "forward", "seqrep")  

# Evaluar todos los subconjuntos posibles
regfit <- regsubsets(mpg ~ ., data = train_data, nvmax = 10)
summary(regfit)
```
`Selection Algorithm: exhaustive`**: Esto es crucial. Significa que el algoritmo probó **todas las combinaciones posibles** de variables para cada tamaño de subconjunto hasta `nvmax = 10` y seleccionó el mejor para cada tamaño. Esto asegura que, para cada número de variables (1, 2, ..., 10), el modelo presentado es el óptimo según el criterio interno de `regsubsets` (generalmente $R^2$ ajustado, BIC o Cp de Mallows, a menos que se especifique otro).


```{r}
# Ver los mejores modelos según el número de variables
plot(regfit, scale = "adjr2")  # O usar "bic" o "Cp" para otros criterios
```

```{r, class.source = "fold-hide"}
require(ggplot2)
# Crear un data frame con las métricas para ggplot2
reg_summary <- summary(regfit)

models_data <- data.frame(
                          num_variables = 1:length(reg_summary$adjr2),
                          adjr2 = reg_summary$adjr2,
                          cp = reg_summary$cp,
                          bic = reg_summary$bic
)

ggplot(models_data, aes(x = num_variables, y = adjr2)) +
 geom_line(color = "blue") +
  geom_point(color = "blue") +
   geom_vline(xintercept = which.max(models_data$adjr2), linetype = "dashed", color = "red") +
    geom_text(aes(x = which.max(models_data$adjr2) + 0.5, y = max(adjr2) * 0.95,
                  label = paste("Max R2 Ajustado con", which.max(models_data$adjr2), "vars")),
                  color = "red", size = 3, hjust = 0) +
     theme(plot.title = element_text(size = 22, hjust = 0.15, family = "Montserrat", face = "bold"),
           plot.subtitle = element_text(size = 18, hjust = 0, family = "Montserrat", face = "bold"),
           plot.caption = element_text(size = 11, hjust = 0.2, vjust = 1, family = "Montserrat"), 
           axis.text = element_text(family = "Montserrat"), 
           axis.title = element_text(family = "Montserrat", size = 15), 
           legend.position = "none"
               ) + 
     scale_x_continuous(breaks = 1:10) + 
     labs(title = "R2 Ajustado vs. Número de Variables",
          x = "Número de Variables",
          y = "R2 Ajustado") +
      theme_minimal()
```


### Variance Inflation Factor (`VIF`)  

El **Factor de Inflación de la Varianza (VIF)** es una herramienta estadística fundamental utilizada para **detectar y cuantificar la multicolinealidad** en modelos de regresión lineal múltiple.

**Problemas que causa la multicolinealidad:**
  
  * **Coeficientes de regresión inestables y difíciles de interpretar:** Cuando las variables predictoras están altamente correlacionadas, es difícil para el modelo determinar la contribución única de cada variable a la variable dependiente. Pequeños cambios en los datos pueden llevar a grandes cambios en los coeficientes estimados, haciendo que sean poco fiables.
* **Errores estándar inflados:** La multicolinealidad aumenta los errores estándar de los coeficientes de regresión, lo que a su vez disminuye los valores de las estadísticas t y aumenta los p-valores. Esto puede llevar a la conclusión errónea de que una variable no es estadísticamente significativa cuando en realidad sí lo es.
* **Poder predictivo reducido:** Aunque la multicolinealidad no necesariamente afecta la capacidad predictiva global del modelo (el $R^2$ ajustado puede seguir siendo alto), sí afecta la precisión de las estimaciones de los coeficientes individuales, lo que hace difícil comprender la relación real entre cada predictor y la variable de respuesta.


El VIF mide **cuánto se "infla" la varianza del coeficiente de regresión estimado de una variable predictora debido a su correlación con otras variables predictoras** en el modelo.

**La fórmula del VIF para una variable $X_j$ es:**
  
  $$VIF_j = \frac{1}{1 - R_j^2}$$
  
  Donde $R_j^2$ es el coeficiente de determinación ($R^2$) de una regresión auxiliar en la que la variable $X_j$ se predice utilizando todas las demás variables independientes del modelo.

#### ¿Cómo se interpreta el VIF?

* **VIF = 1:** Indica que la variable predictora no está correlacionada con ninguna de las otras variables predictoras en el modelo. No hay multicolinealidad.
* **VIF > 1:** Indica que existe algún grado de multicolinealidad. Cuanto mayor sea el valor del VIF, mayor es el grado de multicolinealidad.

**Reglas generales para interpretar los valores de VIF (son reglas de "pulgar" y pueden variar ligeramente según el contexto y el campo de estudio):**
  
  * **VIF ≤ 5:** Generalmente se considera que no hay problemas graves de multicolinealidad.
* **5 < VIF ≤ 10:** Puede indicar un nivel moderado a alto de multicolinealidad que podría justificar una investigación.
* **VIF > 10:** A menudo se considera una indicación clara de multicolinealidad grave y problemática. Es un umbral comúnmente aceptado para señalar que una variable está fuertemente correlacionada con otras, lo que podría afectar la estabilidad y fiabilidad del modelo.

```{r}
require(car)

# Calcular VIF para el modelo original
vif(modelo_ols)
```
**Altísima colinealidad** en:
  
  * `cyl`, `disp`, `wt`, `carb` 

**Sospechosos comunes**:
  
  * `cyl`, `disp`, `hp` y `wt` están muy correlacionados entre sí (motores grandes tienden a pesar más, tener más cilindros y más caballos).


```{r}
# Calcular VIF para el modelo reducido 
vif(modelo_step)
```
Todos los VIF están **por debajo del umbral de 5**, lo que indica:
  
* No hay **colinealidad preocupante** entre las variables.
* El modelo es **estable** y los coeficientes son interpretables con confianza.
* `stepAIC` hizo un buen trabajo al reducir la complejidad y mejorar la interpretabilidad del modelo.

## Evaluación del modelo  

Ahora se va a trabajar con un **modelo reducido (`modelo_step`)**, es importante **usar ese modelo** para hacer predicciones en el conjunto de prueba. Además, como solo algunas variables quedaron en el modelo (`drat`, `wt`, `gear`, `carb`), el `test_data` debe contener esas columnas.

**Verificar qué variables necesita el modelo reducido**

```{r}
# Ver fórmulas del modelo reducido
formula(modelo_step)
```

**Nos aseguramos que las bases de (test / training tengan esas variables**
```{r}
test_data_reducido <- test_data[, c("mpg", "drat", "wt", "gear", "carb")]
```

### Modelo predictivo   

Con la función **`predict()`** genérica en R que se utiliza para obtener predicciones de varios tipos de objetos de modelos (lineales, árboles de decisión, series de tiempo, etc.). Su comportamiento exacto depende de la clase del objeto que se le pasa como primer argumento.  

Esta operación es un paso clave en el flujo de trabajo de modelado predictivo, ya que permite:
  
1.  **Evaluación del modelo:** Una vez que se tienen las `predicciones` para el `test_data`, se puede compararlas con los valores reales (observados) de la variable dependiente en el `test_data` (si se tienen) para **evaluar qué tan bien se desempeña el modelo** en datos no vistos. Esto ayuda a estimar su rendimiento en el mundo real y a detectar problemas como el **sobreajuste (overfitting)**.  
2.  **Uso práctico del modelo:** Después de validar que el modelo es bueno, se puede usarlo para predecir la variable dependiente para nuevas observaciones futuras de las que solo se conocen las variables predictoras (por ejemplo, predecir el precio de una casa basándose en sus características, si el modelo fue entrenado para eso).


```{r}
# 5. Se evalua el modelo en los datos de prueba
predicciones <- predict(modelo_step, newdata = test_data_reducido)
```


```{r}
# Comparar valores reales vs. predichos
resultados <- tibble(Real = test_data_reducido$mpg,
                     Pred = predicciones
)
```

### Evaluar el desempeño predictivo  

#### Mean Absolute Error (`MAE`)

El **MAE (Mean Absolute Error)**, o **Error Absoluto Medio** en español, es una de las métricas más utilizadas para evaluar la precisión de un modelo de regresión. Mide la **magnitud promedio de los errores** entre los valores predichos por un modelo y los valores reales observados.

##### Fórmula del MAE

La fórmula para calcular el MAE es la siguiente:
  
  $$MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$
  
  Donde:
  
  * $n$: Es el número total de observaciones (o puntos de datos) en el conjunto de datos.
* $y_i$: Es el **valor real u observado** de la variable dependiente para la observación $i$.
* $\hat{y}_i$: Es el **valor predicho** por el modelo para la observación $i$.
* $|y_i - \hat{y}_i|$: Es el **valor absoluto** de la diferencia entre el valor real y el valor predicho para la observación $i$. Tomar el valor absoluto es crucial porque evita que los errores positivos y negativos se cancelen entre sí, lo que daría una falsa impresión de precisión.
* $\sum_{i=1}^{n}$: Indica la suma de todos los errores absolutos para todas las $n$ observaciones.


```{r}
# Cálculo del MAE (Mean Absolute Error)
MAE <- mean(abs(predicciones - test_data_reducido$mpg))
MAE
```
Esto significa que, **en promedio**, el modelo reducido predice el consumo de combustible con un error de ±3.68 mpg. Esto es bueno o malo. Depende del rango de la variable `mpg` en `mtcars`.  

```{r}
range(mtcars$mpg)
```
El rango de `mpg` es de **10.4 a 33.9**, es decir, abarca **\~23.5 unidades**.
    
Por tanto:
      
      * Un MAE de **3.68** representa alrededor de un **15% del rango total**. El modelo, en promedio, se equivoca por 3.68 millas por galón en sus predicciones.
    * No es un error enorme, pero **tampoco es excelente**.
    * Si el modelo completo (`modelo_ols`) tenía un MAE menor, podría predecir mejor, aunque con más colinealidad.

#### Root Mean Squared Error (`RMSE`)  

El **RMSE (Root Mean Squared Error)**, o **Raíz del Error Cuadrático Medio**, es una de las métricas más comunes y ampliamente utilizadas para evaluar la precisión de los modelos de regresión. Mide la **magnitud promedio de los errores** entre los valores predichos por un modelo y los valores reales observados, pero dando **mayor peso a los errores más grandes**.

#### Fórmula del RMSE

  $$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$
  
Donde:
  
  * $n$: Es el número total de observaciones (o puntos de datos).
* $y_i$: Es el **valor real u observado** de la variable dependiente para la observación $i$.
* $\hat{y}_i$: Es el **valor predicho** por el modelo para la observación $i$.
* $(y_i - \hat{y}_i)^2$: Es el **cuadrado de la diferencia** entre el valor real y el valor predicho para la observación $i$. Elevar al cuadrado las diferencias tiene dos propósitos principales:
  * Eliminar los signos negativos: Asegura que los errores positivos y negativos no se cancelen entre sí.
* Penalizar más los errores grandes: Los errores más grandes tienen un impacto desproporcionadamente mayor en el resultado final del RMSE que los errores pequeños, debido a la operación al cuadrado.
* $\sum_{i=1}^{n}$: Indica la suma de todos los errores al cuadrado para todas las $n$ observaciones.

##### Interpretación del RMSE:

* El RMSE se expresa en las **mismas unidades que la variable dependiente original**. Esto facilita su interpretación. Por ejemplo, si estás prediciendo la altura en metros y el RMSE es 0.5, significa que, en promedio, las predicciones del modelo se desvían aproximadamente 0.5 metros de la altura real.
* **Un RMSE de 0 (cero) indica un modelo perfecto**, donde todas las predicciones son exactamente iguales a los valores reales.
* **Valores más bajos de RMSE indican un mejor rendimiento del modelo.**
  * **Sensibilidad a valores atípicos:** Debido al término al cuadrado, el RMSE penaliza más fuertemente los errores grandes (valores atípicos) que el MAE. Esto significa que si tu modelo tiene algunos errores de predicción muy grandes, el RMSE será significativamente más alto que el MAE. Esta característica puede ser una ventaja o desventaja dependiendo del contexto:
  * **Ventaja:** Si los errores grandes son particularmente indeseables en tu aplicación, el RMSE es una buena métrica porque los destaca.
* **Desventaja:** Si tu conjunto de datos contiene muchos valores atípicos reales o errores de medición que no son representativos del rendimiento general del modelo, el RMSE podría dar una visión pesimista.


```{r}
# Cálculo del RMSE (Root Mean Squared Error)
RMSE <- sqrt(mean((predicciones - test_data_reducido$mpg)^2))
RMSE
```
* Un RMSE de **4.87** significa que, en promedio, las predicciones difieren de los valores reales por **±4.87 millas por galón**.
  * Como **los errores grandes tienen más peso** (al ser elevados al cuadrado), el RMSE será siempre **igual o mayor al MAE**. Penaliza más los errores grandes que el MAE.
  *  Es **aceptable para predicción en `mtcars`**, pero con espacio para mejora si se requiere mayor precisión.      
  
**Compararlo con el MAE**
  
  * Si **RMSE está mucho más alto que el MAE**, el modelo está cometiendo **errores grandes con frecuencia** (outliers, mala especificación).
  * Si **RMSE ≈ MAE**, los errores están bien distribuidos.
  

#### Coeficiente de determinación $R^2$  

$R^{2}$ (**coeficiente de determinación**) mide qué proporción de la variabilidad de la **variable dependiente** es explicada por el modelo.  

La fórmula general del $R^2$ (coeficiente de determinación) es:
  
  $$R^2 = 1 - \frac{SSE}{SST}$$
  
  Donde:
  
  * **$SSE$** (Sum of Squared Errors) o **$RSS$** (Residual Sum of Squares): Es la **Suma de Cuadrados de los Errores** (o Suma de Cuadrados de los Residuos). Mide la variación no explicada por el modelo. Se calcula como la suma de los cuadrados de las diferencias entre los valores observados ($y_i$) y los valores predichos por el modelo ($\hat{y}_i$):
  $$SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
  
  * **$SST$** (Total Sum of Squares): Es la **Suma Total de Cuadrados**. Mide la variación total de la variable dependiente respecto a su media. Representa la variabilidad total en los datos que el modelo intenta explicar. Se calcula como la suma de los cuadrados de las diferencias entre los valores observados ($y_i$) y la media de la variable dependiente ($\bar{y}$):
  $$SST = \sum_{i=1}^{n} (y_i - \bar{y})^2$$ 
  
##### Interpretación del $R^2$:

* El $R^2$ es una métrica que varía entre **0 y 1** (o 0% y 100%).
* **$R^2 = 0$**: Indica que el modelo no explica ninguna de la variabilidad en la variable dependiente. Es tan bueno como simplemente usar la media de la variable dependiente para la predicción.
* **$R^2 = 1$**: Indica que el modelo explica el 100% de la variabilidad en la variable dependiente. Esto rara vez ocurre en la práctica con datos del mundo real, y a menudo sugiere sobreajuste si sucede en un conjunto de entrenamiento.
* **Valores más altos de $R^2$ indican un mejor ajuste del modelo** a los datos observados, lo que significa que las variables predictoras del modelo explican una mayor proporción de la variabilidad en la variable dependiente.

```{r}
# R² manual (opcional)
SST <- sum((test_data_reducido$mpg - mean(test_data_reducido$mpg))^2)
SSE <- sum((test_data_reducido$mpg - predicciones)^2)
R2 <- 1 - SSE/SST
R2
```

 * Esto significa que el **60.2% de la variabilidad** de `mpg` (consumo de combustible) en los datos de prueba **es explicada por el modelo**.
    * El **39.8% restante** es **variabilidad no explicada** (error, factores no incluidos, ruido). 
    * El modelo predice **más de la mitad de la variabilidad de `mpg`** en los datos de prueba. 
    

#### Distribución de los residuos  

Relación lineal entre variable dependiente e independiente:

Se calculan los residuos para cada observación y se representan (scatterplot). Si las observaciones siguen la línea del modelo, los residuos se deben distribuir aleatoriamente entorno al valor 0.

```{r, class.source = "fold-hide"}
tabla <- data.frame(prediccion = modelo_step$fitted.values,
                    residuos = modelo_step$residuals)

ggplot(data = tabla, aes(x = prediccion, y = residuos)) +
 geom_point(aes(color = residuos)) +
  scale_color_gradient2(low = "blue3", mid = "grey", high = "red") +
   geom_hline(yintercept = 0) +
    geom_segment(aes(xend = prediccion, yend = 0), alpha = 0.2) +
     theme_bw() +
      theme(plot.title = element_text(size = 22, hjust = 0, family = "Montserrat", face = "bold"),
            plot.subtitle = element_text(size = 18, hjust = 0, family = "Montserrat", face = "bold"),
            plot.caption = element_text(size = 11, hjust = 0.2, vjust = 1, family = "Montserrat"), 
            axis.text = element_text(family = "Montserrat"), 
            axis.title = element_text(family = "Montserrat", size = 15), 
            legend.position = "none"
                   ) + 
     labs(title = "Distribución de los residuos", 
          x = "predicción modelo",
          y = "residuo") 
  
```


Los residuos se distribuyen de forma aleatoria entorno al 0 por lo que se acepta la linealidad.  

##### Distribución normal de los residuos

Los residuos se deben distribuir de forma normal con media 0. Para comprobarlo se recurre a histogramas, a los cuantiles normales o a un test de contraste de normalidad.    

```{r, class.source = "fold-hide"}
ggplot(data = tabla, aes(x = residuos)) +
  geom_histogram(aes(y = ..density..)) +
   theme_light() +
    theme(plot.title = element_text(size = 22, hjust = 0, family = "Montserrat", face = "bold"),
          plot.subtitle = element_text(size = 18, hjust = 0, family = "Montserrat", face = "bold"),
          plot.caption = element_text(size = 11, hjust = 0.2, vjust = 1, family = "Montserrat"), 
          axis.text = element_text(family = "Montserrat"), 
          axis.title = element_text(family = "Montserrat", size = 15), 
          legend.position = "none"
                   ) + 
      labs(title = "Histograma de los residuos")
```


#### Q-Q Plot   

Los gráficos Q-Q normales son esenciales para verificar uno de los supuestos clave en la regresión lineal: la **normalidad de los residuos**. Si los residuos siguen una distribución normal, los puntos en el gráfico Q-Q deben caer aproximadamente a lo largo de una línea recta.


```{r, class.source = "fold-hide"}
# Residuos del modelo
residuos <- residuals(modelo_step)

# Calcular los cuantiles teóricos normales
# La función 'qqnorm' internamente calcula estos valores
# Puedes replicarlo manualmente o usar la función 'qqnorm' y luego extraer sus valores
qq_data <- data.frame(teoricos = qqnorm(residuos, plot.it = FALSE)$x,
                      observados = residuos)


ggplot(qq_data, aes(x = teoricos, y = observados)) +
 geom_point(alpha = 0.7, color = "darkblue") + # Puntos de los cuantiles
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") + # Línea de referencia ideal (y=x)
   theme_minimal() + 
   labs(title = "Gráfico Q-Q Normal de los Residuos del Modelo",
        x = "Cuantiles Teóricos Normales",
        y = "Residuos del Modelo") 
```

##### Interpretación del gráfico Q-Q Normal:
  
  * **Si los puntos caen aproximadamente sobre la línea roja diagonal**, indica que los residuos siguen una distribución normal. Esto es lo que idealmente buscas.
* **Si los puntos se desvían de la línea**, puede indicar una violación del supuesto de normalidad:
  * **Forma de "S"**: Los residuos pueden tener colas más pesadas o más ligeras de lo esperado (ej. distribución con curtosis).
* **Curvatura en un extremo o ambos**: Los residuos pueden estar sesgados (skewed) hacia la izquierda o la derecha.


#### Shapiro-Wilk normality test  

El **Test de Normalidad de Shapiro-Wilk** es una prueba estadística que se utiliza para determinar si una muestra de datos procede de una población con distribución normal. Es una de las pruebas de normalidad más potentes y ampliamente recomendadas, especialmente para tamaños de muestra pequeños a moderados.  

Es una prueba estadística que evalúa si los datos provienen de una distribución normal.

* **Hipótesis nula (H₀)**: los datos siguen una distribución normal.
* **Hipótesis alternativa (H₁)**: los datos **no** siguen una distribución normal.

 
```{r}
shapiro.test(modelo_step$residuals)
```
* **p = 0.5222 > 0.05** → **no se rechaza H₀**
  * **Conclusión**: **los residuos del modelo pueden considerarse normalmente distribuidos**.   

### Validación cruzada   

```{r, eval = FALSE}
# 6. Validación cruzada (opcional)
control <- trainControl(method = "cv", number = 10)
modelo_cv <- train(mpg ~ ., data = mtcars, method = "lm", trControl = control)
print(modelo_cv)

# 7. Usar el modelo para predecir nuevos datos (simulación)
nuevo_auto <- data.frame(
  cyl = 6,
  disp = 200,
  hp = 110,
  drat = 3.5,
  wt = 2.8,
  qsec = 18,
  vs = 1,
  am = 1,
  gear = 4,
  carb = 2
)

predict(modelo_ols, newdata = nuevo_auto)

# 8. Interpretación del modelo (con broom)
tidy(modelo_ols)
```

## 📌 Notas

-   Este modelo es considerado **supervisado** porque se entrena con pares de entrada y salida.
-   Aunque la regresión lineal es simple, **sigue siendo un algoritmo de machine learning supervisado** y útil como línea base (baseline).
-   Puedes reemplazar `lm()` por modelos más complejos como `randomForest`, `xgboost`, etc., manteniendo la misma estructura.

### Ventajas y Limitaciones

**Ventajas:**

-   **Simplicidad:** Es fácil de entender e implementar.

-   **Interpretabilidad:** Los coeficientes estimados tienen una interpretación clara.

-   **Eficiencia:** Bajo ciertas condiciones, los estimadores de OLS son los más eficientes.

**Limitaciones:**

-   **Sensibilidad a valores atípicos:** Los valores extremos pueden influir mucho en la línea de regresión.
-   **Sensibilidad a violaciones de supuestos:** Si los supuestos no se cumplen, las estimaciones pueden ser sesgadas o ineficientes, y la inferencia estadística puede ser inválida.
-   **Solo relaciones lineales:** No puede capturar relaciones no lineales a menos que se transformen las variables adecuadamente.

## Referencias

Librerias que se usaron en el documento

```{r, echo = FALSE, eval = TRUE}
sesion_info <- devtools::session_info()
require(knitr)
require(kableExtra)
kable(dplyr::select(tibble::as_tibble(sesion_info$packages %>% dplyr::filter(attached == TRUE)),
                    c(package, loadedversion, source))) %>%
 kable_styling(font_size = 10, 
               bootstrap_options = c("condensed", "responsive", "bordered")) %>%
  kable_classic(full_width = TRUE, html_font = "montserrat") %>% 
   scroll_box(width = "100%", height = "400px") %>%  
    gsub("font-size: initial !important;", "font-size: 10pt !important;", .)
```

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png" alt="Creative Commons Licence" style="border-width:0"/></a><br />This work by [**Diana Villasana Ocampo**]{xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"} is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
