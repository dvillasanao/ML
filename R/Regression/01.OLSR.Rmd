---
title: "Ordinary Least Squares Regression (OLSR)"
author: "Diana Villasana Ocampo"
knit: (function(inputFile, encoding) {
       rmarkdown::render(inputFile, encoding = encoding, output_dir = "../../Output/")
  })
output:
   html_document:
      code_folding: hide
      highlight: tango
      theme: flatly
      toc: true
      toc_depth: 3
      toc_float:
        collapsed: yes
---

```{=html}
<style type="text/css">
body {
text-align: justify;
font-style: normal;
font-family: "Montserrat";
font-size: 12px
}
h1.title {
  font-size: 40px;
  color: #000D3B;
}
h1 {
  color: #B6854D;
}
h2 {
  color: #172984;
}
h3 {
  color: #172984;
}
</style>
```

```{=html}
<style>
.nav>li>a {
    position: relative;
    display: block;
    padding: 10px 15px;
    color: #1C3BA4
}
.nav-pills>li.active>a, .nav-pills>li.active>a:hover, .nav-pills>li>a:focus {
    color: #ffffff;
    background-color: #09C2BC
}
</style>
```

```{=html}
<style>
.tile1-text {
    position: relative;
    display: block;
    padding: 10px 15px;
    color: #0A6A87;
    list-style: none;
}
.top1-tiles a:nth-of-type(1):hover, .top-tiles1 a:nth-of-type(1):focus{
    color: #ffffff;
    background: #0A6A87
}
</style>
```

```{=html}
<style>
.tile2-text {
    position: relative;
    display: block;
    padding: 10px 15px;
    color: #0A6CC8;
    list-style: none;
}
.top2-tiles a:nth-of-type(1):hover, .top2-tiles a:nth-of-type(1):focus{
    color: #ffffff;
    background: #0A6CC8
}
</style>
```

```{=html}
<style>
.math {
  font-size: 15px;
  color: #1e42ab;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, 
                      cache.lazy = FALSE, class.source = "fold-show")
knitr::opts_knit$set(root.dir = here::here())
setwd(here::here())
```

```{r,echo=FALSE, eval=FALSE, }
rm(list = ls())
```

```{r, echo = FALSE, results=FALSE}
# Se descargan las fuentes de la google fonts
require(showtext)
library(extrafont)
# activar showtext
windowsFonts()
```

```{r, echo = FALSE}
# 1. Cargar librer√≠as necesarias
library(tidyverse)
library(caret)     # Para dividir datos y evaluaci√≥n
library(broom)     # Para tidy modelos
library(Metrics)   # Para m√©tricas como RMSE, MAE
require(tibble)
```

La Regresi√≥n por M√≠nimos Cuadrados Ordinarios (Ordinary Least Squares Regression, **OLSR** u **OLS**) es una t√©cnica estad√≠stica fundamental utilizada para estimar la relaci√≥n entre una **variable dependiente** (o de respuesta) y una o m√°s **variables independientes** (o predictoras). Es uno de los m√©todos m√°s comunes en el an√°lisis de regresi√≥n lineal.

## Objetivo

El objetivo principal de OLSR es encontrar la "l√≠nea de mejor ajuste" a trav√©s de un conjunto de puntos de datos. Esta l√≠nea se determina minimizando la suma de los cuadrados de las diferencias entre los valores observados de la variable dependiente y los valores predichos por el modelo. A estas diferencias se les llama **residuos** o **errores**. Al minimizar la suma de los cuadrados, OLS asegura que los errores positivos y negativos no se cancelen entre s√≠, y que los errores grandes tengan un impacto proporcionalmente mayor en la minimizaci√≥n.

## Metodolog√≠a

La metodolog√≠a de OLSR se basa en los siguientes pasos y principios:

1.  **Modelo Lineal:** OLSR asume una relaci√≥n lineal entre las variables. Para una regresi√≥n lineal simple (una variable independiente), la ecuaci√≥n es:\
    $$Y = \beta_0 + \beta_1X + \epsilon$$

    Donde:

    -   $Y$ es la variable dependiente.

-   $X$ es la variable independiente.
-   $\beta_0$ es el intercepto (el valor de $Y$ cuando $X$ es 0).
-   $\beta_1$ es la pendiente (el cambio en $Y$ por cada unidad de cambio en $X$).
-   $\epsilon$ es el t√©rmino de error o residual, que representa la parte de $Y$ que no puede ser explicada por $X$.

Para una regresi√≥n lineal m√∫ltiple (varias variables independientes), la ecuaci√≥n se expande a:\
$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \epsilon$$

2.  **Minimizaci√≥n de la Suma de Cuadrados de Residuos (SSR):** El coraz√≥n de OLS es encontrar los valores de los coeficientes ($\beta_0, \beta_1$, etc.) que minimicen la siguiente funci√≥n:\
    $$\text{Minimizar } \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$\
    Donde:
    -   $y_i$ es el valor observado de la variable dependiente para la observaci√≥n $i$.

-   $\hat{y}_i$ es el valor predicho de la variable dependiente por el modelo para la observaci√≥n $i$.
-   $(y_i - \hat{y}_i)$ es el residual para la observaci√≥n $i$.

Para lograr esta minimizaci√≥n, se utilizan t√©cnicas de c√°lculo (derivadas parciales) para encontrar los valores de los coeficientes que hacen que la pendiente de la funci√≥n de suma de cuadrados sea cero.

3.  **Estimaci√≥n de Coeficientes:** Los valores estimados de los coeficientes, denotados como $\hat{\beta}_0, \hat{\beta}_1$, etc., son aquellos que resultan de la minimizaci√≥n. Estos coeficientes son los que definen la "l√≠nea de mejor ajuste".

4.  **Supuestos del OLS:** Para que los estimadores de OLS sean los "mejores estimadores lineales insesgados" (seg√∫n el Teorema de Gauss-Markov), se deben cumplir ciertas suposiciones:

    -   **Linealidad:** La relaci√≥n entre las variables es lineal.

    -   **Independencia de los errores:** Los errores de una observaci√≥n no est√°n correlacionados con los errores de otra.

    -   **Homocedasticidad:** La varianza de los errores es constante en todos los niveles de las variables independientes.

    -   **Normalidad de los errores:** Los errores se distribuyen normalmente (aunque no es estrictamente necesario para la estimaci√≥n, s√≠ lo es para la inferencia estad√≠stica).

    -   **No multicolinealidad perfecta:** Las variables independientes no est√°n perfectamente correlacionadas entre s√≠.

## **Pasos generales del Machine Learning supervisado**

1.  **Importar y explorar los datos**
2.  **Preprocesamiento**
3.  **Divisi√≥n de los datos (train/test)**
4.  **Entrenamiento del modelo**
5.  **Evaluaci√≥n del modelo**
6.  **Ajustes o validaci√≥n cruzada (si aplica)**
7.  **Predicci√≥n con nuevos datos**
8.  **Interpretaci√≥n de resultados**

------------------------------------------------------------------------

## Base de datos

La base de datos `mtcars` es un conjunto de datos cl√°sico en R que contiene informaci√≥n sobre **32 autom√≥viles** (modelos de 1973‚Äì74), y fue extra√≠do de la revista *Motor Trend US*. Incluye **variables t√©cnicas** del desempe√±o de los autos.

Aqu√≠ est√° una descripci√≥n de cada columna:

| Variable | Significado | Tipo de dato |
|------------------|-----------------------------------|------------------|
| `mpg` | Miles per gallon (millas por gal√≥n) | Num√©rica |
| `cyl` | N√∫mero de cilindros | Entero |
| `disp` | Desplazamiento del motor (en pulgadas c√∫bicas) | Num√©rica |
| `hp` | Caballos de fuerza | Entero |
| `drat` | Relaci√≥n del eje trasero (rear axle ratio) | Num√©rica |
| `wt` | Peso del auto (en miles de libras) | Num√©rica |
| `qsec` | Tiempo en 1/4 de milla (segundos) | Num√©rica |
| `vs` | Tipo de motor: 0 = V-shaped, 1 = straight (en l√≠nea) | Binaria (factor) |
| `am` | Tipo de transmisi√≥n: 0 = autom√°tica, 1 = manual | Binaria (factor) |
| `gear` | N√∫mero de velocidades (marchas) adelante | Entero |
| `carb` | N√∫mero de carburadores | Entero |

```{r}
# 2. Cargar y se exploran los datos
data("mtcars")
```

```{r, echo = FALSE}
require(gt)

mtcars %>% 
 gt() %>%
  tab_header(title = "mtcars data") %>%
   tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Cuando_") ~ px(300),
                   everything() ~ px(50)) %>%
         as_raw_html() 

```

## Entrenamiento de los datos (train/test)

-   Se usa la funci√≥n `createDataPartition()` del paquete `caret` para **dividir los datos**.
-   Se crea un **√≠ndice** con el 80% de las filas del `mtcars`, **estratificado** seg√∫n la variable `mpg` (la variable objetivo).
-   El argumento `p = 0.8` significa que el 80% se usar√° para **entrenamiento** y el 20% restante para **prueba**.
-   `list = FALSE` devuelve los √≠ndices como un vector, no como una lista.

```{r}
# Se quiere predecir `mpg` (millas por gal√≥n) usando otras variables
# mpg ser√° la variable dependiente (target)

# 3. Se dividen losdatos en entrenamiento y prueba
set.seed(123)  # Para reproducibilidad
train_index <- createDataPartition(mtcars$mpg, p = 0.8, list = FALSE)
train_data <- mtcars[train_index, ]
test_data <- mtcars[-train_index, ]
```

-   Particionar los datos, evita el **overfitting** (cuando el modelo memoriza los datos de entrenamiento).
-   Permite una **evaluaci√≥n honesta** del modelo al probarlo en datos que no vio durante el entrenamiento.
-   Es una pr√°ctica est√°ndar en cualquier pipeline de aprendizaje autom√°tico.

## Entrenamiento del modelo

-   la funci√≥n **`lm()`** es la funci√≥n base de R para ajustar un modelo de **regresi√≥n lineal** (OLS = *Ordinary Least Squares*).

-   **`mpg ~ .`** es una f√≥rmula que indica:

    -   `mpg` es la **variable dependiente** (lo que queremos predecir).
    -   `.` significa ‚Äútodas las dem√°s variables‚Äù del conjunto de datos se usar√°n como **variables independientes** (predictoras).
    -   **`data = train_data`** especifica que el modelo debe entrenarse usando el conjunto de **entrenamiento** (80% de los datos).

```{r}
# 4. Entrenar el modelo de regresi√≥n lineal (OLS)
modelo_ols <- lm(mpg ~ ., data = train_data)

# Revisar resumen del modelo
summary(modelo_ols)
```

Interpretemos los resultados:

**Residuales**

````         
```r
Residuals:
  Min      1Q  Median      3Q     Max 
-3.2742 -1.3609 -0.2707  1.1921  4.9877 
```
````

-   Indica la distribuci√≥n de los **errores de predicci√≥n** (diferencia entre valor real y valor predicho).
-   Lo ideal es que est√©n **centrados en 0** y sean **sim√©tricos**, lo que parece cumplirse aqu√≠.
-   El valor m√°ximo del error es aproximadamente ¬±5 millas por gal√≥n.

**Ninguna variable es estad√≠sticamente significativa individualmente** (todas tienen p \> 0.05).

| M√©trica | Valor | Significado |
|------------------|------------------|------------------------------------|
| **R-squared** | 0.8861 | El modelo explica el **88.6%** de la variabilidad de `mpg` |
| **Adjusted R-squared** | 0.8191 | Ajusta el R¬≤ penalizando el n√∫mero de predictores |
| **F-statistic** | 13.23 | El modelo como conjunto es **significativo** |
| **p-value global** | 3.719e-06 | Muy bajo ‚Üí **el modelo es √∫til en conjunto** (aunque variables individuales no lo sean) |

### üìå **Observaciones generales**

-   **El modelo es bueno en conjunto**, pero ninguna variable individual es altamente significativa.\
-   Esto puede ser por **colinealidad** (las variables est√°n correlacionadas entre s√≠).\
-   Se podr√≠a:
    -   Usar un modelo m√°s simple (quitar predictores no √∫tiles).
    -   Aplicar **selecci√≥n de variables** (ej. stepAIC, regsubsets).
    -   Verificar colinealidad con el **VIF** (Variance Inflation Factor).
    -   Visualizar los residuos para ver si el modelo cumple los supuestos.
    
    
```{r, class.source = "fold-hide"}
require(ggplot2)
require(reshape2)

tabla <- mtcars %>% 
          melt(., id = "mpg")

p <- tabla %>% 
      ggplot(aes(y = mpg, x = value, color = variable)) + 
       geom_point() +
        geom_smooth(method = "lm", se = TRUE, color = "black") +
         theme_bw() + 
          theme(plot.title = element_text(size = 22, hjust = 0.15, family = "Montserrat", face = "bold"),
                plot.subtitle = element_text(size = 18, hjust = 0, family = "Montserrat", face = "bold"),
                plot.caption = element_text(size = 11, hjust = 0.2, vjust = 1, family = "Montserrat"), 
                axis.text = element_text(family = "Montserrat"), 
                axis.title = element_text(family = "Montserrat", size = 15), 
                legend.position = "none"
               ) + 
           scale_color_viridis_d(option = "A") +
            facet_wrap(variable~., scales = "free") + 
             labs(title = "Diagramas de dispersi√≥n",
                  x = "",
                  y = "")
p
```
    
## Selecci√≥n de variables   


### Criterio de Informaci√≥n de Akaike   


El Criterio de Informaci√≥n de Akaike (AIC, por sus siglas en ingl√©s, Akaike Information Criterion) es una medida de la bondad de ajuste de un modelo estad√≠stico que penaliza la complejidad del modelo. Su objetivo es seleccionar el modelo que mejor se ajusta a los datos con la menor cantidad de par√°metros, evitando as√≠ el sobreajuste (overfitting).

La f√≥rmula general para calcular el AIC es:
  
$$AIC = 2k - 2\ln(L)$$
  
Donde:
  * $k$ es el n√∫mero de par√°metros del modelo. En un modelo de regresi√≥n, esto incluye el intercepto y los coeficientes de las variables predictoras, m√°s la varianza del error si se estima (en modelos de m√≠nimos cuadrados ordinarios, esto es a menudo el caso, por lo que $k$ a veces se cuenta como el n√∫mero de coeficientes + 1 para la varianza del error, o simplemente el n√∫mero de coeficientes si la varianza del error se considera impl√≠cita en la funci√≥n de verosimilitud).
* $\ln(L)$ es el logaritmo natural del valor m√°ximo de la funci√≥n de verosimilitud (log-likelihood) del modelo. La funci√≥n de verosimilitud mide qu√© tan bien el modelo reproduce los datos observados.

#### C√≥mo se interpreta:

* **Valores m√°s bajos de AIC indican un mejor modelo.** El AIC busca un equilibrio entre la bondad de ajuste del modelo a los datos (representada por $\ln(L)$) y la complejidad del modelo (representada por $2k$).
* El t√©rmino $2k$ es una **penalizaci√≥n por la complejidad**. Cada par√°metro adicional en el modelo aumenta el valor de AIC, lo que desincentiva la inclusi√≥n de variables innecesarias que podr√≠an sobreajustar los datos.  

#### Para modelos de Regresi√≥n por M√≠nimos Cuadrados Ordinarios (OLS):

Aunque la f√≥rmula general del AIC es la que se mencion√≥, para modelos de OLS con residuos normalmente distribuidos, la funci√≥n de log-verosimilitud tiene una forma espec√≠fica, lo que permite reescribir el AIC de una manera m√°s pr√°ctica.

Para un modelo de regresi√≥n lineal con $n$ observaciones y $k$ par√°metros (incluyendo el intercepto), y asumiendo errores normales con varianza constante, el AIC se puede calcular como:
  
  $$AIC = n \cdot \ln\left(\frac{RSS}{n}\right) + 2k$$
  
  Donde:
  * $n$ es el n√∫mero de observaciones (tama√±o de la muestra).
* $RSS$ es la Suma de Cuadrados de los Residuos (Residual Sum of Squares), que es la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos por el modelo.
* $k$ es el n√∫mero de par√°metros del modelo (coeficientes de las variables independientes + intercepto).

**Es importante recordar:**
  
  * El AIC no es una prueba de hip√≥tesis en el sentido tradicional; no te dice si un modelo es "bueno" o "malo" en un sentido absoluto.
* Es una herramienta para la **selecci√≥n de modelos entre un conjunto de modelos candidatos**. Siempre se compara el AIC de diferentes modelos ajustados a los **mismos datos**. El modelo con el AIC m√°s bajo es el preferido.
* Cuando la muestra es peque√±a, a veces se prefiere el **AIC corregido (AICc)**, que a√±ade una penalizaci√≥n adicional por el tama√±o de la muestra:
  $$AICc = AIC + \frac{2k(k+1)}{n-k-1}$$
  A medida que $n$ (tama√±o de la muestra) es grande, el t√©rmino de correcci√≥n se vuelve insignificante y el AICc converge al AIC.


Se realiza una selecci√≥n autom√°tica de variables para un modelo de regresi√≥n lineal, utilizando el **criterio AIC (Criterio de Informaci√≥n de Akaike**)   

```{r}
require(MASS)

# Aplicar selecci√≥n autom√°tica con criterio AIC
modelo_step <- stepAIC(modelo_ols, direction = "both")
```

El procedimiento va eliminando o agregando variables al modelo original para reducir el **AIC (Criterio de Informaci√≥n de Akaike)**. El objetivo es encontrar un modelo m√°s **parsimonioso** (simple) con **buena capacidad predictiva**.  

* El proceso empieza con un modelo completo:
  `mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb`
y un AIC inicial, por ejemplo: `AIC = 57.77`.

* Luego eval√∫a qu√© pasa si **quita** una variable (`- vs`, `- cyl`, etc.).

* Si al eliminar una variable el AIC **baja**, se acepta ese cambio.
Ejemplo:
  `- vs    1    2.1293  102.57  56.353`
Significa que si quitas la variable `vs`, el AIC baja de 57.77 a 56.35 ‚Üí ¬°mejor!
  
  * Se acepta esa modificaci√≥n y se repite el procedimiento, ahora con el nuevo modelo.

* Tambi√©n puede intentar **agregar** variables previamente eliminadas (`+ vs`, `+ hp`, etc.) si esto mejora el AIC.

* Cuando **ya no puede bajar m√°s el AIC** quitando o agregando variables. El modelo final es el que **tiene el AIC m√°s bajo** alcanzado durante el proceso. 

```{r}
# Ver resumen del modelo seleccionado
summary(modelo_step)
```

* **Error est√°ndar residual**: En promedio, el error de predicci√≥n es de ¬±2.2 mpg.
* **R¬≤ = 0.8738**: El modelo explica el **87.4% de la variabilidad** en `mpg`.
* **R¬≤ ajustado = 0.8518**: Ajusta por el n√∫mero de predictores; a√∫n muy bueno.
* **F-statistic = 39.81**, **p-value muy bajo (‚âà0)**: El modelo global es **altamente significativo**.


### Forward or backward stepwise  

Los modelos "forward" (hacia adelante) y "backward" (hacia atr√°s) son dos enfoques comunes dentro de la **regresi√≥n stepwise (paso a paso)**, un m√©todo de selecci√≥n autom√°tica de variables para construir modelos de regresi√≥n. El objetivo de la regresi√≥n stepwise es encontrar un subconjunto √≥ptimo de variables predictoras que expliquen la mayor parte de la varianza en la variable dependiente con la menor complejidad posible, evitando el sobreajuste y mejorando la interpretabilidad del modelo.

Ambos m√©todos operan de manera iterativa, agregando o eliminando variables bas√°ndose en un criterio estad√≠stico (como el p-valor, AIC, BIC, etc.).
 
#### **Forward Selection** (Selecci√≥n Hacia Adelante)

La selecci√≥n hacia adelante comienza con un modelo "nulo", es decir, un modelo que no contiene ninguna variable predictora (solo el intercepto). Luego, en cada paso, eval√∫a todas las variables predictoras no incluidas en el modelo y agrega aquella que, al ser incluida, produce la mejora m√°s significativa en el ajuste del modelo. Este proceso contin√∫a hasta que ninguna de las variables restantes cumple con el criterio de entrada preestablecido (por ejemplo, su p-valor es mayor que un umbral determinado).

#### **Backward Elimination** (Eliminaci√≥n Hacia Atr√°s)

La eliminaci√≥n hacia atr√°s comienza con un modelo "completo", es decir, un modelo que incluye todas las variables predictoras candidatas. Luego, en cada paso, eval√∫a la contribuci√≥n de cada variable predictora en el modelo y elimina aquella que, al ser retirada, tiene el menor impacto negativo en el ajuste del modelo (o la que es menos significativa, por ejemplo, la que tiene el p-valor m√°s alto). Este proceso contin√∫a hasta que todas las variables restantes en el modelo cumplen con un criterio de permanencia preestablecido (por ejemplo, su p-valor es menor que un umbral determinado).


#### Consideraciones Generales sobre la Regresi√≥n Stepwise

Es importante mencionar que, aunque los m√©todos forward y backward son populares por su automatizaci√≥n, tambi√©n tienen sus **cr√≠ticas y desventajas**:
  
  * **Sobreajuste (Overfitting):** Los modelos resultantes pueden ajustarse muy bien a los datos de entrenamiento, pero no generalizar bien a nuevos datos. Esto se debe a que el proceso de selecci√≥n de variables se basa en los datos observados, lo que puede llevar a incluir variables que parecen importantes por puro azar en esa muestra particular.
* **P-valores y Coeficientes Sesgados:** Los p-valores y los coeficientes de regresi√≥n obtenidos de un modelo stepwise pueden ser sesgados. Los p-valores tienden a ser m√°s peque√±os de lo que realmente son, lo que lleva a una falsa confianza en la significancia de las variables.
* **No considera la teor√≠a:** El proceso es puramente estad√≠stico y no incorpora el conocimiento del dominio o la teor√≠a subyacente sobre las relaciones entre las variables. Un modelo estad√≠sticamente "√≥ptimo" podr√≠a carecer de sentido te√≥rico o pr√°ctico.
* **Dependencia del orden:** La selecci√≥n de variables puede ser sensible al orden en que se a√±aden o eliminan, especialmente en la selecci√≥n hacia adelante.


```{r}
require(leaps)

#	Use exhaustive search, forward selection, backward selection or sequential replacement to search.
#method=c("exhaustive", "backward", "forward", "seqrep")  

# Evaluar todos los subconjuntos posibles
regfit <- regsubsets(mpg ~ ., data = train_data, nvmax = 10)
summary(regfit)
```
`Selection Algorithm: exhaustive`**: Esto es crucial. Significa que el algoritmo prob√≥ **todas las combinaciones posibles** de variables para cada tama√±o de subconjunto hasta `nvmax = 10` y seleccion√≥ el mejor para cada tama√±o. Esto asegura que, para cada n√∫mero de variables (1, 2, ..., 10), el modelo presentado es el √≥ptimo seg√∫n el criterio interno de `regsubsets` (generalmente $R^2$ ajustado, BIC o Cp de Mallows, a menos que se especifique otro).


```{r}
# Ver los mejores modelos seg√∫n el n√∫mero de variables
plot(regfit, scale = "adjr2")  # O usar "bic" o "Cp" para otros criterios
```

```{r, class.source = "fold-hide"}
require(ggplot2)
# Crear un data frame con las m√©tricas para ggplot2
reg_summary <- summary(regfit)

models_data <- data.frame(
                          num_variables = 1:length(reg_summary$adjr2),
                          adjr2 = reg_summary$adjr2,
                          cp = reg_summary$cp,
                          bic = reg_summary$bic
)

ggplot(models_data, aes(x = num_variables, y = adjr2)) +
 geom_line(color = "blue") +
  geom_point(color = "blue") +
   geom_vline(xintercept = which.max(models_data$adjr2), linetype = "dashed", color = "red") +
    geom_text(aes(x = which.max(models_data$adjr2) + 0.5, y = max(adjr2) * 0.95,
                  label = paste("Max R2 Ajustado con", which.max(models_data$adjr2), "vars")),
                  color = "red", size = 3, hjust = 0) +
     theme(plot.title = element_text(size = 22, hjust = 0.15, family = "Montserrat", face = "bold"),
           plot.subtitle = element_text(size = 18, hjust = 0, family = "Montserrat", face = "bold"),
           plot.caption = element_text(size = 11, hjust = 0.2, vjust = 1, family = "Montserrat"), 
           axis.text = element_text(family = "Montserrat"), 
           axis.title = element_text(family = "Montserrat", size = 15), 
           legend.position = "none"
               ) + 
     scale_x_continuous(breaks = 1:10) + 
     labs(title = "R2 Ajustado vs. N√∫mero de Variables",
          x = "N√∫mero de Variables",
          y = "R2 Ajustado") +
      theme_minimal()
```


### Variance Inflation Factor (`VIF`)  

El **Factor de Inflaci√≥n de la Varianza (VIF)** es una herramienta estad√≠stica fundamental utilizada para **detectar y cuantificar la multicolinealidad** en modelos de regresi√≥n lineal m√∫ltiple.

**Problemas que causa la multicolinealidad:**
  
  * **Coeficientes de regresi√≥n inestables y dif√≠ciles de interpretar:** Cuando las variables predictoras est√°n altamente correlacionadas, es dif√≠cil para el modelo determinar la contribuci√≥n √∫nica de cada variable a la variable dependiente. Peque√±os cambios en los datos pueden llevar a grandes cambios en los coeficientes estimados, haciendo que sean poco fiables.
* **Errores est√°ndar inflados:** La multicolinealidad aumenta los errores est√°ndar de los coeficientes de regresi√≥n, lo que a su vez disminuye los valores de las estad√≠sticas t y aumenta los p-valores. Esto puede llevar a la conclusi√≥n err√≥nea de que una variable no es estad√≠sticamente significativa cuando en realidad s√≠ lo es.
* **Poder predictivo reducido:** Aunque la multicolinealidad no necesariamente afecta la capacidad predictiva global del modelo (el $R^2$ ajustado puede seguir siendo alto), s√≠ afecta la precisi√≥n de las estimaciones de los coeficientes individuales, lo que hace dif√≠cil comprender la relaci√≥n real entre cada predictor y la variable de respuesta.


El VIF mide **cu√°nto se "infla" la varianza del coeficiente de regresi√≥n estimado de una variable predictora debido a su correlaci√≥n con otras variables predictoras** en el modelo.

**La f√≥rmula del VIF para una variable $X_j$ es:**
  
  $$VIF_j = \frac{1}{1 - R_j^2}$$
  
  Donde $R_j^2$ es el coeficiente de determinaci√≥n ($R^2$) de una regresi√≥n auxiliar en la que la variable $X_j$ se predice utilizando todas las dem√°s variables independientes del modelo.

#### ¬øC√≥mo se interpreta el VIF?

* **VIF = 1:** Indica que la variable predictora no est√° correlacionada con ninguna de las otras variables predictoras en el modelo. No hay multicolinealidad.
* **VIF > 1:** Indica que existe alg√∫n grado de multicolinealidad. Cuanto mayor sea el valor del VIF, mayor es el grado de multicolinealidad.

**Reglas generales para interpretar los valores de VIF (son reglas de "pulgar" y pueden variar ligeramente seg√∫n el contexto y el campo de estudio):**
  
  * **VIF ‚â§ 5:** Generalmente se considera que no hay problemas graves de multicolinealidad.
* **5 < VIF ‚â§ 10:** Puede indicar un nivel moderado a alto de multicolinealidad que podr√≠a justificar una investigaci√≥n.
* **VIF > 10:** A menudo se considera una indicaci√≥n clara de multicolinealidad grave y problem√°tica. Es un umbral com√∫nmente aceptado para se√±alar que una variable est√° fuertemente correlacionada con otras, lo que podr√≠a afectar la estabilidad y fiabilidad del modelo.

```{r}
require(car)

# Calcular VIF para el modelo original
vif(modelo_ols)
```
**Alt√≠sima colinealidad** en:
  
  * `cyl`, `disp`, `wt`, `carb` 

**Sospechosos comunes**:
  
  * `cyl`, `disp`, `hp` y `wt` est√°n muy correlacionados entre s√≠ (motores grandes tienden a pesar m√°s, tener m√°s cilindros y m√°s caballos).


```{r}
# Calcular VIF para el modelo reducido 
vif(modelo_step)
```
Todos los VIF est√°n **por debajo del umbral de 5**, lo que indica:
  
* No hay **colinealidad preocupante** entre las variables.
* El modelo es **estable** y los coeficientes son interpretables con confianza.
* `stepAIC` hizo un buen trabajo al reducir la complejidad y mejorar la interpretabilidad del modelo.

## Evaluaci√≥n del modelo  

Ahora se va a trabajar con un **modelo reducido (`modelo_step`)**, es importante **usar ese modelo** para hacer predicciones en el conjunto de prueba. Adem√°s, como solo algunas variables quedaron en el modelo (`drat`, `wt`, `gear`, `carb`), el `test_data` debe contener esas columnas.

**Verificar qu√© variables necesita el modelo reducido**

```{r}
# Ver f√≥rmulas del modelo reducido
formula(modelo_step)
```

**Nos aseguramos que las bases de (test / training tengan ) de que test_data tenga esas variables**
```{r}
test_data_reducido <- test_data[, c("mpg", "drat", "wt", "gear", "carb")]
```

### Modelo predictivo   

Con la funci√≥n **`predict()`** gen√©rica en R que se utiliza para obtener predicciones de varios tipos de objetos de modelos (lineales, √°rboles de decisi√≥n, series de tiempo, etc.). Su comportamiento exacto depende de la clase del objeto que se le pasa como primer argumento.  

Esta operaci√≥n es un paso clave en el flujo de trabajo de modelado predictivo, ya que permite:
  
1.  **Evaluaci√≥n del modelo:** Una vez que se tienen las `predicciones` para el `test_data`, se puede compararlas con los valores reales (observados) de la variable dependiente en el `test_data` (si se tienen) para **evaluar qu√© tan bien se desempe√±a el modelo** en datos no vistos. Esto ayuda a estimar su rendimiento en el mundo real y a detectar problemas como el **sobreajuste (overfitting)**.  
2.  **Uso pr√°ctico del modelo:** Despu√©s de validar que el modelo es bueno, se puede usarlo para predecir la variable dependiente para nuevas observaciones futuras de las que solo se conocen las variables predictoras (por ejemplo, predecir el precio de una casa bas√°ndose en sus caracter√≠sticas, si el modelo fue entrenado para eso).


```{r}
# 5. Se evalua el modelo en los datos de prueba
predicciones <- predict(modelo_step, newdata = test_data_reducido)
```


```{r}
# Comparar valores reales vs. predichos
resultados <- tibble(Real = test_data_reducido$mpg,
                     Pred = predicciones
)
```

### Evaluar el desempe√±o predictivo  

```{r, eval = FALSE}
# C√°lculo del MAE (Mean Absolute Error)
MAE <- mean(abs(predicciones - test_data_reducido$mpg))
MAE
```

```{r}
# C√°lculo del RMSE (Root Mean Squared Error)
RMSE <- sqrt(mean((predicciones - test_data_reducido$mpg)^2))
RMSE
```

```{r}
# R¬≤ manual (opcional)
SST <- sum((test_data_reducido$mpg - mean(test_data_reducido$mpg))^2)
SSE <- sum((test_data_reducido$mpg - predicciones)^2)
R2 <- 1 - SSE/SST
R2
```


```{r}
# 6. Validaci√≥n cruzada (opcional)
control <- trainControl(method = "cv", number = 10)
modelo_cv <- train(mpg ~ ., data = mtcars, method = "lm", trControl = control)
print(modelo_cv)

# 7. Usar el modelo para predecir nuevos datos (simulaci√≥n)
nuevo_auto <- data.frame(
  cyl = 6,
  disp = 200,
  hp = 110,
  drat = 3.5,
  wt = 2.8,
  qsec = 18,
  vs = 1,
  am = 1,
  gear = 4,
  carb = 2
)

predict(modelo_ols, newdata = nuevo_auto)

# 8. Interpretaci√≥n del modelo (con broom)
tidy(modelo_ols)
```

## üìå Notas

-   Este modelo es considerado **supervisado** porque se entrena con pares de entrada y salida.
-   Aunque la regresi√≥n lineal es simple, **sigue siendo un algoritmo de machine learning supervisado** y √∫til como l√≠nea base (baseline).
-   Puedes reemplazar `lm()` por modelos m√°s complejos como `randomForest`, `xgboost`, etc., manteniendo la misma estructura.

### Ventajas y Limitaciones

**Ventajas:**

-   **Simplicidad:** Es f√°cil de entender e implementar.

-   **Interpretabilidad:** Los coeficientes estimados tienen una interpretaci√≥n clara.

-   **Eficiencia:** Bajo ciertas condiciones, los estimadores de OLS son los m√°s eficientes.

**Limitaciones:**

-   **Sensibilidad a valores at√≠picos:** Los valores extremos pueden influir mucho en la l√≠nea de regresi√≥n.
-   **Sensibilidad a violaciones de supuestos:** Si los supuestos no se cumplen, las estimaciones pueden ser sesgadas o ineficientes, y la inferencia estad√≠stica puede ser inv√°lida.
-   **Solo relaciones lineales:** No puede capturar relaciones no lineales a menos que se transformen las variables adecuadamente.

## Referencias

Librerias que se usaron en el documento

```{r, echo = FALSE, eval = TRUE}
sesion_info <- devtools::session_info()
require(knitr)
require(kableExtra)
kable(dplyr::select(tibble::as_tibble(sesion_info$packages %>% dplyr::filter(attached == TRUE)),
                    c(package, loadedversion, source))) %>%
 kable_styling(font_size = 10, 
               bootstrap_options = c("condensed", "responsive", "bordered")) %>%
  kable_classic(full_width = TRUE, html_font = "montserrat") %>% 
   scroll_box(width = "100%", height = "400px") %>%  
    gsub("font-size: initial !important;", "font-size: 10pt !important;", .)
```

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png" alt="Creative Commons Licence" style="border-width:0"/></a><br />This work by [**Diana Villasana Ocampo**]{xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"} is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
