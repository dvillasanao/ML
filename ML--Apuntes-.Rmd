--- 
title: "Machine Learning (Apuntes) "
author: "Diana Villasana Ocampo"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::bs4_book,
  set in the _output.yml file.
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---

# url: your book url like https://bookdown.org/yihui/bookdown

Placeholder


## üìå Cuadro {-}

<!--chapter:end:index.Rmd-->


# üîç **1. Regressi√≥n** {.unnumbered}

Placeholder


## Ordinary Least Squares Regression (`OLSR`) {-}   
## Linear Regression {.unnumbered}
## Regresi√≥n Log√≠stica {.unnumbered}
## Locally Estimated Scatterplot Smoothing (`LOESS`) {.unnumbered}
## Multivariate Adaptive Regression Splines (`MARS`) {.unnumbered}
## Stepwise Regression {.unnumbered}
## Support Vector Machine (SVM) {.unnumbered}

<!--chapter:end:01-regression.Rmd-->


# üå≤ **2. √Årboles de Decisi√≥n y Derivados** {-}  

Placeholder


## C4.5  {-}   
## C5.0  {-}  
## Classification and Regression Tree (CART)  {-} 
## Chi-squared Automatic Interaction Detection (CHAID)  {-}    
## Conditional Decision Trees (Conditional Inference Trees - CITs)  {-}   
## Decision Stump  {-}  
## Iterative Dichotomiser 3 (ID3)  {-}    
## M5 (Model Tree) {-}  

<!--chapter:end:02-decision_tree.Rmd-->


# üåü **3. Ensambles (Ensemble Methods)** {-}

Placeholder


## Adaptive Boosting (AdaBoost)  {-}  
## Boosting  {-}   
## Bootstrapped Aggregation (Bagging)  {-}    
## Extreme Gradient Boosting (XGBoost)  {-}    
## Gradient Boosting Machines (GBM)  {-}   
## Gradient Boosted Regression Trees (GBRT)  {-}   
## Light Gradient Boosting Machine (LightGBM)  {-}   
## Random Forest  {-}   
## Stacked Generlization (Blending) {-}   

<!--chapter:end:03-ensemble.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üß† **4. Redes Neuronales y Deep Learning** {-}  

**Ejemplos:** MLP, CNN, RNN, Transformers   
**Cu√°ndo usarlo:**   

* Im√°genes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).
* Grandes vol√∫menes de datos no estructurados.

**Ventajas:** Muy poderosos para datos complejos y no estructurados.   
**Limitaciones:** Requieren mucha data y poder computacional. Menor interpretabilidad.

---


## Autoenconder  {-}  

Un **Autoencoder** es un tipo de **red neuronal artificial** dise√±ado para aprender una **representaci√≥n (o codificaci√≥n) eficiente y comprimida** de los datos de entrada, sin supervisi√≥n humana. Su objetivo principal es la **reducci√≥n de dimensionalidad** o el **aprendizaje de caracter√≠sticas**, lo que lo hace √∫til para tareas como la detecci√≥n de anomal√≠as, la denoising de im√°genes, o la generaci√≥n de datos.

La arquitectura b√°sica de un Autoencoder se compone de dos partes principales:

1.  **Encoder (Codificador):** Esta parte de la red toma los datos de entrada y los transforma en una representaci√≥n de menor dimensi√≥n, a menudo llamada **c√≥digo, representaci√≥n latente, o cuello de botella (bottleneck)**. Es decir, comprime la informaci√≥n esencial de la entrada.
2.  **Decoder (Decodificador):** Esta parte toma la representaci√≥n comprimida (el c√≥digo) del encoder y la reconstruye de nuevo a la dimensi√≥n original de los datos de entrada.

El Autoencoder se entrena para **minimizar la diferencia entre la entrada original y su reconstrucci√≥n** generada por el decoder. Esta diferencia se mide a trav√©s de una **funci√≥n de p√©rdida de reconstrucci√≥n** (como el error cuadr√°tico medio para datos continuos o la entrop√≠a cruzada para datos binarios). Al forzar a la red a reconstruir su propia entrada a partir de una representaci√≥n comprimida, el Autoencoder aprende las caracter√≠sticas m√°s salientes y √∫tiles de los datos de forma no supervisada.

Existen varias variantes de Autoencoders, como los **Autoencoders Denoising** (que aprenden a reconstruir datos limpios a partir de datos con ruido), los **Autoencoders Variacionales (VAEs)** (que aprenden una distribuci√≥n probabil√≠stica de la representaci√≥n latente, √∫tiles para la generaci√≥n de datos), y los **Autoencoders Convolucionales** (que usan capas convolucionales, ideales para im√°genes).


**Aprendizaje Global vs. Local:**

Un Autoencoder se considera principalmente un modelo de **aprendizaje global**, aunque con una perspectiva √∫nica debido a su naturaleza de compresi√≥n y reconstrucci√≥n.

* **Aspecto Global:** Un Autoencoder aprende una **transformaci√≥n global** de los datos. El encoder aprende a mapear todo el espacio de entrada a un espacio de representaci√≥n latente, y el decoder aprende a mapear ese espacio latente de vuelta al espacio de salida. Las ponderaciones y sesgos de la red se ajustan para encontrar esta transformaci√≥n que funciona de manera √≥ptima para todo el conjunto de datos de entrenamiento, permitiendo la reconstrucci√≥n m√°s fiel posible en general. La **funci√≥n de p√©rdida de reconstrucci√≥n** se minimiza a nivel de todo el conjunto de datos, no solo en vecindarios espec√≠ficos.

* **Representaci√≥n Local vs. Reconstrucci√≥n Global:** Aunque el objetivo final es una reconstrucci√≥n global de la entrada, la **representaci√≥n latente (el c√≥digo)** puede verse como una forma de capturar **caracter√≠sticas o patrones importantes** que, en cierto sentido, resumen la informaci√≥n "local" o particular de cada instancia de datos de una manera comprimida. Sin embargo, la forma en que estas caracter√≠sticas se aprenden y se utilizan para la reconstrucci√≥n se rige por un conjunto global de par√°metros de la red. No se entrena un modelo separado para cada vecindario de datos, sino una √∫nica red que aprende una funci√≥n de mapeo para todo el dominio.

En resumen, el Autoencoder aprende una representaci√≥n eficiente y una capacidad de reconstrucci√≥n que se aplica de manera consistente a todos los datos, lo que lo clasifica como un modelo de aprendizaje global que busca una soluci√≥n unificada para el problema de la codificaci√≥n y decodificaci√≥n de datos. 

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (aprendizaje no supervisado)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (o categ√≥ricas codificadas)",
  "‚úÖ Captura relaciones complejas y no lineales",
  "‚ùå No aplica (no es un modelo de regresi√≥n)",
  "‚ùå No aplica (no hay errores residuales)",
  "‚ùå No aplica",
  "‚úÖ S√≠, pueden afectar la reconstrucci√≥n",
  "‚úÖ Puede ayudar a reducir efectos de multicolinealidad",
  "‚ö†Ô∏è Baja interpretabilidad (representaciones latentes)",
  "‚ö†Ô∏è Lento en entrenamiento, especialmente con muchas capas o datos",
  "‚ö†Ô∏è Se puede validar con reconstrucci√≥n y autoevaluaci√≥n",
  "‚ùå Datos con mucha dispersi√≥n o sin estructura latente clara"
)

detalles <- c(
  "Red neuronal no supervisada que aprende a codificar y decodificar los datos para reducir dimensionalidad o detectar anomal√≠as.",
  "No predice una variable externa, sino que reproduce la entrada como salida.",
  "Requiere variables num√©ricas (o una codificaci√≥n previa en caso de categ√≥ricas).",
  "Es capaz de capturar estructuras complejas y no lineales al comprimir los datos.",
  "No tiene residuos como un modelo cl√°sico, pero s√≠ errores de reconstrucci√≥n.",
  "No modela errores independientes, ya que no es un modelo predictivo tradicional.",
  "Tampoco se eval√∫a homoscedasticidad, ya que no hay predicci√≥n como tal.",
  "Outliers distorsionan el entrenamiento, especialmente si no se normaliza.",
  "Ayuda a eliminar redundancias en los datos si est√°n correlacionados.",
  "Las capas internas (representaciones) no son directamente interpretables.",
  "Requiere entrenamiento con varias iteraciones y puede tardar con arquitecturas grandes.",
  "Se eval√∫a con p√©rdida de reconstrucci√≥n o aplicando validaci√≥n cruzada si se integra en modelos supervisados.",
  "Pierde eficacia si los datos no tienen una estructura latente √∫til para codificar."
)

tabla_autoencoder <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_autoencoder %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir autoencoder",
             subtitle = "Autoenconder")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Back - Propagation  {-}  

**Back-Propagation (Retropropagaci√≥n)** es el algoritmo fundamental de entrenamiento utilizado para ajustar los pesos de las **redes neuronales artificiales multicapa (MLP)**. La idea central de Back-Propagation es calcular la **contribuci√≥n de cada peso al error global de la red** y luego ajustar esos pesos para reducir dicho error, propagando la informaci√≥n del error "hacia atr√°s" desde la capa de salida hasta la capa de entrada.

A diferencia del Perceptron, que solo puede aprender patrones linealmente separables, Back-Propagation permite entrenar redes neuronales profundas con m√∫ltiples capas ocultas y funciones de activaci√≥n no lineales, lo que les permite modelar relaciones complejas y no lineales en los datos.

El funcionamiento de Back-Propagation se divide en dos fases principales que se repiten iterativamente:

1.  **Fase de Propagaci√≥n hacia Adelante (Forward Pass):**
    * Las entradas se pasan a trav√©s de la red, desde la capa de entrada, a trav√©s de las capas ocultas, hasta la capa de salida.
    * En cada neurona, se calcula la suma ponderada de sus entradas (incluido el sesgo) y se aplica la funci√≥n de activaci√≥n (ej. sigmoide, tanh, ReLU) para producir la salida de esa neurona.
    * La salida final de la red se compara con el valor objetivo real para calcular el **error global** (o "costo") de la red, utilizando una funci√≥n de p√©rdida (ej. error cuadr√°tico medio para regresi√≥n, entrop√≠a cruzada para clasificaci√≥n).

2.  **Fase de Retropropagaci√≥n (Backward Pass):**
    * El error global se propaga **hacia atr√°s** desde la capa de salida, a trav√©s de las capas ocultas, hasta la capa de entrada.
    * En cada capa, se calcula el **gradiente** del error con respecto a los pesos de las conexiones de esa capa. Esto implica el uso de la **regla de la cadena** del c√°lculo diferencial para determinar cu√°nto contribuye cada peso al error final.
    * Una vez calculados los gradientes, los pesos de la red se **actualizan** en la direcci√≥n opuesta al gradiente (es decir, en la direcci√≥n de mayor descenso) para reducir el error. Esta actualizaci√≥n se realiza con una **tasa de aprendizaje** que controla el tama√±o del paso.
    $$w_{ij}^{\text{nuevo}} = w_{ij}^{\text{anterior}} - \alpha \cdot \frac{\partial E}{\partial w_{ij}}$$
    Donde $E$ es el error, $w_{ij}$ es el peso de la conexi√≥n entre la neurona $i$ y la neurona $j$, y $\alpha$ es la tasa de aprendizaje.

En el contexto del **aprendizaje global vs. local**, Back-Propagation es el coraz√≥n del entrenamiento de sistemas de **aprendizaje global** por excelencia (las redes neuronales multicapa). La red neuronal busca aprender una **aproximaci√≥n de funci√≥n global** que mapee las entradas a las salidas, minimizando el error en todo el conjunto de datos. Si los datos no se distribuyen linealmente, Back-Propagation permite que la red aprenda relaciones no lineales complejas a trav√©s de sus m√∫ltiples capas y funciones de activaci√≥n no lineales. A diferencia de LOESS o los m√©todos de regresi√≥n ponderada localmente, Back-Propagation no divide expl√≠citamente el problema en m√∫ltiples problemas locales independientes para minimizar funciones de costo locales. En cambio, busca minimizar una funci√≥n de p√©rdida **global** para toda la red. Sin embargo, su capacidad para ajustar un gran n√∫mero de par√°metros (pesos) le permite construir representaciones internas de los datos que pueden ser incre√≠blemente flexibles y adaptables, superando la limitaci√≥n de que "a veces ning√∫n valor de par√°metro [en un modelo simple] puede proporcionar una aproximaci√≥n suficientemente buena". La retropropagaci√≥n es lo que permiti√≥ a las redes neuronales convertirse en poderosas herramientas de aprendizaje autom√°tico.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas (requiere normalizar), Categ√≥ricas como dummies",
  "‚úÖ Captura relaciones no lineales profundas",
  "‚ùå No requiere",
  "‚úÖ Deseable, aunque no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Elevado (puede requerir robustez ante valores extremos)",
  "‚ö†Ô∏è Puede ralentizar convergencia si es muy alta",
  "‚ö†Ô∏è Baja (modelo de ‚Äúcaja negra‚Äù)",
  "‚ö†Ô∏è Depende de la arquitectura y tama√±o del dataset",
  "‚úÖ Recomendable para ajustar tasas de aprendizaje, capas y neuronas",
  "‚ùå No conviene con datos muy peque√±os o alta dimensionalidad sin regularizar"
)

detalles <- c(
  "Algoritmo para entrenar redes neuronales multicapa ajustando pesos por retropropagaci√≥n del error.",
  "En clasificaci√≥n usa softmax o sigmoide en salida; en regresi√≥n, capa lineal para valor continuo.",
  "Debe escalarse cada caracter√≠stica; las categ√≥ricas transformarse a variables indicadoras antes de entrenar.",
  "Aprende funciones arbitrariamente complejas activando m√∫ltiples capas ocultas con funciones no lineales.",
  "No impone distribuci√≥n espec√≠fica en errores, se optimiza v√≠a descenso de gradiente.",
  "Mejor si las observaciones son independientes; sensible a secuencias sin ajustes espec√≠ficos.",
  "No requiere varianza constante, ya que los pesos se ajustan adaptativamente durante el entrenamiento.",
  "Valores extremos pueden causar activaciones saturadas (vanishing/exploding gradients) si no se manejan.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; Batch Normalization ayuda a mitigar.",
  "Dif√≠cil interpretar cada peso individual; se usan t√©cnicas como LIME o SHAP para explicar decisiones.",
  "El tiempo crece con n√∫mero de capas, neuronas y epochs; GPUs aceleran el proceso.",
  "Cross-validation (o k-fold) ayuda a elegir n√∫mero de capas, neuronas por capa, tasa de aprendizaje y regularizaci√≥n.",
  "No funciona bien con datasets peque√±os (overfitting f√°cil) o ruido elevado sin t√©cnicas de regularizaci√≥n."
)

tabla_backprop <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_backprop %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir back-propagation",
             subtitle = "Back - Propagation")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Convolutional Neural Network (CNN)  {-}   

**Convolutional Neural Networks (CNNs)**, tambi√©n conocidas como **ConvNets**, son una clase especializada de **redes neuronales profundas** que han demostrado ser excepcionalmente efectivas en tareas de **visi√≥n por computadora** (como clasificaci√≥n de im√°genes, detecci√≥n de objetos, reconocimiento facial) y, m√°s recientemente, en procesamiento de lenguaje natural. La idea fundamental de una CNN es imitar el funcionamiento del c√≥rtex visual en el cerebro humano, utilizando **capas de convoluci√≥n** para detectar autom√°ticamente patrones y caracter√≠sticas jer√°rquicas directamente de los datos de entrada sin necesidad de una extracci√≥n manual de caracter√≠sticas.

A diferencia de los Multilayer Perceptrons (MLPs) que conectan cada neurona de una capa con cada neurona de la siguiente capa (lo que resulta en una enorme cantidad de par√°metros para datos de alta dimensi√≥n como im√°genes), las CNNs aprovechan tres ideas arquitect√≥nicas clave:

1.  **Capas de Convoluci√≥n:** Estas capas aplican un peque√±o conjunto de **filtros (kernels)** a la entrada (ej., una imagen). Cada filtro "se desliza" por la entrada (operaci√≥n de convoluci√≥n) y calcula un producto punto entre sus valores y los valores de la regi√≥n de la entrada que est√° cubriendo. Esto genera un **mapa de caracter√≠sticas** que resalta la presencia de patrones espec√≠ficos (bordes, texturas, formas) en diferentes ubicaciones de la entrada. La ventaja es que los mismos filtros se aplican en m√∫ltiples ubicaciones, lo que reduce dr√°sticamente el n√∫mero de par√°metros y captura la **localidad** de los patrones y la **invarianza traslacional**.
2.  **Capas de Pooling (Submuestreo):** Estas capas se insertan peri√≥dicamente entre las capas convolucionales. Su funci√≥n es reducir la dimensionalidad espacial de los mapas de caracter√≠sticas (ej., reduciendo el n√∫mero de p√≠xeles), lo que ayuda a hacer que el modelo sea m√°s robusto a peque√±as variaciones o distorsiones en la posici√≥n de las caracter√≠sticas. Las operaciones comunes son el **max pooling** (tomar el valor m√°ximo de una regi√≥n) o el **average pooling** (tomar el promedio).
3.  **Capas Totalmente Conectadas (Dense):** Despu√©s de varias capas convolucionales y de pooling, los mapas de caracter√≠sticas finales se aplanan en un vector y se conectan a una o m√°s capas totalmente conectadas (similares a las de un MLP). Estas capas finales realizan la clasificaci√≥n o regresi√≥n bas√°ndose en las caracter√≠sticas de alto nivel extra√≠das por las capas anteriores.

El entrenamiento de una CNN se realiza utilizando el algoritmo de **Back-Propagation** y descenso de gradiente (con sus variantes como SGD, Adam, etc.), ajustando los pesos de los filtros y las conexiones de las capas densas para minimizar una funci√≥n de p√©rdida.

En el contexto del **aprendizaje global vs. local**, las CNNs son un ejemplo sobresaliente de un sistema de **aprendizaje global** que, en sus capas iniciales, se beneficia de la detecci√≥n de patrones **locales**. Cada filtro de convoluci√≥n aprende a detectar un patr√≥n local espec√≠fico (un borde vertical, una esquina, etc.) que se repite en diferentes partes de la imagen (lo que es una forma de "regresi√≥n ponderada localmente" en el sentido de que el filtro "aplica" su conocimiento local a diferentes ventanas de entrada). Sin embargo, la combinaci√≥n jer√°rquica de m√∫ltiples capas convolucionales y de pooling, seguida de capas totalmente conectadas, permite que la red construya representaciones cada vez m√°s abstractas y globales del contenido de la imagen. Esto significa que si los datos no se distribuyen linealmente, las CNNs pueden aprender a modelar relaciones extremadamente complejas y no lineales al componer caracter√≠sticas locales en representaciones globales. La arquitectura de CNNs resuelve la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos m√°s simples al permitir que la red aprenda caracter√≠sticas relevantes de forma autom√°tica y jer√°rquica, adapt√°ndose a las complejidades inherentes de datos como im√°genes y videos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Im√°genes (matrices de p√≠xeles) y datos con estructura espacial",
  "‚úÖ Captura relaciones locales y espaciales mediante filtros convolucionales",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias son independientes)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (artefactos o ruido en im√°genes puede afectar)",
  "‚ö†Ô∏è No se eval√∫a colinealidad de predictores, maneja correlaciones espaciales",
  "‚ö†Ô∏è Baja (modelo tipo ‚Äòcaja negra‚Äô, usar t√©cnicas como Grad-CAM para interpretaci√≥n)",
  "‚ö†Ô∏è Lento sin GPU, entrenamiento intensivo en c√≥mputo",
  "‚úÖ Robusto si se aplica k-fold o validaci√≥n en conjunto de im√°genes",
  "‚ùå No funciona bien con pocos datos o sin estructura espacial significativa"
)

detalles <- c(
  "Red neuronal profunda especializada en procesar datos con estructura de grilla (ej. im√°genes).",
  "En clasificaci√≥n utiliza softmax; en regresi√≥n, capa lineal para valores continuos.",
  "Requiere tensores de entrada (canales, altura, ancho); funciones de preprocesamiento para im√°genes.",
  "Filtros convolucionales extraen caracter√≠sticas locales, max-pooling disminuye dimensionalidad manteniendo informaci√≥n relevante.",
  "No impone ninguna distribuci√≥n en los errores, optimiza funci√≥n de p√©rdida directamente.",
  "Ideal si las muestras son independientes; sensible a dependencias temporales o espaciales no modeladas.",
  "No requiere varianza constante pues se basa en convoluciones y pooling, no en un modelo param√©trico de error.",
  "Ruido o artefactos en p√≠xeles pueden alterar el aprendizaje de filtros, es importante usar t√©cnicas de regularizaci√≥n.",
  "La red aprende filtros que capturan patrones locales, por lo que no es necesario verificar colinealidad expl√≠citamente.",
  "Dif√≠cil de interpretar cada filtro y capa; se utilizan mapas de activaci√≥n o Grad-CAM para entender qu√© regiones influyen en la predicci√≥n.",
  "El entrenamiento con m√∫ltiples capas convolucionales y millones de par√°metros es intensivo en GPU/TPU.",
  "Validaci√≥n cruzada o separaci√≥n de conjuntos (train/validation/test) ayuda a evitar overfitting.",
  "No es apropiado para datasets muy peque√±os sin aumentar datos (data augmentation) o sin informaci√≥n espacial clara."
)

tabla_cnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles) 

tabla_cnn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CNN",
             subtitle = "Convolutional Neural Network (CNN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```


## Hopfield Network  {-}   

La **Red de Hopfield** es un tipo de **red neuronal recurrente** o **red neuronal con memoria asociativa**, propuesta por John Hopfield en 1982. A diferencia de las redes neuronales de propagaci√≥n hacia adelante (como el Perceptr√≥n o las MLP entrenadas con Back-Propagation) que se utilizan para el mapeo de entrada a salida, la idea fundamental de una Red de Hopfield es funcionar como un **sistema de memoria asociativa** y un **sistema din√°mico que converge a estados estables**. Su objetivo principal es almacenar y recuperar patrones binarios, as√≠ como resolver problemas de optimizaci√≥n.

El funcionamiento de una Red de Hopfield se basa en los siguientes principios:

1.  **Neuronas Binarias:** La red consta de un conjunto de neuronas (nodos) que son **binarias**, lo que significa que solo pueden tomar dos estados posibles, generalmente $1$ o $-1$.
2.  **Conexiones Ponderadas:** Cada neurona est√° conectada a todas las dem√°s neuronas (excepto a s√≠ misma) mediante **conexiones sim√©tricas y ponderadas**. Los pesos de estas conexiones se calculan de manera que los patrones que se quieren "memorizar" se conviertan en **estados de energ√≠a m√≠nima** de la red. La regla de aprendizaje m√°s com√∫n para establecer estos pesos es la **regla de Hebb**: si dos neuronas se activan juntas para un patr√≥n, el peso entre ellas se incrementa.
3.  **Din√°mica de Activaci√≥n:** Cuando se presenta una entrada a la red (que puede ser un patr√≥n ruidoso o incompleto), las neuronas se actualizan de forma as√≠ncrona o s√≠ncrona. La activaci√≥n de cada neurona se recalcula en funci√≥n de la suma ponderada de las activaciones de las otras neuronas a las que est√° conectada.
    $$S_i = \text{sgn}\left(\sum_{j \neq i} W_{ij} S_j\right)$$
    Donde $S_i$ es el estado de la neurona $i$, $W_{ij}$ es el peso entre la neurona $i$ y $j$, y $\text{sgn}$ es la funci√≥n signo.
4.  **Convergencia a Estados Estables:** Este proceso de actualizaci√≥n se repite hasta que la red alcanza un **estado estable** (un "atractor"), donde las activaciones de las neuronas ya no cambian. Si la red ha sido entrenada correctamente, este estado estable corresponder√° al patr√≥n memorizado m√°s cercano a la entrada inicial (memoria asociativa).
5.  **Funci√≥n de Energ√≠a:** La estabilidad de la red se puede describir mediante una **funci√≥n de energ√≠a de Lyapunov**. Durante la din√°mica de la red, la energ√≠a de la red siempre disminuye hasta que se alcanza un m√≠nimo local (un patr√≥n memorizado).

En el contexto del **aprendizaje global vs. local**, la Red de Hopfield es un sistema de **aprendizaje global** que exhibe un comportamiento de **optimizaci√≥n local**. La regla de aprendizaje (como la regla de Hebb) establece los pesos de todas las conexiones para que los patrones deseados se conviertan en m√≠nimos de energ√≠a en todo el espacio de estados. Es decir, se busca una configuraci√≥n global de pesos para memorizar un conjunto de patrones. Sin embargo, la **din√°mica de recuperaci√≥n** de la red es intr√≠nsecamente un proceso de **convergencia local**: dada una entrada inicial, la red "cae" en el m√≠nimo de energ√≠a m√°s cercano, que corresponde al patr√≥n memorizado.

Si los datos no se distribuyen linealmente, la Red de Hopfield no aplica el concepto de regresi√≥n (o clasificaci√≥n) de la misma manera que LOESS o los √°rboles de decisi√≥n. En cambio, funciona como un sistema de **memoria y recuperaci√≥n de patrones** no lineales. Puede almacenar y recuperar patrones complejos que no son linealmente separables. La red busca una soluci√≥n global (un conjunto de pesos) para almacenar los patrones, y luego, en la recuperaci√≥n, utiliza un proceso de "b√∫squeda" local en el espacio de energ√≠a para converger a un patr√≥n memorizado. Esto aborda la idea de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un modelo de regresi√≥n lineal, ya que la Red de Hopfield no es un modelo de regresi√≥n en s√≠, sino un sistema din√°mico que encuentra estados de equilibrio. Su capacidad para manejar patrones ruidosos o incompletos para recuperar el patr√≥n completo es una de sus principales fortalezas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è No es supervisado en el sentido cl√°sico",
  "‚ö†Ô∏è No hay ‚Äútarget‚Äù continuo o categ√≥rico (almacenamiento de patrones)",
  "‚úÖ Variables binarias o valores discretizados (patrones binarios)",
  "‚úÖ Correlaciona patrones con pesos sim√©tricos entre neuronas",
  "‚ùå No aplica (no hay residuos param√©tricos)",
  "‚úÖ Deseable, aunque no obligatorio (los estados deben actualizarse sin bucles no deseados)",
  "‚ùå No aplica (no modela varianza de errores)",
  "‚ö†Ô∏è Muy sensible (un solo nodo saturado puede distorsionar la red)",
  "‚ö†Ô∏è Puede verse afectado si los patrones de entrenamiento tienen redundancia fuerte",
  "‚ö†Ô∏è Media (la din√°mica de atra√ß√£o es interpretable, pero las conexiones pueden ser complejas)",
  "‚ö†Ô∏è Moderada (dependiendo del n√∫mero de neuronas y estados s√≠ncronos/as√≠ncronos)",
  "‚ùå No se usa tradicionalmente, pero se puede validar estabilidad de memorias con pruebas de convergencia",
  "‚ö†Ô∏è No sirve si los patrones no son binarios o si hay alto ruido en entradas asociativas"
)

detalles <- c(
  "Red neuronal recurrente para recuperaci√≥n asociativa de patrones, no requiere pares X‚Üíy.",
  "No predice una variable externa, recupera patrones completos a partir de entradas parciales o ruidosas.",
  "Requiere que cada elemento del patr√≥n sea binario (¬±1) o est√© discretizado; las variables continuas deben binarizarse.",
  "Los pesos sim√©tricos se calculan por Hebb (p. ej. W = Œ£ p·µ¢ p·µ¢·µÄ), sin umbral expl√≠cito para relaciones lineales.",
  "No hay un t√©rmino de error param√©trico; la din√°mica sigue la funci√≥n de energ√≠a, no un residuo gaussiano.",
  "Es mejor si las actualizaciones de estado son independientes o s√≠ncronas; la dependencia temporal puede generar oscilaciones.",
  "No modela varianza de error, pues busca minimizar energ√≠a, no error cuadr√°tico.",
  "Patrones fuera del rango binario pueden causar saturaci√≥n o estados inestables.",
  "Patrones muy similares (colineales) pueden interferir en recuperaciones correctas (atractores vecinos).",
  "La din√°mica de convergencia hacia un estado estable (atractor) se puede visualizar, pero la topolog√≠a de pesos puede no ser transparente.",
  "Simulaci√≥n de din√°micas es razonable para tama√±os moderados (‚â§1000 neuronas); grandes redes requieren optimizaci√≥n paralela.",
  "No se usa CV tradicional; se analiza la robustez de memorias variando inicializaci√≥n o agregando ruido.",
  "No apto si los datos no pueden discretizarse en patrones binarios, o si se requieren m√∫ltiples clases de salida simult√°neas."
)

tabla_hopfield <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_hopfield %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir hopfield network",
             subtitle = "Hopfield Network")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  

```

## Multilayer Perceptron (MP)  {-}     

El **Multilayer Perceptron (MLP)**, tambi√©n conocido como **red neuronal de propagaci√≥n hacia adelante cl√°sica**, es un tipo fundamental de **red neuronal artificial** utilizada para una amplia gama de tareas de **aprendizaje supervisado**, incluyendo **clasificaci√≥n** y **regresi√≥n**. La idea fundamental del MLP es extender el concepto del Perceptr√≥n simple al incorporar una o m√°s **capas ocultas** entre la capa de entrada y la capa de salida, y utilizando **funciones de activaci√≥n no lineales** en estas capas. Esta arquitectura de m√∫ltiples capas es lo que le confiere a los MLP su capacidad para aprender y modelar relaciones complejas y no lineales en los datos.

La estructura de un MLP t√≠picamente incluye:

1.  **Capa de Entrada:** Recibe las caracter√≠sticas de entrada del problema.
2.  **Capas Ocultas:** Son una o m√°s capas intermedias donde se realizan c√°lculos complejos. Cada neurona en una capa oculta recibe entradas de la capa anterior, calcula una suma ponderada de estas entradas (m√°s un sesgo), y luego aplica una **funci√≥n de activaci√≥n no lineal** (como la funci√≥n sigmoide, tanh o ReLU) a esta suma. Es la no linealidad de estas funciones de activaci√≥n la que permite al MLP aprender relaciones no lineales.
    $$a_j = f\left(\sum_{i=1}^{n} w_{ij} x_i + b_j\right)$$
    Donde $a_j$ es la activaci√≥n de la neurona $j$, $x_i$ son las entradas de la capa anterior, $w_{ij}$ son los pesos, $b_j$ es el sesgo, y $f$ es la funci√≥n de activaci√≥n no lineal.
3.  **Capa de Salida:** Produce la predicci√≥n final de la red. La funci√≥n de activaci√≥n en esta capa depende del tipo de problema (ej., una funci√≥n lineal para regresi√≥n, softmax para clasificaci√≥n multiclase, o sigmoide para clasificaci√≥n binaria).

El entrenamiento de un MLP se realiza t√≠picamente utilizando el algoritmo de **Back-Propagation**, que ajusta los pesos de la red de manera iterativa para minimizar una funci√≥n de p√©rdida (error) calculada en la capa de salida.

En el contexto del **aprendizaje global vs. local**, el Multilayer Perceptron es el paradigma de un sistema de **aprendizaje global**. La red aprende una **aproximaci√≥n de funci√≥n global** que mapea las entradas a las salidas, buscando minimizar la funci√≥n de p√©rdida en todo el conjunto de datos de entrenamiento. A diferencia de los sistemas de aprendizaje local que dividen expl√≠citamente el problema global en m√∫ltiples problemas m√°s peque√±os, el MLP ajusta todos sus pesos de forma interconectada para aprender una representaci√≥n distribuida de los patrones en los datos. Si los datos no se distribuyen linealmente, el MLP es excepcionalmente capaz de modelar estas relaciones complejas gracias a sus capas ocultas y funciones de activaci√≥n no lineales. Esto aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos lineales o m√°s simples, ya que el MLP puede construir representaciones internas de gran complejidad para aproximar casi cualquier funci√≥n continua. Hoy en d√≠a, los MLP son la base de muchas arquitecturas de "Deep Learning".


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas (normalizar) y categ√≥reas (dummies)",
  "‚úÖ Captura relaciones no lineales profundas",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (puede requerir robustez ante outliers)",
  "‚ö†Ô∏è Afecta la convergencia si es muy alta",
  "‚ö†Ô∏è Baja (modelo tipo 'caja negra')",
  "‚ö†Ô∏è Depende de arquitectura y tama√±o del dataset",
  "‚úÖ Recomendable (k-fold o repeated CV)",
  "‚ùå No conviene con pocos datos o ruido elevado"
)

detalles <- c(
  "Red neuronal con m√∫ltiples capas ocultas y funci√≥n de activaci√≥n no lineal.",
  "Clasificaci√≥n con softmax/sigmoide; regresi√≥n con capa lineal en salida.",
  "Debe escalarse cada caracter√≠stica; las categ√≥ricas convierten a variables indicadoras.",
  "Aprende patrones complejos combinando m√∫ltiples capas y neuronas.",
  "No impone distribuci√≥n espec√≠fica de errores, se optimiza con optimizadores basados en gradiente.",
  "Funciona mejor si las muestras son independientes; sensibles a dependencia temporal sin ajustes.",
  "No requiere varianza constante, ajusta pesos en cada mini-batch o lote.",
  "Outliers pueden causar gradientes explosivos o desaparecidos sin mecanismos de robustez.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; batch normalization ayuda.",
  "Dif√≠cil de interpretar cada peso/neuronas; se usan t√©cnicas como SHAP o LIME para explicaci√≥n.",
  "El tiempo de entrenamiento aumenta con cada capa, neuronas y epochs; GPUs aceleran el proceso.",
  "Crucial para ajustar hiperpar√°metros: n√∫mero de capas, neuronas por capa, tasa de aprendizaje, regularizaci√≥n.",
  "No √∫til para datasets muy peque√±os (sobreajuste) o altamente ruidosos sin regularizaci√≥n."
)

tabla_mp <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mp %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MP",
             subtitle = "Multilayer Perceptron (MP)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Perceptron  {-}     

El **Perceptron** es el algoritmo de **aprendizaje supervisado** m√°s simple y uno de los primeros modelos de **redes neuronales artificiales**, propuesto por Frank Rosenblatt en 1957. Est√° dise√±ado para tareas de **clasificaci√≥n binaria**, es decir, para decidir si una entrada pertenece a una de dos clases posibles. Su idea fundamental es modelar c√≥mo una neurona biol√≥gica podr√≠a tomar decisiones.

El funcionamiento de un Perceptron es bastante directo:

1.  **Entradas y Pesos:** Recibe m√∫ltiples **entradas** (caracter√≠sticas) y a cada entrada se le asigna un **peso**. Estos pesos representan la importancia de cada caracter√≠stica.
2.  **Suma Ponderada:** Las entradas se multiplican por sus respectivos pesos y se suman. A esta suma se le a√±ade un **t√©rmino de sesgo (bias)**.
    $$z = \sum_{i=1}^{n} w_i x_i + b$$
    Donde $x_i$ son las entradas, $w_i$ son los pesos, $b$ es el sesgo, y $n$ es el n√∫mero de entradas.
3.  **Funci√≥n de Activaci√≥n:** El resultado de la suma ponderada ($z$) se pasa a trav√©s de una **funci√≥n de activaci√≥n** (generalmente una funci√≥n escal√≥n o *step function*). Esta funci√≥n decide la salida final, que es 1 si la suma excede un umbral (o 0 si no lo excede). Para el Perceptron original, la salida es binaria.
    $$\text{salida} = \begin{cases} 1 & \text{si } z \geq \text{umbral} \\ 0 & \text{si } z < \text{umbral} \end{cases}$$
4.  **Aprendizaje (Regla de Perceptron):** El Perceptron aprende ajustando sus pesos de forma iterativa. Si la predicci√≥n es incorrecta, los pesos se actualizan para reducir el error en la siguiente iteraci√≥n. La regla de actualizaci√≥n de pesos es:
    $$w_i^{\text{nuevo}} = w_i^{\text{anterior}} + \alpha \cdot (y - \hat{y}) \cdot x_i$$
    Donde $\alpha$ es la tasa de aprendizaje, $y$ es el valor real, y $\hat{y}$ es la predicci√≥n del Perceptron.

En el contexto del **aprendizaje global vs. local**, el Perceptron es un sistema de **aprendizaje global** por naturaleza. Busca encontrar un **hiperplano de separaci√≥n lineal** √∫nico que divida el espacio de caracter√≠sticas en dos regiones. La idea es que, si los datos son **linealmente separables** (es decir, si existe una l√≠nea, plano o hiperplano que puede separar perfectamente las dos clases), el Perceptron est√° garantizado para converger y encontrar esa soluci√≥n.

Sin embargo, precisamente porque busca una soluci√≥n lineal global, si los datos no se distribuyen linealmente (es decir, no son linealmente separables), el Perceptron **no puede encontrar una soluci√≥n convergente** y no puede aprender la relaci√≥n. Esto ilustra la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" cuando se busca una soluci√≥n global r√≠gida. El Perceptron original no puede aplicar el concepto de regresi√≥n ponderada localmente ni adaptarse a complejidades no lineales, a diferencia de modelos posteriores como las redes neuronales multicapa con funciones de activaci√≥n no lineales o los algoritmos de √°rboles de decisi√≥n. A pesar de esta limitaci√≥n, el Perceptron sent√≥ las bases para el desarrollo posterior de redes neuronales m√°s complejas.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas (requiere normalizar), Categ√≥ricas como dummies",
  "‚ö†Ô∏è Aprendizaje lineal: separabilidad lineal requerida",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers pueden cambiar el hiperplano)",
  "‚ö†Ô∏è Afecta la convergencia si est√° muy alta",
  "‚ö†Ô∏è Baja (modelo b√°sico de una capa sin capas ocultas)",
  "‚úÖ Muy r√°pido para datasets medianos",
  "‚úÖ √ötil para evaluar margen de separaci√≥n",
  "‚ùå No sirve si las clases no son linealmente separables"
)

detalles <- c(
  "Red neuronal de una sola capa que ajusta un hiperplano separador.",
  "Dise√±ado para clasificaci√≥n binaria; no predice valores continuos.",
  "Todas las features deben ser num√©ricas y escaladas; las categ√≥ricas deben convertirse en indicadores.",
  "Busca maximizar el margen de separaci√≥n lineal entre dos clases; no captura no linealidades.",
  "No exige distribuci√≥n normal de errores ya que optimiza con perceptr√≥n simple.",
  "Funciona mejor si las instancias son independientes; sensible a dependencias temporales sin ajuste.",
  "No se basa en varianza de errores; el algoritmo actualiza pesos sin supuestos de varianza.",
  "Los valores extremos cercanos al margen pueden forzar ajustes bruscos de pesos.",
  "La colinealidad puede ralentizar la convergencia, aunque no impide la definici√≥n de hiperplano.",
  "F√°cil de entender: el peso de cada caracter√≠stica indica direcci√≥n del hiperplano.",
  "Entrenamiento r√°pido usando regla de aprendizaje por error; escalable a datos medianos.",
  "Se usa CV para ajustar tasa de aprendizaje y n√∫mero de √©pocas para evitar bajo/sobreajuste.",
  "In√∫til si las clases no se pueden separar linealmente; requiere extensiones (por ejemplo, kernel) para no linealidad."
)

tabla_perceptron <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_perceptron %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir perceptron",
             subtitle = "Perceptron")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Radial Basis Function Network (RBFN)  {-}      

**Radial Basis Function Network (RBFN)** es un tipo de **red neuronal artificial** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**. A diferencia de las redes neuronales multicapa perceptr√≥n tradicionales que utilizan funciones de activaci√≥n sigmoide o ReLU, las RBFN emplean **funciones de base radial** como sus funciones de activaci√≥n en la capa oculta. Su estructura es t√≠picamente m√°s simple que un perceptr√≥n multicapa, consistiendo generalmente en tres capas: una capa de entrada, una capa oculta con neuronas de base radial, y una capa de salida.

La idea fundamental de una RBFN radica en su capacidad para modelar relaciones no lineales al mapear datos de entrada a un espacio de caracter√≠sticas de mayor dimensi√≥n donde pueden ser **linealmente separables** (para clasificaci√≥n) o donde una **funci√≥n lineal** puede aproximar la relaci√≥n (para regresi√≥n). Esto se logra a trav√©s de las neuronas de la capa oculta, cada una de las cuales representa un "centro" en el espacio de caracter√≠sticas.

El funcionamiento de una RBFN implica:

1.  **Capa de Entrada:** Recibe las caracter√≠sticas de entrada.
2.  **Capa Oculta (Neuronas de Base Radial):** Cada neurona en esta capa tiene un **centro** ($c_i$) y un **radio (o desviaci√≥n est√°ndar, $\sigma_i$)**. La funci√≥n de activaci√≥n de estas neuronas (com√∫nmente una **funci√≥n Gaussiana**) calcula la **distancia** entre el vector de entrada ($x$) y el centro de la neurona ($c_i$), y luego aplica la funci√≥n de base radial. Cuanto m√°s cerca est√© la entrada del centro de la neurona, mayor ser√° la activaci√≥n de esa neurona.
    $$\phi_i(x) = \exp\left(-\frac{\|x - c_i\|^2}{2\sigma_i^2}\right)$$
    Donde $\phi_i(x)$ es la salida de la neurona $i$, $\|x - c_i\|$ es la distancia euclidiana entre la entrada $x$ y el centro $c_i$, y $\sigma_i$ es el radio (ancho) de la funci√≥n Gaussiana.
3.  **Capa de Salida:** Las salidas de las neuronas de la capa oculta se combinan linealmente (ponderadas por unos coeficientes, $w_{ij}$) para producir la salida final de la red. Para regresi√≥n, es una suma ponderada; para clasificaci√≥n, a menudo se usa una funci√≥n de activaci√≥n softmax.
    $$y_j = \sum_{i=1}^{M} w_{ij}\phi_i(x)$$
    Donde $y_j$ es la salida $j$, $M$ es el n√∫mero de neuronas ocultas, y $w_{ij}$ son los pesos de la capa de salida.

En el contexto del **aprendizaje global vs. local**, las RBFN son intr√≠nsecamente sistemas de **aprendizaje local**. Cada neurona de la capa oculta es sensible a una **regi√≥n espec√≠fica** del espacio de entrada, definida por su centro y su radio. La red como un todo es una combinaci√≥n de estas respuestas locales. Si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se aplica de forma muy eficaz mediante esta naturaleza de **regresi√≥n ponderada localmente**. Las RBFN pueden aproximar cualquier funci√≥n continua con la suficiente cantidad de neuronas de base radial. Esto aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo global, ya que la red puede adaptarse localmente a las caracter√≠sticas de diferentes regiones del espacio de datos. Son particularmente √∫tiles para problemas de aproximaci√≥n de funciones, series de tiempo y reconocimiento de patrones.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas (requiere normalizaci√≥n), Categ√≥ricas como dummies",
  "‚úÖ Captura no linealidades mediante funciones base radiales",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "‚ùå No asume varianza constante",
  "‚ö†Ô∏è Moderadamente (centros pueden verse alterados por outliers)",
  "‚ö†Ô∏è Puede influir en la selecci√≥n de centros, pero no tan cr√≠tico como en OLS",
  "‚ö†Ô∏è Baja (la capa oculta con RBF es dif√≠cil de interpretar)",
  "‚ö†Ô∏è Moderada (depende de n√∫mero de centros y dimensiones)",
  "‚úÖ Recomendable para ajustar n√∫mero de bases y spread",
  "‚ùå Datos muy grandes o alta dimensionalidad sin reducci√≥n, mucho ruido"
)

detalles <- c(
  "Red neuronal de una capa oculta con funciones radial basis como activaci√≥n.",
  "Para regresi√≥n predice un valor continuo; para clasificaci√≥n usa votaci√≥n o softmax sobre salidas.",
  "Requiere que las caracter√≠sticas num√©ricas est√©n escaladas; las categ√≥ricas deben convertirse a variables indicadoras.",
  "Cada neurona oculta calcula una funci√≥n gaussiana (u otra RBF) centrada en un punto, captando curvas suaves.",
  "No impone distribuci√≥n normal en los errores, pues optimiza en funci√≥n de m√≠nimos cuadrados o cross-entropy.",
  "Funciona mejor si las observaciones son independientes; sensible a estructuras de dependencia sin modelar.",
  "No requiere homocedasticidad ya que no se basa en un modelo param√©trico de error con varianza fija.",
  "Los valores extremos pueden desplazar los centros de las RBF, afectando la forma del modelo.",
  "La colinealidad puede dificultar la determinaci√≥n de centros √≥ptimos, pero no invalida el ajuste.",
  "Las neuronas ocultas representan combinaciones complejas de caracter√≠sticas, por lo que el modelo es tipo 'caja negra'.",
  "El entrenamiento implica fijar o aprender centros y spreads; para muchos centros o dimensiones altas, el costo crece r√°pido.",
  "Se usa CV para elegir el n√∫mero de bases (centros) y el par√°metro de ancho (`sigma` o `spread`) para evitar sobreajuste.",
  "No conviene cuando hay decenas de miles de caracter√≠sticas sin reducci√≥n previa o cuando el ruido es muy alto."
)

tabla_rbfn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rbfn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RBFN",
             subtitle = "Radial Basis Function Network (RBFN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Recurrent Neural Networks (RNNs) {-}   

**Recurrent Neural Networks (RNNs)** son un tipo de **red neuronal artificial** dise√±ado espec√≠ficamente para manejar **datos secuenciales** o temporales, donde la informaci√≥n de pasos anteriores en la secuencia es relevante para la predicci√≥n actual. A diferencia de las redes de propagaci√≥n hacia adelante (como MLP o CNN) que asumen que las entradas son independientes entre s√≠, las RNNs tienen "memoria" o **conexiones recurrentes** que les permiten mantener un **estado interno** que encapsula informaci√≥n de pasos de tiempo anteriores. Esta caracter√≠stica las hace ideales para tareas como el procesamiento de lenguaje natural (PLN), el reconocimiento de voz, la traducci√≥n autom√°tica y la predicci√≥n de series de tiempo.

La idea fundamental de una RNN es que una **unidad recurrente** aplica la misma funci√≥n de transformaci√≥n a cada elemento de una secuencia, con la particularidad de que la salida de la unidad en un paso de tiempo dado se realimenta como entrada para el mismo proceso en el siguiente paso de tiempo. Esto permite que la red "recuerde" y utilice informaci√≥n pasada al procesar la secuencia actual.

El funcionamiento b√°sico de una RNN en un paso de tiempo ($t$) implica:

1.  **Entrada actual ($x_t$):** El elemento actual de la secuencia.
2.  **Estado oculto anterior ($h_{t-1}$):** La "memoria" o estado interno de la red del paso de tiempo anterior.
3.  **C√°lculo del Estado Oculto Actual ($h_t$):** Se combina la entrada actual y el estado oculto anterior, y se aplica una funci√≥n de activaci√≥n (ej., tanh o ReLU).
    $$h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$
    Donde $W_{hh}$ son los pesos de la conexi√≥n recurrente, $W_{xh}$ son los pesos de la entrada, y $b_h$ es el sesgo.
4.  **Salida Actual ($y_t$):** Se genera una salida a partir del estado oculto actual.
    $$y_t = W_{hy} h_t + b_y$$
    Donde $W_{hy}$ son los pesos de la salida y $b_y$ es el sesgo.

Este proceso de actualizaci√≥n de estado y salida se repite para cada elemento de la secuencia. La "memoria" de la RNN est√° codificada en el estado oculto que se pasa de un paso de tiempo al siguiente.

El entrenamiento de las RNNs se realiza mediante una variante del algoritmo de Back-Propagation llamada **Back-Propagation Through Time (BPTT)**. BPTT desenrolla la red a lo largo del tiempo, tratando cada paso de tiempo como una capa separada, y luego aplica la retropropagaci√≥n de manera similar a c√≥mo se entrena un MLP, pero propagando los errores a trav√©s de las conexiones recurrentes. Sin embargo, las RNNs simples pueden sufrir de problemas como el **desvanecimiento del gradiente** (vanishing gradient) o el **explosi√≥n del gradiente** (exploding gradient) para secuencias largas, lo que llev√≥ al desarrollo de arquitecturas m√°s avanzadas como **LSTM (Long Short-Term Memory)** y **GRU (Gated Recurrent Unit)**.

En el contexto del **aprendizaje global vs. local**, las RNNs son sistemas de **aprendizaje global** que est√°n dise√±ados para aprender y modelar **dependencias temporales y patrones secuenciales** en un dominio global. A diferencia de los m√©todos de regresi√≥n ponderada localmente como LOESS, que se enfocan en ajustar curvas en regiones espec√≠ficas de datos, las RNNs intentan aprender una funci√≥n de mapeo compleja que considera toda la secuencia hist√≥rica para producir una predicci√≥n. Si los datos (secuenciales) no se distribuyen linealmente, las RNNs son extremadamente efectivas para capturar estas relaciones no lineales y dependencias a largo plazo. Al tener un estado interno que recuerda informaci√≥n pasada, abordan directamente la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos est√°ticos o lineales, ya que pueden adaptar sus predicciones din√°micamente en funci√≥n del contexto secuencial, lo que las convierte en una herramienta fundamental para el an√°lisis de series de tiempo y el procesamiento de lenguaje.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado secuencial",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n) en secuencias",
  "‚úÖ Series temporales y datos secuenciales (texto, audio, series) convertidos a vectores",
  "‚úÖ Captura dependencias temporales y de largo plazo entre pasos de la secuencia",
  "‚ùå No requiere supuestos de normalidad de residuos",
  "‚ö†Ô∏è Ideal si las secuencias son independientes entre s√≠; no modela dependencia ex√≥gena autom√°ticamente",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers en la serie pueden sesgar el entrenamiento si no se detectan)",
  "‚ö†Ô∏è La colinealidad en caracter√≠sticas secuenciales puede afectar la convergencia (usar embeddings o reducci√≥n)",
  "‚ö†Ô∏è Baja (‚Äúcaja negra‚Äù con muchas capas; usar t√©cnicas de atenci√≥n o visualizaci√≥n de activaciones)",
  "‚ö†Ô∏è Lento sin GPU/TPU; entrenamiento costoso para secuencias largas o redes profundas",
  "‚ö†Ô∏è Usar validaci√≥n cronol√≥gica (time series split) es m√°s apropiado que k-fold cl√°sico",
  "‚ùå No conviene con muy pocas muestras temporales, secuencias extremadamente largas sin truncar, o datos muy ruidosos"
)

detalles <- c(
  "Red neuronal recurrente que procesa datos en pasos temporales manteniendo un estado interno.",
  "En clasificaci√≥n, etiqueta cada elemento o secuencia; en regresi√≥n, predice valores continuos a lo largo del tiempo.",
  "Las entradas deben transformarse en vectores o embeddings; por ejemplo, texto a √≠ndices, series normalizadas.",
  "La arquitectura RNN (LSTM, GRU) retiene informaci√≥n de pasos anteriores para afectar salidas posteriores.",
  "No impone distribuci√≥n en errores, ya que se optimiza v√≠a descenso de gradiente sobre secuencias.",
  "Funciona mejor si cada secuencia (serie) es independiente; para datos con autocorrelaci√≥n compleja, usar variantes especializadas.",
  "No requiere varianza constante, pues se basa en propagaci√≥n de estado y no en un t√©rmino de error param√©trico.",
  "Valores at√≠picos en la serie pueden provocar gradientes explosivos o desvanecidos sin mecanismos como clipping.",
  "La representaci√≥n internal de patrones secuenciales puede verse afectada si hay caracter√≠sticas muy correlacionadas; usar regularizaci√≥n.",
  "Dif√≠cil interpretar pesos internos; se usan mec√°nicas como atenci√≥n (attention) o visualizaci√≥n de celdas LSTM.",
  "El entrenamiento con backpropagation through time es intensivo; GPUs o TPUs aceleran enormemente el proceso.",
  "Para series temporales, se prefiere validaci√≥n basada en ventanas de tiempo (rolling/expanding window) en lugar de random split.",
  "No apto si las secuencias son muy cortas o muy pocas, o hay mucho ruido sin filtrado; en esos casos, usar modelos estad√≠sticos simples."
)

tabla_rnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rnn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RNN",
             subtitle = "Recurrent Neural Networks (RNNs)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Transformers  {-}  

Los **Transformers** son una arquitectura de **red neuronal profunda** que ha revolucionado el campo del **Procesamiento de Lenguaje Natural (PLN)** y, m√°s recientemente, se ha expandido a la visi√≥n por computadora y otras √°reas. Introducidos en el art√≠culo "Attention Is All You Need" (Vaswani et al., 2017), la idea fundamental de los Transformers es prescindir de la naturaleza recurrente de las RNNs y las convolucionales de las CNNs, bas√°ndose enteramente en un mecanismo llamado **auto-atenci√≥n (self-attention)** para capturar dependencias de largo alcance en las secuencias de entrada.

Antes de los Transformers, las RNNs eran el modelo dominante para datos secuenciales. Sin embargo, las RNNs ten√≠an limitaciones como la dificultad para capturar dependencias a muy largo plazo (problema del gradiente desvanecido) y la imposibilidad de paralelizar completamente el procesamiento de secuencias (debido a su naturaleza secuencial). Los Transformers resuelven estos problemas al permitir que cada elemento de la secuencia interact√∫e directamente con todos los dem√°s elementos de la secuencia, sin importar su distancia.

Los componentes clave de un Transformer incluyen:

1.  **Mecanismo de Auto-Atenci√≥n (Self-Attention):** Este es el coraz√≥n del Transformer. Para cada token (palabra) en una secuencia, el mecanismo de auto-atenci√≥n calcula una puntuaci√≥n de "relevancia" entre ese token y todos los dem√°s tokens de la secuencia. Esto permite que el modelo "pese" la importancia de cada token al generar la representaci√≥n de otro token. Este proceso se implementa a trav√©s de tres vectores para cada token: **Query (Q)**, **Key (K)** y **Value (V)**.
    $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
    Donde $d_k$ es la dimensi√≥n de los vectores Key.

2.  **Atenci√≥n Multi-Cabeza (Multi-Head Attention):** Para mejorar la capacidad del modelo de enfocarse en diferentes aspectos de la secuencia, el mecanismo de auto-atenci√≥n se aplica m√∫ltiples veces en paralelo con diferentes conjuntos de matrices de pesos (cabezas). Las salidas de estas cabezas se concatenan y se transforman linealmente.

3.  **Capas Feed-Forward (Posici√≥n por Posici√≥n):** Despu√©s del mecanismo de atenci√≥n, hay una red neuronal de propagaci√≥n hacia adelante (un MLP simple) que se aplica de forma independiente a cada posici√≥n en la secuencia.

4.  **Codificador-Decodificador (Encoder-Decoder Architecture):** El Transformer original consta de un **codificador** y un **decodificador**.
    * El **codificador** toma la secuencia de entrada y genera una representaci√≥n. Consiste en m√∫ltiples capas id√©nticas, cada una con una capa de auto-atenci√≥n multi-cabeza y una capa feed-forward.
    * El **decodificador** toma la representaci√≥n del codificador y genera la secuencia de salida (por ejemplo, la traducci√≥n). Tambi√©n consiste en m√∫ltiples capas, cada una con auto-atenci√≥n multi-cabeza, atenci√≥n multi-cabeza (que atiende a la salida del codificador) y una capa feed-forward.

5.  **Codificaci√≥n Posicional (Positional Encoding):** Dado que los Transformers procesan secuencias en paralelo y no tienen una noci√≥n inherente de la posici√≥n de los tokens (a diferencia de las RNNs), se a√±ade informaci√≥n de la posici√≥n de cada token a sus incrustaciones de entrada.

En el contexto del **aprendizaje global vs. local**, los Transformers son un sistema de **aprendizaje global** que, gracias a su mecanismo de atenci√≥n, pueden aprender **dependencias a largo alcance** y relaciones complejas que son inherentemente globales en la secuencia. Aunque los c√°lculos individuales de atenci√≥n pueden verse como una forma de ponderaci√≥n de la importancia local de los tokens, la red en su conjunto construye una representaci√≥n global de la secuencia. Si los datos (secuenciales) no se distribuyen linealmente, los Transformers son excepcionalmente capaces de modelar estas relaciones no lineales y dependencias a trav√©s de su capacidad para "observar" toda la secuencia a la vez y ponderar la relevancia de cada parte. Esto resuelve de manera fundamental la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos secuenciales anteriores, ya que la arquitectura de atenci√≥n les permite aprender patrones complejos y no lineales en datos secuenciales sin las restricciones de memoria de las RNNs, lo que los convierte en la arquitectura dominante para tareas de PLN avanzadas.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado (frecuentemente secuencial o multitarea)",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n en secuencias)",
  "‚úÖ Texto, secuencias, im√°genes en vectores o embeddings",
  "‚úÖ Captura dependencia secuencial y global mediante mecanismos de atenci√≥n",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚ö†Ô∏è Ideal si las muestras o secuencias son independientes; para datos correlacionados usar variantes espec√≠ficas",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers en embeddings o entradas ruidosas pueden afectar atenci√≥n)",
  "‚ö†Ô∏è La colinealidad en embedding space puede ralentizar aprendizaje; usar regularizaci√≥n",
  "‚ö†Ô∏è Baja (modelo de ‚Äôcaja negra‚Äô, requieren m√©todos como attention visualization o interpretabilidad basada en pesos)",
  "‚ö†Ô∏è Lento sin hardware especializado (secuencialidad en atenci√≥n puede ser costosa)",
  "‚ö†Ô∏è Validaci√≥n temporal o k-fold anidada, seg√∫n tarea; en NLP se prefiere holdout sobre texto sin mezclar",
  "‚ùå No es apropiado con muy pocos datos de entrenamiento o sin estructura secuencial clara"
)

detalles <- c(
  "Arquitectura basada en capas de atenci√≥n para procesar secuencias completas en paralelo.",
  "Modelos como BERT, GPT, T5 pueden usarse para tareas de clasificaci√≥n, traducci√≥n, regresi√≥n de valores continuos en secuencias.",
  "Entradas requieren tokenizaci√≥n y conversi√≥n a embeddings; pueden combinarse varias modalidades.",
  "La auto‚Äêatenci√≥n global permite capturar relaciones a largo y corto plazo sin sesgo posicional estricto.",
  "No impone distribuci√≥n param√©trica de errores; se entrena con optimizadores basados en p√©rdidas cross‚Äêentropy o MSE.",
  "Se prefiere que las secuencias en el batch no sean dependientes; para series de tiempo, usar variantes como Time‚ÄêSeries Transformer.",
  "No se modela varianza del error; el entrenamiento se enfoca en minimizar funci√≥n de p√©rdida directa.",
  "Ruido en texto (typos) o en datos num√©ricos de entrada puede inducir atenci√≥n err√°tica; usar limpieza de datos y regularizaci√≥n.",
  "Los embeddings pueden contener informaci√≥n redundante de caracter√≠sticas correlacionadas; ajustar tama√±o de embedding y regularizaci√≥n.",
  "Interpretabilidad limitada; se usan t√©cnicas como visualizaci√≥n de mapas de atenci√≥n, LIME, SHAP para entender decisiones.",
  "El c√≥mputo de atenci√≥n es O(n¬≤) en longitud de secuencia; GPUs/TPUs o variantes eficientes (Linformer, Performer) alivian costo.",
  "Para tareas de texto, a veces se usa train/validation/test sin CV cl√°sica; para tareas generales, k-fold anidada ayuda a elegir hiperpar√°metros.",
  "No es adecuado con datasets muy peque√±os, sin preentrenamiento o sin estructuras secuenciales definidas."
)

tabla_transformers <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_transformers %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir transformers",
             subtitle = "Transformers")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```





<!--chapter:end:04-neural-networks.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
# üß© **5. Reducci√≥n de Dimensionalidad** {-}   

**Ejemplos:** PCA, t-SNE, UMAP   
**Cu√°ndo usarlo:**   

* Visualizaci√≥n de datos de alta dimensi√≥n.
* Preprocesamiento para eliminar ruido o multicolinealidad.

**Ventajas:** Mejora desempe√±o y velocidad de otros modelos.    
**Limitaciones:** Puede perder interpretabilidad; no siempre mejora modelos.

---

## Flexible Discriminant Analysis (FDA)  {-}   

**Flexible Discriminant Analysis (FDA)** es un m√©todo de **clasificaci√≥n** que generaliza el An√°lisis Discriminante Lineal (LDA) para manejar **relaciones no lineales** entre las variables predictoras y las clases. A diferencia de LDA, que asume l√≠mites de decisi√≥n lineales y distribuciones gaussianas con matrices de covarianza iguales, FDA es mucho m√°s adaptable.

FDA logra esta flexibilidad al combinar dos conceptos:
1.  **Optimal Scoring:** Transforma las variables de respuesta categ√≥ricas en valores num√©ricos (scores √≥ptimos) de manera que las clases sean m√°s f√°cilmente separables linealmente.
2.  **Modelos de Regresi√≥n No Param√©tricos:** En lugar de usar una regresi√≥n lineal simple (como en LDA), FDA utiliza m√©todos de regresi√≥n no param√©tricos m√°s flexibles, como las **Multivariate Adaptive Regression Splines (MARS)**. Esto permite que la relaci√≥n entre las variables transformadas y los scores √≥ptimos sea no lineal, lo que a su vez se traduce en fronteras de decisi√≥n no lineales en el espacio original de los datos.

Es decir, FDA toma los datos, los transforma de una manera inteligente para que sean m√°s f√°ciles de separar, y luego aplica una discriminaci√≥n lineal en ese espacio transformado, lo que resulta en una frontera de decisi√≥n compleja y flexible en el espacio original.

En el contexto del **aprendizaje global vs. local**, FDA se considera un modelo que **integra aspectos de ambos**.

* **Aspecto Global:** El objetivo final de FDA es encontrar una **funci√≥n discriminante global** que separe las clases en el espacio transformado. Los scores √≥ptimos y las funciones base del m√©todo de regresi√≥n (como MARS) se aprenden considerando la estructura general de los datos para lograr la mejor separaci√≥n a nivel global. El modelo resultante es una funci√≥n que se aplica de manera consistente a cualquier nueva observaci√≥n.

* **Aspecto Local (debido al uso de modelos no param√©tricos como MARS):** La flexibilidad de FDA proviene de su uso de m√©todos como MARS, que dividen el espacio de las caracter√≠sticas en **regiones locales** y ajustan relaciones simples dentro de cada una. Esto permite que el modelo se adapte a no linealidades y a cambios en la relaci√≥n entre las variables en diferentes partes del espacio de datos. As√≠, si los datos no se distribuyen linealmente, FDA puede construir fronteras de decisi√≥n que capturan esas complejidades al "localizar" las relaciones importantes.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas (puede usar transformaciones)",
  "‚úÖ No lineal (usa regresi√≥n flexible en el espacio transformado)",
  "‚ùå No aplica (no es regresi√≥n de residuos)",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è FDA suaviza este supuesto al modelar relaciones no lineales",
  "‚ö†Ô∏è Puede ser sensible a outliers, dependiendo del m√©todo de ajuste",
  "‚ö†Ô∏è Puede mitigar multicolinealidad si se usa penalizaci√≥n",
  "‚ö†Ô∏è Menos interpretable que LDA, pero permite mayor flexibilidad",
  "‚ö†Ô∏è Menor eficiencia que LDA por mayor complejidad computacional",
  "‚úÖ Validaci√≥n cruzada √∫til para seleccionar transformaciones o suavizados",
  "‚ùå En datos con pocos casos o ruido excesivo puede sobreajustarse"
)

detalles <- c(
  "Extensi√≥n de LDA que permite relaciones no lineales entre predictores y clases mediante t√©cnicas como splines o regresi√≥n flexible.",
  "Clasifica observaciones en clases categ√≥ricas bas√°ndose en predictores transformados.",
  "Admite variables num√©ricas, las cuales pueden ser transformadas de forma no lineal.",
  "Usa regresi√≥n no lineal flexible (como splines) para modelar relaciones complejas en el espacio de discriminaci√≥n.",
  "No genera residuos como regresi√≥n tradicional; es un modelo de clasificaci√≥n.",
  "No se enfoca en errores secuenciales o dependientes.",
  "Relaja la homocedasticidad al no asumir distribuci√≥n gaussiana estricta.",
  "Puede verse afectado por valores extremos, seg√∫n el m√©todo de suavizado.",
  "La transformaci√≥n flexible puede reducir colinealidad, pero no siempre la elimina.",
  "Los coeficientes y funciones discriminantes pueden ser dif√≠ciles de interpretar si se usan transformaciones complejas.",
  "Mayor costo computacional que LDA, pero m√°s potente en patrones no lineales.",
  "Se recomienda CV para evaluar desempe√±o y evitar overfitting en el proceso de ajuste flexible.",
  "Si los datos no requieren flexibilidad o el tama√±o muestral es bajo, FDA puede ser innecesariamente complejo."
)

tabla_fda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_fda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir FDA",
             subtitle = "Flexible Discriminant Analysis (FDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



  
  
## Linear Discriminant Analysis (LDA)  {-}    

El **An√°lisis Discriminante Lineal (LDA)** es un m√©todo de **clasificaci√≥n** y **reducci√≥n de dimensionalidad** utilizado para encontrar una combinaci√≥n lineal de caracter√≠sticas que mejor separe dos o m√°s clases de objetos o eventos. Su objetivo principal es modelar la diferencia entre las clases, lo que lo hace muy √∫til para tareas de clasificaci√≥n supervisada.

LDA funciona proyectando los puntos de datos a un espacio de menor dimensi√≥n (generalmente una o pocas dimensiones) de tal manera que las clases est√©n lo m√°s separadas posible. Para lograr esto, busca una direcci√≥n (un eje) que maximice la **separaci√≥n entre las medias de las clases** (varianza entre clases) mientras minimiza la **varianza dentro de cada clase** (varianza intraclase). En un problema de clasificaci√≥n binaria, esto significa encontrar la l√≠nea √≥ptima para proyectar los datos de modo que las dos clases se superpongan lo menos posible.

A diferencia de modelos como la Regresi√≥n Log√≠stica, que buscan modelar la probabilidad de pertenencia a una clase, LDA modela directamente la distribuci√≥n de los datos dentro de cada clase y luego utiliza el Teorema de Bayes para asignar una nueva observaci√≥n a la clase m√°s probable. LDA asume que las **varianzas (o matrices de covarianza) de las clases son iguales** y que los datos est√°n distribuidos normalmente.

**Aprendizaje Global vs. Local:**

El An√°lisis Discriminante Lineal (LDA) es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** LDA busca una **√∫nica transformaci√≥n lineal** o un conjunto de direcciones (ejes) que se aplican a **todos los datos** para lograr la m√°xima separaci√≥n entre las clases en un espacio de menor dimensi√≥n. La frontera de decisi√≥n que resulta de LDA es siempre **lineal** y se define globalmente a partir de las medias y las varianzas combinadas (asumidas como iguales) de todas las clases. El modelo es "fijo" y se aplica uniformemente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n espec√≠fica en el espacio de caracter√≠sticas. No se ajustan modelos diferentes para distintos vecindarios de datos, sino que se aprende una regla de separaci√≥n que es v√°lida para todo el dominio.

Por lo tanto, si los datos no se distribuyen linealmente o las fronteras de decisi√≥n entre las clases son inherentemente no lineales (por ejemplo, si una clase rodea a otra), LDA puede no ser el m√©todo m√°s adecuado. En esos escenarios, modelos de aprendizaje local o m√°s flexibles (como los √°rboles de decisi√≥n, SVM con kernels no lineales, o FDA que extiende LDA para no linealidades) suelen ofrecer un mejor rendimiento.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas (preferentemente)",
  "‚úÖ Asume relaciones lineales entre variables y clases",
  "‚úÖ Supone normalidad multivariante de los predictores dentro de cada clase",
  "‚úÖ Supone independencia entre observaciones",
  "‚úÖ Asume varianza-covarianza igual entre clases (homocedasticidad)",
  "‚ö†Ô∏è Sensible a valores at√≠picos",
  "‚ö†Ô∏è Puede verse afectado negativamente por alta colinealidad",
  "‚úÖ Alta, coeficientes discriminantes son interpretables",
  "‚úÖ Muy eficiente computacionalmente",
  "‚úÖ Se recomienda para evaluar estabilidad y evitar sobreajuste",
  "‚ùå Mal desempe√±o si no se cumplen supuestos de normalidad y homocedasticidad"
)

detalles <- c(
  "Modelo supervisado cl√°sico para clasificaci√≥n que encuentra combinaciones lineales de predictores que separan clases.",
  "Requiere una variable categ√≥rica como objetivo, con dos o m√°s clases.",
  "Mejor con predictores num√©ricos continuos; categ√≥ricos requieren codificaci√≥n previa.",
  "Calcula funciones discriminantes lineales que maximizan la separaci√≥n entre clases.",
  "Cada grupo debe seguir una distribuci√≥n normal multivariante para resultados √≥ptimos.",
  "Las observaciones deben ser independientes para validez de inferencia.",
  "Supone igual matriz de covarianzas entre grupos; si no se cumple, usar QDA.",
  "Outliers influyen en la media y la varianza estimada, distorsionando fronteras.",
  "Multicolinealidad puede hacer que los coeficientes discriminantes sean inestables.",
  "Las funciones discriminantes se interpretan como direcciones de m√°xima separaci√≥n.",
  "Requiere bajo costo computacional y se entrena r√°pidamente.",
  "Se puede usar validaci√≥n cruzada para elegir el n√∫mero de componentes o verificar precisi√≥n.",
  "Cuando los datos no cumplen normalidad ni homocedasticidad, el modelo pierde precisi√≥n."
)

tabla_lda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LDA",
             subtitle = "Linear Discriminant Analysis (LDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

   
   
## Mixture Discriminant Analysis (MDA)  {-}   

El **An√°lisis Discriminante de Mezclas (MDA)** es una extensi√≥n del An√°lisis Discriminante Lineal (LDA) y del An√°lisis Discriminante Cuadr√°tico (QDA) que aborda la limitaci√≥n de que estas t√©cnicas asumen que cada clase proviene de una √∫nica distribuci√≥n normal (o gaussiana). MDA relaja esta suposici√≥n al permitir que **cada clase sea modelada como una mezcla de m√∫ltiples distribuciones gaussianas**. Esto le otorga una capacidad significativamente mayor para manejar clases con formas complejas o multimodales, que no pueden ser descritas adecuadamente por una sola distribuci√≥n normal.

MDA funciona de la siguiente manera:
1.  **Modelado por Componentes de Mezcla:** Para cada clase, MDA estima los par√°metros (media y matriz de covarianza) de varias distribuciones gaussianas ("componentes de mezcla") en lugar de solo una. Es similar al proceso de agrupamiento de mezclas gaussianas (Gaussian Mixture Models - GMM) aplicado dentro de cada clase.
2.  **Asignaci√≥n a la Clase:** Una vez que se han modelado las distribuciones de mezcla para cada clase, para una nueva observaci√≥n, MDA calcula la probabilidad de que esa observaci√≥n pertenezca a cada componente de mezcla en cada clase. Luego, asigna la observaci√≥n a la clase que maximiza la probabilidad posterior, es decir, la clase que es m√°s probable que haya generado esa observaci√≥n.
3.  **Fronteras de Decisi√≥n Flexibles:** Al modelar cada clase como una mezcla de gaussianas, MDA puede generar fronteras de decisi√≥n que son mucho m√°s flexibles y no lineales que las de LDA (que son lineales) o QDA (que son cuadr√°ticas). Esto le permite adaptarse a clases con estructuras complejas, que pueden tener "agrupaciones" internas o formas irregulares.

Los par√°metros del modelo (las medias, covarianzas y pesos de los componentes de mezcla para cada clase) se suelen estimar utilizando un algoritmo iterativo como la **Maximizaci√≥n de Expectativas (Expectation-Maximization - EM)**.

**Aprendizaje Global vs. Local:**

El An√°lisis Discriminante de Mezclas (MDA) se encuentra en un punto intermedio, inclin√°ndose hacia un modelo que **combina aspectos de aprendizaje global y local**, con una mayor flexibilidad para capturar la estructura local de los datos en comparaci√≥n con LDA o QDA.

* **Aspecto Global:** Al igual que LDA, el objetivo final de MDA es crear un **clasificador global** que pueda asignar cualquier nueva observaci√≥n a una de las clases. Las distribuciones de mezcla para cada clase se aprenden a partir de todo el conjunto de datos de entrenamiento para esas clases, y el clasificador resultante se aplica de manera consistente en todo el espacio de caracter√≠sticas. La regla de decisi√≥n final es una funci√≥n que se deriva de las distribuciones aprendidas para todas las clases.

* **Aspecto Local:** La "flexibilidad" de MDA y su capacidad para manejar no linealidades proviene de su suposici√≥n de que cada clase puede estar compuesta por **m√∫ltiples componentes gaussianos**. Esto significa que, dentro de una misma clase, puede haber sub-agrupaciones o densidades locales que son modeladas individualmente. Al permitir estas m√∫ltiples distribuciones gaussianas dentro de cada clase, MDA puede adaptarse mejor a las caracter√≠sticas y densidades de los datos en diferentes **vecindarios o subregiones** del espacio de caracter√≠sticas. Si los datos no se distribuyen linealmente y tienen formas complejas (como clusters separados dentro de una clase), MDA puede "localizar" y modelar estas estructuras, llevando a fronteras de decisi√≥n mucho m√°s complejas y no lineales que se ajustan mejor a la forma real de las clases.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas",
  "‚úÖ No lineal (usa mezclas de gaussianas para modelar clases)",
  "‚ùå No aplica como en regresi√≥n",
  "‚ùå No se eval√∫a como en regresi√≥n",
  "‚ö†Ô∏è Supone varianza homog√©nea dentro de componentes, pero puede variar entre clases",
  "‚ö†Ô∏è Puede ser sensible a outliers (afectan las medias y covarianzas)",
  "‚ö†Ô∏è Puede verse afectado, aunque usa reducci√≥n dimensional",
  "‚ö†Ô∏è Moderadamente interpretable (depende de componentes gaussianos)",
  "‚ö†Ô∏è M√°s lento que LDA/QDA, pero m√°s flexible",
  "‚úÖ Recomendable para elegir n√∫mero de componentes y evitar sobreajuste",
  "‚ùå Mal desempe√±o si la distribuci√≥n dentro de clases no es bien modelada por gaussianas"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n que combina regresi√≥n discriminante con mezclas gaussianas dentro de cada clase.",
  "Se usa para clasificar observaciones en grupos definidos por una variable categ√≥rica.",
  "Requiere predictores num√©ricos para ajustar distribuciones normales multivariadas.",
  "Modela cada clase como una combinaci√≥n de distribuciones gaussianas, permitiendo formas no lineales.",
  "No hay residuos como en regresi√≥n, ya que se trata de una tarea de clasificaci√≥n.",
  "No eval√∫a independencia cl√°sica de errores; se enfoca en estimar la densidad condicional.",
  "Permite varianza distinta entre componentes, pero se puede ajustar homogeneidad seg√∫n implementaci√≥n.",
  "Outliers pueden afectar las medias y varianzas estimadas de las mezclas gaussianas.",
  "La multicolinealidad puede dificultar la estimaci√≥n de matrices de covarianza.",
  "Interpretar los componentes internos (medias y pesos) puede ser complejo, pero ofrece buena visualizaci√≥n.",
  "Es m√°s lento que LDA o QDA por su naturaleza iterativa y uso de EM (Expectation-Maximization).",
  "Se puede usar validaci√≥n cruzada para seleccionar el n√∫mero √≥ptimo de mezclas por clase.",
  "Si las clases no se ajustan bien a combinaciones de gaussianas, el modelo pierde precisi√≥n."
)

tabla_mda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MDA",
             subtitle = "Mixture Discriminant Analysis (MDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```





## Multidimensional Scaling (MDS)  {-}    

El **Escalamiento Multidimensional (MDS)** es una t√©cnica de **reducci√≥n de dimensionalidad** utilizada para visualizar y explorar las **similitudes o disimilitudes** entre un conjunto de objetos. Su objetivo principal es tomar datos de alta dimensi√≥n, donde las relaciones entre los puntos pueden ser dif√≠ciles de entender, y representarlos en un **espacio de menor dimensi√≥n** (t√≠picamente 2D o 3D) de tal manera que las **distancias entre los puntos en el nuevo espacio reflejen lo m√°s fielmente posible las distancias (o disimilitudes) originales** entre los objetos.

Imagina que tienes una tabla de distancias de viaje entre varias ciudades. MDS intentar√≠a dibujar un mapa de esas ciudades donde las distancias en el mapa se correspondieran lo m√°s posible con las distancias de la tabla.

El proceso general de MDS implica:

1.  **Matriz de Disimilitud:** Se necesita una matriz que contenga las disimilitudes (distancias) entre cada par de objetos. Estas disimilitudes pueden ser distancias euclidianas, correlaciones, o cualquier otra medida de qu√© tan diferentes (o similares) son dos objetos.
2.  **Optimizaci√≥n:** El algoritmo busca una configuraci√≥n de puntos en el espacio de menor dimensi√≥n que minimice una **funci√≥n de "estr√©s" o "ajuste"**. Esta funci√≥n mide qu√© tan bien las distancias en el espacio reducido se corresponden con las disimilitudes originales. Una funci√≥n de estr√©s baja indica un buen ajuste.
3.  **Visualizaci√≥n:** Los puntos resultantes en el espacio de menor dimensi√≥n pueden ser graficados para revelar patrones, clusters o la estructura subyacente de los datos que no eran evidentes en las dimensiones originales.

Existen varias variantes de MDS, como el **MDS Cl√°sico (o M√©trica)**, que asume que las disimilitudes son distancias euclidianas y busca una soluci√≥n anal√≠tica, y el **MDS No-M√©trico**, que solo busca preservar el **orden** de las disimilitudes (es decir, si A es m√°s diferente de B que de C, esa relaci√≥n se mantendr√° en el espacio reducido, sin que las distancias exactas tengan que ser iguales).


**Aprendizaje Global vs. Local:**

El Escalamiento Multidimensional (MDS) se considera predominantemente una t√©cnica de **aprendizaje global**.

* **Aspecto Global:** MDS busca una **configuraci√≥n √∫nica de puntos** en el espacio de baja dimensi√≥n que optimice el ajuste de **todas las disimilitudes** en el conjunto de datos de manera simult√°nea. La funci√≥n de estr√©s que se minimiza considera las distancias entre *todos* los pares de puntos, buscando una soluci√≥n que sea globalmente la mejor representaci√≥n de esas relaciones. El objetivo es preservar la estructura general de las distancias en el conjunto de datos completo, no solo las relaciones en vecindarios espec√≠ficos. La soluci√≥n que se encuentra es una "vista a√©rea" o un "mapa" de las relaciones de todo el conjunto de datos.

Aunque las disimilitudes originales son "locales" en el sentido de que son medidas entre pares de puntos, la forma en que MDS utiliza todas estas medidas para construir un mapa coherente y de baja dimensi√≥n es un proceso global de optimizaci√≥n. No se ajustan modelos separados para diferentes subconjuntos de datos; en su lugar, se busca una representaci√≥n unificada que capture la estructura general de similaridad/disimilitud de todos los datos. Por lo tanto, si los datos tienen una estructura global bien definida basada en distancias, MDS es una herramienta efectiva para revelar esa estructura. 


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (requiere matriz de distancias)",
  "‚úÖ No lineal en MDS no cl√°sico; lineal en MDS cl√°sico",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è S√≠, valores at√≠picos afectan distancias",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Interpretaci√≥n visual en 2D o 3D, no en ejes significativos",
  "‚ùå Lento si se usan distancias complejas o muchos puntos",
  "‚ö†Ô∏è Validaci√≥n mediante 'stress' y visualizaci√≥n",
  "‚ùå Mal desempe√±o con datos sin estructura o ruido elevado"
)

detalles <- c(
  "M√©todo no supervisado que proyecta datos de alta dimensi√≥n en espacios de 2D o 3D preservando distancias entre puntos.",
  "No busca predecir una variable, solo representar relaciones de cercan√≠a entre observaciones.",
  "Se basa en distancias euclidianas u otras m√©tricas aplicadas a datos num√©ricos.",
  "MDS cl√°sico es lineal; el no cl√°sico (por ejemplo metric o non-metric MDS) puede modelar relaciones no lineales.",
  "No se modelan residuos, por lo que no aplica la normalidad.",
  "No hay errores de predicci√≥n, por tanto no aplica este supuesto.",
  "No hay varianzas residuales, por lo que este supuesto tampoco aplica.",
  "Valores extremos modifican distancias y distorsionan la representaci√≥n espacial.",
  "Al no haber regresores, la multicolinealidad no es un problema.",
  "El mapa generado se interpreta por proximidad relativa, no por pesos o coeficientes.",
  "Puede ser costoso computacionalmente si hay muchos puntos o si se optimiza la funci√≥n de estr√©s.",
  "Se eval√∫a qu√© tan bien se preservan las distancias originales con la m√©trica de estr√©s o visualmente.",
  "No funciona bien si los datos no tienen estructura clara, est√°n muy dispersos o contienen ruido irrelevante."
)

tabla_mds <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mds %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MDS",
             subtitle = "Multidimensional Scaling (MDS)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Quadratic Discriminant Analysis (QDA) {-}  


El **An√°lisis Discriminante Cuadr√°tico (QDA)** es un m√©todo de **clasificaci√≥n** que, al igual que el An√°lisis Discriminante Lineal (LDA), modela la distribuci√≥n de cada clase para clasificar nuevas observaciones. Sin embargo, QDA es una extensi√≥n de LDA que relaja una de sus suposiciones clave: mientras que LDA asume que todas las clases comparten la misma matriz de covarianza (es decir, las distribuciones tienen la misma "forma" o "orientaci√≥n"), **QDA permite que cada clase tenga su propia matriz de covarianza distinta**.

Esta diferencia es fundamental:
* **LDA:** Asume que la variaci√≥n de los datos es la misma en todas las clases, lo que resulta en **fronteras de decisi√≥n lineales** entre las clases.
* **QDA:** Permite que la variaci√≥n de los datos sea diferente para cada clase, lo que resulta en **fronteras de decisi√≥n cuadr√°ticas** entre las clases. Esto significa que las fronteras de decisi√≥n pueden ser curvas (elipsoides, par√°bolas, hip√©rbolas), lo que permite a QDA modelar relaciones m√°s complejas y no lineales entre las variables y las clases.

El funcionamiento de QDA implica:
1.  **Modelado de Distribuciones:** Para cada clase, QDA estima la media y la matriz de covarianza espec√≠ficas de esa clase, asumiendo una distribuci√≥n normal multivariada.
2.  **Clasificaci√≥n:** Para una nueva observaci√≥n, QDA calcula la probabilidad de que esa observaci√≥n provenga de cada clase, utilizando las distribuciones normales modeladas para cada clase. Luego, asigna la observaci√≥n a la clase con la probabilidad posterior m√°s alta (aplicando el Teorema de Bayes).

**Aprendizaje Global vs. Local:**

El An√°lisis Discriminante Cuadr√°tico (QDA) es, al igual que LDA, un modelo de **aprendizaje global**.

* **Aspecto Global:** QDA construye un **clasificador global** basado en las distribuciones de probabilidad aprendidas para cada clase. Las medias y las matrices de covarianza se estiman a partir de todo el conjunto de datos de entrenamiento para cada clase, y estos par√°metros definen una funci√≥n discriminante que se aplica de manera uniforme a cualquier nueva observaci√≥n en el espacio de caracter√≠sticas. La frontera de decisi√≥n, aunque cuadr√°tica y no lineal, es una √∫nica funci√≥n matem√°tica definida a nivel global por los par√°metros del modelo. No se ajustan modelos separados para diferentes vecindarios de datos.

* **Mayor Flexibilidad Globalmente:** Aunque sigue siendo un modelo global, la capacidad de QDA para tener matrices de covarianza separadas para cada clase le otorga una **mayor flexibilidad para adaptarse a formas de clase m√°s diversas** en comparaci√≥n con LDA. Esto significa que QDA puede modelar situaciones donde las clases tienen diferentes orientaciones o dispersiones en el espacio de caracter√≠sticas, lo que resulta en fronteras de decisi√≥n que pueden capturar ciertas no linealidades de manera global. Sin embargo, sigue asumiendo distribuciones gaussianas para cada clase y una forma cuadr√°tica para las fronteras, lo que puede ser una limitaci√≥n si la verdadera complejidad de los datos es a√∫n mayor o no se ajusta a estas suposiciones.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas",
  "‚úÖ Modela separaci√≥n cuadr√°tica entre clases",
  "‚ùå No aplica (clasificaci√≥n, no regresi√≥n)",
  "‚ùå No aplica (se asume independencia dentro de clases)",
  "‚ùå No se asume homoscedasticidad (cada clase tiene su propia matriz de covarianza)",
  "‚ö†Ô∏è Puede ser muy sensible a outliers (afectan las matrices de covarianza)",
  "‚ö†Ô∏è Puede verse afectado, especialmente si hay pocos datos",
  "‚úÖ Relativamente interpretable (fronteras no lineales entre clases)",
  "‚ö†Ô∏è M√°s costoso que LDA; ineficiente con pocos datos o muchas variables",
  "‚úÖ Recomendado para evitar overfitting, especialmente con pocos datos",
  "‚ùå Si hay pocos datos por clase, estimar matrices de covarianza es inestable"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n que permite que cada clase tenga su propia matriz de covarianza.",
  "Se utiliza para predecir a qu√© clase pertenece una observaci√≥n con base en sus caracter√≠sticas.",
  "Requiere predictores num√©ricos continuos, ya que calcula medias y covarianzas.",
  "A diferencia de LDA, permite fronteras no lineales al no asumir varianzas iguales entre clases.",
  "No tiene residuos como en regresi√≥n, por lo que el supuesto de normalidad de errores no aplica.",
  "No aplica el supuesto de independencia de errores; se enfoca en la distribuci√≥n conjunta por clase.",
  "Cada clase tiene su propia varianza y covarianza, lo que lo hace m√°s flexible que LDA.",
  "Valores extremos pueden distorsionar la estimaci√≥n de medias y covarianzas de cada clase.",
  "Multicolinealidad puede dificultar la inversi√≥n de la matriz de covarianza en clases peque√±as.",
  "Los coeficientes y decisiones son interpretables en t√©rminos de separaciones estad√≠sticas entre clases.",
  "M√°s lento y costoso computacionalmente que LDA, especialmente con muchas variables.",
  "La validaci√≥n cruzada ayuda a prevenir sobreajuste y a seleccionar caracter√≠sticas relevantes.",
  "Con clases poco representadas o muchas variables, las matrices de covarianza pueden volverse inestables."
)

tabla_qda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles) 

tabla_qda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir QDA",
             subtitle = "Quadratic Discriminant Analysis (QDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

  
  
## Partial Least Squares Regression (PLSR)  {-}    

**Partial Least Squares Regression (PLSR)** es una t√©cnica de **regresi√≥n multivariada** que combina caracter√≠sticas de la **regresi√≥n por m√≠nimos cuadrados ordinarios (OLS)** y el **an√°lisis de componentes principales (PCA)**. Se utiliza para modelar la relaci√≥n entre un conjunto de variables predictoras (X) y uno o m√°s conjuntos de variables de respuesta (Y), siendo particularmente √∫til en situaciones donde hay un gran n√∫mero de variables predictoras, **multicolinealidad** (altas correlaciones entre las variables predictoras), o cuando el n√∫mero de predictoras excede el n√∫mero de observaciones.

La idea fundamental de PLSR es encontrar un conjunto de **componentes latentes** (tambi√©n conocidos como "factores" o "variables latentes") tanto en el espacio de las variables X como en el de las variables Y. Estos componentes se construyen de tal manera que **maximizan la covarianza** entre las variables predictoras y las variables de respuesta. A diferencia de PCA, que solo busca componentes que expliquen la m√°xima varianza en X, PLSR busca componentes que sean relevantes para explicar la varianza en X *y* que tambi√©n est√©n altamente correlacionados con Y. Una vez que se extraen estos componentes, se realiza una regresi√≥n de m√≠nimos cuadrados ordinarios de Y sobre estos componentes latentes.

El proceso general de PLSR implica:

1.  **Extracci√≥n de Componentes Latentes:** PLSR construye iterativamente un conjunto de componentes latentes. En cada paso:
    * Identifica una combinaci√≥n lineal de las variables X (un componente de X) y una combinaci√≥n lineal de las variables Y (un componente de Y) que tienen la mayor covarianza entre s√≠.
    * Estos componentes representan las direcciones en el espacio de datos que explican la mayor cantidad de la relaci√≥n entre X y Y.
    * Una vez que se extrae un componente, la varianza explicada por ese componente se "deflacta" (se elimina) de las matrices X e Y, y el proceso se repite con los residuos para encontrar el siguiente componente ortogonal.
2.  **Regresi√≥n:** Una vez que se ha determinado el n√∫mero √≥ptimo de componentes latentes (a menudo a trav√©s de validaci√≥n cruzada), se realiza una regresi√≥n lineal est√°ndar de las variables Y sobre estos componentes latentes de X.

**Ventajas clave de PLSR:**

* **Manejo de Multicolinealidad:** Es muy efectivo en la reducci√≥n de dimensionalidad y el manejo de predictoras altamente correlacionadas, donde la regresi√≥n OLS fallar√≠a o producir√≠a estimaciones inestables.
* **Manejo de Datos de Alta Dimensionalidad:** Funciona bien cuando el n√∫mero de variables predictoras es mayor que el n√∫mero de observaciones.
* **Enfoque Predictivo:** Se centra en desarrollar modelos con una fuerte capacidad predictiva.

**Aprendizaje Global vs. Local:**
  
La Regresi√≥n por M√≠nimos Cuadrados Parciales (PLSR) se considera un modelo de **aprendizaje global**.

* **Aspecto Global:** PLSR construye un **modelo lineal global** que relaciona las variables predictoras con la variable de respuesta a trav√©s de sus componentes latentes. Los componentes PLS se derivan de la estructura de covarianza de **todas las variables** (tanto predictoras como de respuesta) en el conjunto de datos completo, y el modelo de regresi√≥n final se ajusta sobre estos componentes, generando una ecuaci√≥n que se aplica de manera consistente a cualquier nueva observaci√≥n. No se ajustan modelos separados para diferentes vecindarios de datos; en cambio, se busca una transformaci√≥n global de los datos que facilite la predicci√≥n.

Si bien PLSR no es un m√©todo de **regresi√≥n ponderada localmente** como LOESS (que ajusta modelos simples a subconjuntos locales de datos), comparte con ellos el objetivo de modelar relaciones complejas. Sin embargo, lo hace de una manera diferente. En lugar de dividir el espacio de caracter√≠sticas y aplicar modelos locales, PLSR transforma el espacio de caracter√≠sticas de forma global para encontrar una representaci√≥n de menor dimensionalidad que sea √≥ptima para la predicci√≥n. Cuando los datos no se distribuyen linealmente, PLSR puede no ser la herramienta m√°s adecuada en su forma lineal b√°sica, ya que sigue siendo una t√©cnica lineal. Sin embargo, al encontrar las direcciones m√°s relevantes en el espacio de los datos, puede capturar aspectos importantes de la estructura de los datos que son √∫tiles incluso si la relaci√≥n subyacente es no lineal. Para manejar la no linealidad expl√≠citamente, existen extensiones como **Nonlinear Partial Least Squares (NPLS)** o **Kernel PLS (KPLS)**, que introducen funciones kernel para mapear los datos a un espacio de caracter√≠sticas de mayor dimensi√≥n donde la relaci√≥n podr√≠a ser linealmente modelable por PLS.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado (regresi√≥n y clasificaci√≥n con adaptaci√≥n)",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n si se transforma)",
  "‚úÖ Num√©ricas (requiere escalado), categ√≥ricas como dummies",
  "‚úÖ Captura relaciones lineales y reduce dimensiones simult√°neamente",
  "‚ùå No requiere estrictamente, pero mejora con residuos normales",
  "‚úÖ Deseable, aunque no cr√≠tico",
  "‚úÖ Deseable para homogeneizar varianza tras escalado",
  "‚ö†Ô∏è Moderado (outliers pueden influir en componentes latentes)",
  "‚úÖ Dise√±ado para alta colinealidad entre predictores",
  "‚ö†Ô∏è Media (componentes latentes son interpretables, pero relaciones pueden ser complejas)",
  "‚ö†Ô∏è Moderada (depende de n√∫mero de componentes y tama√±o del dataset)",
  "‚úÖ Usar k-fold para elegir n√∫mero de componentes √≥ptimos",
  "‚ùå No funciona bien si relaciones son muy no lineales o datos muy ruidosos sin preprocesar"
)

detalles <- c(
  "Modelo que proyecta predictores y respuesta a espacios latentes para maximizar covarianza.",
  "PLSR encuentra componentes que explican varianza en X y covarianza con Y.",
  "Todas las variables num√©ricas deben escalarse; convertir categ√≥ricas en indicadores.",
  "Combina reducci√≥n de dimensi√≥n (PCA-like) con regresi√≥n en componentes latentes.",
  "No impone supuestos estrictos, pero residuos normales facilitan inferencia estad√≠stica.",
  "Mejor si muestras son independientes; RLSR en datos correlacionados requiere cuidado.",
  "Escalar y homogeneizar predictores e incluso respuesta mejora la estabilidad.",
  "Outliers extremos pueden distorsionar c√°lculo de componentes; usar robust PLSR para mitigarlo.",
  "PLSR maneja colinealidad al construir pocas componentes que representan grupos de variables correlacionadas.",
  "Componentes latentes tienen pesos interpretables, pero interpretar combinaciones puede ser complejo.",
  "El m√©todo usa descomposici√≥n de matrices; eficiente con BLAS/LAPACK optimizado.",
  "Validaci√≥n cruzada ayuda a determinar el n√∫mero √≥ptimo de componentes latentes a usar.",
  "No es adecuado para relaciones puramente no lineales; en ese caso usar Kernel PLSR o m√©todos no lineales."
)

tabla_plsr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_plsr %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PLSR",
             subtitle = "Partial Least Squares Regression (PLSR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
``` 


## Partial Least Squares Discriminant Analysis (PLSDA)  {-}   

El **An√°lisis Discriminante de M√≠nimos Cuadrados Parciales (PLSDA)** es una extensi√≥n del algoritmo de **Regresi√≥n por M√≠nimos Cuadrados Parciales (PLSR)**, adaptada para problemas de **clasificaci√≥n**. Al igual que PLSR, PLSDA es particularmente √∫til cuando se tienen muchas **variables predictoras (X)** y estas est√°n **altamente correlacionadas (multicolinealidad)**, situaciones comunes en campos como la metabol√≥mica, la prote√≥mica o la espectroscopia.

En esencia, PLSDA transforma un problema de clasificaci√≥n en un problema de regresi√≥n. Esto se logra de la siguiente manera:

1.  **Codificaci√≥n de la Variable de Clase:** La variable de respuesta categ√≥rica (la clase a la que pertenece una observaci√≥n) se transforma en una o m√°s variables num√©ricas. Por ejemplo, en un problema de clasificaci√≥n binaria, una clase puede codificarse como '0' y la otra como '1'. Para m√∫ltiples clases, se puede usar una codificaci√≥n "one-hot encoding" (ej., [1,0,0] para Clase A, [0,1,0] para Clase B, etc.).
2.  **Extracci√≥n de Componentes Latentes:** Similar a PLSR, PLSDA construye componentes latentes (factores PLS) que son combinaciones lineales de las variables predictoras. Estos componentes se eligen para maximizar la covarianza entre las variables predictoras y las variables de respuesta codificadas. Esto asegura que los componentes capturen la varianza en X que es relevante para la separaci√≥n de clases en Y.
3.  **Clasificaci√≥n:** Una vez que se han obtenido los componentes PLS y se ha realizado la regresi√≥n sobre ellos para predecir los valores codificados de la clase, se aplica una regla de decisi√≥n (por ejemplo, un umbral o un clasificador lineal simple) a las predicciones para asignar cada observaci√≥n a una clase. Si se usa codificaci√≥n one-hot, la observaci√≥n se asigna a la clase con el valor predicho m√°s alto.

PLSDA es ventajoso porque puede manejar conjuntos de datos con muchas m√°s variables que observaciones (problemas $p \gg n$), y es robusto a la multicolinealidad.


**Aprendizaje Global vs. Local:**

El An√°lisis Discriminante de M√≠nimos Cuadrados Parciales (PLSDA) es un modelo de **aprendizaje global**.

* **Aspecto Global:** PLSDA busca una **transformaci√≥n lineal global** de las variables predictoras a componentes latentes, y luego una **relaci√≥n lineal global** entre esos componentes y la variable de respuesta codificada (clase). Los componentes PLS se derivan de la estructura de covarianza de todo el conjunto de datos, y el modelo de regresi√≥n final (que se usa para la clasificaci√≥n) se aplica de manera consistente a cualquier nueva observaci√≥n. La frontera de decisi√≥n impl√≠cita en PLSDA es t√≠picamente lineal en el espacio de los componentes PLS (y por lo tanto lineal o una combinaci√≥n lineal de las variables originales), lo que resulta en un clasificador que opera globalmente en el espacio de caracter√≠sticas.

* **Enfoque en la Relevancia Global:** Aunque reduce la dimensionalidad y selecciona componentes que son relevantes para la respuesta, la soluci√≥n final es un mapeo y una regla de decisi√≥n que son v√°lidos para todo el dominio de los datos. No ajusta modelos locales para diferentes regiones del espacio de caracter√≠sticas. Por lo tanto, PLSDA es una t√©cnica eficiente para encontrar patrones globales de separaci√≥n de clases en presencia de alta dimensionalidad y multicolinealidad, pero si las relaciones entre las variables y las clases son inherentemente no lineales o tienen estructuras muy complejas que no pueden ser capturadas por una transformaci√≥n lineal, su capacidad puede ser limitada.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (binaria o multicategor√≠a)",
  "‚úÖ Num√©ricas (se proyectan a componentes)",
  "‚úÖ Captura relaciones lineales y no lineales a trav√©s de proyecciones",
  "‚ùå No aplica directamente (modelo de clasificaci√≥n)",
  "‚ùå No aplica como en regresi√≥n cl√°sica",
  "‚ùå No se eval√∫a como en modelos de regresi√≥n",
  "‚ö†Ô∏è Algo sensible a outliers (pueden influir en componentes)",
  "‚úÖ Muy √∫til si hay multicolinealidad",
  "‚ö†Ô∏è Menos interpretable que modelos cl√°sicos; depende de componentes",
  "‚úÖ Eficiente, especialmente con datos de alta dimensi√≥n",
  "‚úÖ Se recomienda usar validaci√≥n cruzada para elegir el n√∫mero de componentes",
  "‚ùå Si las proyecciones no separan bien las clases o hay mucho ruido"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n basado en PLS (Partial Least Squares) que proyecta los datos para maximizar la separaci√≥n entre clases.",
  "Se requiere que la variable dependiente sea categ√≥rica. PLS-DA funciona bien con 2 o m√°s clases.",
  "Las variables predictoras deben ser num√©ricas para que el modelo pueda proyectarlas en componentes latentes.",
  "El modelo encuentra combinaciones de predictores que mejor separan las clases en el espacio proyectado.",
  "No se eval√∫a normalidad de residuos como en modelos de regresi√≥n; la salida es de clasificaci√≥n.",
  "Tampoco aplica la independencia cl√°sica de errores ya que se clasifican observaciones.",
  "El supuesto de homoscedasticidad no es relevante aqu√≠.",
  "Outliers pueden afectar la construcci√≥n de componentes, distorsionando la separaci√≥n de clases.",
  "PLS-DA es √∫til cuando los predictores est√°n altamente correlacionados, ya que crea componentes ortogonales.",
  "Los componentes no son directamente interpretables como las variables originales, aunque se pueden analizar los pesos de carga.",
  "Es un algoritmo relativamente eficiente, especialmente para conjuntos con muchas variables.",
  "La validaci√≥n cruzada es cr√≠tica para seleccionar el n√∫mero √≥ptimo de componentes y evitar overfitting.",
  "No funciona bien si las clases no est√°n bien separadas en el espacio proyectado o si hay demasiado ruido en los datos."
)

tabla_plsda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_plsda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PLSDA",
             subtitle = "Partial Least Squares Discriminant Analysis (PLSDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Principal Component Analysis (PCA)  {-}   

El **An√°lisis de Componentes Principales (PCA)** es una t√©cnica fundamental de **reducci√≥n de dimensionalidad** no supervisada. Su objetivo principal es simplificar conjuntos de datos complejos con muchas variables, transform√°ndolos en un conjunto m√°s peque√±o de nuevas variables, llamadas **componentes principales**, sin perder demasiada informaci√≥n. Estos componentes principales son combinaciones lineales de las variables originales y son **ortogonales (no correlacionados)** entre s√≠.

PCA funciona identificando las direcciones en el espacio de datos donde la **varianza es m√°xima**. La primera componente principal (PC1) captura la mayor cantidad de varianza posible en los datos. La segunda componente principal (PC2) captura la mayor varianza restante, sujeta a ser ortogonal a la primera, y as√≠ sucesivamente. De esta manera, PCA organiza la varianza en los datos en un conjunto jer√°rquico de componentes.

Los usos comunes de PCA incluyen:
* **Reducci√≥n de dimensionalidad:** Disminuir el n√∫mero de variables en un dataset, lo que puede acelerar los algoritmos de Machine Learning y reducir el riesgo de sobreajuste.
* **Visualizaci√≥n de datos:** Proyectar datos de alta dimensi√≥n en 2D o 3D para facilitar su visualizaci√≥n y la identificaci√≥n de patrones, clusters o outliers.
* **Denoising:** Eliminar el ruido de los datos al retener solo los componentes principales que capturan la se√±al real.


**Aprendizaje Global vs. Local:**

El An√°lisis de Componentes Principales (PCA) es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** PCA busca una **transformaci√≥n lineal global** del espacio de caracter√≠sticas. Los componentes principales se derivan de la matriz de covarianza (o correlaci√≥n) de **todo el conjunto de datos**. Esto significa que las direcciones de m√°xima varianza se determinan considerando la estructura de dispersi√≥n general de todos los puntos de datos. El conjunto de componentes principales que se obtiene es un sistema de coordenadas global al que se proyecta cualquier punto de datos. No se ajustan diferentes transformaciones para distintas regiones o vecindarios de datos; en su lugar, se aprende una √∫nica proyecci√≥n que se aplica uniformemente a todo el dominio.

Por lo tanto, si la estructura de los datos es consistentemente lineal o tiene relaciones de varianza que se extienden linealmente a lo largo del espacio, PCA funcionar√° muy bien. Sin embargo, si los datos tienen estructuras no lineales complejas (por ejemplo, datos que forman una espiral o una esfera), PCA puede tener limitaciones para capturar estas relaciones, ya que solo busca direcciones lineales de m√°xima varianza.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è No supervisado (reducci√≥n de dimensiones)",
  "‚ùå No aplica (no hay target a predecir)",
  "‚úÖ Num√©ricas (requiere escalado), categ√≥ricas procesadas como dummies",
  "‚úÖ Captura correlaciones lineales entre predictores",
  "‚ùå No requiere supuestos de distribuci√≥n en residuos",
  "‚ö†Ô∏è Ideal si las observaciones son independientes, aunque no cr√≠tico",
  "‚úÖ Deseable (datos homogenizados tras escalado)",
  "‚ö†Ô∏è Moderado (outliers pueden distorsionar componentes principales)",
  "‚úÖ Sensible a colinealidad (reduce variables correlacionadas a componentes)",
  "‚ö†Ô∏è Media (componentes lineales son interpretables, pero combinaciones pueden no serlo)",
  "‚úÖ R√°pido en datasets medianos; escalable con √°lgebra lineal optimizada",
  "‚ö†Ô∏è No se aplica CV cl√°sico; se puede usar reconstrucci√≥n de error o validaci√≥n por bloques",
  "‚ùå No funciona bien si las relaciones son no lineales o datos muy ruidosos sin preprocesar"
)

detalles <- c(
  "M√©todo no supervisado para reducir la dimensi√≥n del espacio de predictores.",
  "No predice variables, se centra en variabilidad interna de los datos.",
  "Todas las variables num√©ricas deben escalarse; las categ√≥ricas convertir a variables indicadoras.",
  "Busca direcciones (componentes) que maximizan varianza lineal entre predictores.",
  "No impone supuestos sobre errores; se basa en descomposici√≥n de la matriz de covarianza.",
  "Mejor si las muestras no est√°n correlacionadas en el tiempo o espacialmente.",
  "Escalar y homogeneizar mejora el c√°lculo de componentes principales.",
  "Outliers extremos pueden sesgar la direcci√≥n de los componentes principales.",
  "Reduce colinealidad al combinar variables correlacionadas en componentes ortogonales.",
  "Componentes iniciales pueden interpretarse mediante pesos, pero componentes posteriores son combinaciones lineales complejas.",
  "Computaci√≥n depende de descomposici√≥n de matrices (SVD), es eficiente con optimizaci√≥n BLAS.",
  "Se puede evaluar n√∫mero √≥ptimo de componentes con validaci√≥n de reconstrucci√≥n o bootstrap de SVD.",
  "No apto para relaciones no lineales complejas; en tal caso usar Kernel PCA o m√©todos no lineales."
)

tabla_pca <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pca %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PCA",
             subtitle = "Principal Component Analysis (PCA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Principal Component Regression (PCR)  {-}    

La **Regresi√≥n de Componentes Principales (PCR)** es un m√©todo de **regresi√≥n** que combina el **An√°lisis de Componentes Principales (PCA)** con la **Regresi√≥n por M√≠nimos Cuadrados Ordinarios (OLS)**. Su principal utilidad radica en situaciones donde se tienen muchas **variables predictoras (X)** y existe una **alta multicolinealidad** (fuerte correlaci√≥n entre ellas), lo que puede hacer que los modelos de regresi√≥n OLS sean inestables o ineficientes.

El proceso de PCR consta de dos pasos principales:

1.  **Reducci√≥n de Dimensionalidad con PCA:** Primero, se aplica PCA a las variables predictoras (X) para transformarlas en un conjunto m√°s peque√±o de **componentes principales**. Estos componentes son combinaciones lineales no correlacionadas de las variables originales y capturan la mayor parte de la varianza en las variables X. Se selecciona un subconjunto de estos componentes principales (aquellos que explican la mayor parte de la varianza total) para retener. Es importante destacar que, en este paso, PCA no tiene conocimiento de la variable de respuesta (Y); solo se enfoca en la estructura de las variables X.
2.  **Regresi√≥n OLS sobre Componentes:** Una vez que se han obtenido los componentes principales seleccionados, se realiza una regresi√≥n lineal est√°ndar (OLS) de la variable de respuesta (Y) sobre estos componentes. Como los componentes principales son ortogonales, la multicolinealidad ya no es un problema en este paso de regresi√≥n.

El beneficio de PCR es que permite construir un modelo de regresi√≥n en escenarios con multicolinealidad severa, reduciendo el n√∫mero de variables a un conjunto m√°s manejable y estable, mientras se intenta preservar la mayor cantidad de informaci√≥n de las variables predictoras.

**Aprendizaje Global vs. Local:**

La Regresi√≥n de Componentes Principales (PCR) es un modelo de **aprendizaje global**.

* **Aspecto Global:** Ambos pasos de PCR son intr√≠nsecamente globales.
    1.  **PCA (Paso Global):** Como se mencion√≥ anteriormente, PCA es una t√©cnica global que encuentra una transformaci√≥n lineal de los datos que se aplica de manera uniforme a todo el espacio de caracter√≠sticas. Los componentes principales se derivan de la estructura de varianza global de las variables predictoras.
    2.  **OLS (Paso Global):** La regresi√≥n realizada sobre los componentes principales es un modelo OLS est√°ndar, que tambi√©n es una t√©cnica global. Busca una √∫nica relaci√≥n lineal que se aplica a todos los datos transformados.

En conjunto, PCR construye una **funci√≥n de regresi√≥n global** que mapea el espacio de caracter√≠sticas original (transformado a componentes principales) a la variable de respuesta. La soluci√≥n resultante es una ecuaci√≥n que se aplica de manera consistente para todas las observaciones, sin ajustar modelos diferentes para subconjuntos locales de datos. Esto significa que si la relaci√≥n entre las variables predictoras y la respuesta es no lineal o cambia dr√°sticamente en diferentes regiones del espacio de caracter√≠sticas, PCR podr√≠a no ser la opci√≥n m√°s flexible, ya que se basa en transformaciones y regresiones lineales globales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (combinaci√≥n de PCA + regresi√≥n)",
  "‚úÖ Variable continua (num√©rica)",
  "‚úÖ Num√©ricas (se aplica PCA primero)",
  "‚úÖ Puede capturar relaciones lineales (con reducci√≥n de dimensionalidad)",
  "‚ö†Ô∏è Requiere verificar residuos del modelo final",
  "‚ö†Ô∏è Se deben revisar los residuos como en regresi√≥n cl√°sica",
  "‚ö†Ô∏è Requiere diagn√≥stico posterior a la regresi√≥n",
  "‚ö†Ô∏è PCA puede estar influenciada por outliers",
  "‚úÖ Reduce multicolinealidad usando componentes ortogonales",
  "‚ö†Ô∏è Menos interpretable (usa componentes, no variables originales)",
  "‚úÖ Eficiente, especialmente con datos de alta dimensi√≥n",
  "‚úÖ Puede usar validaci√≥n cruzada para elegir n√∫mero de componentes",
  "‚ùå Si las primeras componentes no explican bien la variable respuesta"
)

detalles <- c(
  "Modelo supervisado que aplica PCA a los predictores y luego ajusta una regresi√≥n lineal sobre los componentes principales seleccionados.",
  "Se requiere que la variable dependiente sea num√©rica (continua).",
  "Se espera que los predictores sean num√©ricos para aplicar PCA adecuadamente.",
  "PCR puede detectar relaciones lineales al reducir la dimensionalidad primero y luego ajustar la regresi√≥n.",
  "Aunque el PCA es no supervisado, los residuos de la regresi√≥n deben ser normales para cumplir los supuestos de OLS.",
  "Es necesario revisar la independencia de errores como en cualquier regresi√≥n lineal.",
  "Tambi√©n deben analizarse posibles problemas de heterocedasticidad en los residuos.",
  "Outliers pueden influir en los componentes principales y, por lo tanto, en el modelo final.",
  "PCR es muy √∫til cuando los predictores est√°n altamente correlacionados.",
  "Interpretar los resultados puede ser dif√≠cil porque las componentes no corresponden a variables originales.",
  "El proceso es r√°pido incluso con muchos predictores, ya que PCA reduce la dimensi√≥n.",
  "Usualmente se usa validaci√≥n cruzada para determinar cu√°ntas componentes usar.",
  "No es efectivo si los primeros componentes (con mayor varianza) no est√°n relacionados con la variable dependiente."
)

tabla_pcr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pcr %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PCR",
             subtitle = "Principal Component Regression (PCR)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Projection Pursuit (PP)  {-}     

**Projection Pursuit (PP)** es una t√©cnica estad√≠stica de **reducci√≥n de dimensionalidad y an√°lisis exploratorio de datos** utilizada para encontrar las proyecciones "m√°s interesantes" de datos multivariados de alta dimensi√≥n en un espacio de menor dimensi√≥n (generalmente 1D o 2D). La clave de PP es que las proyecciones "interesantes" son aquellas que **se desv√≠an m√°s de una distribuci√≥n normal (gaussiana)**, ya que las estructuras como agrupaciones, valores at√≠picos, o formas inusuales tienden a ser m√°s evidentes en proyecciones no gaussianas.

El algoritmo de PP no busca simplemente la mayor varianza (como PCA), sino que intenta encontrar direcciones de proyecci√≥n que revelen la estructura subyacente y las caracter√≠sticas no lineales de los datos. Lo hace **maximizando un "√≠ndice de proyecci√≥n"** que mide la "interesante" o la "no-gaussianidad" de la proyecci√≥n. Diferentes √≠ndices pueden enfocarse en diferentes aspectos, como la asimetr√≠a, la curtosis, o la presencia de m√∫ltiples modos (grupos).

Existen variantes de PP para diferentes prop√≥sitos, como:
* **Exploratory Projection Pursuit (EPP):** Para visualizaci√≥n y detecci√≥n de estructuras.
* **Projection Pursuit Regression (PPR):** Para construir modelos de regresi√≥n no lineales.
* **Projection Pursuit Classification (PPC):** Para tareas de clasificaci√≥n.

**Aprendizaje Global vs. Local:**

Projection Pursuit (PP) se puede considerar como un modelo que **combina aspectos de aprendizaje global y local**, con un fuerte √©nfasis en la detecci√≥n de caracter√≠sticas locales en un contexto global.

* **Aspecto Global:** PP busca una **transformaci√≥n lineal global** (la direcci√≥n de proyecci√≥n) que se aplica a todo el conjunto de datos para encontrar las proyecciones "m√°s interesantes". La optimizaci√≥n del √≠ndice de proyecci√≥n se realiza sobre todo el espacio de caracter√≠sticas para identificar estas direcciones. Las funciones resultantes (como en PPR o PPC) son combinaciones de funciones no lineales aplicadas a estas proyecciones globales.

* **Aspecto Local (al revelar estructuras):** Donde PP exhibe un car√°cter "local" es en su capacidad para **resaltar estructuras que son intr√≠nsecamente locales** (como clusters o valores at√≠picos) que podr√≠an estar ocultas en las altas dimensiones o en proyecciones puramente globales (como PCA). Al buscar desviaciones de la normalidad, PP es capaz de "perseguir" (de ah√≠ "pursuit") las direcciones que exponen agrupaciones densas o huecos en los datos, que son fen√≥menos locales. La idea es que si los datos no se distribuyen linealmente o tienen estructuras complejas, PP puede encontrar proyecciones donde la "densidad" o "forma" local de los datos es m√°s informativa, permitiendo al usuario o a un algoritmo posterior identificar estas estructuras que son una forma de "regresi√≥n ponderada localmente" o un an√°lisis local de patrones.

En resumen, PP es una t√©cnica potente para explorar la estructura de datos de alta dimensi√≥n, especialmente cuando las relaciones son no lineales o complejas. Si bien el proceso de b√∫squeda de proyecciones es global, el "inter√©s" de estas proyecciones a menudo radica en su capacidad para revelar caracter√≠sticas locales y no gaussianas que son cruciales para entender los datos.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (requiere matriz de datos)",
  "‚úÖ Detecta proyecciones no lineales con estructura interesante",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede ser sensible a valores extremos",
  "‚ö†Ô∏è Puede verse afectado si hay alta redundancia",
  "‚ö†Ô∏è Interpretaci√≥n m√°s dif√≠cil que PCA; proyecciones no son ortogonales",
  "‚ùå Puede ser lento por b√∫squeda iterativa de proyecciones",
  "‚ö†Ô∏è Validaci√≥n subjetiva o basada en heur√≠sticas de inter√©s",
  "‚ùå No √∫til si no hay estructuras no gaussianas en los datos"
)

detalles <- c(
  "M√©todo no supervisado que busca proyecciones de los datos donde se maximice cierta 'interesantitud' (varianza no gaussiana, agrupamientos, etc.).",
  "No est√° dise√±ado para predicci√≥n, sino para exploraci√≥n visual o estructural.",
  "Se aplica a datos num√©ricos, generalmente estandarizados, buscando direcciones relevantes.",
  "A diferencia del PCA (que busca m√°xima varianza), PP busca patrones como colas pesadas, clusters, o distribuciones no normales.",
  "No es un modelo predictivo, por tanto no se calculan residuos.",
  "No se modela el error; se enfoca en la estructura interna de los datos.",
  "No tiene varianzas residuales, por lo que no aplica homoscedasticidad.",
  "Proyecciones pueden verse distorsionadas por valores extremos.",
  "Variables muy correlacionadas pueden dominar las proyecciones si no se controlan.",
  "Proyecciones son dif√≠ciles de interpretar directamente; pueden requerir an√°lisis posterior.",
  "Requiere m√©todos num√©ricos iterativos para encontrar direcciones de inter√©s, lo que lo vuelve computacionalmente intensivo.",
  "Puede usarse validaci√≥n visual (por ejemplo, si se detectan agrupamientos) o criterios como 'kurtosis'.",
  "Si los datos son gaussianos y no contienen patrones relevantes, PP no encuentra proyecciones √∫tiles."
)

tabla_pp <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_pp %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir PP",
             subtitle = "Projection Pursuit (PP) ")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Sammon Mapping  {-}   

**Sammon Mapping** es una t√©cnica de **reducci√≥n de dimensionalidad no lineal** que se utiliza para visualizar datos de alta dimensi√≥n en un espacio de menor dimensi√≥n (generalmente 2D o 3D). Su principal objetivo es **preservar la estructura de distancia local** de los datos originales en la representaci√≥n de menor dimensi√≥n.

A diferencia de t√©cnicas como PCA que buscan preservar la varianza global (y por lo tanto las distancias euclidianas globales), Sammon Mapping se enfoca en que las **distancias peque√±as** (entre puntos cercanos) en el espacio original sean representadas con **mayor fidelidad** en el espacio reducido que las distancias grandes. Esto lo hace particularmente bueno para revelar agrupaciones o clusters que podr√≠an estar ocultos en proyecciones lineales o en otras t√©cnicas de reducci√≥n de dimensionalidad que no priorizan las distancias locales.

El algoritmo de Sammon Mapping funciona minimizando una **funci√≥n de "error" o "estr√©s"** espec√≠fica, conocida como el **"estr√©s de Sammon"**. Esta funci√≥n penaliza m√°s fuertemente las grandes discrepancias en las distancias peque√±as que las grandes discrepancias en las distancias grandes. La minimizaci√≥n de esta funci√≥n se realiza mediante un proceso iterativo de descenso de gradiente.

**Aprendizaje Global vs. Local:**

Sammon Mapping es un modelo que exhibe un fuerte car√°cter de **aprendizaje local**, aunque la optimizaci√≥n se realiza sobre la totalidad de los datos.

* **Aspecto Local:** La caracter√≠stica distintiva de Sammon Mapping es su √©nfasis en la **preservaci√≥n de las distancias locales**. Al penalizar m√°s las distancias peque√±as que se deforman en la proyecci√≥n, el algoritmo se esfuerza por mantener a los puntos que estaban cerca en el espacio original, cerca en el espacio de menor dimensi√≥n. Esto es crucial para revelar la **estructura local y las agrupaciones** dentro de los datos. Es como si el algoritmo estuviera haciendo una serie de "regresiones ponderadas localmente" para cada vecindario de puntos, ajustando las posiciones en el mapa de baja dimensi√≥n para que las relaciones cercanas se mantengan. Esta prioridad en las relaciones de vecindad es una marca del aprendizaje local.

* **Optimizaci√≥n Global:** A pesar de su enfoque local, la funci√≥n de estr√©s de Sammon se calcula y se minimiza sobre **todos los pares de puntos** en el conjunto de datos. La soluci√≥n final es una configuraci√≥n global de puntos en el espacio de baja dimensi√≥n. Por lo tanto, el proceso de optimizaci√≥n es global, pero su criterio de "mejor ajuste" da una importancia desproporcionada a la preservaci√≥n de las relaciones locales.

En resumen, Sammon Mapping es una t√©cnica poderosa para visualizar datos de alta dimensi√≥n, especialmente cuando los clusters o las estructuras locales son importantes. Si los datos no se distribuyen linealmente y lo que se busca es entender c√≥mo se agrupan los puntos en sus vecindarios, Sammon Mapping ofrece una representaci√≥n donde las relaciones locales son el foco principal, lo que lo convierte en una excelente herramienta para la exploraci√≥n de estructuras no lineales y la detecci√≥n de agrupaciones.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (distancias euclidianas)",
  "‚úÖ No lineal, mantiene distancias entre puntos",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è S√≠, es sensible a valores at√≠picos",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Interpretaci√≥n visual en 2D o 3D; no en componentes",
  "‚ùå Lento para conjuntos grandes (algoritmo iterativo)",
  "‚ö†Ô∏è Se puede validar visualmente o con estr√©s",
  "‚ùå Mal desempe√±o en datos ruidosos o de alta dimensi√≥n sin estructura"

)

detalles <- c(
  "M√©todo no supervisado para proyectar datos de alta dimensi√≥n en espacios de menor dimensi√≥n preservando distancias.",
  "No busca predecir, sino representar relaciones de cercan√≠a entre observaciones.",
  "Usa distancias entre puntos; solo variables num√©ricas tienen sentido.",
  "A diferencia de PCA, Sammon busca preservar distancias relativas entre puntos originales y proyectados.",
  "No genera residuos como un modelo predictivo, por lo tanto no se aplica la normalidad.",
  "No hay modelo de error porque no hay predicci√≥n.",
  "No aplica el supuesto de homoscedasticidad.",
  "Valores extremos alteran las distancias y distorsionan el mapa resultante.",
  "Como es una t√©cnica de reducci√≥n, no le afecta multicolinealidad directamente.",
  "El mapa resultante puede interpretarse en t√©rminos de proximidad, no de pesos o coeficientes.",
  "Implementaci√≥n cl√°sica es iterativa y costosa computacionalmente en datasets grandes.",
  "Puede usarse estr√©s (error entre distancias originales y proyectadas) como m√©trica de calidad.",
  "Si las distancias no reflejan bien la estructura real (por ruido o dimensiones irrelevantes), el m√©todo falla en representar datos √∫tiles."
)

tabla_sammon <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_sammon %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir sammon mapping",
             subtitle = "Sammon Mapping")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Regularized Discriminant Analysis (RDA)  {-}    

El **An√°lisis Discriminante Regularizado (RDA)** es un m√©todo de **clasificaci√≥n** que act√∫a como un **intermedio flexible entre el An√°lisis Discriminante Lineal (LDA) y el An√°lisis Discriminante Cuadr√°tico (QDA)**. Fue desarrollado por Jerome Friedman para abordar las limitaciones de LDA (que asume covarianzas iguales para todas las clases, lo que resulta en fronteras lineales) y QDA (que permite covarianzas separadas pero puede ser inestable con pocos datos o muchas variables).

RDA introduce dos **par√°metros de regularizaci√≥n**, $\alpha$ y $\gamma$, que controlan la flexibilidad del modelo y su capacidad para adaptarse a los datos:

1.  **Par√°metro $\alpha$ (alpha):** Controla el grado en que la matriz de covarianza de cada clase se **contrae hacia una matriz de covarianza com√∫n** (como en LDA).
    * Si $\alpha = 0$, RDA se comporta como **QDA** (cada clase tiene su propia matriz de covarianza).
    * Si $\alpha = 1$, RDA se comporta como **LDA** (todas las clases comparten una matriz de covarianza com√∫n).
    * Para valores entre 0 y 1, RDA utiliza un promedio ponderado de la matriz de covarianza espec√≠fica de la clase y la matriz de covarianza com√∫n. Esto ayuda a estabilizar las estimaciones de covarianza en QDA, especialmente cuando los tama√±os de muestra son peque√±os o el n√∫mero de variables es grande.

2.  **Par√°metro $\gamma$ (gamma):** Controla el grado en que la matriz de covarianza (ya sea com√∫n o espec√≠fica de la clase, dependiendo de $\alpha$) se **contrae hacia una matriz diagonal**.
    * Si $\gamma = 0$, no hay contracci√≥n diagonal adicional.
    * Si $\gamma = 1$, la matriz de covarianza se contrae completamente a una matriz diagonal (lo que implica independencia entre las variables).
    * Para valores entre 0 y 1, se aplica una contracci√≥n hacia la diagonal, lo que puede ser √∫til cuando hay multicolinealidad.

Al sintonizar estos dos par√°metros (generalmente mediante validaci√≥n cruzada), RDA puede encontrar un equilibrio √≥ptimo entre la simplicidad de LDA y la flexibilidad de QDA, adapt√°ndose mejor a la estructura de covarianza real de los datos y mejorando la estabilidad del modelo.

**Aprendizaje Global vs. Local:**

El An√°lisis Discriminante Regularizado (RDA) es un modelo de **aprendizaje global** que incorpora un grado de **adaptaci√≥n local** a trav√©s de su regularizaci√≥n.

* **Aspecto Global:** Al igual que LDA y QDA, RDA construye un **clasificador global** basado en las distribuciones de probabilidad modeladas para cada clase. Las matrices de covarianza regularizadas y las medias de las clases se estiman a partir de todo el conjunto de datos de entrenamiento, y la regla de clasificaci√≥n resultante se aplica de manera consistente en todo el espacio de caracter√≠sticas. La frontera de decisi√≥n que RDA define es una funci√≥n global (que puede ser lineal o cuadr√°tica, o una combinaci√≥n de ambas, dependiendo de los par√°metros de regularizaci√≥n).

* **Adaptaci√≥n Local (a trav√©s de la regularizaci√≥n de covarianza):** La flexibilidad de RDA para ajustarse mejor a los datos que LDA o QDA proviene de su capacidad para modelar las **estructuras de covarianza de las clases de una manera m√°s matizada**. Al permitir una contracci√≥n parcial de las matrices de covarianza hacia una com√∫n (par√°metro $\alpha$) o hacia una diagonal (par√°metro $\gamma$), RDA puede adaptar las formas de las distribuciones de las clases. Esto permite que el modelo capture mejor las caracter√≠sticas de dispersi√≥n de los datos en diferentes regiones, lo que en √∫ltima instancia se traduce en fronteras de decisi√≥n m√°s adaptables que pueden manejar cierto grado de no linealidad o formas complejas de clase. No es un ajuste local en el sentido de LOESS, sino una forma de adaptar la complejidad del modelo global a la estructura de covarianza percibida de cada clase.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica (clases)",
  "‚úÖ Num√©ricas",
  "‚úÖ No lineal (transici√≥n entre LDA y QDA)",
  "‚ùå No aplica (no es un modelo de regresi√≥n)",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Controla la homoscedasticidad mediante regularizaci√≥n",
  "‚ö†Ô∏è Puede ser sensible, aunque la regularizaci√≥n reduce impacto",
  "‚úÖ Reduce impacto mediante regularizaci√≥n de covarianzas",
  "‚ö†Ô∏è Menos interpretable que LDA/QDA puro, pero con mayor flexibilidad",
  "‚úÖ M√°s eficiente que QDA en conjuntos peque√±os o ruidosos",
  "‚úÖ Muy √∫til para evitar overfitting, sobre todo con validaci√≥n cruzada",
  "‚ùå Puede no mejorar sobre LDA/QDA si no hay problemas de varianza o sobreajuste"
)

detalles <- c(
  "Modelo supervisado de clasificaci√≥n que combina LDA y QDA usando par√°metros de regularizaci√≥n.",
  "Clasifica observaciones en clases discretas bas√°ndose en variables num√©ricas predictoras.",
  "Requiere variables num√©ricas para calcular medias y covarianzas por clase.",
  "Introduce par√°metros de mezcla que ajustan la matriz de covarianza hacia la identidad (como ridge) y hacia la covarianza com√∫n.",
  "No genera residuos como un modelo de regresi√≥n, por lo tanto el supuesto no aplica.",
  "No se enfoca en errores independientes, sino en distribuciones de clase.",
  "La regularizaci√≥n suaviza las diferencias entre covarianzas, mitigando problemas de homoscedasticidad.",
  "Los valores at√≠picos pueden influir en la estimaci√≥n, pero se reduce con regularizaci√≥n.",
  "Mejor manejo de multicolinealidad que QDA gracias a la matriz regularizada.",
  "La interpretaci√≥n depende de los valores de regularizaci√≥n elegidos; m√°s flexible pero menos directa.",
  "Reduce complejidad computacional respecto a QDA; √∫til con pocas observaciones por clase.",
  "Es com√∫n usar validaci√≥n cruzada para seleccionar los par√°metros de regularizaci√≥n √≥ptimos.",
  "No aporta mejoras significativas si los supuestos de LDA o QDA se cumplen perfectamente sin sobreajuste."
)

tabla_rda <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_rda %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RDA",
             subtitle = "Regularized Discriminant Analysis (RDA)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Uniform Manifold Approximation and Projection (UMAP) {-}   

**Uniform Manifold Approximation and Projection (UMAP)** es una t√©cnica de **reducci√≥n de dimensionalidad no lineal** de vanguardia, utilizada principalmente para la **visualizaci√≥n de datos** de alta dimensi√≥n y para el **aprendizaje de caracter√≠sticas (feature learning)**. Fue desarrollada por Leland McInnes, John Healy y James Melville. UMAP es una alternativa m√°s reciente y a menudo m√°s r√°pida y escalable a t-SNE (t-Distributed Stochastic Neighbor Embedding), manteniendo su capacidad para preservar la estructura local y global de los datos.

La idea central de UMAP se basa en la **teor√≠a de los conjuntos difusos (fuzzy set theory)** y la **geometr√≠a riemanniana**. Intenta construir una representaci√≥n de baja dimensi√≥n de los datos asumiendo que los datos de alta dimensi√≥n residen en una **variedad (manifold) subyacente de baja dimensi√≥n**. El algoritmo opera en dos fases:

1.  **Construcci√≥n del Grafo de Vecindad Difusa:**
    * Primero, UMAP construye un **grafo ponderado difuso** en el espacio de alta dimensi√≥n. Los nodos del grafo son los puntos de datos y los pesos de las aristas representan la probabilidad de que dos puntos est√©n conectados (es decir, qu√© tan similares o cercanos son).
    * Para ello, UMAP estima las distancias entre los puntos en el manifold subyacente y luego convierte estas distancias en probabilidades de conectividad. Esto es crucial porque le permite adaptarse a la densidad local de los datos (puntos en regiones densas pueden estar cerca incluso con distancias euclidianas grandes, y viceversa en regiones dispersas).

2.  **Optimizaci√≥n del Dise√±o en Baja Dimensi√≥n:**
    * Luego, UMAP optimiza el dise√±o de los puntos en un espacio de baja dimensi√≥n (ej., 2D) para que la **estructura del grafo construido en alta dimensi√≥n sea lo m√°s similar posible** al grafo construido en baja dimensi√≥n.
    * Esto se logra minimizando una funci√≥n de costo que intenta hacer que las probabilidades de conectividad en el espacio de baja dimensi√≥n coincidan con las probabilidades de conectividad del grafo de alta dimensi√≥n.

UMAP es valorado por su velocidad, escalabilidad a grandes conjuntos de datos, y su capacidad para preservar simult√°neamente la **estructura local y global** de los datos, lo que lo hace ideal para visualizar agrupaciones y relaciones complejas.

**Aprendizaje Global vs. Local:**

UMAP es un excelente ejemplo de un modelo que logra un equilibrio sofisticado entre el **aprendizaje local y global**.

* **Aspecto Local:** UMAP pone un fuerte √©nfasis en la **preservaci√≥n de la estructura local**. Al construir el grafo de vecindad difusa, se enfoca en las relaciones de los vecinos m√°s cercanos de cada punto (controlado por el par√°metro `n_neighbors`). La forma en que calcula las probabilidades de conectividad se adapta a la densidad local de los datos, asegurando que los cl√∫steres y las agrupaciones cercanas se mantengan cohesivos en la representaci√≥n de baja dimensi√≥n. Las relaciones "locales" son las que definen el "manifold" en primera instancia. Esto significa que si los datos no se distribuyen linealmente y tienen estructuras complejas con vecindarios distintos (como diferentes clusters o ramas en una estructura), UMAP es capaz de capturarlas con alta fidelidad, de forma similar a como una "regresi√≥n ponderada localmente" operar√≠a en cada vecindario.

* **Aspecto Global:** A pesar de su √©nfasis local, UMAP tambi√©n hace un esfuerzo consciente por preservar la **estructura global** de los datos. Al minimizar la funci√≥n de costo para que la estructura del grafo se mantenga en el espacio de baja dimensi√≥n, UMAP no solo se asegura de que los puntos cercanos permanezcan cercanos, sino que tambi√©n intenta que los grupos de puntos que estaban globalmente separados en la alta dimensi√≥n permanezcan separados en la baja dimensi√≥n. El par√°metro `min_dist` ayuda a controlar cu√°n compactos deben ser los cl√∫steres, lo que influye en la separaci√≥n global. Esta capacidad de equilibrar ambos aspectos es una de las principales ventajas de UMAP sobre t√©cnicas que a veces sacrifican la estructura global (como t-SNE, que puede "romper" grandes cl√∫steres).


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (o categ√≥ricas codificadas)",
  "‚úÖ Captura relaciones no lineales y estructura local/global",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Algo sensible a outliers (puede distorsionar estructuras)",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Visual en 2D o 3D; dif√≠cil interpretaci√≥n formal",
  "‚úÖ Muy r√°pido incluso en grandes conjuntos de datos",
  "‚ö†Ô∏è No usa validaci√≥n cruzada cl√°sica, pero puede evaluarse la estabilidad",
  "‚ùå Datos con mucho ruido, escalas mal ajustadas o sin estructura latente clara"
)

detalles <- c(
  "T√©cnica no supervisada de reducci√≥n de dimensionalidad que preserva tanto estructura local como global de los datos.",
  "No busca predecir, sino proyectar observaciones a un espacio de menor dimensi√≥n.",
  "Funciona con datos num√©ricos; variables categ√≥ricas deben ser codificadas antes.",
  "A diferencia de PCA, puede descubrir relaciones no lineales m√°s complejas.",
  "No genera residuos; no aplica el supuesto de normalidad.",
  "No hay modelo de error residual, por lo que no aplica la independencia.",
  "No es un modelo predictivo, as√≠ que no se eval√∫a homoscedasticidad.",
  "Outliers pueden influir en el mapa de manera desproporcionada.",
  "Como es una t√©cnica de reducci√≥n, la multicolinealidad no le afecta directamente.",
  "La interpretaci√≥n se limita a la distribuci√≥n visual de puntos.",
  "UMAP es computacionalmente eficiente y escalable a grandes vol√∫menes de datos.",
  "No utiliza validaci√≥n cruzada directa, pero puede evaluarse la estabilidad de la proyecci√≥n.",
  "Cuando no existe una estructura clara en los datos, la proyecci√≥n puede ser confusa o poco √∫til."
)

tabla_umap <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_umap %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir UMAP",
             subtitle = "Uniform Manifold Approximation and Projection (UMAP)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


<!--chapter:end:05-dimensionality_reduction.Rmd-->


# üß¨ **6. Bayesianos** {-}  

Placeholder


## Averaged One - Dependence Estimators (AODE)  {-}   
## Bayesian Network (BN)  {-}   
## Bayesian Belief Network (BBN)  {-}  
## Gaussian Naive Bayes (GNB) {-}    
## Multinomial Naive Bayes (MNB) {-}  
## Naive Bayes (NB) {-}  

<!--chapter:end:06-bayesian.Rmd-->


# üßÆ **7. Regularizaci√≥n** {-}  

Placeholder


## Elastic Net  {-}  
## Ridge Regression  {-}   
## Least Absolute Shrinkage and Selection Operator (LASSO)  {-}  
## Least Angle Regression (LARS)  {-}   

<!--chapter:end:07-regularization.Rmd-->


# üîç **8. Instance-Based (Basados en Instancias)** {-}  

Placeholder


## k - Nearest Neighbour (kNN)  {-}   
## Learning Vector Quantization (LVQ)  {-}   
## Locally Weighted Learning (LWL)  {-}   
## Self - Organizing Map (SOM)  {-} 

<!--chapter:end:08-instance_based.Rmd-->


# üìè **9. Clustering (No Supervisado)** {-}  

Placeholder


## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)  {-}    
## Expectation Maximization (EM) {-}  
## Hierarchical Clustering (hclust) {-}  
## k-Means  {-}  
## k-Medians  {-}   

<!--chapter:end:09-clustering.Rmd-->


# üìê **10. Sistemas Basados en Reglas (Rule-Based Systems)** {-}

Placeholder


## Cubist  {-}  
## Decision Rules  {-}  
## Fuzzy Logic {-}
## One Rule (OneR)  {-}   
## Repeated Incremental Pruning to Produce Error Reduction (RIPPER)  {-} 
## Rule Fit  {-}   
## Zero Rule (ZeroR)  {-}    

<!--chapter:end:10-rule_based_systems.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 80)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE
)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(here::here())
```
`r if (knitr::is_html_output()) '
# References {-}
'`

Sagi, S. (2019). ML Algorithms: One SD (œÉ). The obvious questions to ask when‚Ä¶ | by Sagi Shaier | Medium. https://medium.com/@Shaier/ml-algorithms-one-sd-%CF%83-74bcb28fafb6 

Kuhn, M. (2019). The caret Package. https://topepo.github.io/caret/index.html

<!--chapter:end:11-references.Rmd-->

