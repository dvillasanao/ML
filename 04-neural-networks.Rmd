# üß† **4. Redes Neuronales y Deep Learning** {-}  

**Ejemplos:** MLP, CNN, RNN, Transformers   
**Cu√°ndo usarlo:**   

* Im√°genes (CNN), texto y lenguaje natural (Transformers), series temporales (RNN/LSTM).
* Grandes vol√∫menes de datos no estructurados.

**Ventajas:** Muy poderosos para datos complejos y no estructurados.   
**Limitaciones:** Requieren mucha data y poder computacional. Menor interpretabilidad.

---


## Radial Basis Function Network (RBFN)  {-}     

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas (requiere normalizaci√≥n), Categ√≥ricas como dummies",
  "‚úÖ Captura no linealidades mediante funciones base radiales",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "‚ùå No asume varianza constante",
  "‚ö†Ô∏è Moderadamente (centros pueden verse alterados por outliers)",
  "‚ö†Ô∏è Puede influir en la selecci√≥n de centros, pero no tan cr√≠tico como en OLS",
  "‚ö†Ô∏è Baja (la capa oculta con RBF es dif√≠cil de interpretar)",
  "‚ö†Ô∏è Moderada (depende de n√∫mero de centros y dimensiones)",
  "‚úÖ Recomendable para ajustar n√∫mero de bases y spread",
  "‚ùå Datos muy grandes o alta dimensionalidad sin reducci√≥n, mucho ruido"
)

detalles <- c(
  "Red neuronal de una capa oculta con funciones radial basis como activaci√≥n.",
  "Para regresi√≥n predice un valor continuo; para clasificaci√≥n usa votaci√≥n o softmax sobre salidas.",
  "Requiere que las caracter√≠sticas num√©ricas est√©n escaladas; las categ√≥ricas deben convertirse a variables indicadoras.",
  "Cada neurona oculta calcula una funci√≥n gaussiana (u otra RBF) centrada en un punto, captando curvas suaves.",
  "No impone distribuci√≥n normal en los errores, pues optimiza en funci√≥n de m√≠nimos cuadrados o cross-entropy.",
  "Funciona mejor si las observaciones son independientes; sensible a estructuras de dependencia sin modelar.",
  "No requiere homocedasticidad ya que no se basa en un modelo param√©trico de error con varianza fija.",
  "Los valores extremos pueden desplazar los centros de las RBF, afectando la forma del modelo.",
  "La colinealidad puede dificultar la determinaci√≥n de centros √≥ptimos, pero no invalida el ajuste.",
  "Las neuronas ocultas representan combinaciones complejas de caracter√≠sticas, por lo que el modelo es tipo 'caja negra'.",
  "El entrenamiento implica fijar o aprender centros y spreads; para muchos centros o dimensiones altas, el costo crece r√°pido.",
  "Se usa CV para elegir el n√∫mero de bases (centros) y el par√°metro de ancho (`sigma` o `spread`) para evitar sobreajuste.",
  "No conviene cuando hay decenas de miles de caracter√≠sticas sin reducci√≥n previa o cuando el ruido es muy alto."
)

tabla_rbfn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rbfn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RBFN",
             subtitle = "Radial Basis Function Network (RBFN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Perceptron  {-}    

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas (requiere normalizar), Categ√≥ricas como dummies",
  "‚ö†Ô∏è Aprendizaje lineal: separabilidad lineal requerida",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers pueden cambiar el hiperplano)",
  "‚ö†Ô∏è Afecta la convergencia si est√° muy alta",
  "‚ö†Ô∏è Baja (modelo b√°sico de una capa sin capas ocultas)",
  "‚úÖ Muy r√°pido para datasets medianos",
  "‚úÖ √ötil para evaluar margen de separaci√≥n",
  "‚ùå No sirve si las clases no son linealmente separables"
)

detalles <- c(
  "Red neuronal de una sola capa que ajusta un hiperplano separador.",
  "Dise√±ado para clasificaci√≥n binaria; no predice valores continuos.",
  "Todas las features deben ser num√©ricas y escaladas; las categ√≥ricas deben convertirse en indicadores.",
  "Busca maximizar el margen de separaci√≥n lineal entre dos clases; no captura no linealidades.",
  "No exige distribuci√≥n normal de errores ya que optimiza con perceptr√≥n simple.",
  "Funciona mejor si las instancias son independientes; sensible a dependencias temporales sin ajuste.",
  "No se basa en varianza de errores; el algoritmo actualiza pesos sin supuestos de varianza.",
  "Los valores extremos cercanos al margen pueden forzar ajustes bruscos de pesos.",
  "La colinealidad puede ralentizar la convergencia, aunque no impide la definici√≥n de hiperplano.",
  "F√°cil de entender: el peso de cada caracter√≠stica indica direcci√≥n del hiperplano.",
  "Entrenamiento r√°pido usando regla de aprendizaje por error; escalable a datos medianos.",
  "Se usa CV para ajustar tasa de aprendizaje y n√∫mero de √©pocas para evitar bajo/sobreajuste.",
  "In√∫til si las clases no se pueden separar linealmente; requiere extensiones (por ejemplo, kernel) para no linealidad."
)

tabla_perceptron <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_perceptron %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir perceptron",
             subtitle = "Perceptron")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Back - Propagation  {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas (requiere normalizar), Categ√≥ricas como dummies",
  "‚úÖ Captura relaciones no lineales profundas",
  "‚ùå No requiere",
  "‚úÖ Deseable, aunque no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Elevado (puede requerir robustez ante valores extremos)",
  "‚ö†Ô∏è Puede ralentizar convergencia si es muy alta",
  "‚ö†Ô∏è Baja (modelo de ‚Äúcaja negra‚Äù)",
  "‚ö†Ô∏è Depende de la arquitectura y tama√±o del dataset",
  "‚úÖ Recomendable para ajustar tasas de aprendizaje, capas y neuronas",
  "‚ùå No conviene con datos muy peque√±os o alta dimensionalidad sin regularizar"
)

detalles <- c(
  "Algoritmo para entrenar redes neuronales multicapa ajustando pesos por retropropagaci√≥n del error.",
  "En clasificaci√≥n usa softmax o sigmoide en salida; en regresi√≥n, capa lineal para valor continuo.",
  "Debe escalarse cada caracter√≠stica; las categ√≥ricas transformarse a variables indicadoras antes de entrenar.",
  "Aprende funciones arbitrariamente complejas activando m√∫ltiples capas ocultas con funciones no lineales.",
  "No impone distribuci√≥n espec√≠fica en errores, se optimiza v√≠a descenso de gradiente.",
  "Mejor si las observaciones son independientes; sensible a secuencias sin ajustes espec√≠ficos.",
  "No requiere varianza constante, ya que los pesos se ajustan adaptativamente durante el entrenamiento.",
  "Valores extremos pueden causar activaciones saturadas (vanishing/exploding gradients) si no se manejan.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; Batch Normalization ayuda a mitigar.",
  "Dif√≠cil interpretar cada peso individual; se usan t√©cnicas como LIME o SHAP para explicar decisiones.",
  "El tiempo crece con n√∫mero de capas, neuronas y epochs; GPUs aceleran el proceso.",
  "Cross-validation (o k-fold) ayuda a elegir n√∫mero de capas, neuronas por capa, tasa de aprendizaje y regularizaci√≥n.",
  "No funciona bien con datasets peque√±os (overfitting f√°cil) o ruido elevado sin t√©cnicas de regularizaci√≥n."
)

tabla_backprop <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_backprop %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir back-propagation",
             subtitle = "Back - Propagation")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Hopfield Network  {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è No es supervisado en el sentido cl√°sico",
  "‚ö†Ô∏è No hay ‚Äútarget‚Äù continuo o categ√≥rico (almacenamiento de patrones)",
  "‚úÖ Variables binarias o valores discretizados (patrones binarios)",
  "‚úÖ Correlaciona patrones con pesos sim√©tricos entre neuronas",
  "‚ùå No aplica (no hay residuos param√©tricos)",
  "‚úÖ Deseable, aunque no obligatorio (los estados deben actualizarse sin bucles no deseados)",
  "‚ùå No aplica (no modela varianza de errores)",
  "‚ö†Ô∏è Muy sensible (un solo nodo saturado puede distorsionar la red)",
  "‚ö†Ô∏è Puede verse afectado si los patrones de entrenamiento tienen redundancia fuerte",
  "‚ö†Ô∏è Media (la din√°mica de atra√ß√£o es interpretable, pero las conexiones pueden ser complejas)",
  "‚ö†Ô∏è Moderada (dependiendo del n√∫mero de neuronas y estados s√≠ncronos/as√≠ncronos)",
  "‚ùå No se usa tradicionalmente, pero se puede validar estabilidad de memorias con pruebas de convergencia",
  "‚ö†Ô∏è No sirve si los patrones no son binarios o si hay alto ruido en entradas asociativas"
)

detalles <- c(
  "Red neuronal recurrente para recuperaci√≥n asociativa de patrones, no requiere pares X‚Üíy.",
  "No predice una variable externa, recupera patrones completos a partir de entradas parciales o ruidosas.",
  "Requiere que cada elemento del patr√≥n sea binario (¬±1) o est√© discretizado; las variables continuas deben binarizarse.",
  "Los pesos sim√©tricos se calculan por Hebb (p. ej. W = Œ£ p·µ¢ p·µ¢·µÄ), sin umbral expl√≠cito para relaciones lineales.",
  "No hay un t√©rmino de error param√©trico; la din√°mica sigue la funci√≥n de energ√≠a, no un residuo gaussiano.",
  "Es mejor si las actualizaciones de estado son independientes o s√≠ncronas; la dependencia temporal puede generar oscilaciones.",
  "No modela varianza de error, pues busca minimizar energ√≠a, no error cuadr√°tico.",
  "Patrones fuera del rango binario pueden causar saturaci√≥n o estados inestables.",
  "Patrones muy similares (colineales) pueden interferir en recuperaciones correctas (atractores vecinos).",
  "La din√°mica de convergencia hacia un estado estable (atractor) se puede visualizar, pero la topolog√≠a de pesos puede no ser transparente.",
  "Simulaci√≥n de din√°micas es razonable para tama√±os moderados (‚â§1000 neuronas); grandes redes requieren optimizaci√≥n paralela.",
  "No se usa CV tradicional; se analiza la robustez de memorias variando inicializaci√≥n o agregando ruido.",
  "No apto si los datos no pueden discretizarse en patrones binarios, o si se requieren m√∫ltiples clases de salida simult√°neas."
)

tabla_hopfield <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_hopfield %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir hopfield network",
             subtitle = "Hopfield Network")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  

```

## Multilayer Perceptron (MP)  {-}    

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas (normalizar) y categ√≥reas (dummies)",
  "‚úÖ Captura relaciones no lineales profundas",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (puede requerir robustez ante outliers)",
  "‚ö†Ô∏è Afecta la convergencia si es muy alta",
  "‚ö†Ô∏è Baja (modelo tipo 'caja negra')",
  "‚ö†Ô∏è Depende de arquitectura y tama√±o del dataset",
  "‚úÖ Recomendable (k-fold o repeated CV)",
  "‚ùå No conviene con pocos datos o ruido elevado"
)

detalles <- c(
  "Red neuronal con m√∫ltiples capas ocultas y funci√≥n de activaci√≥n no lineal.",
  "Clasificaci√≥n con softmax/sigmoide; regresi√≥n con capa lineal en salida.",
  "Debe escalarse cada caracter√≠stica; las categ√≥ricas convierten a variables indicadoras.",
  "Aprende patrones complejos combinando m√∫ltiples capas y neuronas.",
  "No impone distribuci√≥n espec√≠fica de errores, se optimiza con optimizadores basados en gradiente.",
  "Funciona mejor si las muestras son independientes; sensibles a dependencia temporal sin ajustes.",
  "No requiere varianza constante, ajusta pesos en cada mini-batch o lote.",
  "Outliers pueden causar gradientes explosivos o desaparecidos sin mecanismos de robustez.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; batch normalization ayuda.",
  "Dif√≠cil de interpretar cada peso/neuronas; se usan t√©cnicas como SHAP o LIME para explicaci√≥n.",
  "El tiempo de entrenamiento aumenta con cada capa, neuronas y epochs; GPUs aceleran el proceso.",
  "Crucial para ajustar hiperpar√°metros: n√∫mero de capas, neuronas por capa, tasa de aprendizaje, regularizaci√≥n.",
  "No √∫til para datasets muy peque√±os (sobreajuste) o altamente ruidosos sin regularizaci√≥n."
)

tabla_mp <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mp %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MP",
             subtitle = "Multilayer Perceptron (MP)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Convolutional Neural Network (CNN)  {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Im√°genes (matrices de p√≠xeles) y datos con estructura espacial",
  "‚úÖ Captura relaciones locales y espaciales mediante filtros convolucionales",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias son independientes)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (artefactos o ruido en im√°genes puede afectar)",
  "‚ö†Ô∏è No se eval√∫a colinealidad de predictores, maneja correlaciones espaciales",
  "‚ö†Ô∏è Baja (modelo tipo ‚Äòcaja negra‚Äô, usar t√©cnicas como Grad-CAM para interpretaci√≥n)",
  "‚ö†Ô∏è Lento sin GPU, entrenamiento intensivo en c√≥mputo",
  "‚úÖ Robusto si se aplica k-fold o validaci√≥n en conjunto de im√°genes",
  "‚ùå No funciona bien con pocos datos o sin estructura espacial significativa"
)

detalles <- c(
  "Red neuronal profunda especializada en procesar datos con estructura de grilla (ej. im√°genes).",
  "En clasificaci√≥n utiliza softmax; en regresi√≥n, capa lineal para valores continuos.",
  "Requiere tensores de entrada (canales, altura, ancho); funciones de preprocesamiento para im√°genes.",
  "Filtros convolucionales extraen caracter√≠sticas locales, max-pooling disminuye dimensionalidad manteniendo informaci√≥n relevante.",
  "No impone ninguna distribuci√≥n en los errores, optimiza funci√≥n de p√©rdida directamente.",
  "Ideal si las muestras son independientes; sensible a dependencias temporales o espaciales no modeladas.",
  "No requiere varianza constante pues se basa en convoluciones y pooling, no en un modelo param√©trico de error.",
  "Ruido o artefactos en p√≠xeles pueden alterar el aprendizaje de filtros, es importante usar t√©cnicas de regularizaci√≥n.",
  "La red aprende filtros que capturan patrones locales, por lo que no es necesario verificar colinealidad expl√≠citamente.",
  "Dif√≠cil de interpretar cada filtro y capa; se utilizan mapas de activaci√≥n o Grad-CAM para entender qu√© regiones influyen en la predicci√≥n.",
  "El entrenamiento con m√∫ltiples capas convolucionales y millones de par√°metros es intensivo en GPU/TPU.",
  "Validaci√≥n cruzada o separaci√≥n de conjuntos (train/validation/test) ayuda a evitar overfitting.",
  "No es apropiado para datasets muy peque√±os sin aumentar datos (data augmentation) o sin informaci√≥n espacial clara."
)

tabla_cnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles) 

tabla_mp %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CNN",
             subtitle = "Convolutional Neural Network (CNN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Recurrent Neural Networks (RNNs) {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado secuencial",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n) en secuencias",
  "‚úÖ Series temporales y datos secuenciales (texto, audio, series) convertidos a vectores",
  "‚úÖ Captura dependencias temporales y de largo plazo entre pasos de la secuencia",
  "‚ùå No requiere supuestos de normalidad de residuos",
  "‚ö†Ô∏è Ideal si las secuencias son independientes entre s√≠; no modela dependencia ex√≥gena autom√°ticamente",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers en la serie pueden sesgar el entrenamiento si no se detectan)",
  "‚ö†Ô∏è La colinealidad en caracter√≠sticas secuenciales puede afectar la convergencia (usar embeddings o reducci√≥n)",
  "‚ö†Ô∏è Baja (‚Äúcaja negra‚Äù con muchas capas; usar t√©cnicas de atenci√≥n o visualizaci√≥n de activaciones)",
  "‚ö†Ô∏è Lento sin GPU/TPU; entrenamiento costoso para secuencias largas o redes profundas",
  "‚ö†Ô∏è Usar validaci√≥n cronol√≥gica (time series split) es m√°s apropiado que k-fold cl√°sico",
  "‚ùå No conviene con muy pocas muestras temporales, secuencias extremadamente largas sin truncar, o datos muy ruidosos"
)

detalles <- c(
  "Red neuronal recurrente que procesa datos en pasos temporales manteniendo un estado interno.",
  "En clasificaci√≥n, etiqueta cada elemento o secuencia; en regresi√≥n, predice valores continuos a lo largo del tiempo.",
  "Las entradas deben transformarse en vectores o embeddings; por ejemplo, texto a √≠ndices, series normalizadas.",
  "La arquitectura RNN (LSTM, GRU) retiene informaci√≥n de pasos anteriores para afectar salidas posteriores.",
  "No impone distribuci√≥n en errores, ya que se optimiza v√≠a descenso de gradiente sobre secuencias.",
  "Funciona mejor si cada secuencia (serie) es independiente; para datos con autocorrelaci√≥n compleja, usar variantes especializadas.",
  "No requiere varianza constante, pues se basa en propagaci√≥n de estado y no en un t√©rmino de error param√©trico.",
  "Valores at√≠picos en la serie pueden provocar gradientes explosivos o desvanecidos sin mecanismos como clipping.",
  "La representaci√≥n internal de patrones secuenciales puede verse afectada si hay caracter√≠sticas muy correlacionadas; usar regularizaci√≥n.",
  "Dif√≠cil interpretar pesos internos; se usan mec√°nicas como atenci√≥n (attention) o visualizaci√≥n de celdas LSTM.",
  "El entrenamiento con backpropagation through time es intensivo; GPUs o TPUs aceleran enormemente el proceso.",
  "Para series temporales, se prefiere validaci√≥n basada en ventanas de tiempo (rolling/expanding window) en lugar de random split.",
  "No apto si las secuencias son muy cortas o muy pocas, o hay mucho ruido sin filtrado; en esos casos, usar modelos estad√≠sticos simples."
)

tabla_rnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rnn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RNN",
             subtitle = "Recurrent Neural Networks (RNNs)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Transformers  {-}  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado (frecuentemente secuencial o multitarea)",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n en secuencias)",
  "‚úÖ Texto, secuencias, im√°genes en vectores o embeddings",
  "‚úÖ Captura dependencia secuencial y global mediante mecanismos de atenci√≥n",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚ö†Ô∏è Ideal si las muestras o secuencias son independientes; para datos correlacionados usar variantes espec√≠ficas",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers en embeddings o entradas ruidosas pueden afectar atenci√≥n)",
  "‚ö†Ô∏è La colinealidad en embedding space puede ralentizar aprendizaje; usar regularizaci√≥n",
  "‚ö†Ô∏è Baja (modelo de ‚Äôcaja negra‚Äô, requieren m√©todos como attention visualization o interpretabilidad basada en pesos)",
  "‚ö†Ô∏è Lento sin hardware especializado (secuencialidad en atenci√≥n puede ser costosa)",
  "‚ö†Ô∏è Validaci√≥n temporal o k-fold anidada, seg√∫n tarea; en NLP se prefiere holdout sobre texto sin mezclar",
  "‚ùå No es apropiado con muy pocos datos de entrenamiento o sin estructura secuencial clara"
)

detalles <- c(
  "Arquitectura basada en capas de atenci√≥n para procesar secuencias completas en paralelo.",
  "Modelos como BERT, GPT, T5 pueden usarse para tareas de clasificaci√≥n, traducci√≥n, regresi√≥n de valores continuos en secuencias.",
  "Entradas requieren tokenizaci√≥n y conversi√≥n a embeddings; pueden combinarse varias modalidades.",
  "La auto‚Äêatenci√≥n global permite capturar relaciones a largo y corto plazo sin sesgo posicional estricto.",
  "No impone distribuci√≥n param√©trica de errores; se entrena con optimizadores basados en p√©rdidas cross‚Äêentropy o MSE.",
  "Se prefiere que las secuencias en el batch no sean dependientes; para series de tiempo, usar variantes como Time‚ÄêSeries Transformer.",
  "No se modela varianza del error; el entrenamiento se enfoca en minimizar funci√≥n de p√©rdida directa.",
  "Ruido en texto (typos) o en datos num√©ricos de entrada puede inducir atenci√≥n err√°tica; usar limpieza de datos y regularizaci√≥n.",
  "Los embeddings pueden contener informaci√≥n redundante de caracter√≠sticas correlacionadas; ajustar tama√±o de embedding y regularizaci√≥n.",
  "Interpretabilidad limitada; se usan t√©cnicas como visualizaci√≥n de mapas de atenci√≥n, LIME, SHAP para entender decisiones.",
  "El c√≥mputo de atenci√≥n es O(n¬≤) en longitud de secuencia; GPUs/TPUs o variantes eficientes (Linformer, Performer) alivian costo.",
  "Para tareas de texto, a veces se usa train/validation/test sin CV cl√°sica; para tareas generales, k-fold anidada ayuda a elegir hiperpar√°metros.",
  "No es adecuado con datasets muy peque√±os, sin preentrenamiento o sin estructuras secuenciales definidas."
)

tabla_transformers <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_transformers %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir transformers",
             subtitle = "Transformers")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




