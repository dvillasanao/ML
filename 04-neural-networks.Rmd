# üß† 4. Redes Neuronales {-}  

**Ejemplos:** MLP, CNN, RNN, Transformers.  
**Uso:** Perfectas para **im√°genes** (CNN), **texto** (Transformers) y **series temporales** (RNN/LSTM), especialmente con **grandes vol√∫menes de datos no estructurados**.  
**Ventajas:** Muy poderosas para datos complejos.  
**Limitaciones:** Requieren **mucha data** y **computaci√≥n**, y tienen **menor interpretabilidad**.  

---

## Autoenconder  {-}  

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_01_Autoenconder.html" style="color: blue;">
  Autoenconder en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Autoenconder.png"))
```


Un **Autoencoder** es un tipo de **red neuronal artificial** dise√±ado para aprender una **representaci√≥n (o codificaci√≥n) eficiente y comprimida** de los datos de entrada, sin supervisi√≥n humana. Su objetivo principal es la **reducci√≥n de dimensionalidad** o el **aprendizaje de caracter√≠sticas**, lo que lo hace √∫til para tareas como la detecci√≥n de anomal√≠as, la denoising de im√°genes, o la generaci√≥n de datos.

La arquitectura b√°sica de un Autoencoder se compone de dos partes principales:

1.  **Encoder (Codificador):** Esta parte de la red toma los datos de entrada y los transforma en una representaci√≥n de menor dimensi√≥n, a menudo llamada **c√≥digo, representaci√≥n latente, o cuello de botella (bottleneck)**. Es decir, comprime la informaci√≥n esencial de la entrada.
2.  **Decoder (Decodificador):** Esta parte toma la representaci√≥n comprimida (el c√≥digo) del encoder y la reconstruye de nuevo a la dimensi√≥n original de los datos de entrada.

El Autoencoder se entrena para **minimizar la diferencia entre la entrada original y su reconstrucci√≥n** generada por el decoder. Esta diferencia se mide a trav√©s de una **funci√≥n de p√©rdida de reconstrucci√≥n** (como el error cuadr√°tico medio para datos continuos o la entrop√≠a cruzada para datos binarios). Al forzar a la red a reconstruir su propia entrada a partir de una representaci√≥n comprimida, el Autoencoder aprende las caracter√≠sticas m√°s salientes y √∫tiles de los datos de forma no supervisada.

Existen varias variantes de Autoencoders, como los **Autoencoders Denoising** (que aprenden a reconstruir datos limpios a partir de datos con ruido), los **Autoencoders Variacionales (VAEs)** (que aprenden una distribuci√≥n probabil√≠stica de la representaci√≥n latente, √∫tiles para la generaci√≥n de datos), y los **Autoencoders Convolucionales** (que usan capas convolucionales, ideales para im√°genes).


**Aprendizaje Global vs. Local:**

Un Autoencoder se considera principalmente un modelo de **aprendizaje global**, aunque con una perspectiva √∫nica debido a su naturaleza de compresi√≥n y reconstrucci√≥n.

* **Aspecto Global:** Un Autoencoder aprende una **transformaci√≥n global** de los datos. El encoder aprende a mapear todo el espacio de entrada a un espacio de representaci√≥n latente, y el decoder aprende a mapear ese espacio latente de vuelta al espacio de salida. Las ponderaciones y sesgos de la red se ajustan para encontrar esta transformaci√≥n que funciona de manera √≥ptima para todo el conjunto de datos de entrenamiento, permitiendo la reconstrucci√≥n m√°s fiel posible en general. La **funci√≥n de p√©rdida de reconstrucci√≥n** se minimiza a nivel de todo el conjunto de datos, no solo en vecindarios espec√≠ficos.

* **Representaci√≥n Local vs. Reconstrucci√≥n Global:** Aunque el objetivo final es una reconstrucci√≥n global de la entrada, la **representaci√≥n latente (el c√≥digo)** puede verse como una forma de capturar **caracter√≠sticas o patrones importantes** que, en cierto sentido, resumen la informaci√≥n "local" o particular de cada instancia de datos de una manera comprimida. Sin embargo, la forma en que estas caracter√≠sticas se aprenden y se utilizan para la reconstrucci√≥n se rige por un conjunto global de par√°metros de la red. No se entrena un modelo separado para cada vecindario de datos, sino una √∫nica red que aprende una funci√≥n de mapeo para todo el dominio.

En resumen, el Autoencoder aprende una representaci√≥n eficiente y una capacidad de reconstrucci√≥n que se aplica de manera consistente a todos los datos, lo que lo clasifica como un modelo de aprendizaje global que busca una soluci√≥n unificada para el problema de la codificaci√≥n y decodificaci√≥n de datos. 

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (aprendizaje no supervisado)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (o categ√≥ricas codificadas)",
  "‚úÖ Captura relaciones complejas y no lineales",
  "‚ùå No aplica (no es un modelo de regresi√≥n)",
  "‚ùå No aplica (no hay errores residuales)",
  "‚ùå No aplica",
  "‚úÖ S√≠, pueden afectar la reconstrucci√≥n",
  "‚úÖ Puede ayudar a reducir efectos de multicolinealidad",
  "‚ö†Ô∏è Baja interpretabilidad (representaciones latentes)",
  "‚ö†Ô∏è Lento en entrenamiento, especialmente con muchas capas o datos",
  "‚ö†Ô∏è Se puede validar con reconstrucci√≥n y autoevaluaci√≥n",
  "‚ùå Datos con mucha dispersi√≥n o sin estructura latente clara"
)

detalles <- c(
  "Red neuronal no supervisada que aprende a codificar y decodificar los datos para reducir dimensionalidad o detectar anomal√≠as.",
  "No predice una variable externa, sino que reproduce la entrada como salida.",
  "Requiere variables num√©ricas (o una codificaci√≥n previa en caso de categ√≥ricas).",
  "Es capaz de capturar estructuras complejas y no lineales al comprimir los datos.",
  "No tiene residuos como un modelo cl√°sico, pero s√≠ errores de reconstrucci√≥n.",
  "No modela errores independientes, ya que no es un modelo predictivo tradicional.",
  "Tampoco se eval√∫a homoscedasticidad, ya que no hay predicci√≥n como tal.",
  "Outliers distorsionan el entrenamiento, especialmente si no se normaliza.",
  "Ayuda a eliminar redundancias en los datos si est√°n correlacionados.",
  "Las capas internas (representaciones) no son directamente interpretables.",
  "Requiere entrenamiento con varias iteraciones y puede tardar con arquitecturas grandes.",
  "Se eval√∫a con p√©rdida de reconstrucci√≥n o aplicando validaci√≥n cruzada si se integra en modelos supervisados.",
  "Pierde eficacia si los datos no tienen una estructura latente √∫til para codificar."
)

tabla_autoencoder <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_autoencoder %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir autoencoder",
             subtitle = "Autoenconder")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Back - Propagation  {-}  

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_02_Back---Propagation.html" style="color: blue;">
  Back - Propagation en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Back - Propagation.png"))
```


**Back-Propagation (Retropropagaci√≥n)** es el algoritmo fundamental de entrenamiento utilizado para ajustar los pesos de las **redes neuronales artificiales multicapa (MLP)**. La idea central de Back-Propagation es calcular la **contribuci√≥n de cada peso al error global de la red** y luego ajustar esos pesos para reducir dicho error, propagando la informaci√≥n del error "hacia atr√°s" desde la capa de salida hasta la capa de entrada.

A diferencia del Perceptron, que solo puede aprender patrones linealmente separables, Back-Propagation permite entrenar redes neuronales profundas con m√∫ltiples capas ocultas y funciones de activaci√≥n no lineales, lo que les permite modelar relaciones complejas y no lineales en los datos.

El funcionamiento de Back-Propagation se divide en dos fases principales que se repiten iterativamente:

1.  **Fase de Propagaci√≥n hacia Adelante (Forward Pass):**
    * Las entradas se pasan a trav√©s de la red, desde la capa de entrada, a trav√©s de las capas ocultas, hasta la capa de salida.
    * En cada neurona, se calcula la suma ponderada de sus entradas (incluido el sesgo) y se aplica la funci√≥n de activaci√≥n (ej. sigmoide, tanh, ReLU) para producir la salida de esa neurona.
    * La salida final de la red se compara con el valor objetivo real para calcular el **error global** (o "costo") de la red, utilizando una funci√≥n de p√©rdida (ej. error cuadr√°tico medio para regresi√≥n, entrop√≠a cruzada para clasificaci√≥n).

2.  **Fase de Retropropagaci√≥n (Backward Pass):**
    * El error global se propaga **hacia atr√°s** desde la capa de salida, a trav√©s de las capas ocultas, hasta la capa de entrada.
    * En cada capa, se calcula el **gradiente** del error con respecto a los pesos de las conexiones de esa capa. Esto implica el uso de la **regla de la cadena** del c√°lculo diferencial para determinar cu√°nto contribuye cada peso al error final.
    * Una vez calculados los gradientes, los pesos de la red se **actualizan** en la direcci√≥n opuesta al gradiente (es decir, en la direcci√≥n de mayor descenso) para reducir el error. Esta actualizaci√≥n se realiza con una **tasa de aprendizaje** que controla el tama√±o del paso.
    $$w_{ij}^{\text{nuevo}} = w_{ij}^{\text{anterior}} - \alpha \cdot \frac{\partial E}{\partial w_{ij}}$$
    Donde $E$ es el error, $w_{ij}$ es el peso de la conexi√≥n entre la neurona $i$ y la neurona $j$, y $\alpha$ es la tasa de aprendizaje.

En el contexto del **aprendizaje global vs. local**, Back-Propagation es el coraz√≥n del entrenamiento de sistemas de **aprendizaje global** por excelencia (las redes neuronales multicapa). La red neuronal busca aprender una **aproximaci√≥n de funci√≥n global** que mapee las entradas a las salidas, minimizando el error en todo el conjunto de datos. Si los datos no se distribuyen linealmente, Back-Propagation permite que la red aprenda relaciones no lineales complejas a trav√©s de sus m√∫ltiples capas y funciones de activaci√≥n no lineales. A diferencia de LOESS o los m√©todos de regresi√≥n ponderada localmente, Back-Propagation no divide expl√≠citamente el problema en m√∫ltiples problemas locales independientes para minimizar funciones de costo locales. En cambio, busca minimizar una funci√≥n de p√©rdida **global** para toda la red. Sin embargo, su capacidad para ajustar un gran n√∫mero de par√°metros (pesos) le permite construir representaciones internas de los datos que pueden ser incre√≠blemente flexibles y adaptables, superando la limitaci√≥n de que "a veces ning√∫n valor de par√°metro [en un modelo simple] puede proporcionar una aproximaci√≥n suficientemente buena". La retropropagaci√≥n es lo que permiti√≥ a las redes neuronales convertirse en poderosas herramientas de aprendizaje autom√°tico.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas (requiere normalizar), Categ√≥ricas como dummies",
  "‚úÖ Captura relaciones no lineales profundas",
  "‚ùå No requiere",
  "‚úÖ Deseable, aunque no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Elevado (puede requerir robustez ante valores extremos)",
  "‚ö†Ô∏è Puede ralentizar convergencia si es muy alta",
  "‚ö†Ô∏è Baja (modelo de ‚Äúcaja negra‚Äù)",
  "‚ö†Ô∏è Depende de la arquitectura y tama√±o del dataset",
  "‚úÖ Recomendable para ajustar tasas de aprendizaje, capas y neuronas",
  "‚ùå No conviene con datos muy peque√±os o alta dimensionalidad sin regularizar"
)

detalles <- c(
  "Algoritmo para entrenar redes neuronales multicapa ajustando pesos por retropropagaci√≥n del error.",
  "En clasificaci√≥n usa softmax o sigmoide en salida; en regresi√≥n, capa lineal para valor continuo.",
  "Debe escalarse cada caracter√≠stica; las categ√≥ricas transformarse a variables indicadoras antes de entrenar.",
  "Aprende funciones arbitrariamente complejas activando m√∫ltiples capas ocultas con funciones no lineales.",
  "No impone distribuci√≥n espec√≠fica en errores, se optimiza v√≠a descenso de gradiente.",
  "Mejor si las observaciones son independientes; sensible a secuencias sin ajustes espec√≠ficos.",
  "No requiere varianza constante, ya que los pesos se ajustan adaptativamente durante el entrenamiento.",
  "Valores extremos pueden causar activaciones saturadas (vanishing/exploding gradients) si no se manejan.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; Batch Normalization ayuda a mitigar.",
  "Dif√≠cil interpretar cada peso individual; se usan t√©cnicas como LIME o SHAP para explicar decisiones.",
  "El tiempo crece con n√∫mero de capas, neuronas y epochs; GPUs aceleran el proceso.",
  "Cross-validation (o k-fold) ayuda a elegir n√∫mero de capas, neuronas por capa, tasa de aprendizaje y regularizaci√≥n.",
  "No funciona bien con datasets peque√±os (overfitting f√°cil) o ruido elevado sin t√©cnicas de regularizaci√≥n."
)

tabla_backprop <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_backprop %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir back-propagation",
             subtitle = "Back - Propagation")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Convolutional Neural Network (CNN)  {-}   

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_03_CNN.html" style="color: blue;">
  Convolutional Neural Network (CNN) en R
</a>



```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/CNN.png"))
```

**Convolutional Neural Networks (CNNs)**, tambi√©n conocidas como **ConvNets**, son una clase especializada de **redes neuronales profundas** que han demostrado ser excepcionalmente efectivas en tareas de **visi√≥n por computadora** (como clasificaci√≥n de im√°genes, detecci√≥n de objetos, reconocimiento facial) y, m√°s recientemente, en procesamiento de lenguaje natural. La idea fundamental de una CNN es imitar el funcionamiento del c√≥rtex visual en el cerebro humano, utilizando **capas de convoluci√≥n** para detectar autom√°ticamente patrones y caracter√≠sticas jer√°rquicas directamente de los datos de entrada sin necesidad de una extracci√≥n manual de caracter√≠sticas.

A diferencia de los Multilayer Perceptrons (MLPs) que conectan cada neurona de una capa con cada neurona de la siguiente capa (lo que resulta en una enorme cantidad de par√°metros para datos de alta dimensi√≥n como im√°genes), las CNNs aprovechan tres ideas arquitect√≥nicas clave:

1.  **Capas de Convoluci√≥n:** Estas capas aplican un peque√±o conjunto de **filtros (kernels)** a la entrada (ej., una imagen). Cada filtro "se desliza" por la entrada (operaci√≥n de convoluci√≥n) y calcula un producto punto entre sus valores y los valores de la regi√≥n de la entrada que est√° cubriendo. Esto genera un **mapa de caracter√≠sticas** que resalta la presencia de patrones espec√≠ficos (bordes, texturas, formas) en diferentes ubicaciones de la entrada. La ventaja es que los mismos filtros se aplican en m√∫ltiples ubicaciones, lo que reduce dr√°sticamente el n√∫mero de par√°metros y captura la **localidad** de los patrones y la **invarianza traslacional**.
2.  **Capas de Pooling (Submuestreo):** Estas capas se insertan peri√≥dicamente entre las capas convolucionales. Su funci√≥n es reducir la dimensionalidad espacial de los mapas de caracter√≠sticas (ej., reduciendo el n√∫mero de p√≠xeles), lo que ayuda a hacer que el modelo sea m√°s robusto a peque√±as variaciones o distorsiones en la posici√≥n de las caracter√≠sticas. Las operaciones comunes son el **max pooling** (tomar el valor m√°ximo de una regi√≥n) o el **average pooling** (tomar el promedio).
3.  **Capas Totalmente Conectadas (Dense):** Despu√©s de varias capas convolucionales y de pooling, los mapas de caracter√≠sticas finales se aplanan en un vector y se conectan a una o m√°s capas totalmente conectadas (similares a las de un MLP). Estas capas finales realizan la clasificaci√≥n o regresi√≥n bas√°ndose en las caracter√≠sticas de alto nivel extra√≠das por las capas anteriores.

El entrenamiento de una CNN se realiza utilizando el algoritmo de **Back-Propagation** y descenso de gradiente (con sus variantes como SGD, Adam, etc.), ajustando los pesos de los filtros y las conexiones de las capas densas para minimizar una funci√≥n de p√©rdida.

En el contexto del **aprendizaje global vs. local**, las CNNs son un ejemplo sobresaliente de un sistema de **aprendizaje global** que, en sus capas iniciales, se beneficia de la detecci√≥n de patrones **locales**. Cada filtro de convoluci√≥n aprende a detectar un patr√≥n local espec√≠fico (un borde vertical, una esquina, etc.) que se repite en diferentes partes de la imagen (lo que es una forma de "regresi√≥n ponderada localmente" en el sentido de que el filtro "aplica" su conocimiento local a diferentes ventanas de entrada). Sin embargo, la combinaci√≥n jer√°rquica de m√∫ltiples capas convolucionales y de pooling, seguida de capas totalmente conectadas, permite que la red construya representaciones cada vez m√°s abstractas y globales del contenido de la imagen. Esto significa que si los datos no se distribuyen linealmente, las CNNs pueden aprender a modelar relaciones extremadamente complejas y no lineales al componer caracter√≠sticas locales en representaciones globales. La arquitectura de CNNs resuelve la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos m√°s simples al permitir que la red aprenda caracter√≠sticas relevantes de forma autom√°tica y jer√°rquica, adapt√°ndose a las complejidades inherentes de datos como im√°genes y videos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Im√°genes (matrices de p√≠xeles) y datos con estructura espacial",
  "‚úÖ Captura relaciones locales y espaciales mediante filtros convolucionales",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚úÖ Deseable, aunque no obligatorio (mejor si instancias son independientes)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderadamente (artefactos o ruido en im√°genes puede afectar)",
  "‚ö†Ô∏è No se eval√∫a colinealidad de predictores, maneja correlaciones espaciales",
  "‚ö†Ô∏è Baja (modelo tipo ‚Äòcaja negra‚Äô, usar t√©cnicas como Grad-CAM para interpretaci√≥n)",
  "‚ö†Ô∏è Lento sin GPU, entrenamiento intensivo en c√≥mputo",
  "‚úÖ Robusto si se aplica k-fold o validaci√≥n en conjunto de im√°genes",
  "‚ùå No funciona bien con pocos datos o sin estructura espacial significativa"
)

detalles <- c(
  "Red neuronal profunda especializada en procesar datos con estructura de grilla (ej. im√°genes).",
  "En clasificaci√≥n utiliza softmax; en regresi√≥n, capa lineal para valores continuos.",
  "Requiere tensores de entrada (canales, altura, ancho); funciones de preprocesamiento para im√°genes.",
  "Filtros convolucionales extraen caracter√≠sticas locales, max-pooling disminuye dimensionalidad manteniendo informaci√≥n relevante.",
  "No impone ninguna distribuci√≥n en los errores, optimiza funci√≥n de p√©rdida directamente.",
  "Ideal si las muestras son independientes; sensible a dependencias temporales o espaciales no modeladas.",
  "No requiere varianza constante pues se basa en convoluciones y pooling, no en un modelo param√©trico de error.",
  "Ruido o artefactos en p√≠xeles pueden alterar el aprendizaje de filtros, es importante usar t√©cnicas de regularizaci√≥n.",
  "La red aprende filtros que capturan patrones locales, por lo que no es necesario verificar colinealidad expl√≠citamente.",
  "Dif√≠cil de interpretar cada filtro y capa; se utilizan mapas de activaci√≥n o Grad-CAM para entender qu√© regiones influyen en la predicci√≥n.",
  "El entrenamiento con m√∫ltiples capas convolucionales y millones de par√°metros es intensivo en GPU/TPU.",
  "Validaci√≥n cruzada o separaci√≥n de conjuntos (train/validation/test) ayuda a evitar overfitting.",
  "No es apropiado para datasets muy peque√±os sin aumentar datos (data augmentation) o sin informaci√≥n espacial clara."
)

tabla_cnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles) 

tabla_cnn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CNN",
             subtitle = "Convolutional Neural Network (CNN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```


## Hopfield Network  {-}    

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_04_Hopfield-Network.html" style="color: blue;">
  Hopfield Network en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Hopfield Network.png"))
```

La **Red de Hopfield** es un tipo de **red neuronal recurrente** o **red neuronal con memoria asociativa**, propuesta por John Hopfield en 1982. A diferencia de las redes neuronales de propagaci√≥n hacia adelante (como el Perceptr√≥n o las MLP entrenadas con Back-Propagation) que se utilizan para el mapeo de entrada a salida, la idea fundamental de una Red de Hopfield es funcionar como un **sistema de memoria asociativa** y un **sistema din√°mico que converge a estados estables**. Su objetivo principal es almacenar y recuperar patrones binarios, as√≠ como resolver problemas de optimizaci√≥n.

El funcionamiento de una Red de Hopfield se basa en los siguientes principios:

1.  **Neuronas Binarias:** La red consta de un conjunto de neuronas (nodos) que son **binarias**, lo que significa que solo pueden tomar dos estados posibles, generalmente $1$ o $-1$.
2.  **Conexiones Ponderadas:** Cada neurona est√° conectada a todas las dem√°s neuronas (excepto a s√≠ misma) mediante **conexiones sim√©tricas y ponderadas**. Los pesos de estas conexiones se calculan de manera que los patrones que se quieren "memorizar" se conviertan en **estados de energ√≠a m√≠nima** de la red. La regla de aprendizaje m√°s com√∫n para establecer estos pesos es la **regla de Hebb**: si dos neuronas se activan juntas para un patr√≥n, el peso entre ellas se incrementa.
3.  **Din√°mica de Activaci√≥n:** Cuando se presenta una entrada a la red (que puede ser un patr√≥n ruidoso o incompleto), las neuronas se actualizan de forma as√≠ncrona o s√≠ncrona. La activaci√≥n de cada neurona se recalcula en funci√≥n de la suma ponderada de las activaciones de las otras neuronas a las que est√° conectada.
    $$S_i = \text{sgn}\left(\sum_{j \neq i} W_{ij} S_j\right)$$
    Donde $S_i$ es el estado de la neurona $i$, $W_{ij}$ es el peso entre la neurona $i$ y $j$, y $\text{sgn}$ es la funci√≥n signo.
4.  **Convergencia a Estados Estables:** Este proceso de actualizaci√≥n se repite hasta que la red alcanza un **estado estable** (un "atractor"), donde las activaciones de las neuronas ya no cambian. Si la red ha sido entrenada correctamente, este estado estable corresponder√° al patr√≥n memorizado m√°s cercano a la entrada inicial (memoria asociativa).
5.  **Funci√≥n de Energ√≠a:** La estabilidad de la red se puede describir mediante una **funci√≥n de energ√≠a de Lyapunov**. Durante la din√°mica de la red, la energ√≠a de la red siempre disminuye hasta que se alcanza un m√≠nimo local (un patr√≥n memorizado).

En el contexto del **aprendizaje global vs. local**, la Red de Hopfield es un sistema de **aprendizaje global** que exhibe un comportamiento de **optimizaci√≥n local**. La regla de aprendizaje (como la regla de Hebb) establece los pesos de todas las conexiones para que los patrones deseados se conviertan en m√≠nimos de energ√≠a en todo el espacio de estados. Es decir, se busca una configuraci√≥n global de pesos para memorizar un conjunto de patrones. Sin embargo, la **din√°mica de recuperaci√≥n** de la red es intr√≠nsecamente un proceso de **convergencia local**: dada una entrada inicial, la red "cae" en el m√≠nimo de energ√≠a m√°s cercano, que corresponde al patr√≥n memorizado.

Si los datos no se distribuyen linealmente, la Red de Hopfield no aplica el concepto de regresi√≥n (o clasificaci√≥n) de la misma manera que LOESS o los √°rboles de decisi√≥n. En cambio, funciona como un sistema de **memoria y recuperaci√≥n de patrones** no lineales. Puede almacenar y recuperar patrones complejos que no son linealmente separables. La red busca una soluci√≥n global (un conjunto de pesos) para almacenar los patrones, y luego, en la recuperaci√≥n, utiliza un proceso de "b√∫squeda" local en el espacio de energ√≠a para converger a un patr√≥n memorizado. Esto aborda la idea de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un modelo de regresi√≥n lineal, ya que la Red de Hopfield no es un modelo de regresi√≥n en s√≠, sino un sistema din√°mico que encuentra estados de equilibrio. Su capacidad para manejar patrones ruidosos o incompletos para recuperar el patr√≥n completo es una de sus principales fortalezas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è No es supervisado en el sentido cl√°sico",
  "‚ö†Ô∏è No hay ‚Äútarget‚Äù continuo o categ√≥rico (almacenamiento de patrones)",
  "‚úÖ Variables binarias o valores discretizados (patrones binarios)",
  "‚úÖ Correlaciona patrones con pesos sim√©tricos entre neuronas",
  "‚ùå No aplica (no hay residuos param√©tricos)",
  "‚úÖ Deseable, aunque no obligatorio (los estados deben actualizarse sin bucles no deseados)",
  "‚ùå No aplica (no modela varianza de errores)",
  "‚ö†Ô∏è Muy sensible (un solo nodo saturado puede distorsionar la red)",
  "‚ö†Ô∏è Puede verse afectado si los patrones de entrenamiento tienen redundancia fuerte",
  "‚ö†Ô∏è Media (la din√°mica de atra√ß√£o es interpretable, pero las conexiones pueden ser complejas)",
  "‚ö†Ô∏è Moderada (dependiendo del n√∫mero de neuronas y estados s√≠ncronos/as√≠ncronos)",
  "‚ùå No se usa tradicionalmente, pero se puede validar estabilidad de memorias con pruebas de convergencia",
  "‚ö†Ô∏è No sirve si los patrones no son binarios o si hay alto ruido en entradas asociativas"
)

detalles <- c(
  "Red neuronal recurrente para recuperaci√≥n asociativa de patrones, no requiere pares X‚Üíy.",
  "No predice una variable externa, recupera patrones completos a partir de entradas parciales o ruidosas.",
  "Requiere que cada elemento del patr√≥n sea binario (¬±1) o est√© discretizado; las variables continuas deben binarizarse.",
  "Los pesos sim√©tricos se calculan por Hebb (p. ej. W = Œ£ p·µ¢ p·µ¢·µÄ), sin umbral expl√≠cito para relaciones lineales.",
  "No hay un t√©rmino de error param√©trico; la din√°mica sigue la funci√≥n de energ√≠a, no un residuo gaussiano.",
  "Es mejor si las actualizaciones de estado son independientes o s√≠ncronas; la dependencia temporal puede generar oscilaciones.",
  "No modela varianza de error, pues busca minimizar energ√≠a, no error cuadr√°tico.",
  "Patrones fuera del rango binario pueden causar saturaci√≥n o estados inestables.",
  "Patrones muy similares (colineales) pueden interferir en recuperaciones correctas (atractores vecinos).",
  "La din√°mica de convergencia hacia un estado estable (atractor) se puede visualizar, pero la topolog√≠a de pesos puede no ser transparente.",
  "Simulaci√≥n de din√°micas es razonable para tama√±os moderados (‚â§1000 neuronas); grandes redes requieren optimizaci√≥n paralela.",
  "No se usa CV tradicional; se analiza la robustez de memorias variando inicializaci√≥n o agregando ruido.",
  "No apto si los datos no pueden discretizarse en patrones binarios, o si se requieren m√∫ltiples clases de salida simult√°neas."
)

tabla_hopfield <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_hopfield %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir hopfield network",
             subtitle = "Hopfield Network")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  

```

## Gated Recurrent Unit (GRU) {-}   

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_12_GRU.html" style="color: blue;">
  Gated Recurrent Unit (GRU) en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/GRU.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_gru <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_gru <- c(
  "‚úÖ Supervisado",
  "‚úÖ Num√©rica o Categ√≥rica (secuencial)",
  "‚úÖ Num√©ricas y categ√≥ricas (secuenciales)",
  "‚úÖ Muy compleja (dependencias temporales)",
  "‚ùå No es requisito",
  "‚ùå No aplica (dependencia temporal)",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (datos ruidosos)",
  "‚úÖ Maneja bien (caracter√≠sticas dentro de secuencias)",
  "‚ùå Muy baja (caja negra)",
  "‚úÖ Moderada a Alta",
  "‚úÖ Compatible y recomendada",
  "‚ùå Series muy cortas o sin dependencias"
)

detalles_gru <- c(
  "Red neuronal recurrente (RNN) utilizada para procesar secuencias de datos. Puede ser para regresi√≥n (predicci√≥n de valores futuros) o clasificaci√≥n (etiquetado de secuencias).",
  "La variable respuesta puede ser num√©rica (ej., precio de acciones, temperatura) o categ√≥rica (ej., clase de una secuencia de audio), siempre en un contexto secuencial.",
  "Las variables predictoras pueden ser de cualquier tipo, pero se espera que formen una secuencia (ej., series de tiempo, texto, audio). Requieren pre-procesamiento adecuado (ej., embedding para texto).",
  "Captura dependencias temporales y patrones no lineales complejos a lo largo de una secuencia. Usa 'compuertas' (gates) para controlar el flujo de informaci√≥n y resolver el problema del 'gradiente que se desvanece'.",
  "No asume ninguna distribuci√≥n espec√≠fica para los residuos. Se enfoca en la capacidad predictiva sobre secuencias.",
  "A diferencia de muchos modelos, los GRU est√°n dise√±ados para modelar la **dependencia temporal** entre observaciones. Las observaciones *dentro de una secuencia* no son independientes.",
  "No asume homoscedasticidad. Su capacidad de modelado es flexible y puede capturar heterogeneidad en la varianza a lo largo del tiempo.",
  "S√≠, las GRU pueden ser sensibles al ruido y a los valores at√≠picos, especialmente si estos afectan las dependencias temporales cruciales. Requiere una limpieza de datos adecuada.",
  "Los GRU pueden manejar caracter√≠sticas correlacionadas dentro de las secuencias, ya que aprenden a extraer caracter√≠sticas relevantes a lo largo del tiempo.",
  "Como todas las redes neuronales profundas, los GRU son modelos de 'caja negra'. La interpretabilidad de c√≥mo se toman las decisiones es muy baja.",
  "Moderadamente eficiente. M√°s r√°pido que las LSTMs debido a su arquitectura m√°s simple (menos compuertas), pero m√°s lento que modelos no recurrentes. Requiere GPUs para un entrenamiento eficiente en grandes datasets.",
  "Esencial para evaluar la capacidad de generalizaci√≥n del modelo en datos no vistos y para la sintonizaci√≥n de hiperpar√°metros. Com√∫nmente se usa validaci√≥n cruzada basada en el tiempo.",
  "No funcionar√° bien con secuencias muy cortas donde no hay dependencias temporales significativas que aprender. Tambi√©n, si las caracter√≠sticas no tienen una estructura secuencial inherente, un GRU podr√≠a ser una sobreingenier√≠a."
)

tabla_gru <- data.frame(Criterio = criterios_gru, Aplica = aplica_gru, Detalles = detalles_gru)

tabla_gru %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Gated Recurrent Unit (GRU)",
    subtitle = "Red Neuronal Recurrente para Secuencias"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Generative Adversarial Networks (GANs) {-}  

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_05_GANs.html" style="color: blue;">
  Generative Adversarial Networks (GANs) en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/GANs.png"))
```


```{r, echo = FALSE}
library(gt)

criterios_gan <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_gan <- c(
  "‚úÖ Generativo (No Supervisado/Semi-supervisado)",
  "‚ùå No aplica directamente (genera datos)",
  "‚ùå No aplica directamente (genera a partir de ruido)",
  "‚úÖ Muy compleja (captura distribuciones subyacentes)",
  "‚ùå No es requisito",
  "‚ùå No aplica directamente",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (inestabilidad, colapso de modo)",
  "‚ùå No aplica directamente",
  "‚ùå Muy baja (caja negra compleja)",
  "‚ö†Ô∏è Moderada a Baja (entrenamiento intensivo)",
  "‚ùå No aplica directamente (evaluaci√≥n cualitativa/m√©trica)",
  "‚ùå Datos insuficientes, falta de diversidad, inestabilidad"
)

detalles_gan <- c(
  "Modelo generativo que aprende a crear nuevos datos con las mismas caracter√≠sticas que los datos de entrenamiento. Consiste en dos redes: un Generador y un Discriminador, que compiten entre s√≠.",
  "Las GANs no tienen una 'variable respuesta' en el sentido tradicional. Su objetivo es generar *nuevas muestras de datos* (ej., im√°genes, texto, audio) que imiten la distribuci√≥n de los datos reales.",
  "Las GANs no toman 'variables predictoras' directas del dataset. El Generador toma como entrada un vector de ruido aleatorio (ruido latente) para crear nuevas muestras de datos.",
  "Capturan relaciones intrincadas y de alta dimensi√≥n en los datos, aprendiendo la distribuci√≥n de probabilidad subyacente de las muestras reales para replicarla.",
  "No hacen suposiciones sobre la normalidad de los residuos, ya que no predicen un valor num√©rico ni modelan errores de predicci√≥n.",
  "Aunque los datos de entrenamiento individuales son idealmente independientes, la interacci√≥n entre el Generador y el Discriminador durante el entrenamiento es un proceso complejo y din√°mico, no de errores independientes.",
  "No asume homoscedasticidad. Su objetivo es aprender la variabilidad inherente en los datos reales para generar muestras diversas.",
  "Las GANs pueden ser sensibles a datos ruidosos o an√≥malos, lo que puede llevar a inestabilidades en el entrenamiento o a un 'colapso de modo' (generar poca diversidad de muestras).",
  "No aplica la multicolinealidad de predictores de la misma manera que en modelos supervisados, ya que no hay 'predictores' en el sentido usual. El Generador aprende correlaciones y estructuras en el espacio latente.",
  "La interpretabilidad de c√≥mo las GANs generan datos es extremadamente baja. Son modelos complejos de 'caja negra'.",
  "El entrenamiento de GANs es computacionalmente muy intensivo y puede ser lento, especialmente para modelos grandes o datos de alta resoluci√≥n. Requiere GPUs potentes y a menudo mucho tiempo.",
  "La validaci√≥n cruzada no se aplica directamente para evaluar GANs. Su rendimiento se mide a menudo cualitativamente (inspecci√≥n visual de las muestras generadas) y/o mediante m√©tricas como Inception Score (IS), Fr√©chet Inception Distance (FID) o Generative Adversarial Metric (GAM).",
  "Las GANs no funcionan bien si: 1) el tama√±o del conjunto de datos es muy peque√±o, 2) hay poca diversidad en los datos de entrenamiento, 3) el entrenamiento es inestable y conduce a un 'colapso de modo' (cuando el generador produce una gama muy limitada de salidas). Adem√°s, pueden ser dif√≠ciles de entrenar."
)

tabla_gan <- data.frame(Criterio = criterios_gan, Aplica = aplica_gan, Detalles = detalles_gan)

tabla_gan %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Generative Adversarial Networks (GANs)",
    subtitle = "Redes Generativas Antag√≥nicas"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```

## Long Short-Term Memory (LSTM) {-}  

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_13_LSTM.html" style="color: blue;">
  Long Short-Term Memory (LSTM) en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/LSTM.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_lstm <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_lstm <- c(
  "‚úÖ Supervisado",
  "‚úÖ Num√©rica o Categ√≥rica (secuencial)",
  "‚úÖ Num√©ricas y categ√≥ricas (secuenciales)",
  "‚úÖ Muy compleja (dependencias temporales largas)",
  "‚ùå No es requisito",
  "‚ùå No aplica (dependencia temporal)",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (datos ruidosos)",
  "‚úÖ Maneja bien (caracter√≠sticas dentro de secuencias)",
  "‚ùå Muy baja (caja negra)",
  "‚ö†Ô∏è Moderada (m√°s lento que GRU)",
  "‚úÖ Compatible y recomendada",
  "‚ùå Series muy cortas o sin dependencias"
)

detalles_lstm <- c(
  "Red neuronal recurrente (RNN) avanzada utilizada para procesar y aprender de secuencias de datos. Adecuada para tareas de regresi√≥n, clasificaci√≥n y generaci√≥n de secuencias.",
  "La variable respuesta puede ser num√©rica continua (ej., predicci√≥n de series de tiempo) o categ√≥rica (ej., clasificaci√≥n de texto), siempre en un contexto de datos secuenciales.",
  "Las variables predictoras pueden ser de cualquier tipo, pero deben estar estructuradas como secuencias (ej., palabras en una frase, valores en una serie temporal). Requieren pre-procesamiento como embeddings para datos categ√≥ricos.",
  "Especialmente dise√±ada para capturar y recordar dependencias a largo plazo en los datos secuenciales, gracias a su arquitectura con 'celdas de memoria' y 'compuertas' (input, forget, output gates).",
  "No asume ninguna distribuci√≥n espec√≠fica para los errores o residuos. Es un modelo no param√©trico que aprende patrones complejos de datos.",
  "A diferencia de modelos estad√≠sticos, las LSTMs est√°n dise√±adas para modelar y explotar la **dependencia temporal** entre observaciones dentro de una secuencia. Las observaciones individuales *no* son independientes.",
  "No asume homoscedasticidad. Su flexibilidad le permite aprender patrones donde la varianza puede cambiar a lo largo de la secuencia.",
  "S√≠, al igual que otras redes neuronales, las LSTMs pueden ser sensibles a datos ruidosos y valores at√≠picos que pueden distorsionar los patrones aprendidos, especialmente si ocurren en puntos cr√≠ticos de la secuencia.",
  "Las LSTMs son robustas a la multicolinealidad entre caracter√≠sticas dentro de una secuencia, ya que aprenden a extraer representaciones √∫tiles a partir de las entradas correlacionadas a lo largo del tiempo.",
  "Como modelos de aprendizaje profundo, las LSTMs son 'cajas negras'. Es muy dif√≠cil interpretar directamente c√≥mo las celdas de memoria y compuertas influyen en una predicci√≥n espec√≠fica. Se usan m√©todos de explicabilidad global o local (ej., LRP, atenci√≥n).",
  "Son computacionalmente m√°s intensivas que las GRU debido a su arquitectura m√°s compleja (m√°s compuertas). El entrenamiento en datasets grandes y secuencias largas requiere recursos significativos como GPUs.",
  "Es fundamental para evaluar la capacidad de generalizaci√≥n del modelo y para el ajuste de hiperpar√°metros. Se utilizan t√©cnicas de validaci√≥n cruzada adaptadas a series de tiempo (ej., validaci√≥n progresiva).",
  "No funcionan bien si las secuencias son muy cortas o si no hay dependencias temporales significativas entre los elementos de la secuencia. En estos casos, modelos m√°s simples podr√≠an ser m√°s eficientes y suficientes."
)

tabla_lstm <- data.frame(Criterio = criterios_lstm, Aplica = aplica_lstm, Detalles = detalles_lstm)

tabla_lstm %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Long Short-Term Memory (LSTM)",
    subtitle = "Redes Recurrentes para Dependencias a Largo Plazo"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Multilayer Perceptron (MP)  {-}     

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_06_MP.html" style="color: blue;">
  Multilayer Perceptron (MP) en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/MP.png"))
```

El **Multilayer Perceptron (MLP)**, tambi√©n conocido como **red neuronal de propagaci√≥n hacia adelante cl√°sica**, es un tipo fundamental de **red neuronal artificial** utilizada para una amplia gama de tareas de **aprendizaje supervisado**, incluyendo **clasificaci√≥n** y **regresi√≥n**. La idea fundamental del MLP es extender el concepto del Perceptr√≥n simple al incorporar una o m√°s **capas ocultas** entre la capa de entrada y la capa de salida, y utilizando **funciones de activaci√≥n no lineales** en estas capas. Esta arquitectura de m√∫ltiples capas es lo que le confiere a los MLP su capacidad para aprender y modelar relaciones complejas y no lineales en los datos.

La estructura de un MLP t√≠picamente incluye:

1.  **Capa de Entrada:** Recibe las caracter√≠sticas de entrada del problema.
2.  **Capas Ocultas:** Son una o m√°s capas intermedias donde se realizan c√°lculos complejos. Cada neurona en una capa oculta recibe entradas de la capa anterior, calcula una suma ponderada de estas entradas (m√°s un sesgo), y luego aplica una **funci√≥n de activaci√≥n no lineal** (como la funci√≥n sigmoide, tanh o ReLU) a esta suma. Es la no linealidad de estas funciones de activaci√≥n la que permite al MLP aprender relaciones no lineales.
    $$a_j = f\left(\sum_{i=1}^{n} w_{ij} x_i + b_j\right)$$
    Donde $a_j$ es la activaci√≥n de la neurona $j$, $x_i$ son las entradas de la capa anterior, $w_{ij}$ son los pesos, $b_j$ es el sesgo, y $f$ es la funci√≥n de activaci√≥n no lineal.
3.  **Capa de Salida:** Produce la predicci√≥n final de la red. La funci√≥n de activaci√≥n en esta capa depende del tipo de problema (ej., una funci√≥n lineal para regresi√≥n, softmax para clasificaci√≥n multiclase, o sigmoide para clasificaci√≥n binaria).

El entrenamiento de un MLP se realiza t√≠picamente utilizando el algoritmo de **Back-Propagation**, que ajusta los pesos de la red de manera iterativa para minimizar una funci√≥n de p√©rdida (error) calculada en la capa de salida.

En el contexto del **aprendizaje global vs. local**, el Multilayer Perceptron es el paradigma de un sistema de **aprendizaje global**. La red aprende una **aproximaci√≥n de funci√≥n global** que mapea las entradas a las salidas, buscando minimizar la funci√≥n de p√©rdida en todo el conjunto de datos de entrenamiento. A diferencia de los sistemas de aprendizaje local que dividen expl√≠citamente el problema global en m√∫ltiples problemas m√°s peque√±os, el MLP ajusta todos sus pesos de forma interconectada para aprender una representaci√≥n distribuida de los patrones en los datos. Si los datos no se distribuyen linealmente, el MLP es excepcionalmente capaz de modelar estas relaciones complejas gracias a sus capas ocultas y funciones de activaci√≥n no lineales. Esto aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos lineales o m√°s simples, ya que el MLP puede construir representaciones internas de gran complejidad para aproximar casi cualquier funci√≥n continua. Hoy en d√≠a, los MLP son la base de muchas arquitecturas de "Deep Learning".


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n)",
  "‚úÖ Num√©ricas (normalizar) y categ√≥reas (dummies)",
  "‚úÖ Captura relaciones no lineales profundas",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, aunque no obligatorio",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (puede requerir robustez ante outliers)",
  "‚ö†Ô∏è Afecta la convergencia si es muy alta",
  "‚ö†Ô∏è Baja (modelo tipo 'caja negra')",
  "‚ö†Ô∏è Depende de arquitectura y tama√±o del dataset",
  "‚úÖ Recomendable (k-fold o repeated CV)",
  "‚ùå No conviene con pocos datos o ruido elevado"
)

detalles <- c(
  "Red neuronal con m√∫ltiples capas ocultas y funci√≥n de activaci√≥n no lineal.",
  "Clasificaci√≥n con softmax/sigmoide; regresi√≥n con capa lineal en salida.",
  "Debe escalarse cada caracter√≠stica; las categ√≥ricas convierten a variables indicadoras.",
  "Aprende patrones complejos combinando m√∫ltiples capas y neuronas.",
  "No impone distribuci√≥n espec√≠fica de errores, se optimiza con optimizadores basados en gradiente.",
  "Funciona mejor si las muestras son independientes; sensibles a dependencia temporal sin ajustes.",
  "No requiere varianza constante, ajusta pesos en cada mini-batch o lote.",
  "Outliers pueden causar gradientes explosivos o desaparecidos sin mecanismos de robustez.",
  "Predictores muy correlacionados pueden ralentizar la convergencia; batch normalization ayuda.",
  "Dif√≠cil de interpretar cada peso/neuronas; se usan t√©cnicas como SHAP o LIME para explicaci√≥n.",
  "El tiempo de entrenamiento aumenta con cada capa, neuronas y epochs; GPUs aceleran el proceso.",
  "Crucial para ajustar hiperpar√°metros: n√∫mero de capas, neuronas por capa, tasa de aprendizaje, regularizaci√≥n.",
  "No √∫til para datasets muy peque√±os (sobreajuste) o altamente ruidosos sin regularizaci√≥n."
)

tabla_mp <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_mp %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MP",
             subtitle = "Multilayer Perceptron (MP)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()  
```

## Perceptron  {-}     

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_07_Perceptron.html" style="color: blue;">
  Perceptron en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Perceptron.png"))
```

El **Perceptron** es el algoritmo de **aprendizaje supervisado** m√°s simple y uno de los primeros modelos de **redes neuronales artificiales**, propuesto por Frank Rosenblatt en 1957. Est√° dise√±ado para tareas de **clasificaci√≥n binaria**, es decir, para decidir si una entrada pertenece a una de dos clases posibles. Su idea fundamental es modelar c√≥mo una neurona biol√≥gica podr√≠a tomar decisiones.

El funcionamiento de un Perceptron es bastante directo:

1.  **Entradas y Pesos:** Recibe m√∫ltiples **entradas** (caracter√≠sticas) y a cada entrada se le asigna un **peso**. Estos pesos representan la importancia de cada caracter√≠stica.
2.  **Suma Ponderada:** Las entradas se multiplican por sus respectivos pesos y se suman. A esta suma se le a√±ade un **t√©rmino de sesgo (bias)**.
    $$z = \sum_{i=1}^{n} w_i x_i + b$$
    Donde $x_i$ son las entradas, $w_i$ son los pesos, $b$ es el sesgo, y $n$ es el n√∫mero de entradas.
3.  **Funci√≥n de Activaci√≥n:** El resultado de la suma ponderada ($z$) se pasa a trav√©s de una **funci√≥n de activaci√≥n** (generalmente una funci√≥n escal√≥n o *step function*). Esta funci√≥n decide la salida final, que es 1 si la suma excede un umbral (o 0 si no lo excede). Para el Perceptron original, la salida es binaria.
    $$\text{salida} = \begin{cases} 1 & \text{si } z \geq \text{umbral} \\ 0 & \text{si } z < \text{umbral} \end{cases}$$
4.  **Aprendizaje (Regla de Perceptron):** El Perceptron aprende ajustando sus pesos de forma iterativa. Si la predicci√≥n es incorrecta, los pesos se actualizan para reducir el error en la siguiente iteraci√≥n. La regla de actualizaci√≥n de pesos es:
    $$w_i^{\text{nuevo}} = w_i^{\text{anterior}} + \alpha \cdot (y - \hat{y}) \cdot x_i$$
    Donde $\alpha$ es la tasa de aprendizaje, $y$ es el valor real, y $\hat{y}$ es la predicci√≥n del Perceptron.

En el contexto del **aprendizaje global vs. local**, el Perceptron es un sistema de **aprendizaje global** por naturaleza. Busca encontrar un **hiperplano de separaci√≥n lineal** √∫nico que divida el espacio de caracter√≠sticas en dos regiones. La idea es que, si los datos son **linealmente separables** (es decir, si existe una l√≠nea, plano o hiperplano que puede separar perfectamente las dos clases), el Perceptron est√° garantizado para converger y encontrar esa soluci√≥n.

Sin embargo, precisamente porque busca una soluci√≥n lineal global, si los datos no se distribuyen linealmente (es decir, no son linealmente separables), el Perceptron **no puede encontrar una soluci√≥n convergente** y no puede aprender la relaci√≥n. Esto ilustra la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" cuando se busca una soluci√≥n global r√≠gida. El Perceptron original no puede aplicar el concepto de regresi√≥n ponderada localmente ni adaptarse a complejidades no lineales, a diferencia de modelos posteriores como las redes neuronales multicapa con funciones de activaci√≥n no lineales o los algoritmos de √°rboles de decisi√≥n. A pesar de esta limitaci√≥n, el Perceptron sent√≥ las bases para el desarrollo posterior de redes neuronales m√°s complejas.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas (requiere normalizar), Categ√≥ricas como dummies",
  "‚ö†Ô∏è Aprendizaje lineal: separabilidad lineal requerida",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "‚ùå No se asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers pueden cambiar el hiperplano)",
  "‚ö†Ô∏è Afecta la convergencia si est√° muy alta",
  "‚ö†Ô∏è Baja (modelo b√°sico de una capa sin capas ocultas)",
  "‚úÖ Muy r√°pido para datasets medianos",
  "‚úÖ √ötil para evaluar margen de separaci√≥n",
  "‚ùå No sirve si las clases no son linealmente separables"
)

detalles <- c(
  "Red neuronal de una sola capa que ajusta un hiperplano separador.",
  "Dise√±ado para clasificaci√≥n binaria; no predice valores continuos.",
  "Todas las features deben ser num√©ricas y escaladas; las categ√≥ricas deben convertirse en indicadores.",
  "Busca maximizar el margen de separaci√≥n lineal entre dos clases; no captura no linealidades.",
  "No exige distribuci√≥n normal de errores ya que optimiza con perceptr√≥n simple.",
  "Funciona mejor si las instancias son independientes; sensible a dependencias temporales sin ajuste.",
  "No se basa en varianza de errores; el algoritmo actualiza pesos sin supuestos de varianza.",
  "Los valores extremos cercanos al margen pueden forzar ajustes bruscos de pesos.",
  "La colinealidad puede ralentizar la convergencia, aunque no impide la definici√≥n de hiperplano.",
  "F√°cil de entender: el peso de cada caracter√≠stica indica direcci√≥n del hiperplano.",
  "Entrenamiento r√°pido usando regla de aprendizaje por error; escalable a datos medianos.",
  "Se usa CV para ajustar tasa de aprendizaje y n√∫mero de √©pocas para evitar bajo/sobreajuste.",
  "In√∫til si las clases no se pueden separar linealmente; requiere extensiones (por ejemplo, kernel) para no linealidad."
)

tabla_perceptron <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_perceptron %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir perceptron",
             subtitle = "Perceptron")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Radial Basis Function Network (RBFN)  {-}  

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_08_RBFN.html" style="color: blue;">
  Radial Basis Function Network (RBFN) en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/RBFN.png"))
```

**Radial Basis Function Network (RBFN)** es un tipo de **red neuronal artificial** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**. A diferencia de las redes neuronales multicapa perceptr√≥n tradicionales que utilizan funciones de activaci√≥n sigmoide o ReLU, las RBFN emplean **funciones de base radial** como sus funciones de activaci√≥n en la capa oculta. Su estructura es t√≠picamente m√°s simple que un perceptr√≥n multicapa, consistiendo generalmente en tres capas: una capa de entrada, una capa oculta con neuronas de base radial, y una capa de salida.

La idea fundamental de una RBFN radica en su capacidad para modelar relaciones no lineales al mapear datos de entrada a un espacio de caracter√≠sticas de mayor dimensi√≥n donde pueden ser **linealmente separables** (para clasificaci√≥n) o donde una **funci√≥n lineal** puede aproximar la relaci√≥n (para regresi√≥n). Esto se logra a trav√©s de las neuronas de la capa oculta, cada una de las cuales representa un "centro" en el espacio de caracter√≠sticas.

El funcionamiento de una RBFN implica:

1.  **Capa de Entrada:** Recibe las caracter√≠sticas de entrada.
2.  **Capa Oculta (Neuronas de Base Radial):** Cada neurona en esta capa tiene un **centro** ($c_i$) y un **radio (o desviaci√≥n est√°ndar, $\sigma_i$)**. La funci√≥n de activaci√≥n de estas neuronas (com√∫nmente una **funci√≥n Gaussiana**) calcula la **distancia** entre el vector de entrada ($x$) y el centro de la neurona ($c_i$), y luego aplica la funci√≥n de base radial. Cuanto m√°s cerca est√© la entrada del centro de la neurona, mayor ser√° la activaci√≥n de esa neurona.
    $$\phi_i(x) = \exp\left(-\frac{\|x - c_i\|^2}{2\sigma_i^2}\right)$$
    Donde $\phi_i(x)$ es la salida de la neurona $i$, $\|x - c_i\|$ es la distancia euclidiana entre la entrada $x$ y el centro $c_i$, y $\sigma_i$ es el radio (ancho) de la funci√≥n Gaussiana.
3.  **Capa de Salida:** Las salidas de las neuronas de la capa oculta se combinan linealmente (ponderadas por unos coeficientes, $w_{ij}$) para producir la salida final de la red. Para regresi√≥n, es una suma ponderada; para clasificaci√≥n, a menudo se usa una funci√≥n de activaci√≥n softmax.
    $$y_j = \sum_{i=1}^{M} w_{ij}\phi_i(x)$$
    Donde $y_j$ es la salida $j$, $M$ es el n√∫mero de neuronas ocultas, y $w_{ij}$ son los pesos de la capa de salida.

En el contexto del **aprendizaje global vs. local**, las RBFN son intr√≠nsecamente sistemas de **aprendizaje local**. Cada neurona de la capa oculta es sensible a una **regi√≥n espec√≠fica** del espacio de entrada, definida por su centro y su radio. La red como un todo es una combinaci√≥n de estas respuestas locales. Si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se aplica de forma muy eficaz mediante esta naturaleza de **regresi√≥n ponderada localmente**. Las RBFN pueden aproximar cualquier funci√≥n continua con la suficiente cantidad de neuronas de base radial. Esto aborda directamente la desventaja de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo global, ya que la red puede adaptarse localmente a las caracter√≠sticas de diferentes regiones del espacio de datos. Son particularmente √∫tiles para problemas de aproximaci√≥n de funciones, series de tiempo y reconocimiento de patrones.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas (requiere normalizaci√≥n), Categ√≥ricas como dummies",
  "‚úÖ Captura no linealidades mediante funciones base radiales",
  "‚ùå No requiere supuestos de normalidad",
  "‚úÖ Deseable, pero no obligatorio (mejor si muestras i.i.d.)",
  "‚ùå No asume varianza constante",
  "‚ö†Ô∏è Moderadamente (centros pueden verse alterados por outliers)",
  "‚ö†Ô∏è Puede influir en la selecci√≥n de centros, pero no tan cr√≠tico como en OLS",
  "‚ö†Ô∏è Baja (la capa oculta con RBF es dif√≠cil de interpretar)",
  "‚ö†Ô∏è Moderada (depende de n√∫mero de centros y dimensiones)",
  "‚úÖ Recomendable para ajustar n√∫mero de bases y spread",
  "‚ùå Datos muy grandes o alta dimensionalidad sin reducci√≥n, mucho ruido"
)

detalles <- c(
  "Red neuronal de una capa oculta con funciones radial basis como activaci√≥n.",
  "Para regresi√≥n predice un valor continuo; para clasificaci√≥n usa votaci√≥n o softmax sobre salidas.",
  "Requiere que las caracter√≠sticas num√©ricas est√©n escaladas; las categ√≥ricas deben convertirse a variables indicadoras.",
  "Cada neurona oculta calcula una funci√≥n gaussiana (u otra RBF) centrada en un punto, captando curvas suaves.",
  "No impone distribuci√≥n normal en los errores, pues optimiza en funci√≥n de m√≠nimos cuadrados o cross-entropy.",
  "Funciona mejor si las observaciones son independientes; sensible a estructuras de dependencia sin modelar.",
  "No requiere homocedasticidad ya que no se basa en un modelo param√©trico de error con varianza fija.",
  "Los valores extremos pueden desplazar los centros de las RBF, afectando la forma del modelo.",
  "La colinealidad puede dificultar la determinaci√≥n de centros √≥ptimos, pero no invalida el ajuste.",
  "Las neuronas ocultas representan combinaciones complejas de caracter√≠sticas, por lo que el modelo es tipo 'caja negra'.",
  "El entrenamiento implica fijar o aprender centros y spreads; para muchos centros o dimensiones altas, el costo crece r√°pido.",
  "Se usa CV para elegir el n√∫mero de bases (centros) y el par√°metro de ancho (`sigma` o `spread`) para evitar sobreajuste.",
  "No conviene cuando hay decenas de miles de caracter√≠sticas sin reducci√≥n previa o cuando el ruido es muy alto."
)

tabla_rbfn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rbfn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RBFN",
             subtitle = "Radial Basis Function Network (RBFN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Recurrent Neural Networks (RNNs) {-}  

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_09_RNNs.html" style="color: blue;">
  Recurrent Neural Networks (RNNs) en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/RNNs.png"))
```

**Recurrent Neural Networks (RNNs)** son un tipo de **red neuronal artificial** dise√±ado espec√≠ficamente para manejar **datos secuenciales** o temporales, donde la informaci√≥n de pasos anteriores en la secuencia es relevante para la predicci√≥n actual. A diferencia de las redes de propagaci√≥n hacia adelante (como MLP o CNN) que asumen que las entradas son independientes entre s√≠, las RNNs tienen "memoria" o **conexiones recurrentes** que les permiten mantener un **estado interno** que encapsula informaci√≥n de pasos de tiempo anteriores. Esta caracter√≠stica las hace ideales para tareas como el procesamiento de lenguaje natural (PLN), el reconocimiento de voz, la traducci√≥n autom√°tica y la predicci√≥n de series de tiempo.

La idea fundamental de una RNN es que una **unidad recurrente** aplica la misma funci√≥n de transformaci√≥n a cada elemento de una secuencia, con la particularidad de que la salida de la unidad en un paso de tiempo dado se realimenta como entrada para el mismo proceso en el siguiente paso de tiempo. Esto permite que la red "recuerde" y utilice informaci√≥n pasada al procesar la secuencia actual.

El funcionamiento b√°sico de una RNN en un paso de tiempo ($t$) implica:

1.  **Entrada actual ($x_t$):** El elemento actual de la secuencia.
2.  **Estado oculto anterior ($h_{t-1}$):** La "memoria" o estado interno de la red del paso de tiempo anterior.
3.  **C√°lculo del Estado Oculto Actual ($h_t$):** Se combina la entrada actual y el estado oculto anterior, y se aplica una funci√≥n de activaci√≥n (ej., tanh o ReLU).
    $$h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$
    Donde $W_{hh}$ son los pesos de la conexi√≥n recurrente, $W_{xh}$ son los pesos de la entrada, y $b_h$ es el sesgo.
4.  **Salida Actual ($y_t$):** Se genera una salida a partir del estado oculto actual.
    $$y_t = W_{hy} h_t + b_y$$
    Donde $W_{hy}$ son los pesos de la salida y $b_y$ es el sesgo.

Este proceso de actualizaci√≥n de estado y salida se repite para cada elemento de la secuencia. La "memoria" de la RNN est√° codificada en el estado oculto que se pasa de un paso de tiempo al siguiente.

El entrenamiento de las RNNs se realiza mediante una variante del algoritmo de Back-Propagation llamada **Back-Propagation Through Time (BPTT)**. BPTT desenrolla la red a lo largo del tiempo, tratando cada paso de tiempo como una capa separada, y luego aplica la retropropagaci√≥n de manera similar a c√≥mo se entrena un MLP, pero propagando los errores a trav√©s de las conexiones recurrentes. Sin embargo, las RNNs simples pueden sufrir de problemas como el **desvanecimiento del gradiente** (vanishing gradient) o el **explosi√≥n del gradiente** (exploding gradient) para secuencias largas, lo que llev√≥ al desarrollo de arquitecturas m√°s avanzadas como **LSTM (Long Short-Term Memory)** y **GRU (Gated Recurrent Unit)**.

En el contexto del **aprendizaje global vs. local**, las RNNs son sistemas de **aprendizaje global** que est√°n dise√±ados para aprender y modelar **dependencias temporales y patrones secuenciales** en un dominio global. A diferencia de los m√©todos de regresi√≥n ponderada localmente como LOESS, que se enfocan en ajustar curvas en regiones espec√≠ficas de datos, las RNNs intentan aprender una funci√≥n de mapeo compleja que considera toda la secuencia hist√≥rica para producir una predicci√≥n. Si los datos (secuenciales) no se distribuyen linealmente, las RNNs son extremadamente efectivas para capturar estas relaciones no lineales y dependencias a largo plazo. Al tener un estado interno que recuerda informaci√≥n pasada, abordan directamente la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos est√°ticos o lineales, ya que pueden adaptar sus predicciones din√°micamente en funci√≥n del contexto secuencial, lo que las convierte en una herramienta fundamental para el an√°lisis de series de tiempo y el procesamiento de lenguaje.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado secuencial",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n) en secuencias",
  "‚úÖ Series temporales y datos secuenciales (texto, audio, series) convertidos a vectores",
  "‚úÖ Captura dependencias temporales y de largo plazo entre pasos de la secuencia",
  "‚ùå No requiere supuestos de normalidad de residuos",
  "‚ö†Ô∏è Ideal si las secuencias son independientes entre s√≠; no modela dependencia ex√≥gena autom√°ticamente",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers en la serie pueden sesgar el entrenamiento si no se detectan)",
  "‚ö†Ô∏è La colinealidad en caracter√≠sticas secuenciales puede afectar la convergencia (usar embeddings o reducci√≥n)",
  "‚ö†Ô∏è Baja (‚Äúcaja negra‚Äù con muchas capas; usar t√©cnicas de atenci√≥n o visualizaci√≥n de activaciones)",
  "‚ö†Ô∏è Lento sin GPU/TPU; entrenamiento costoso para secuencias largas o redes profundas",
  "‚ö†Ô∏è Usar validaci√≥n cronol√≥gica (time series split) es m√°s apropiado que k-fold cl√°sico",
  "‚ùå No conviene con muy pocas muestras temporales, secuencias extremadamente largas sin truncar, o datos muy ruidosos"
)

detalles <- c(
  "Red neuronal recurrente que procesa datos en pasos temporales manteniendo un estado interno.",
  "En clasificaci√≥n, etiqueta cada elemento o secuencia; en regresi√≥n, predice valores continuos a lo largo del tiempo.",
  "Las entradas deben transformarse en vectores o embeddings; por ejemplo, texto a √≠ndices, series normalizadas.",
  "La arquitectura RNN (LSTM, GRU) retiene informaci√≥n de pasos anteriores para afectar salidas posteriores.",
  "No impone distribuci√≥n en errores, ya que se optimiza v√≠a descenso de gradiente sobre secuencias.",
  "Funciona mejor si cada secuencia (serie) es independiente; para datos con autocorrelaci√≥n compleja, usar variantes especializadas.",
  "No requiere varianza constante, pues se basa en propagaci√≥n de estado y no en un t√©rmino de error param√©trico.",
  "Valores at√≠picos en la serie pueden provocar gradientes explosivos o desvanecidos sin mecanismos como clipping.",
  "La representaci√≥n internal de patrones secuenciales puede verse afectada si hay caracter√≠sticas muy correlacionadas; usar regularizaci√≥n.",
  "Dif√≠cil interpretar pesos internos; se usan mec√°nicas como atenci√≥n (attention) o visualizaci√≥n de celdas LSTM.",
  "El entrenamiento con backpropagation through time es intensivo; GPUs o TPUs aceleran enormemente el proceso.",
  "Para series temporales, se prefiere validaci√≥n basada en ventanas de tiempo (rolling/expanding window) en lugar de random split.",
  "No apto si las secuencias son muy cortas o muy pocas, o hay mucho ruido sin filtrado; en esos casos, usar modelos estad√≠sticos simples."
)

tabla_rnn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_rnn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir RNN",
             subtitle = "Recurrent Neural Networks (RNNs)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Restricted Boltzmann Machine (RBM) {-}   

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_10_RBM.html" style="color: blue;">
  Restricted Boltzmann Machine (RBM) en R
</a>


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/RBM.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_rbm <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_rbm <- c(
  "‚úÖ Generativo (No Supervisado)",
  "‚ùå No aplica directamente",
  "‚úÖ Num√©ricas y categ√≥ricas (binarizadas/escaladas)",
  "‚úÖ Compleja (probabil√≠stica, latente)",
  "‚ùå No es requisito",
  "‚ùå No aplica directamente",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (puede ser influenciada)",
  "‚úÖ Maneja bien (aprende representaciones latentes)",
  "‚ùå Muy baja (caja negra)",
  "‚ö†Ô∏è Moderada a Baja",
  "‚ùå No aplica directamente (m√©trica de reconstrucci√≥n)",
  "‚ùå Datos ruidosos o falta de patrones complejos"
)

detalles_rbm <- c(
  "Modelo generativo y no supervisado. Aprende la distribuci√≥n de probabilidad de entrada para extraer caracter√≠sticas o reducir la dimensionalidad. A menudo se usa como bloque de construcci√≥n para Deep Belief Networks.",
  "Las RBM no tienen una 'variable respuesta' en el sentido supervisado. Su objetivo es aprender las caracter√≠sticas latentes de los datos de entrada.",
  "Las variables de entrada pueden ser num√©ricas (escaladas entre 0 y 1) o categ√≥ricas (binarizadas o codificadas one-hot). Las RBM m√°s b√°sicas son binarias (Bernoulli).",
  "Aprende relaciones complejas y no lineales entre las variables visibles (entrada) y las variables ocultas (latentes) a trav√©s de un modelo probabil√≠stico basado en energ√≠a.",
  "No hace suposiciones sobre la distribuci√≥n de los residuos, ya que no predice un valor continuo ni modela errores de predicci√≥n.",
  "No aplica directamente; las RBM modelan las correlaciones entre las variables de entrada y las caracter√≠sticas latentes, no la independencia de los errores residuales.",
  "No asume homoscedasticidad. Se enfoca en aprender la estructura de la distribuci√≥n de los datos.",
  "S√≠, las RBM pueden ser influenciadas por outliers, ya que intentan modelar la distribuci√≥n de todos los datos. Outliers extremos pueden distorsionar las caracter√≠sticas latentes aprendidas.",
  "Maneja bien la multicolinealidad. Al aprender representaciones latentes, las RBM pueden capturar las relaciones subyacentes entre variables correlacionadas de manera eficiente.",
  "La interpretabilidad de las caracter√≠sticas latentes aprendidas por las RBM es muy baja. Es dif√≠cil atribuir un significado directo a las activaciones de las unidades ocultas.",
  "El entrenamiento de las RBM (especialmente usando el algoritmo de Contrastive Divergence) puede ser moderadamente intensivo computacionalmente y lento para grandes datasets o redes profundas. La inferencia es m√°s r√°pida.",
  "La validaci√≥n cruzada no se aplica de la misma manera que en modelos supervisados. La evaluaci√≥n se realiza a menudo mediante la capacidad de reconstrucci√≥n de los datos de entrada o su utilidad como pre-entrenamiento para modelos supervisados.",
  "No funciona bien si los datos son puramente aleatorios o no contienen patrones complejos que el modelo pueda aprender. Tambi√©n, puede ser dif√≠cil de entrenar si hay muchos datos ruidosos o si el algoritmo de entrenamiento no converge."
)

tabla_rbm <- data.frame(Criterio = criterios_rbm, Aplica = aplica_rbm, Detalles = detalles_rbm)

tabla_rbm %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Restricted Boltzmann Machine (RBM)",
    subtitle = "M√°quinas de Boltzmann Restringidas para Aprendizaje No Supervisado"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Transformers  {-}  

<a href="https://dvillasanao.github.io/ML_Examples/Output/Neural%20Networks/04_11_Transformers.html" style="color: blue;">
  Transformers en R
</a>



```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Neural Networks/Transformers.png"))
```

Los **Transformers** son una arquitectura de **red neuronal profunda** que ha revolucionado el campo del **Procesamiento de Lenguaje Natural (PLN)** y, m√°s recientemente, se ha expandido a la visi√≥n por computadora y otras √°reas. Introducidos en el art√≠culo "Attention Is All You Need" (Vaswani et al., 2017), la idea fundamental de los Transformers es prescindir de la naturaleza recurrente de las RNNs y las convolucionales de las CNNs, bas√°ndose enteramente en un mecanismo llamado **auto-atenci√≥n (self-attention)** para capturar dependencias de largo alcance en las secuencias de entrada.

Antes de los Transformers, las RNNs eran el modelo dominante para datos secuenciales. Sin embargo, las RNNs ten√≠an limitaciones como la dificultad para capturar dependencias a muy largo plazo (problema del gradiente desvanecido) y la imposibilidad de paralelizar completamente el procesamiento de secuencias (debido a su naturaleza secuencial). Los Transformers resuelven estos problemas al permitir que cada elemento de la secuencia interact√∫e directamente con todos los dem√°s elementos de la secuencia, sin importar su distancia.

Los componentes clave de un Transformer incluyen:

1.  **Mecanismo de Auto-Atenci√≥n (Self-Attention):** Este es el coraz√≥n del Transformer. Para cada token (palabra) en una secuencia, el mecanismo de auto-atenci√≥n calcula una puntuaci√≥n de "relevancia" entre ese token y todos los dem√°s tokens de la secuencia. Esto permite que el modelo "pese" la importancia de cada token al generar la representaci√≥n de otro token. Este proceso se implementa a trav√©s de tres vectores para cada token: **Query (Q)**, **Key (K)** y **Value (V)**.
    $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
    Donde $d_k$ es la dimensi√≥n de los vectores Key.

2.  **Atenci√≥n Multi-Cabeza (Multi-Head Attention):** Para mejorar la capacidad del modelo de enfocarse en diferentes aspectos de la secuencia, el mecanismo de auto-atenci√≥n se aplica m√∫ltiples veces en paralelo con diferentes conjuntos de matrices de pesos (cabezas). Las salidas de estas cabezas se concatenan y se transforman linealmente.

3.  **Capas Feed-Forward (Posici√≥n por Posici√≥n):** Despu√©s del mecanismo de atenci√≥n, hay una red neuronal de propagaci√≥n hacia adelante (un MLP simple) que se aplica de forma independiente a cada posici√≥n en la secuencia.

4.  **Codificador-Decodificador (Encoder-Decoder Architecture):** El Transformer original consta de un **codificador** y un **decodificador**.
    * El **codificador** toma la secuencia de entrada y genera una representaci√≥n. Consiste en m√∫ltiples capas id√©nticas, cada una con una capa de auto-atenci√≥n multi-cabeza y una capa feed-forward.
    * El **decodificador** toma la representaci√≥n del codificador y genera la secuencia de salida (por ejemplo, la traducci√≥n). Tambi√©n consiste en m√∫ltiples capas, cada una con auto-atenci√≥n multi-cabeza, atenci√≥n multi-cabeza (que atiende a la salida del codificador) y una capa feed-forward.

5.  **Codificaci√≥n Posicional (Positional Encoding):** Dado que los Transformers procesan secuencias en paralelo y no tienen una noci√≥n inherente de la posici√≥n de los tokens (a diferencia de las RNNs), se a√±ade informaci√≥n de la posici√≥n de cada token a sus incrustaciones de entrada.

En el contexto del **aprendizaje global vs. local**, los Transformers son un sistema de **aprendizaje global** que, gracias a su mecanismo de atenci√≥n, pueden aprender **dependencias a largo alcance** y relaciones complejas que son inherentemente globales en la secuencia. Aunque los c√°lculos individuales de atenci√≥n pueden verse como una forma de ponderaci√≥n de la importancia local de los tokens, la red en su conjunto construye una representaci√≥n global de la secuencia. Si los datos (secuenciales) no se distribuyen linealmente, los Transformers son excepcionalmente capaces de modelar estas relaciones no lineales y dependencias a trav√©s de su capacidad para "observar" toda la secuencia a la vez y ponderar la relevancia de cada parte. Esto resuelve de manera fundamental la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en modelos secuenciales anteriores, ya que la arquitectura de atenci√≥n les permite aprender patrones complejos y no lineales en datos secuenciales sin las restricciones de memoria de las RNNs, lo que los convierte en la arquitectura dominante para tareas de PLN avanzadas.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ö†Ô∏è Supervisado (frecuentemente secuencial o multitarea)",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n en secuencias)",
  "‚úÖ Texto, secuencias, im√°genes en vectores o embeddings",
  "‚úÖ Captura dependencia secuencial y global mediante mecanismos de atenci√≥n",
  "‚ùå No requiere supuestos de normalidad en residuos",
  "‚ö†Ô∏è Ideal si las muestras o secuencias son independientes; para datos correlacionados usar variantes espec√≠ficas",
  "‚ùå No asume homoscedasticidad",
  "‚ö†Ô∏è Moderado (outliers en embeddings o entradas ruidosas pueden afectar atenci√≥n)",
  "‚ö†Ô∏è La colinealidad en embedding space puede ralentizar aprendizaje; usar regularizaci√≥n",
  "‚ö†Ô∏è Baja (modelo de ‚Äôcaja negra‚Äô, requieren m√©todos como attention visualization o interpretabilidad basada en pesos)",
  "‚ö†Ô∏è Lento sin hardware especializado (secuencialidad en atenci√≥n puede ser costosa)",
  "‚ö†Ô∏è Validaci√≥n temporal o k-fold anidada, seg√∫n tarea; en NLP se prefiere holdout sobre texto sin mezclar",
  "‚ùå No es apropiado con muy pocos datos de entrenamiento o sin estructura secuencial clara"
)

detalles <- c(
  "Arquitectura basada en capas de atenci√≥n para procesar secuencias completas en paralelo.",
  "Modelos como BERT, GPT, T5 pueden usarse para tareas de clasificaci√≥n, traducci√≥n, regresi√≥n de valores continuos en secuencias.",
  "Entradas requieren tokenizaci√≥n y conversi√≥n a embeddings; pueden combinarse varias modalidades.",
  "La auto‚Äêatenci√≥n global permite capturar relaciones a largo y corto plazo sin sesgo posicional estricto.",
  "No impone distribuci√≥n param√©trica de errores; se entrena con optimizadores basados en p√©rdidas cross‚Äêentropy o MSE.",
  "Se prefiere que las secuencias en el batch no sean dependientes; para series de tiempo, usar variantes como Time‚ÄêSeries Transformer.",
  "No se modela varianza del error; el entrenamiento se enfoca en minimizar funci√≥n de p√©rdida directa.",
  "Ruido en texto (typos) o en datos num√©ricos de entrada puede inducir atenci√≥n err√°tica; usar limpieza de datos y regularizaci√≥n.",
  "Los embeddings pueden contener informaci√≥n redundante de caracter√≠sticas correlacionadas; ajustar tama√±o de embedding y regularizaci√≥n.",
  "Interpretabilidad limitada; se usan t√©cnicas como visualizaci√≥n de mapas de atenci√≥n, LIME, SHAP para entender decisiones.",
  "El c√≥mputo de atenci√≥n es O(n¬≤) en longitud de secuencia; GPUs/TPUs o variantes eficientes (Linformer, Performer) alivian costo.",
  "Para tareas de texto, a veces se usa train/validation/test sin CV cl√°sica; para tareas generales, k-fold anidada ayuda a elegir hiperpar√°metros.",
  "No es adecuado con datasets muy peque√±os, sin preentrenamiento o sin estructuras secuenciales definidas."
)

tabla_transformers <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_transformers %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir transformers",
             subtitle = "Transformers")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




