# üßÆ 7. Regularizaci√≥n {-}  

**Ejemplos:** L1 (Lasso), L2 (Ridge), Elastic Net.  
**Uso:** Esencial para **prevenir el sobreajuste** en modelos, especialmente los lineales y las redes neuronales. Muy √∫til cuando trabajas con **muchas variables** (alta dimensionalidad).  
**Ventajas:** Su principal beneficio es que **penaliza la complejidad del modelo**, forz√°ndolo a ser m√°s simple y generalizable.  
**Limitaciones:** Si se aplica en exceso, la regularizaci√≥n puede **eliminar variables √∫tiles** y, por lo tanto, afectar el rendimiento del modelo.  

---

## Elastic Net  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/ENet.png"))
```

**Elastic Net** es un m√©todo de **regresi√≥n lineal regularizada** que combina las penalizaciones de **Ridge Regression (regresi√≥n L2)** y **Lasso Regression (regresi√≥n L1)**. Fue desarrollado para superar las limitaciones de Lasso, que puede tener problemas cuando hay un gran n√∫mero de variables predictoras o cuando estas variables est√°n altamente correlacionadas (multicolinealidad). Elastic Net es una herramienta muy vers√°til para la **selecci√≥n de caracter√≠sticas**, la **reducci√≥n de sobreajuste** y el manejo de **datos de alta dimensi√≥n**.

La funci√≥n de costo de Elastic Net a√±ade dos t√©rminos de penalizaci√≥n a la suma de los errores cuadrados de los residuos (como en la regresi√≥n OLS):

1.  **Penalizaci√≥n L1 (Lasso):** La suma del valor absoluto de los coeficientes. Esta penalizaci√≥n tiende a **reducir los coeficientes de las variables menos importantes a cero**, realizando as√≠ una **selecci√≥n autom√°tica de caracter√≠sticas**.
2.  **Penalizaci√≥n L2 (Ridge):** La suma del cuadrado de los coeficientes. Esta penalizaci√≥n **encoge los coeficientes** hacia cero, pero no los fuerza a ser exactamente cero. Es particularmente √∫til para manejar la **multicolinealidad**, ya que tiende a distribuir la influencia de las variables correlacionadas de manera m√°s equitativa.

Elastic Net utiliza dos hiperpar√°metros de sintonizaci√≥n:

* **$\alpha$ (alpha):** Controla el **balance entre las penalizaciones L1 y L2**.
    * Si $\alpha = 0$, Elastic Net se convierte en **Ridge Regression**.
    * Si $\alpha = 1$, Elastic Net se convierte en **Lasso Regression**.
    * Para valores entre 0 y 1, es una mezcla de ambas.
* **$\lambda$ (lambda):** Controla la **fuerza general de la regularizaci√≥n**. Un $\lambda$ m√°s grande implica una mayor penalizaci√≥n y, por lo tanto, coeficientes m√°s peque√±os.

Al combinar L1 y L2, Elastic Net logra lo mejor de ambos mundos: realiza selecci√≥n de caracter√≠sticas como Lasso y maneja la multicolinealidad y la estabilidad de los coeficientes como Ridge. Esto lo hace muy robusto en escenarios donde hay muchas variables correlacionadas.


**Aprendizaje Global vs. Local:**

Elastic Net es un modelo de **aprendizaje global**.

* **Aspecto Global:** Elastic Net construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes de la regresi√≥n se estiman optimizando una funci√≥n de costo que considera todos los puntos de datos simult√°neamente. La penalizaci√≥n se aplica a todos los coeficientes de manera uniforme, lo que busca una soluci√≥n que minimice el error de predicci√≥n y controle la complejidad del modelo a nivel global. La ecuaci√≥n de regresi√≥n final es una funci√≥n que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n en el espacio de caracter√≠sticas.

* **Influencia de la Regularizaci√≥n:** Aunque la regresi√≥n en s√≠ es global, las penalizaciones de regularizaci√≥n pueden tener un efecto que podr√≠amos considerar "adaptativo" en el sentido de que ajustan la influencia de las variables en funci√≥n de su relaci√≥n con otras variables y la respuesta. Por ejemplo, la penalizaci√≥n L1 puede "localizar" las variables m√°s importantes al poner otras a cero, y la L2 puede distribuir la importancia entre variables correlacionadas. Sin embargo, estas son propiedades de la optimizaci√≥n global del modelo, no de ajustar modelos separados para diferentes subregiones del espacio de datos. La Elastic Net, al igual que OLS, Ridge y Lasso, busca una √∫nica relaci√≥n lineal que describa la tendencia general de los datos.


```{r, echo = FALSE}
library(gt)

criterios_elastic_net <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_elastic_net <- c(
  "‚úÖ Supervisado (Regresi√≥n/Clasificaci√≥n)",
  "‚úÖ Num√©rica (continua) / Categ√≥rica (para clasificaci√≥n)",
  "‚úÖ Num√©ricas (o categ√≥ricas codificadas)",
  "‚úÖ Lineal (o linealizable)",
  "‚ùå No es requisito directo (pero asume errores i.i.d.)",
  "‚úÖ Necesaria",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠ (pero menos que OLS)",
  "‚úÖ Muy bien (su punto fuerte)",
  "‚úÖ Moderada a Alta (coeficientes reducidos/cero)",
  "‚úÖ Alta (especialmente con algoritmos eficientes)",
  "‚úÖ Esencial",
  "‚ùå Relaciones altamente no lineales no capturadas"
)

detalles_elastic_net <- c(
  "Elastic Net es una t√©cnica de regresi√≥n lineal regularizada que combina las penalizaciones L1 (Lasso) y L2 (Ridge). Su objetivo es realizar selecci√≥n de variables y regularizaci√≥n simult√°neamente, lo que la hace √∫til para modelos predictivos y de alta dimensionalidad.",
  "La variable dependiente puede ser **num√©rica continua** para problemas de regresi√≥n o **categ√≥rica** (generalmente binaria) para problemas de clasificaci√≥n (usando una funci√≥n de enlace, como en la regresi√≥n log√≠stica).",
  "Las variables predictoras deben ser **num√©ricas**. Las variables categ√≥ricas deben ser transformadas a num√©ricas (ej., one-hot encoding). Es crucial escalar las caracter√≠sticas antes de aplicar Elastic Net.",
  "Asume una relaci√≥n **lineal** entre las variables predictoras y la variable respuesta (o una transformaci√≥n de esta, como en regresi√≥n log√≠stica). Sin embargo, puede manejar interacciones si se crean expl√≠citamente como nuevas caracter√≠sticas.",
  "No asume estrictamente la normalidad de los residuos para el proceso de estimaci√≥n, pero los errores suelen asumirse independientes e id√©nticamente distribuidos (i.i.d.) para inferencia estad√≠stica.",
  "S√≠, se asume que las observaciones y sus errores asociados son independientes entre s√≠. Esto es fundamental para la validez de los estimadores.",
  "Es deseable que la varianza de los errores sea constante (homoscedasticidad), aunque la penalizaci√≥n puede hacer el modelo m√°s robusto a desviaciones leves que el M√≠nimos Cuadrados Ordinarios (OLS).",
  "S√≠, Elastic Net, como otros modelos lineales, es sensible a los outliers en los datos. Un outlier puede influir desproporcionadamente en los coeficientes. Sin embargo, la penalizaci√≥n L2 (Ridge) tiende a ser un poco m√°s robusta a los outliers que OLS, pero no es una soluci√≥n robusta per se.",
  "Este es uno de los **grandes puntos fuertes** de Elastic Net. Maneja la multicolinealidad **muy bien** al combinar las propiedades de Lasso (que tiende a seleccionar una de las variables correlacionadas y anular las otras) y Ridge (que reparte los pesos entre las variables correlacionadas). Esto es especialmente √∫til en conjuntos de datos con muchas caracter√≠sticas correlacionadas.",
  "La interpretabilidad es **moderada a alta**. Dado que Elastic Net realiza selecci√≥n de variables (gracias al componente L1), puede producir modelos con menos caracter√≠sticas, lo que los hace m√°s f√°ciles de interpretar que modelos con todos los predictores. Los coeficientes no nulos indican la importancia relativa de las variables.",
  "La velocidad y eficiencia son **altas**. El entrenamiento de Elastic Net es computacionalmente eficiente, incluso para datasets con un gran n√∫mero de caracter√≠sticas (alta dimensionalidad), gracias a algoritmos de optimizaci√≥n especializados.",
  "Es **esencial**. La validaci√≥n cruzada es crucial para sintonizar los dos hiperpar√°metros clave de Elastic Net: `alpha` (que controla la mezcla entre Lasso y Ridge) y `lambda` (la fuerza total de la regularizaci√≥n). Sin una validaci√≥n cruzada adecuada, se corre el riesgo de sobreajuste o subajuste.",
  "No funciona bien si: 1) las **relaciones subyacentes son altamente no lineales** y no pueden ser capturadas por transformaciones de caracter√≠sticas, 2) el **n√∫mero de caracter√≠sticas es extremadamente bajo** (ya que la regularizaci√≥n podr√≠a no ser necesaria), o 3) se busca un modelo puramente interpretable con *todos* los coeficientes no nulos y sin ninguna penalizaci√≥n de complejidad."
)

tabla_elastic_net <- data.frame(Criterio = criterios_elastic_net, Aplica = aplica_elastic_net, Detalles = detalles_elastic_net)

tabla_elastic_net %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Elastic Net",
    subtitle = "Regresi√≥n Lineal Regularizada para Selecci√≥n de Variables"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```




## Ridge Regression  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/RR.png"))
```

**Ridge Regression (Regresi√≥n Ridge)** es un m√©todo de **regresi√≥n lineal regularizada** que se utiliza para mejorar la estimaci√≥n de los coeficientes en modelos lineales, especialmente cuando existe **multicolinealidad** (alta correlaci√≥n entre las variables predictoras) o cuando el n√∫mero de predictores es grande en relaci√≥n con el n√∫mero de observaciones. Ridge Regression fue una de las primeras t√©cnicas de regularizaci√≥n y es fundamental para comprender m√©todos m√°s avanzados como Lasso o Elastic Net.

La Regresi√≥n Ridge aborda los problemas de la regresi√≥n por m√≠nimos cuadrados ordinarios (OLS) al a√±adir un **t√©rmino de penalizaci√≥n L2** a la funci√≥n de costo de los m√≠nimos cuadrados. La funci√≥n de costo que minimiza Ridge Regression es:

$$\text{RSS} + \lambda \sum_{j=1}^{p} \beta_j^2$$

Donde:
* $\text{RSS}$ es la suma de los errores cuadrados de los residuos (Residual Sum of Squares), que es lo que minimiza OLS.
* $\lambda$ (lambda) es un **par√°metro de sintonizaci√≥n (hiperpar√°metro)** no negativo. Este par√°metro controla la **fuerza de la penalizaci√≥n**.
* $\sum_{j=1}^{p} \beta_j^2$ es la **penalizaci√≥n L2**, que es la suma de los cuadrados de los coeficientes de regresi√≥n (excluyendo el intercepto).

**Efecto de la Penalizaci√≥n L2:**
* **Encogimiento de Coeficientes:** La penalizaci√≥n L2 **encoge los coeficientes** hacia cero. Cuanto mayor sea el valor de $\lambda$, mayor ser√° el encogimiento y m√°s peque√±os ser√°n los coeficientes.
* **Reducci√≥n de Varianza:** Este encogimiento reduce la varianza de las estimaciones de los coeficientes, haci√©ndolos m√°s estables y menos sensibles a peque√±as variaciones en los datos de entrenamiento. Esto ayuda a **reducir el sobreajuste**.
* **Manejo de Multicolinealidad:** En presencia de multicolinealidad, OLS puede asignar grandes valores a los coeficientes de variables correlacionadas. Ridge Regression distribuye la influencia entre las variables correlacionadas de manera m√°s uniforme y reduce la magnitud de estos coeficientes, lo que resulta en un modelo m√°s robusto.
* **No realiza selecci√≥n de caracter√≠sticas:** A diferencia de Lasso, Ridge Regression encoge los coeficientes, pero **rara vez los fuerza a ser exactamente cero**. Esto significa que todas las variables predictoras (o casi todas) seguir√°n en el modelo.

El valor √≥ptimo de $\lambda$ se selecciona t√≠picamente mediante t√©cnicas de validaci√≥n cruzada.

**Aprendizaje Global vs. Local:**

Ridge Regression es un modelo de **aprendizaje global**.

* **Aspecto Global:** Ridge Regression construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes se estiman optimizando una funci√≥n de costo que considera todos los puntos de datos simult√°neamente. La penalizaci√≥n L2 se aplica a todos los coeficientes para controlar la complejidad y la estabilidad del modelo a nivel global. La ecuaci√≥n de regresi√≥n resultante es una funci√≥n √∫nica que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n espec√≠fica en el espacio de caracter√≠sticas.

* **Estabilizaci√≥n Global:** Aunque la regularizaci√≥n L2 mejora la estabilidad de las estimaciones de los coeficientes y ayuda a manejar la multicolinealidad, lo hace como parte de una optimizaci√≥n global. No implica la creaci√≥n de m√∫ltiples modelos locales o la adaptaci√≥n a subregiones espec√≠ficas de los datos. La Regresi√≥n Ridge busca una relaci√≥n lineal subyacente que sea la mejor aproximaci√≥n para el conjunto de datos completo, penalizando la complejidad para mejorar la generalizaci√≥n global.


```{r, echo = FALSE}
library(gt)

criterios_ridge <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_ridge <- c(
  "‚úÖ Supervisado (Regresi√≥n/Clasificaci√≥n)",
  "‚úÖ Num√©rica (continua) / Categ√≥rica (para clasificaci√≥n)",
  "‚úÖ Num√©ricas (o categ√≥ricas codificadas)",
  "‚úÖ Lineal (o linealizable)",
  "‚ùå No es requisito directo (pero asume errores i.i.d.)",
  "‚úÖ Necesaria",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠ (pero m√°s robusta que OLS)",
  "‚úÖ Muy bien (su punto fuerte)",
  "‚úÖ Moderada (todos los coeficientes son no nulos)",
  "‚úÖ Alta (especialmente con algoritmos eficientes)",
  "‚úÖ Esencial",
  "‚ùå Se desea selecci√≥n de variables o un modelo muy esparso"
)

detalles_ridge <- c(
  "Ridge Regression es una t√©cnica de regresi√≥n lineal que a√±ade una penalizaci√≥n L2 (la suma de los cuadrados de los coeficientes) a la funci√≥n de costo de los M√≠nimos Cuadrados Ordinarios (OLS). Esto ayuda a reducir la magnitud de los coeficientes y a prevenir el sobreajuste, siendo especialmente √∫til en presencia de multicolinealidad.",
  "La variable dependiente puede ser **num√©rica continua** para problemas de regresi√≥n. Tambi√©n se puede extender a problemas de clasificaci√≥n (ej., regresi√≥n log√≠stica Ridge) donde la variable es **categ√≥rica** (generalmente binaria).",
  "Las variables predictoras deben ser **num√©ricas**. Las variables categ√≥ricas deben ser transformadas a num√©ricas (ej., one-hot encoding). Es **crucial escalar las caracter√≠sticas** antes de aplicar Ridge Regression para asegurar que la penalizaci√≥n L2 se aplique de manera justa a todos los coeficientes.",
  "Asume una relaci√≥n **lineal** entre las variables predictoras y la variable respuesta (o una transformaci√≥n de esta, como en regresi√≥n log√≠stica). Aunque es un modelo lineal, la regularizaci√≥n permite manejar conjuntos de datos con m√°s predictores que observaciones.",
  "No asume estrictamente la normalidad de los residuos para el proceso de estimaci√≥n, pero los errores suelen asumirse independientes e id√©nticamente distribuidos (i.i.d.) para inferencia estad√≠stica. La penalizaci√≥n se enfoca en la reducci√≥n de la varianza de los estimadores.",
  "S√≠, se asume que las observaciones y sus errores asociados son independientes entre s√≠. Esto es fundamental para la validez de los estimadores y la efectividad de la regularizaci√≥n.",
  "Es deseable que la varianza de los errores sea constante (homoscedasticidad). La penalizaci√≥n L2 hace el modelo m√°s robusto a desviaciones de esta asunci√≥n que el M√≠nimos Cuadrados Ordinarios (OLS).",
  "S√≠, Ridge Regression es sensible a los **outliers** en los datos, ya que minimiza la suma de errores al cuadrado. Sin embargo, debido a la penalizaci√≥n L2 que encoge los coeficientes, tiende a ser **menos sensible** que la regresi√≥n OLS pura, haciendo los estimadores m√°s estables frente a peque√±as perturbaciones de los datos. No es una soluci√≥n robusta para outliers severos.",
  "Este es el **principal punto fuerte** de Ridge Regression. Maneja la **multicolinealidad muy bien** encogiendo los coeficientes de las variables correlacionadas de manera proporcional, en lugar de anularlos como Lasso. Esto estabiliza los estimadores y mejora la generalizaci√≥n del modelo.",
  "La interpretabilidad es **moderada**. Aunque Ridge encoge los coeficientes, **nunca los reduce a cero absolutos** (a menos que el `lambda` sea infinito). Esto significa que todas las caracter√≠sticas siguen siendo parte del modelo, lo que puede dificultar la identificaci√≥n de las caracter√≠sticas m√°s importantes si hay muchas. Sin embargo, los coeficientes a√∫n indican la direcci√≥n y magnitud de la relaci√≥n.",
  "La velocidad y eficiencia son **altas**. El entrenamiento de Ridge Regression es computacionalmente eficiente, incluso para conjuntos de datos con un gran n√∫mero de caracter√≠sticas (alta dimensionalidad), gracias a soluciones de forma cerrada o algoritmos iterativos r√°pidos.",
  "Es **esencial**. La validaci√≥n cruzada es crucial para sintonizar el hiperpar√°metro `lambda` (la fuerza de la regularizaci√≥n). Un `lambda` mal elegido puede llevar a un sobreajuste (si es muy bajo) o a un subajuste severo (si es muy alto).",
  "No funciona bien si: 1) las **relaciones subyacentes son altamente no lineales** y no pueden ser capturadas por transformaciones de caracter√≠sticas, 2) se busca un modelo **esparso** donde las caracter√≠sticas irrelevantes se excluyan autom√°ticamente (para eso Lasso o Elastic Net son mejores), o 3) el **n√∫mero de caracter√≠sticas es extremadamente bajo** y la regularizaci√≥n podr√≠a no ser necesaria."
)

tabla_ridge <- data.frame(Criterio = criterios_ridge, Aplica = aplica_ridge, Detalles = detalles_ridge)

tabla_ridge %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Ridge Regression",
    subtitle = "Regresi√≥n Lineal Regularizada para Mitigar la Multicolinealidad"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```



## Least Absolute Shrinkage and Selection Operator (LASSO)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/LASSO.png"))
```



**LASSO (Least Absolute Shrinkage and Selection Operator)** es un m√©todo de **regresi√≥n lineal regularizada** que, al igual que Ridge Regression, se utiliza para mejorar la estimaci√≥n de los coeficientes en modelos lineales y para abordar el **sobreajuste**, especialmente en escenarios con un gran n√∫mero de variables predictoras o cuando algunas de ellas son irrelevantes. LASSO es particularmente famoso por su capacidad para realizar **selecci√≥n autom√°tica de caracter√≠sticas**.

LASSO logra esto a√±adiendo un **t√©rmino de penalizaci√≥n L1** a la funci√≥n de costo de los m√≠nimos cuadrados. La funci√≥n de costo que minimiza LASSO es:

$$\text{RSS} + \lambda \sum_{j=1}^{p} |\beta_j|$$

Donde:
* $\text{RSS}$ es la suma de los errores cuadrados de los residuos.
* $\lambda$ (lambda) es un **par√°metro de sintonizaci√≥n (hiperpar√°metro)** no negativo que controla la **fuerza de la penalizaci√≥n**.
* $\sum_{j=1}^{p} |\beta_j|$ es la **penalizaci√≥n L1**, que es la suma del valor absoluto de los coeficientes de regresi√≥n (excluyendo el intercepto).

**Efecto de la Penalizaci√≥n L1:**
* **Encogimiento de Coeficientes:** Similar a Ridge, la penalizaci√≥n L1 encoge los coeficientes hacia cero.
* **Selecci√≥n de Caracter√≠sticas:** La caracter√≠stica distintiva de LASSO es que, debido a la naturaleza de la penalizaci√≥n L1 (la suma de los valores absolutos), **puede forzar los coeficientes de las variables menos importantes a ser exactamente cero**. Esto significa que LASSO no solo encoge los coeficientes, sino que tambi√©n **realiza una selecci√≥n autom√°tica de caracter√≠sticas**, eliminando efectivamente las variables irrelevantes del modelo. Esto resulta en modelos m√°s simples y f√°ciles de interpretar.
* **Manejo de Multicolinealidad (con cuidado):** Aunque LASSO puede manejar la multicolinealidad, tiende a seleccionar arbitrariamente una de las variables correlacionadas y poner a cero las dem√°s, lo que puede ser una desventaja en comparaci√≥n con Ridge (que distribuye la influencia). Elastic Net surgi√≥ para abordar esto.

El valor √≥ptimo de $\lambda$ se selecciona t√≠picamente mediante t√©cnicas de validaci√≥n cruzada.


**Aprendizaje Global vs. Local:**

LASSO es un modelo de **aprendizaje global**.

* **Aspecto Global:** LASSO construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes se estiman optimizando una funci√≥n de costo que considera todos los puntos de datos simult√°neamente. La penalizaci√≥n L1 se aplica a todos los coeficientes para controlar la complejidad y realizar la selecci√≥n de caracter√≠sticas a nivel global. La ecuaci√≥n de regresi√≥n final es una funci√≥n √∫nica que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n en el espacio de caracter√≠sticas.

* **Selecci√≥n Global de Caracter√≠sticas:** Aunque LASSO puede "localizar" qu√© variables son importantes al reducir sus coeficientes a cero, esto se hace como parte de un proceso de optimizaci√≥n global que eval√∫a la contribuci√≥n de cada variable a la predicci√≥n general del modelo. No implica la creaci√≥n de m√∫ltiples modelos locales o la adaptaci√≥n a subregiones espec√≠ficas de los datos. LASSO busca la relaci√≥n lineal m√°s parsimoniosa que mejor se ajuste al conjunto de datos completo.


```{r, echo = FALSE}
library(gt)

criterios_lasso <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_lasso <- c(
  "‚úÖ Supervisado (Regresi√≥n/Clasificaci√≥n)",
  "‚úÖ Num√©rica (continua) / Categ√≥rica (para clasificaci√≥n)",
  "‚úÖ Num√©ricas (o categ√≥ricas codificadas)",
  "‚úÖ Lineal (o linealizable)",
  "‚ùå No es requisito directo (pero asume errores i.i.d.)",
  "‚úÖ Necesaria",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠ (m√°s que Ridge)",
  "‚ö†Ô∏è Puede ser problem√°tico (selecciona una entre correlacionadas)",
  "‚úÖ Muy alta (genera modelos esparsos)",
  "‚úÖ Alta (especialmente con algoritmos eficientes)",
  "‚úÖ Esencial",
  "‚ùå Datos con grupos de variables correlacionadas donde todas son importantes"
)

detalles_lasso <- c(
  "LASSO (Least Absolute Shrinkage and Selection Operator) es una t√©cnica de regresi√≥n lineal regularizada que a√±ade una penalizaci√≥n L1 (la suma de los valores absolutos de los coeficientes) a la funci√≥n de costo de los M√≠nimos Cuadrados Ordinarios (OLS). Esto fuerza a algunos coeficientes a ser exactamente cero, realizando as√≠ selecci√≥n de variables.",
  "La variable dependiente puede ser **num√©rica continua** para problemas de regresi√≥n. Tambi√©n se puede extender a problemas de clasificaci√≥n (ej., regresi√≥n log√≠stica Lasso) donde la variable es **categ√≥rica** (generalmente binaria).",
  "Las variables predictoras deben ser **num√©ricas**. Las variables categ√≥ricas deben ser transformadas a num√©ricas (ej., one-hot encoding). Es **crucial escalar las caracter√≠sticas** antes de aplicar LASSO para asegurar que la penalizaci√≥n L1 se aplique de manera justa a todos los coeficientes.",
  "Asume una relaci√≥n **lineal** entre las variables predictoras y la variable respuesta (o una transformaci√≥n de esta, como en regresi√≥n log√≠stica). Aunque es un modelo lineal, la regularizaci√≥n permite manejar conjuntos de datos con m√°s predictores que observaciones.",
  "No asume estrictamente la normalidad de los residuos para el proceso de estimaci√≥n, pero los errores suelen asumirse independientes e id√©nticamente distribuidos (i.i.d.) para inferencia estad√≠stica. La penalizaci√≥n se enfoca en la creaci√≥n de modelos esparsos.",
  "S√≠, se asume que las observaciones y sus errores asociados son independientes entre s√≠. Esto es fundamental para la validez de los estimadores y la efectividad de la regularizaci√≥n.",
  "Es deseable que la varianza de los errores sea constante (homoscedasticidad). Sin embargo, la penalizaci√≥n L1 puede hacer el modelo m√°s robusto a desviaciones leves que el M√≠nimos Cuadrados Ordinarios (OLS), aunque no est√° dise√±ado espec√≠ficamente para heterocedasticidad.",
  "S√≠, LASSO es sensible a los **outliers** en los datos. Como minimiza la suma de errores al cuadrado (como OLS), los outliers pueden influir desproporcionadamente en los coeficientes antes de la penalizaci√≥n. Puede ser m√°s sensible a outliers que Ridge Regression.",
  "Puede ser **problem√°tico en presencia de alta multicolinealidad**. Si hay un grupo de variables altamente correlacionadas, LASSO tiende a seleccionar solo una de ellas arbitrariamente y anular las dem√°s. Esto es bueno para la escasez pero puede no ser ideal si todas las variables correlacionadas son importantes para el dominio del problema.",
  "La interpretabilidad es **muy alta**. La principal fortaleza de LASSO es su capacidad para realizar **selecci√≥n de variables autom√°tica**, forzando los coeficientes de las variables menos importantes a ser exactamente cero. Esto resulta en modelos m√°s esparsos y f√°ciles de interpretar, ya que solo las caracter√≠sticas m√°s relevantes permanecen en la ecuaci√≥n.",
  "La velocidad y eficiencia son **altas**. El entrenamiento de LASSO es computacionalmente eficiente, incluso para conjuntos de datos con un gran n√∫mero de caracter√≠sticas (alta dimensionalidad), gracias a algoritmos de optimizaci√≥n especializados (ej., m√≠nimos cuadrados coordinados).",
  "Es **esencial**. La validaci√≥n cruzada es crucial para sintonizar el hiperpar√°metro `lambda` (la fuerza de la regularizaci√≥n L1). Un `lambda` mal elegido puede llevar a un sobreajuste (si es muy bajo) o a un subajuste severo y un modelo vac√≠o (si es muy alto).",
  "No funciona bien si: 1) las **relaciones subyacentes son altamente no lineales** y no pueden ser capturadas por transformaciones de caracter√≠sticas, 2) hay **grupos de variables predictoras altamente correlacionadas y se espera que todas ellas sean relevantes** (en este caso, Elastic Net es una mejor opci√≥n), o 3) se busca un modelo donde *todos* los coeficientes deben ser no nulos por razones de interpretabilidad."
)

tabla_lasso <- data.frame(Criterio = criterios_lasso, Aplica = aplica_lasso, Detalles = detalles_lasso)

tabla_lasso %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Least Absolute Shrinkage and Selection Operator (LASSO)",
    subtitle = "Regresi√≥n Lineal Regularizada para Selecci√≥n de Variables y Escasez"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```







