# üßÆ 7. Regularizaci√≥n {-}  

**Ejemplos:** L1 (Lasso), L2 (Ridge), Elastic Net.  
**Uso:** Esencial para **prevenir el sobreajuste** en modelos, especialmente los lineales y las redes neuronales. Muy √∫til cuando trabajas con **muchas variables** (alta dimensionalidad).  
**Ventajas:** Su principal beneficio es que **penaliza la complejidad del modelo**, forz√°ndolo a ser m√°s simple y generalizable.  
**Limitaciones:** Si se aplica en exceso, la regularizaci√≥n puede **eliminar variables √∫tiles** y, por lo tanto, afectar el rendimiento del modelo.  

---

## Elastic Net  {-}   

**Elastic Net** es un m√©todo de **regresi√≥n lineal regularizada** que combina las penalizaciones de **Ridge Regression (regresi√≥n L2)** y **Lasso Regression (regresi√≥n L1)**. Fue desarrollado para superar las limitaciones de Lasso, que puede tener problemas cuando hay un gran n√∫mero de variables predictoras o cuando estas variables est√°n altamente correlacionadas (multicolinealidad). Elastic Net es una herramienta muy vers√°til para la **selecci√≥n de caracter√≠sticas**, la **reducci√≥n de sobreajuste** y el manejo de **datos de alta dimensi√≥n**.

La funci√≥n de costo de Elastic Net a√±ade dos t√©rminos de penalizaci√≥n a la suma de los errores cuadrados de los residuos (como en la regresi√≥n OLS):

1.  **Penalizaci√≥n L1 (Lasso):** La suma del valor absoluto de los coeficientes. Esta penalizaci√≥n tiende a **reducir los coeficientes de las variables menos importantes a cero**, realizando as√≠ una **selecci√≥n autom√°tica de caracter√≠sticas**.
2.  **Penalizaci√≥n L2 (Ridge):** La suma del cuadrado de los coeficientes. Esta penalizaci√≥n **encoge los coeficientes** hacia cero, pero no los fuerza a ser exactamente cero. Es particularmente √∫til para manejar la **multicolinealidad**, ya que tiende a distribuir la influencia de las variables correlacionadas de manera m√°s equitativa.

Elastic Net utiliza dos hiperpar√°metros de sintonizaci√≥n:

* **$\alpha$ (alpha):** Controla el **balance entre las penalizaciones L1 y L2**.
    * Si $\alpha = 0$, Elastic Net se convierte en **Ridge Regression**.
    * Si $\alpha = 1$, Elastic Net se convierte en **Lasso Regression**.
    * Para valores entre 0 y 1, es una mezcla de ambas.
* **$\lambda$ (lambda):** Controla la **fuerza general de la regularizaci√≥n**. Un $\lambda$ m√°s grande implica una mayor penalizaci√≥n y, por lo tanto, coeficientes m√°s peque√±os.

Al combinar L1 y L2, Elastic Net logra lo mejor de ambos mundos: realiza selecci√≥n de caracter√≠sticas como Lasso y maneja la multicolinealidad y la estabilidad de los coeficientes como Ridge. Esto lo hace muy robusto en escenarios donde hay muchas variables correlacionadas.


**Aprendizaje Global vs. Local:**

Elastic Net es un modelo de **aprendizaje global**.

* **Aspecto Global:** Elastic Net construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes de la regresi√≥n se estiman optimizando una funci√≥n de costo que considera todos los puntos de datos simult√°neamente. La penalizaci√≥n se aplica a todos los coeficientes de manera uniforme, lo que busca una soluci√≥n que minimice el error de predicci√≥n y controle la complejidad del modelo a nivel global. La ecuaci√≥n de regresi√≥n final es una funci√≥n que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n en el espacio de caracter√≠sticas.

* **Influencia de la Regularizaci√≥n:** Aunque la regresi√≥n en s√≠ es global, las penalizaciones de regularizaci√≥n pueden tener un efecto que podr√≠amos considerar "adaptativo" en el sentido de que ajustan la influencia de las variables en funci√≥n de su relaci√≥n con otras variables y la respuesta. Por ejemplo, la penalizaci√≥n L1 puede "localizar" las variables m√°s importantes al poner otras a cero, y la L2 puede distribuir la importancia entre variables correlacionadas. Sin embargo, estas son propiedades de la optimizaci√≥n global del modelo, no de ajustar modelos separados para diferentes subregiones del espacio de datos. La Elastic Net, al igual que OLS, Ridge y Lasso, busca una √∫nica relaci√≥n lineal que describa la tendencia general de los datos.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal (como OLS)",
  "‚ö†Ô∏è Deseable, pero no esencial",
  "‚úÖ Supuesto importante",
  "‚úÖ Requiere homoscedasticidad",
  "‚ö†Ô∏è Afectado por outliers (no tan robusto)",
  "‚úÖ Ideal para multicolinealidad alta (mejor que LASSO)",
  "‚ö†Ô∏è Puede ser menos interpretable que LASSO si hay muchas variables seleccionadas",
  "‚úÖ R√°pido incluso con datos grandes",
  "‚úÖ Requiere validar los hiperpar√°metros `lambda` y `alpha`",
  "‚ùå Relaci√≥n no lineal, o pocos datos con muchas variables no relevantes"
)

detalles <- c(
  "Modelo de regresi√≥n penalizada que combina LASSO (L1) y Ridge (L2) en un solo modelo.",
  "Predice una variable continua a partir de variables independientes num√©ricas.",
  "Las variables deben estar estandarizadas para evitar que la penalizaci√≥n sesgue los coeficientes.",
  "Asume que la relaci√≥n entre variables es lineal.",
  "La normalidad ayuda para inferencia, pero no es cr√≠tica para predicci√≥n.",
  "Errores deben ser independientes para que los coeficientes sean v√°lidos.",
  "Varianza constante de los errores es un supuesto clave.",
  "Aunque regulariza, no es inmune a valores at√≠picos.",
  "Funciona bien cuando hay muchas variables correlacionadas entre s√≠.",
  "Mezcla selecci√≥n de variables (L1) y regularizaci√≥n (L2), lo cual puede dificultar la interpretaci√≥n directa.",
  "A pesar de usar dos penalizaciones, sigue siendo eficiente con librer√≠as como `glmnet`.",
  "Validaci√≥n cruzada se usa para seleccionar los mejores valores de `lambda` y `alpha`.",
  "Puede tener bajo rendimiento si no hay una relaci√≥n lineal o si las variables relevantes no est√°n presentes en el conjunto."
)

tabla_elastic_net <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_elastic_net %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir elastic net",
             subtitle = "Elastic Net")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Ridge Regression  {-}    

**Ridge Regression (Regresi√≥n Ridge)** es un m√©todo de **regresi√≥n lineal regularizada** que se utiliza para mejorar la estimaci√≥n de los coeficientes en modelos lineales, especialmente cuando existe **multicolinealidad** (alta correlaci√≥n entre las variables predictoras) o cuando el n√∫mero de predictores es grande en relaci√≥n con el n√∫mero de observaciones. Ridge Regression fue una de las primeras t√©cnicas de regularizaci√≥n y es fundamental para comprender m√©todos m√°s avanzados como Lasso o Elastic Net.

La Regresi√≥n Ridge aborda los problemas de la regresi√≥n por m√≠nimos cuadrados ordinarios (OLS) al a√±adir un **t√©rmino de penalizaci√≥n L2** a la funci√≥n de costo de los m√≠nimos cuadrados. La funci√≥n de costo que minimiza Ridge Regression es:

$$\text{RSS} + \lambda \sum_{j=1}^{p} \beta_j^2$$

Donde:
* $\text{RSS}$ es la suma de los errores cuadrados de los residuos (Residual Sum of Squares), que es lo que minimiza OLS.
* $\lambda$ (lambda) es un **par√°metro de sintonizaci√≥n (hiperpar√°metro)** no negativo. Este par√°metro controla la **fuerza de la penalizaci√≥n**.
* $\sum_{j=1}^{p} \beta_j^2$ es la **penalizaci√≥n L2**, que es la suma de los cuadrados de los coeficientes de regresi√≥n (excluyendo el intercepto).

**Efecto de la Penalizaci√≥n L2:**
* **Encogimiento de Coeficientes:** La penalizaci√≥n L2 **encoge los coeficientes** hacia cero. Cuanto mayor sea el valor de $\lambda$, mayor ser√° el encogimiento y m√°s peque√±os ser√°n los coeficientes.
* **Reducci√≥n de Varianza:** Este encogimiento reduce la varianza de las estimaciones de los coeficientes, haci√©ndolos m√°s estables y menos sensibles a peque√±as variaciones en los datos de entrenamiento. Esto ayuda a **reducir el sobreajuste**.
* **Manejo de Multicolinealidad:** En presencia de multicolinealidad, OLS puede asignar grandes valores a los coeficientes de variables correlacionadas. Ridge Regression distribuye la influencia entre las variables correlacionadas de manera m√°s uniforme y reduce la magnitud de estos coeficientes, lo que resulta en un modelo m√°s robusto.
* **No realiza selecci√≥n de caracter√≠sticas:** A diferencia de Lasso, Ridge Regression encoge los coeficientes, pero **rara vez los fuerza a ser exactamente cero**. Esto significa que todas las variables predictoras (o casi todas) seguir√°n en el modelo.

El valor √≥ptimo de $\lambda$ se selecciona t√≠picamente mediante t√©cnicas de validaci√≥n cruzada.

**Aprendizaje Global vs. Local:**

Ridge Regression es un modelo de **aprendizaje global**.

* **Aspecto Global:** Ridge Regression construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes se estiman optimizando una funci√≥n de costo que considera todos los puntos de datos simult√°neamente. La penalizaci√≥n L2 se aplica a todos los coeficientes para controlar la complejidad y la estabilidad del modelo a nivel global. La ecuaci√≥n de regresi√≥n resultante es una funci√≥n √∫nica que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n espec√≠fica en el espacio de caracter√≠sticas.

* **Estabilizaci√≥n Global:** Aunque la regularizaci√≥n L2 mejora la estabilidad de las estimaciones de los coeficientes y ayuda a manejar la multicolinealidad, lo hace como parte de una optimizaci√≥n global. No implica la creaci√≥n de m√∫ltiples modelos locales o la adaptaci√≥n a subregiones espec√≠ficas de los datos. La Regresi√≥n Ridge busca una relaci√≥n lineal subyacente que sea la mejor aproximaci√≥n para el conjunto de datos completo, penalizando la complejidad para mejorar la generalizaci√≥n global.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal (como OLS)",
  "‚ö†Ô∏è Supuesto deseable pero no estricto",
  "‚úÖ Supuesto necesario",
  "‚úÖ Supuesto necesario",
  "‚ö†Ô∏è Puede verse afectado, pero menos que OLS",
  "‚úÖ Dise√±ado para mitigarla mediante penalizaci√≥n",
  "‚ö†Ô∏è Menos interpretable que OLS (coeficientes sesgados)",
  "‚úÖ Eficiente incluso con muchas variables",
  "‚úÖ Requiere validaci√≥n para ajustar par√°metro lambda",
  "‚ùå Si la relaci√≥n no es lineal o hay muchas variables irrelevantes"
)

detalles <- c(
  "Extensi√≥n de la regresi√≥n lineal que agrega penalizaci√≥n L2 para reducir sobreajuste y manejar multicolinealidad.",
  "Se utiliza cuando se desea predecir una variable num√©rica continua.",
  "Las variables deben ser num√©ricas y estar estandarizadas para que la penalizaci√≥n tenga sentido.",
  "Asume relaci√≥n lineal entre predictores y variable respuesta, como la regresi√≥n lineal.",
  "La normalidad es deseable para inferencia, pero no indispensable para predicci√≥n.",
  "Se espera independencia entre observaciones para que el modelo sea v√°lido.",
  "Es importante que los errores tengan varianza constante para predicciones fiables.",
  "Reduce varianza, pero valores extremos a√∫n pueden afectar los resultados.",
  "La penalizaci√≥n reduce varianza al achicar coeficientes, √∫til con predictores correlacionados.",
  "Coeficientes penalizados dificultan la interpretaci√≥n directa, pero mejoran estabilidad.",
  "R√°pido y adecuado para problemas con muchas variables; incluso p > n.",
  "Se usa validaci√≥n cruzada para elegir el mejor valor de lambda (par√°metro de regularizaci√≥n).",
  "No se recomienda cuando la relaci√≥n entre variables es no lineal o se requiere interpretaci√≥n clara."
)

tabla_ridge <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_ridge %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir ridge",
             subtitle = "Ridge Regression")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Least Absolute Shrinkage and Selection Operator (LASSO)  {-}  


**LASSO (Least Absolute Shrinkage and Selection Operator)** es un m√©todo de **regresi√≥n lineal regularizada** que, al igual que Ridge Regression, se utiliza para mejorar la estimaci√≥n de los coeficientes en modelos lineales y para abordar el **sobreajuste**, especialmente en escenarios con un gran n√∫mero de variables predictoras o cuando algunas de ellas son irrelevantes. LASSO es particularmente famoso por su capacidad para realizar **selecci√≥n autom√°tica de caracter√≠sticas**.

LASSO logra esto a√±adiendo un **t√©rmino de penalizaci√≥n L1** a la funci√≥n de costo de los m√≠nimos cuadrados. La funci√≥n de costo que minimiza LASSO es:

$$\text{RSS} + \lambda \sum_{j=1}^{p} |\beta_j|$$

Donde:
* $\text{RSS}$ es la suma de los errores cuadrados de los residuos.
* $\lambda$ (lambda) es un **par√°metro de sintonizaci√≥n (hiperpar√°metro)** no negativo que controla la **fuerza de la penalizaci√≥n**.
* $\sum_{j=1}^{p} |\beta_j|$ es la **penalizaci√≥n L1**, que es la suma del valor absoluto de los coeficientes de regresi√≥n (excluyendo el intercepto).

**Efecto de la Penalizaci√≥n L1:**
* **Encogimiento de Coeficientes:** Similar a Ridge, la penalizaci√≥n L1 encoge los coeficientes hacia cero.
* **Selecci√≥n de Caracter√≠sticas:** La caracter√≠stica distintiva de LASSO es que, debido a la naturaleza de la penalizaci√≥n L1 (la suma de los valores absolutos), **puede forzar los coeficientes de las variables menos importantes a ser exactamente cero**. Esto significa que LASSO no solo encoge los coeficientes, sino que tambi√©n **realiza una selecci√≥n autom√°tica de caracter√≠sticas**, eliminando efectivamente las variables irrelevantes del modelo. Esto resulta en modelos m√°s simples y f√°ciles de interpretar.
* **Manejo de Multicolinealidad (con cuidado):** Aunque LASSO puede manejar la multicolinealidad, tiende a seleccionar arbitrariamente una de las variables correlacionadas y poner a cero las dem√°s, lo que puede ser una desventaja en comparaci√≥n con Ridge (que distribuye la influencia). Elastic Net surgi√≥ para abordar esto.

El valor √≥ptimo de $\lambda$ se selecciona t√≠picamente mediante t√©cnicas de validaci√≥n cruzada.


**Aprendizaje Global vs. Local:**

LASSO es un modelo de **aprendizaje global**.

* **Aspecto Global:** LASSO construye un **modelo lineal global** que se aplica a todo el conjunto de datos. Los coeficientes se estiman optimizando una funci√≥n de costo que considera todos los puntos de datos simult√°neamente. La penalizaci√≥n L1 se aplica a todos los coeficientes para controlar la complejidad y realizar la selecci√≥n de caracter√≠sticas a nivel global. La ecuaci√≥n de regresi√≥n final es una funci√≥n √∫nica que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar su ubicaci√≥n en el espacio de caracter√≠sticas.

* **Selecci√≥n Global de Caracter√≠sticas:** Aunque LASSO puede "localizar" qu√© variables son importantes al reducir sus coeficientes a cero, esto se hace como parte de un proceso de optimizaci√≥n global que eval√∫a la contribuci√≥n de cada variable a la predicci√≥n general del modelo. No implica la creaci√≥n de m√∫ltiples modelos locales o la adaptaci√≥n a subregiones espec√≠ficas de los datos. LASSO busca la relaci√≥n lineal m√°s parsimoniosa que mejor se ajuste al conjunto de datos completo.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal (como OLS)",
  "‚ö†Ô∏è Deseable pero no estrictamente necesaria",
  "‚úÖ Requiere independencia de errores",
  "‚úÖ Requiere homoscedasticidad",
  "‚ö†Ô∏è Puede verse afectado por outliers extremos",
  "‚úÖ Maneja multicolinealidad mediante regularizaci√≥n",
  "‚úÖ Realiza selecci√≥n de variables (coeficientes pueden ser 0)",
  "‚úÖ Eficiente en alta dimensi√≥n; mejor que OLS",
  "‚úÖ Validaci√≥n cruzada necesaria para lambda",
  "‚ùå No es adecuado si la relaci√≥n es no lineal o hay muchas variables correlacionadas con igual relevancia"
)

detalles <- c(
  "Modelo de regresi√≥n penalizada que agrega penalizaci√≥n L1, capaz de forzar coeficientes a cero (selecci√≥n de variables).",
  "Se usa cuando se desea predecir una variable continua.",
  "Las variables predictoras deben estandarizarse para que la penalizaci√≥n sea justa entre coeficientes.",
  "Asume una relaci√≥n lineal entre los predictores y la respuesta.",
  "La normalidad ayuda para inferencia, pero no es cr√≠tica para predicci√≥n.",
  "Los errores deben ser independientes para que las estimaciones sean v√°lidas.",
  "Es deseable que la varianza de los errores sea constante a lo largo de los valores ajustados.",
  "Puede verse afectado por valores at√≠picos, aunque penaliza el modelo.",
  "Disminuye la varianza de los coeficientes y ayuda a estabilizar el modelo frente a multicolinealidad.",
  "Permite eliminar autom√°ticamente variables irrelevantes, facilitando modelos m√°s simples y explicables.",
  "Es computacionalmente eficiente, incluso cuando hay m√°s variables que observaciones.",
  "Lambda (par√°metro de penalizaci√≥n) se selecciona generalmente v√≠a validaci√≥n cruzada.",
  "Si hay muchas variables correlacionadas, LASSO tiende a seleccionar solo una de ellas, lo que puede ser inadecuado en algunos contextos."
)

tabla_lasso <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lasso %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LASSO",
             subtitle = "Least Absolute Shrinkage and Selection Operator (LASSO)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```





## Least Angle Regression (LARS)  {-}   

**Least Angle Regression (LARS)** es un algoritmo de **regresi√≥n lineal** desarrollado por Bradley Efron, Trevor Hastie, Iain Johnstone y Robert Tibshirani. Es particularmente interesante porque puede considerarse como una **versi√≥n m√°s eficiente y paso a paso de LASSO** (Least Absolute Shrinkage and Selection Operator) y es √∫til para **seleccionar caracter√≠sticas** y manejar datos de alta dimensi√≥n.

A diferencia de OLS, que calcula todos los coeficientes de una vez, o de Lasso, que requiere optimizaci√≥n m√°s compleja, LARS opera de manera incremental. Su idea central es avanzar los coeficientes de forma que su √°ngulo con el vector de residuos sea siempre el mismo y que sea el "m√°s peque√±o" posible.

El proceso de LARS se puede resumir as√≠:

1.  **Inicio:** Todos los coeficientes se inicializan en cero.
2.  **Identificaci√≥n del Predictor m√°s Correlacionado:** El algoritmo encuentra la variable predictora que est√° m√°s correlacionada con la variable de respuesta (o con el residuo actual).
3.  **Movimiento en la Direcci√≥n del Predictor:** El coeficiente de esa variable predictora se mueve gradualmente desde cero en la direcci√≥n del signo de su correlaci√≥n. A medida que el coeficiente se mueve, el residuo cambia.
4.  **Activaci√≥n de Nuevos Predictores:** Cuando otra variable predictora alcanza la misma correlaci√≥n con el residuo actual que la variable que ya est√° activa, el algoritmo cambia de direcci√≥n. Ahora, los coeficientes de *ambas* variables activas se mueven juntas en un "√°ngulo equiestad√≠stico" de tal manera que permanecen igualmente correlacionadas con el residuo.
5.  **Proceso Iterativo:** Este proceso contin√∫a, a√±adiendo nuevas variables al conjunto de variables "activas" (es decir, aquellas con coeficientes distintos de cero) a medida que estas alcanzan la misma correlaci√≥n con el residuo. Los coeficientes se mueven de forma coordinada.
6.  **Criterio de Parada:** El algoritmo se detiene cuando todos los predictores han sido incluidos en el modelo, o cuando se alcanza un n√∫mero predefinido de pasos o de variables.

**Relaci√≥n con otros modelos:**
* Si LARS se detiene cuando los coeficientes de las variables no activas son menores o iguales a la correlaci√≥n actual de las variables activas (y los coeficientes de las variables no activas se fijan en cero si su correlaci√≥n es menor), entonces genera la **soluci√≥n completa del camino de LASSO**.
* Tambi√©n puede generar el camino de soluciones para la **Ridge Regression** si se modifica ligeramente.

LARS es eficiente porque solo requiere un n√∫mero de pasos igual al n√∫mero de variables, o menos si se detiene antes.

**Aprendizaje Global vs. Local:**

Least Angle Regression (LARS) es un modelo de **aprendizaje global**.

* **Aspecto Global:** LARS construye un **modelo lineal global** paso a paso. Aunque el algoritmo a√±ade variables una por una y ajusta sus coeficientes de manera incremental, el modelo resultante en cada paso es una ecuaci√≥n de regresi√≥n lineal que se aplica a todo el conjunto de datos. La decisi√≥n de qu√© variable a√±adir y c√≥mo ajustar los coeficientes se basa en las correlaciones globales entre las variables predictoras y la respuesta (o el residuo). La finalidad es encontrar los coeficientes √≥ptimos para una funci√≥n de regresi√≥n que se aplica a todo el espacio de caracter√≠sticas.

* **Selecci√≥n de Caracter√≠sticas Globalmente:** La capacidad de LARS para realizar selecci√≥n de caracter√≠sticas (al igual que LASSO) es un proceso global. Se identifican las variables m√°s influyentes en el contexto de todo el conjunto de datos, y su inclusi√≥n en el modelo contribuye a la formaci√≥n de una relaci√≥n global entre los predictores y la respuesta. No se construyen modelos separados para diferentes subregiones de los datos; en cambio, se construye un √∫nico modelo global de manera progresiva.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal",
  "‚ö†Ô∏è Deseable para inferencia cl√°sica",
  "‚úÖ Necesaria",
  "‚úÖ Supuesto importante",
  "‚ö†Ô∏è Afectado por valores extremos",
  "‚úÖ Maneja bien multicolinealidad como LASSO",
  "‚úÖ Muy interpretable (secuencia de modelos anidados)",
  "‚úÖ Muy eficiente, especialmente con muchas variables",
  "‚úÖ √ötil para elegir n√∫mero de variables con validaci√≥n cruzada",
  "‚ùå Datos ruidosos o no lineales; ‚ùå si hay muchas interacciones no capturadas"
)

detalles <- c(
  "Algoritmo de regresi√≥n eficiente que selecciona variables secuencialmente como alternativa a LASSO.",
  "Busca predecir una variable continua usando m√∫ltiples predictores.",
  "Usa variables num√©ricas estandarizadas; es sensible a escala.",
  "Asume relaci√≥n lineal entre predictores y respuesta.",
  "No exige normalidad para predicci√≥n, pero s√≠ para inferencia estad√≠stica.",
  "Errores deben ser independientes entre s√≠.",
  "Supone varianza constante de los errores.",
  "Puede verse afectado si hay valores extremos en las variables.",
  "Muy √∫til cuando los predictores est√°n correlacionados; elige uno a la vez.",
  "Produce una ruta de modelos f√°cilmente interpretable con selecci√≥n progresiva.",
  "M√°s r√°pido que LASSO al generar trayectorias de coeficientes.",
  "Puede aplicarse validaci√≥n cruzada para seleccionar el mejor modelo en la secuencia.",
  "No captura relaciones no lineales o interacciones sin modificaci√≥n previa del modelo."
)

tabla_lars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lars %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LARS",
             subtitle = "Least Angle Regression (LARS)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


