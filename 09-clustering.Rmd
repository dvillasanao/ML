# üìè 9. Clustering (Aprendizaje No Supervisado) {-}  

**Ejemplos:** K-Means, DBSCAN, Agrupamiento Jer√°rquico.   
**Uso:** Excelente para **agrupar datos sin etiquetas previas**, permiti√©ndote descubrir **estructuras ocultas** o identificar **segmentos de mercado** dentro de tus conjuntos de datos. Es una herramienta clave en la exploraci√≥n de datos.   
**Ventajas:** Es incre√≠blemente √∫til para la **exploraci√≥n de datos** y para **reducir la complejidad** al encontrar patrones inherentes.  
**Limitaciones:** Generalmente, necesitas **elegir el n√∫mero de grupos** de antemano (excepto en DBSCAN), lo cual puede ser un desaf√≠o. Adem√°s, algunos algoritmos pueden ser **sensibles a la escala** de las caracter√≠sticas de tus datos.

---

## Affinity Propagation {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/AF.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_affinity_prop <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_affinity_prop <- c(
  "‚úÖ No Supervisado (Clustering)",
  "‚ùå No aplica directamente",
  "‚úÖ Num√©ricas (o datos de similitud/distancia)",
  "‚úÖ Muy compleja (determina prototipos y agrupaciones)",
  "‚ùå No es requisito",
  "‚ùå No aplica directamente",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (la elecci√≥n de 'preferencia' es clave)",
  "‚úÖ Maneja bien (si la similitud es adecuada)",
  "‚úÖ Moderada (los 'ejemplares' son interpretables)",
  "‚ö†Ô∏è Baja (costo computacional cuadr√°tico)",
  "‚ùå No aplica directamente (evaluaci√≥n de la coherencia del cl√∫ster)",
  "‚ùå Datasets muy grandes, ruido excesivo, significado de distancias no euclidianas"
)

detalles_affinity_prop <- c(
  "Algoritmo de clustering no supervisado que no requiere especificar el n√∫mero de cl√∫steres de antemano. Identifica 'ejemplares' (puntos de datos representativos) y agrupa otros puntos alrededor de ellos bas√°ndose en un proceso de 'paso de mensajes' entre todos los puntos.",
  "Affinity Propagation no tiene una 'variable respuesta'. Su objetivo es agrupar puntos de datos similares en cl√∫steres, identificando autom√°ticamente los centros de cl√∫ster (ejemplares).",
  "Las variables de entrada deben ser num√©ricas para calcular las similitudes/distancias. Alternativamente, se le puede proporcionar directamente una matriz de similitud o distancia precalculada.",
  "Capta relaciones complejas al determinar la similitud entre cada par de puntos y luego pasar 'mensajes' de responsabilidad y disponibilidad para identificar ejemplares y asignaciones de cl√∫ster. No asume formas de cl√∫ster espec√≠ficas (ej., esferas).",
  "No hace suposiciones sobre la normalidad de los residuos ni la distribuci√≥n de los datos, ya que no es un modelo param√©trico de regresi√≥n o clasificaci√≥n.",
  "El concepto de independencia de errores no aplica directamente. El algoritmo se basa en las interacciones y el paso de mensajes entre todos los puntos para determinar las agrupaciones.",
  "No asume homoscedasticidad. Las decisiones de agrupaci√≥n se basan en similitudes y la identificaci√≥n de ejemplares, no en la varianza constante.",
  "S√≠, Affinity Propagation puede ser sensible a los outliers, especialmente a trav√©s de la elecci√≥n del par√°metro de 'preferencia'. Un outlier puede convertirse en un ejemplar si su preferencia es alta, o influir en las similitudes si no se maneja adecuadamente. Puede crear cl√∫steres de un solo miembro si hay muchos outliers.",
  "Maneja bien la multicolinealidad, siempre y cuando la m√©trica de similitud subyacente (ej., similitud euclidiana, si es el caso) sea adecuada para los datos. El algoritmo se enfoca en las relaciones de similitud entre puntos, no en las relaciones directas entre caracter√≠sticas.",
  "La interpretabilidad es moderada a alta. Los centros de los cl√∫steres son puntos de datos reales ('ejemplares'), lo que facilita su inspecci√≥n y comprensi√≥n. Esto puede ser una gran ventaja en aplicaciones donde la representatividad es clave.",
  "La velocidad y eficiencia son bajas. La complejidad computacional de Affinity Propagation es O(N^2 * log N) o O(N^2) en el n√∫mero de muestras (N), lo que la hace poco pr√°ctica para datasets con m√°s de unos pocos miles de puntos. El pre-procesamiento para reducir el n√∫mero de puntos es a menudo necesario para grandes datasets.",
  "La validaci√≥n cruzada no se aplica directamente en el clustering, ya que no hay etiquetas verdaderas para predecir. La evaluaci√≥n se realiza utilizando m√©tricas de coherencia de cl√∫ster (ej., coeficiente de silueta) o comparando con agrupaciones conocidas si se tienen etiquetas de referencia (√≠ndice de Rand ajustado, V-measure).",
  "No funciona bien si: 1) el **dataset es muy grande** debido a su alta complejidad computacional, 2) hay **mucho ruido** que dificulta la identificaci√≥n de similitudes reales, 3) la estructura de cl√∫steres es muy difusa, o 4) la elecci√≥n del par√°metro de 'preferencia' (que controla el n√∫mero de cl√∫steres) es dif√≠cil de ajustar para el dominio espec√≠fico."
)

tabla_affinity_prop <- data.frame(Criterio = criterios_affinity_prop, Aplica = aplica_affinity_prop, Detalles = detalles_affinity_prop)

tabla_affinity_prop %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Affinity Propagation",
    subtitle = "Agrupamiento Basado en el Paso de Mensajes"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```



## Agglomerative Clustering {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/Agglomerative Clustering.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_agglomerative <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_agglomerative <- c(
  "‚úÖ No Supervisado (Clustering Jer√°rquico)",
  "‚ùå No aplica directamente",
  "‚úÖ Num√©ricas (o datos de distancia)",
  "‚úÖ Muy compleja (estructura anidada de cl√∫steres)",
  "‚ùå No es requisito",
  "‚ùå No aplica directamente",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (puede distorsionar las uniones)",
  "‚úÖ Maneja bien (si la distancia es adecuada)",
  "‚úÖ Muy alta (a trav√©s del dendrograma)",
  "‚ö†Ô∏è Baja (costo computacional alto)",
  "‚ùå No aplica directamente (evaluaci√≥n de la coherencia)",
  "‚ùå Datasets muy grandes o ruido excesivo"
)

detalles_agglomerative <- c(
  "Algoritmo de clustering no supervisado que construye una jerarqu√≠a de cl√∫steres. Comienza con cada punto de datos como su propio cl√∫ster y fusiona iterativamente los pares de cl√∫steres m√°s similares hasta que todos los puntos est√©n en un solo cl√∫ster o se alcance un criterio de parada. El resultado es un dendrograma.",
  "Agglomerative Clustering no tiene una 'variable respuesta'. Su objetivo es identificar grupos naturales de puntos de datos y la relaci√≥n jer√°rquica entre ellos.",
  "Las variables de entrada deben ser num√©ricas para calcular las distancias (o similitudes). Es crucial escalar las caracter√≠sticas adecuadamente antes de calcular las distancias.",
  "Capta relaciones complejas al organizar los datos en una estructura de √°rbol anidada (dendrograma), lo que permite visualizar agrupaciones en diferentes niveles de granularidad. No asume formas de cl√∫ster espec√≠ficas (ej., esferas).",
  "No hace suposiciones sobre la normalidad de los residuos ni la distribuci√≥n subyacente de los datos, ya que no es un modelo param√©trico.",
  "El concepto de independencia de errores no aplica directamente. El algoritmo se basa en las distancias entre puntos y cl√∫steres para realizar fusiones.",
  "No asume homoscedasticidad. Las decisiones de agrupaci√≥n se basan en la proximidad o la conexi√≥n entre cl√∫steres, no en la varianza constante.",
  "S√≠, Agglomerative Clustering es sensible a los outliers. Un punto an√≥malo puede permanecer como un cl√∫ster de un solo miembro durante mucho tiempo o, si se une, puede forzar una uni√≥n sub√≥ptima y distorsionar la estructura del cl√∫ster a niveles superiores. El ruido puede afectar las distancias y, por lo tanto, las fusiones.",
  "Maneja bien la multicolinealidad, siempre y cuando la m√©trica de distancia subyacente sea adecuada para los datos. El algoritmo se enfoca en las relaciones de distancia entre puntos, no en las relaciones directas entre caracter√≠sticas.",
  "La interpretabilidad es **muy alta** debido a la generaci√≥n del dendrograma. Este √°rbol visual permite al usuario entender c√≥mo se forman los cl√∫steres a diferentes niveles de similitud y elegir el n√∫mero de cl√∫steres '√≥ptimo' cortando el √°rbol.",
  "La velocidad y eficiencia son bajas. Su complejidad computacional es t√≠picamente O(N^3) en el n√∫mero de muestras (N) debido a la recalculaci√≥n de distancias en cada paso, o O(N^2) si se usa una matriz de distancia precalculada. Esto lo hace poco pr√°ctico para datasets con m√°s de unos pocos miles de puntos.",
  "La validaci√≥n cruzada no se aplica directamente en el clustering, ya que no hay etiquetas verdaderas para predecir. La evaluaci√≥n se realiza utilizando m√©tricas de coherencia de cl√∫ster (ej., coeficiente de silueta) o comparando con agrupaciones conocidas si se tienen etiquetas de referencia (√≠ndice de Rand ajustado, V-measure).",
  "No funciona bien si: 1) el **dataset es muy grande** debido a su alta complejidad computacional y requisitos de memoria, 2) hay **mucho ruido** que puede llevar a fusiones incorrectas en los niveles iniciales, o 3) no hay una estructura jer√°rquica clara en los datos, haciendo que el dendrograma sea menos informativo."
)

tabla_agglomerative <- data.frame(Criterio = criterios_agglomerative, Aplica = aplica_agglomerative, Detalles = detalles_agglomerative)

tabla_agglomerative %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Agglomerative Clustering",
    subtitle = "Agrupamiento Jer√°rquico Ascendente"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/DBSCAN.png"))
```

**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** es un algoritmo de **agrupamiento (clustering) no supervisado** que se distingue de los algoritmos basados en centroides (como k-Means) por su capacidad para encontrar **clusters de formas arbitrarias** y para identificar **puntos de ruido (outliers)**. Su idea central es que los clusters son regiones densas de puntos en el espacio de caracter√≠sticas, separadas por regiones de baja densidad.

DBSCAN define tres tipos de puntos:

1.  **Punto N√∫cleo (Core Point):** Un punto es un punto n√∫cleo si, dentro de un radio especificado ($\epsilon$ o `eps`), contiene un n√∫mero m√≠nimo de otros puntos ( `MinPts`).
2.  **Punto Frontera (Border Point):** Un punto es un punto frontera si est√° dentro del radio $\epsilon$ de un punto n√∫cleo, pero no es un punto n√∫cleo en s√≠ mismo (no tiene `MinPts` vecinos dentro de su propio radio $\epsilon$).
3.  **Punto de Ruido (Noise Point):** Cualquier punto que no es un punto n√∫cleo ni un punto frontera. Estos puntos son considerados outliers.

El algoritmo de DBSCAN opera de la siguiente manera:

1.  **Inicializaci√≥n:** Selecciona un punto arbitrario del conjunto de datos que a√∫n no ha sido visitado.
2.  **Expansi√≥n de Cluster:**
    * Si el punto seleccionado es un **punto n√∫cleo**, se inicia un nuevo cluster. Todos sus vecinos dentro del radio $\epsilon$ se a√±aden al cluster.
    * Recursivamente, se visitan y a√±aden los vecinos de esos nuevos puntos. Si un vecino es tambi√©n un punto n√∫cleo, sus propios vecinos tambi√©n se a√±aden al cluster. Este proceso contin√∫a hasta que no se puedan a√±adir m√°s puntos al cluster (es decir, todos los puntos alcanzables por densidad han sido encontrados).
    * Si el punto seleccionado **no es un punto n√∫cleo**, se marca como ruido (o se deja para ser procesado m√°s tarde si es un punto frontera de otro cluster ya formado).
3.  **Iteraci√≥n:** El proceso se repite con otro punto no visitado hasta que todos los puntos han sido procesados.

DBSCAN es particularmente √∫til para encontrar clusters complejos en conjuntos de datos ruidosos y no requiere que el usuario especifique el n√∫mero de clusters de antemano. Sus dos hiperpar√°metros clave son `eps` (el radio de b√∫squeda de vecindad) y `MinPts` (el n√∫mero m√≠nimo de puntos para formar un n√∫cleo).

**Aprendizaje Global vs. Local:**

DBSCAN es un algoritmo de **agrupamiento inherentemente local**, aunque el resultado final es una partici√≥n global de los datos en clusters y ruido.

* **Aspecto Local:** El coraz√≥n de DBSCAN reside en la definici√≥n de densidad local y la conectividad. Las decisiones sobre si un punto es un n√∫cleo, un frontera o ruido, y si dos puntos pertenecen al mismo cl√∫ster, se basan **exclusivamente en la densidad de puntos en un vecindario muy localizado** definido por el radio $\epsilon$ y el `MinPts`. El algoritmo "expande" los cl√∫steres al moverse de un punto n√∫cleo a sus vecinos, y de estos a sus vecinos, y as√≠ sucesivamente. Esta capacidad de crecer y formar cl√∫steres org√°nicamente a partir de las densidades locales es lo que permite a DBSCAN descubrir formas arbitrarias y adaptarse a la estructura local de los datos. No hay una funci√≥n global o centroides predefinidos que gu√≠en la agrupaci√≥n; todo se deriva de las propiedades de densidad local.

* **Resultado Global (Partici√≥n):** Aunque el proceso es local, el resultado final es una **partici√≥n global del conjunto de datos** en varios cl√∫steres y un conjunto de puntos de ruido. Una vez que todos los puntos han sido procesados y los cl√∫steres expandidos, se obtiene una vista global de la estructura de agrupamiento.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (distancias euclidianas u otras m√©tricas)",
  "‚úÖ Detecta clusters basados en densidad, no forma lineal",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚úÖ Robusto a outliers (los detecta como ruido)",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Clusters pueden ser arbitrarios, pero es intuitivo identificar ruido",
  "‚úÖ Razonablemente r√°pido para conjuntos medianos",
  "‚ùå No usa validaci√≥n cruzada cl√°sica; se eval√∫a con m√©tricas de clustering",
  "‚ùå No funciona bien con clusters de densidades muy diferentes o alta dimensionalidad"
)

detalles <- c(
  "Algoritmo de clustering basado en densidad que agrupa puntos cercanos y marca puntos aislados como ruido.",
  "No busca predecir, sino agrupar observaciones.",
  "Se basa en distancias; variables num√©ricas adecuadas; variables categ√≥ricas necesitan transformaci√≥n.",
  "No asume formas de clusters lineales ni convexas; puede detectar clusters arbitrarios.",
  "No genera residuos; no aplica normalidad.",
  "No hay modelo de error residual, no aplica independencia.",
  "No es un modelo predictivo, no aplica homoscedasticidad.",
  "Detecta outliers etiquet√°ndolos como ruido, siendo robusto frente a ellos.",
  "No hay predictores en sentido tradicional, por lo que multicolinealidad no afecta.",
  "Interpretaci√≥n basada en grupos densos y puntos aislados (ruido).",
  "Es eficiente, aunque su rendimiento puede disminuir en alta dimensionalidad.",
  "No utiliza validaci√≥n cruzada est√°ndar; evaluaci√≥n se basa en √≠ndices de clustering como Silhouette.",
  "Dificultades con clusters con diferentes densidades y cuando la dimensionalidad es muy alta."
)

tabla_dbscan <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_dbscan %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir DBSCAN",
             subtitle = "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Expectation Maximization (EM) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/EM.png"))
```

El algoritmo **Expectation-Maximization (EM)** es un m√©todo iterativo utilizado en estad√≠stica para encontrar las **estimaciones de m√°xima verosimilitud (MLE)** o las estimaciones de m√°xima a posteriori (MAP) de los par√°metros en modelos estad√≠sticos, especialmente cuando el modelo depende de **variables latentes (no observadas o "ocultas")** o cuando los datos est√°n "incompletos".

EM es particularmente √∫til para modelos de mezcla, donde se asume que los datos observados son una mezcla de varias distribuciones subyacentes, y la pertenencia de cada punto de datos a una distribuci√≥n espec√≠fica es la variable latente. El algoritmo consta de dos pasos principales que se alternan hasta la convergencia:

1.  **Paso E (Expectation Step - Paso de Expectativa):**
    * En este paso, dadas las estimaciones actuales de los par√°metros del modelo, se calculan las **probabilidades esperadas (o "responsabilidades")** de que cada punto de datos observado pertenezca a cada una de las componentes latentes (o de que las variables latentes tomen ciertos valores).
    * Esencialmente, se est√° haciendo una "suposici√≥n" sobre los valores de las variables latentes bas√°ndose en los par√°metros actuales del modelo y los datos observados.

2.  **Paso M (Maximization Step - Paso de Maximizaci√≥n):**
    * En este paso, utilizando las "responsabilidades" calculadas en el Paso E (trat√°ndolas como si fueran observaciones completas), se **re-estiman los par√°metros del modelo** para maximizar la verosimilitud esperada.
    * Esto es t√≠picamente un problema de optimizaci√≥n m√°s simple que el problema original de m√°xima verosimilitud con datos incompletos. Se ajustan los par√°metros (ej., medias, varianzas, pesos de mezcla) para que el modelo se ajuste mejor a los datos, considerando las asignaciones "blandas" a las variables latentes.

Los Pasos E y M se repiten iterativamente. La verosimilitud del modelo est√° garantizada para no disminuir en cada iteraci√≥n, y el algoritmo converge a un **m√°ximo local** de la funci√≥n de verosimilitud.

**Aplicaciones comunes:**
* **Modelos de Mezcla Gaussiana (GMMs):** Un uso protot√≠pico del EM para el clustering no supervisado.
* **Modelos Ocultos de Markov (HMMs):** Para problemas de reconocimiento de voz y bioinform√°tica.
* **Imputaci√≥n de datos faltantes:** Para estimar valores faltantes en un conjunto de datos.
* **An√°lisis de componentes latentes.**


**Aprendizaje Global vs. Local:**

El algoritmo Expectation-Maximization (EM) es un m√©todo de **aprendizaje global**, pero es importante entender un matiz sobre su convergencia.

* **Aspecto Global:** EM tiene como objetivo encontrar los **par√°metros de un modelo probabil√≠stico global** (como un GMM completo que describe la distribuci√≥n de todo el conjunto de datos) que maximicen la verosimilitud de los datos observados. Los par√°metros que se estiman (medias, covarianzas, pesos de mezcla en un GMM) son v√°lidos para todo el espacio de caracter√≠sticas. El algoritmo itera sobre todo el conjunto de datos en cada paso E y M para actualizar estos par√°metros globales. La soluci√≥n que busca EM es una representaci√≥n unificada y global de las distribuciones subyacentes de los datos.

* **Convergencia a M√°ximos Locales:** Aunque EM busca una soluci√≥n global, una limitaci√≥n cr√≠tica es que **solo est√° garantizado para converger a un m√°ximo local** de la funci√≥n de verosimilitud, no necesariamente al m√°ximo global. Esto significa que el resultado final puede depender de la **inicializaci√≥n** de los par√°metros del modelo. Si la funci√≥n de verosimilitud tiene m√∫ltiples "picos" (m√°ximos locales), EM puede quedar "atrapado" en uno de ellos. Para mitigar esto, es una pr√°ctica com√∫n ejecutar EM varias veces con diferentes inicializaciones aleatorias y seleccionar el resultado con la verosimilitud m√°s alta.

Por lo tanto, mientras que el objetivo de EM es aprender un modelo global que abarque todo el espacio de datos, su m√©todo iterativo de optimizaci√≥n lo hace susceptible a encontrar √≥ptimos locales en la funci√≥n de verosimilitud. La forma en que un modelo probabil√≠stico como un GMM puede modelar **relaciones no lineales** en los datos es que, al combinar m√∫ltiples distribuciones gaussianas (lineales), el modelo resultante puede capturar formas y densidades complejas y no lineales en el espacio de caracter√≠sticas. EM es el algoritmo que permite aprender estos componentes subyacentes.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (estimaci√≥n de par√°metros en modelos con datos incompletos o mixtos)",
  "‚ùå No aplica directamente (modelo probabil√≠stico)",
  "‚úÖ Variables num√©ricas o categ√≥ricas seg√∫n modelo",
  "‚úÖ Estima par√°metros m√°ximos de verosimilitud, puede manejar modelos complejos",
  "‚ö†Ô∏è Depende del modelo espec√≠fico usado con EM",
  "‚ö†Ô∏è Depende del modelo; errores independientes si asume modelo estad√≠stico cl√°sico",
  "‚ö†Ô∏è Depende del modelo estad√≠stico subyacente",
  "‚ö†Ô∏è Puede ser sensible a outliers dependiendo del modelo y datos",
  "‚ö†Ô∏è Depende del modelo y las variables involucradas",
  "‚ö†Ô∏è La interpretaci√≥n depende del modelo y par√°metros estimados",
  "‚ùå Puede ser lento si el modelo es complejo o datos muy grandes",
  "‚ùå Validaci√≥n cruzada depende del modelo, no es intr√≠nseco a EM",
  "‚ùå Puede converger a m√°ximos locales; requiere buen punto inicial y modelo adecuado"
)

detalles <- c(
  "Algoritmo iterativo para estimar par√°metros de modelos estad√≠sticos con datos faltantes o variables latentes.",
  "No genera predicciones directas, sino estima par√°metros para modelos probabil√≠sticos.",
  "Aplicable a datos num√©ricos o categ√≥ricos dependiendo del modelo (mezcla de Gaussianas, por ejemplo).",
  "Maximiza la funci√≥n de verosimilitud de manera iterativa, estimando variables latentes y par√°metros.",
  "La normalidad depende del modelo (por ejemplo, mezcla de Gaussianas asume normalidad).",
  "Si el modelo asume errores independientes, entonces s√≠; depende del modelo estad√≠stico usado.",
  "Homoscedasticidad depende del modelo estad√≠stico subyacente.",
  "Sensibilidad a outliers var√≠a seg√∫n la robustez del modelo y datos.",
  "Multicolinealidad afecta seg√∫n la estructura del modelo y variables involucradas.",
  "Interpretaci√≥n es sobre par√°metros estimados y variables latentes, no sobre coeficientes directos.",
  "Puede requerir muchas iteraciones, afectando velocidad en modelos complejos.",
  "La validaci√≥n cruzada depende del modelo aplicado tras la estimaci√≥n por EM.",
  "Puede quedarse atrapado en soluciones sub√≥ptimas; se recomienda m√∫ltiples inicios."
)

tabla_em <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_em %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir EM",
             subtitle = "Expectation Maximization (EM)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Gaussian Mixture Models (GMMs) {-}   


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/GMMs.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_gmm <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_gmm <- c(
  "‚úÖ No Supervisado (Clustering, Estimaci√≥n de Densidad)",
  "‚ùå No aplica directamente",
  "‚úÖ Num√©ricas",
  "‚úÖ Muy compleja (modelado probabil√≠stico de formas de cl√∫ster)",
  "‚ùå No es requisito directo (pero componentes son Gaussianos)",
  "‚ùå No aplica directamente",
  "‚ùå No es requisito directo (componentes pueden tener diferentes covarianzas)",
  "‚ö†Ô∏è S√≠ (muy)",
  "‚úÖ Maneja bien (a trav√©s de la covarianza)",
  "‚ö†Ô∏è Moderada (si los componentes tienen sentido)",
  "‚úÖ Moderada a Alta (para datos de tama√±o razonable)",
  "‚úÖ Compatible (para selecci√≥n de hiperpar√°metros)",
  "‚ùå Datos con formas no el√≠pticas, ruido excesivo, selecci√≥n err√≥nea de K"
)

detalles_gmm <- c(
  "Modelo de probabilidad que representa la distribuci√≥n de un conjunto de datos como una suma ponderada de varias distribuciones Gaussianas (normales). Se utiliza principalmente para clustering probabil√≠stico y estimaci√≥n de densidad.",
  "GMMs no tienen una 'variable respuesta'. Su objetivo es modelar la distribuci√≥n subyacente de los datos y agruparlos en 'componentes Gaussianos', donde cada punto pertenece a cada cl√∫ster con una cierta probabilidad.",
  "Las variables de entrada deben ser num√©ricas. Es importante escalar las caracter√≠sticas para asegurar que las m√©tricas de distancia y covarianza se interpreten correctamente.",
  "Capta relaciones muy complejas y no lineales al modelar cl√∫steres de forma el√≠ptica o esf√©rica con diferentes centros, varianzas y orientaciones. Permite que un punto pertenezca a m√∫ltiples cl√∫steres con diferentes probabilidades.",
  "Aunque el modelo general no asume normalidad global, cada **componente individual** del GMM se asume que es Gaussiano. Por lo tanto, los datos dentro de cada cl√∫ster modelado deben aproximarse a una distribuci√≥n normal.",
  "No aplica el concepto de independencia de errores como en modelos predictivos. En cambio, se estima la probabilidad de que cada punto pertenezca a cada componente Gaussiano.",
  "No asume homoscedasticidad global. Cada componente Gaussiano dentro de la mezcla puede tener su propia matriz de covarianza, lo que permite modelar cl√∫steres con diferentes formas y orientaciones.",
  "S√≠, GMMs son muy sensibles a outliers. Un outlier puede afectar significativamente la estimaci√≥n de los par√°metros (media y covarianza) de un componente Gaussiano, o incluso hacer que el algoritmo asigne un componente entero para modelar solo los outliers.",
  "Maneja bien la multicolinealidad, ya que su enfoque se basa en la estimaci√≥n de matrices de covarianza para cada componente Gaussiano. Las relaciones entre las caracter√≠sticas se capturan dentro de estas matrices.",
  "La interpretabilidad es moderada. Cada componente Gaussiano puede ser interpretado como un cl√∫ster, y sus par√°metros (media, covarianza) describen la forma y ubicaci√≥n de ese cl√∫ster. Sin embargo, el significado de los cl√∫steres a menudo requiere an√°lisis posterior.",
  "La velocidad y eficiencia son moderadas a altas para datasets de tama√±o razonable. El algoritmo de Expectation-Maximization (EM) utilizado para el entrenamiento puede converger r√°pidamente, pero puede ser lento para un n√∫mero muy grande de componentes o datos de muy alta dimensi√≥n.",
  "Es compatible y muy √∫til. La validaci√≥n cruzada (o criterios de informaci√≥n como AIC/BIC) se utiliza com√∫nmente para seleccionar el n√∫mero √≥ptimo de componentes Gaussianos (K) y el tipo de matriz de covarianza a utilizar.",
  "No funciona bien si: 1) los cl√∫steres en los datos no tienen una forma el√≠ptica o esf√©rica, 2) el **n√∫mero de componentes (K)** no se elige correctamente (demasiados pueden sobreajustar, pocos pueden subajustar), 3) hay **ruido excesivo** que dificulta la estimaci√≥n de las distribuciones, o 4) hay **demasiados outliers** sin pre-procesamiento."
)

tabla_gmm <- data.frame(Criterio = criterios_gmm, Aplica = aplica_gmm, Detalles = detalles_gmm)

tabla_gmm %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Gaussian Mixture Models (GMMs)",
    subtitle = "Modelos de Mezclas Gaussianas para Clustering y Estimaci√≥n de Densidad"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Hierarchical Clustering (hclust) {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/hclust.png"))
```

El **Agrupamiento Jer√°rquico (Hierarchical Clustering)**, a menudo abreviado como **hclust**, es un m√©todo de **agrupamiento (clustering) no supervisado** que construye una **jerarqu√≠a de clusters** en lugar de una partici√≥n plana de los datos (como k-Means). El resultado de un agrupamiento jer√°rquico se visualiza com√∫nmente como un **dendrograma**, un diagrama en forma de √°rbol que muestra la secuencia de fusiones o divisiones de los clusters.

Existen dos tipos principales de agrupamiento jer√°rquico:

1.  **Agrupamiento Aglomerativo ("Bottom-Up"):** Es el tipo m√°s com√∫n.
    * Comienza tratando **cada punto de datos como un cluster individual**.
    * En cada paso, **fusiona los dos clusters m√°s cercanos** en un nuevo cluster.
    * Este proceso contin√∫a hasta que todos los puntos de datos pertenecen a un √∫nico cluster grande.
    * La "cercan√≠a" entre clusters se define por una **m√©trica de enlace (linkage)**. Las m√©tricas de enlace comunes incluyen:
        * **Enlace √önico (Single Linkage):** Distancia m√≠nima entre dos puntos en diferentes clusters. Tiende a formar clusters "largos" y "delgados".
        * **Enlace Completo (Complete Linkage):** Distancia m√°xima entre dos puntos en diferentes clusters. Tiende a formar clusters compactos.
        * **Enlace Promedio (Average Linkage):** Distancia promedio entre todos los pares de puntos en diferentes clusters.
        * **M√©todo de Ward:** Minimiza la varianza total dentro de los clusters despu√©s de la fusi√≥n. Tiende a formar clusters compactos de tama√±o similar.

2.  **Agrupamiento Divisivo ("Top-Down"):**
    * Comienza con **todos los puntos en un solo cluster grande**.
    * En cada paso, **divide el cluster actual en dos sub-clusters** m√°s peque√±os.
    * Este proceso contin√∫a hasta que cada punto de datos est√° en su propio cluster individual.
    * Es menos com√∫n en la pr√°ctica debido a su mayor complejidad computacional.

La principal ventaja de hclust es que no requiere especificar el n√∫mero de clusters de antemano; en cambio, el n√∫mero de clusters se puede determinar inspeccionando el dendrograma y "cort√°ndolo" a una altura apropiada. Tambi√©n es muy bueno para revelar la estructura anidada de los datos.


**Aprendizaje Global vs. Local:**

El Agrupamiento Jer√°rquico (hclust) es un algoritmo que se puede clasificar como de **aprendizaje local** en su construcci√≥n incremental, pero que al final revela una **estructura global** de los datos.

* **Aspecto Local (Proceso de Fusi√≥n/Divisi√≥n):** En cada paso del agrupamiento aglomerativo, la decisi√≥n de qu√© clusters fusionar se basa **exclusivamente en la distancia (o similitud) entre los clusters m√°s cercanos en ese momento**. Esta es una decisi√≥n puramente local, ya que solo se consideran los pares de clusters m√°s pr√≥ximos. El algoritmo construye la jerarqu√≠a fusionando iterativamente los vecinos m√°s cercanos, lo que le permite adaptarse a la forma y densidad local de los datos. Las fronteras de los cl√∫steres no est√°n predefinidas por un modelo global; en cambio, emergen de las relaciones de proximidad locales. Esto permite a hclust descubrir clusters de **formas arbitrarias** y **relaciones no lineales** que podr√≠an no ser detectadas por m√©todos que asumen formas espec√≠ficas de clusters (como k-Means con suposiciones esf√©ricas).

* **Aspecto Global (Dendrograma):** Aunque las decisiones de fusi√≥n son locales, el resultado final (el dendrograma) es una **representaci√≥n jer√°rquica global** de las relaciones de todos los puntos de datos. Proporciona una visi√≥n completa de c√≥mo todos los puntos se agrupan en diferentes niveles de granularidad, desde clusters individuales hasta un solo cluster grande. Esta estructura global revela patrones de anidamiento y relaciones a diferentes escalas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering jer√°rquico)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Variables num√©ricas o categ√≥ricas (seg√∫n medida de distancia)",
  "‚úÖ Agrupa observaciones en base a similitud o distancia",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Sensible a valores at√≠picos que pueden distorsionar distancias",
  "‚ö†Ô∏è No afecta directamente (no hay predictores ni multicolinealidad)",
  "‚úÖ Dendrograma facilita interpretaci√≥n visual de grupos",
  "‚ö†Ô∏è Puede ser lento en datasets muy grandes",
  "‚ùå No se suele usar validaci√≥n cruzada, pero s√≠ m√©todos de evaluaci√≥n interna",
  "‚ùå Resultados muy sensibles a elecci√≥n de distancia y m√©todo de enlace"
)

detalles <- c(
  "M√©todo no supervisado para agrupar observaciones en una jerarqu√≠a basada en distancias.",
  "No busca predecir, sino identificar grupos o clusters.",
  "Puede trabajar con variables num√©ricas y categ√≥ricas si se define distancia adecuada.",
  "Construye dendrograma que muestra agrupamientos sucesivos desde observaciones individuales hasta un solo cluster.",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia de errores.",
  "No requiere homoscedasticidad.",
  "Valores at√≠picos pueden alterar significativamente la estructura del dendrograma.",
  "Como es una t√©cnica de agrupamiento, no existe multicolinealidad entre variables predictoras.",
  "Dendrograma permite interpretar las relaciones y agrupamientos entre observaciones.",
  "La complejidad aumenta r√°pido con el n√∫mero de observaciones (O(n^3)).",
  "Se eval√∫an √≠ndices de validaci√≥n de clusters (silhouette, Dunn, etc.) en lugar de CV.",
  "La elecci√≥n de m√©trica de distancia (Euclidiana, Manhattan) y m√©todo de enlace (completo, promedio, single) afecta mucho los resultados."
)

tabla_hclust <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_hclust %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir HClust",
             subtitle = "Hierarchical Clustering (hclust)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## k-Means  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/k-means.png"))
```

**k-Means** es uno de los algoritmos de **agrupamiento (clustering) no supervisado** m√°s populares y ampliamente utilizados. Su objetivo es particionar un conjunto de $n$ observaciones en $k$ grupos o "clusters", donde cada observaci√≥n pertenece al cluster cuyo centroide (media) es el m√°s cercano.

El algoritmo k-Means opera de la siguiente manera:

1.  **Inicializaci√≥n:**
    * Se elige un n√∫mero predefinido de clusters, $k$. Este es un **hiperpar√°metro** que debe ser especificado por el usuario.
    * Se inicializan $k$ **centroides** (puntos centrales de los clusters). Esto se puede hacer de forma aleatoria seleccionando $k$ puntos de datos al azar como centroides iniciales, o utilizando m√©todos m√°s sofisticados como k-Means++.

2.  **Paso de Asignaci√≥n (Expectation / E-step):**
    * Para cada punto de datos en el conjunto, se calcula su distancia (com√∫nmente euclidiana) a cada uno de los $k$ centroides.
    * Cada punto de datos se **asigna al cluster cuyo centroide es el m√°s cercano**.

3.  **Paso de Actualizaci√≥n (Maximization / M-step):**
    * Para cada uno de los $k$ clusters, se **recalcula la posici√≥n del centroide** como la media (promedio) de todos los puntos de datos que han sido asignados a ese cluster.

4.  **Iteraci√≥n:**
    * Los pasos de Asignaci√≥n y Actualizaci√≥n se repiten iterativamente.
    * El algoritmo converge cuando las asignaciones de los puntos a los clusters ya no cambian, o cuando las posiciones de los centroides no cambian significativamente entre iteraciones.

El objetivo del algoritmo es minimizar la **suma de los cuadrados de las distancias** de cada punto a su centroide asignado (tambi√©n conocida como la inercia del cluster o la suma de cuadrados dentro del cluster - WCSS).

**Ventajas:** Es simple de implementar, computacionalmente eficiente y escalable para grandes conjuntos de datos.
**Limitaciones:** Requiere que el n√∫mero de clusters $k$ sea especificado de antemano, es sensible a la inicializaci√≥n de los centroides, y tiende a formar clusters esf√©ricos de tama√±o similar, lo que puede ser una desventaja si los clusters tienen formas arbitrarias o densidades muy diferentes. Tambi√©n es sensible a los outliers.


**Aprendizaje Global vs. Local:**

k-Means es un modelo de **aprendizaje global**.

* **Aspecto Global:** k-Means busca una **partici√≥n global de todo el conjunto de datos** en $k$ clusters. El objetivo de la optimizaci√≥n (minimizar la suma de los cuadrados de las distancias a los centroides) se calcula sobre **todos los puntos de datos** y todos los clusters simult√°neamente. Los centroides, una vez convergidos, representan los "centros" de los clusters en el espacio de caracter√≠sticas, y estos se utilizan para asignar cualquier nuevo punto a su cluster correspondiente. La soluci√≥n final es una asignaci√≥n de cada punto a un cluster que se aplica a nivel global.

* **Asignaciones Locales dentro de una Optimizaci√≥n Global:** Aunque en cada iteraci√≥n los puntos se asignan a su centroide "local" m√°s cercano, esta asignaci√≥n es parte de un proceso iterativo que busca optimizar un criterio global (la inercia total del cluster). Los centroides mismos son influenciados por todos los puntos asignados a su cluster, y la reubicaci√≥n de los centroides afecta las asignaciones de todos los puntos en la siguiente iteraci√≥n. El resultado son **fronteras de decisi√≥n lineales (hiperplanos)** entre los clusters (cuyas combinaciones pueden formar pol√≠gonos de Voronoi), que son una caracter√≠stica de un modelo global que divide el espacio. Si los datos no se distribuyen linealmente y los clusters tienen formas no esf√©ricas o densidades muy diferentes, k-Means puede tener dificultades para descubrirlos, precisamente por su naturaleza global de optimizaci√≥n de la distancia euclidiana a un centroide.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering por partici√≥n)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Variables num√©ricas (recomendado estandarizar)",
  "‚úÖ Agrupa observaciones seg√∫n distancia a centroides",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Sensible a valores at√≠picos y centroides iniciales",
  "‚ö†Ô∏è No afecta directamente (no hay predictores ni multicolinealidad)",
  "‚úÖ F√°cil interpretaci√≥n de clusters y centroides",
  "‚úÖ R√°pido y eficiente para datasets grandes",
  "‚ùå No se usa validaci√≥n cruzada, pero s√≠ √≠ndices de cluster (silhouette, etc.)",
  "‚ùå No funciona bien con clusters no esf√©ricos o tama√±os muy dispares"
)

detalles <- c(
  "M√©todo no supervisado que particiona datos en k clusters minimizando suma de cuadrados dentro de clusters.",
  "No busca predecir, sino encontrar grupos o clusters.",
  "Requiere variables num√©ricas; es com√∫n estandarizarlas para evitar sesgos por escala.",
  "Cada observaci√≥n se asigna al cluster con el centroide m√°s cercano (distancia Euclidiana).",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia.",
  "No requiere homoscedasticidad.",
  "Los outliers pueden mover centroides y distorsionar clusters.",
  "Como t√©cnica de agrupamiento, no hay multicolinealidad entre variables.",
  "Centroides y clusters son f√°ciles de interpretar y visualizar.",
  "Algoritmo r√°pido, converge r√°pido en general.",
  "Se usan √≠ndices externos e internos para evaluar calidad del clustering, no validaci√≥n cruzada.",
  "No funciona bien si los clusters tienen formas complejas, tama√±os muy distintos o solapamientos."
)

tabla_kmeans <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_kmeans %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir k-means",
             subtitle = "K - Means")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## k-Medians  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/k-medians.png"))
```

**k-Medians** es un algoritmo de **agrupamiento (clustering) no supervisado** que es una variante de **k-Means**. Al igual que k-Means, su objetivo es particionar un conjunto de $n$ observaciones en $k$ grupos o "clusters". La principal diferencia radica en c√≥mo se calcula el "centro" de cada cluster y la m√©trica de distancia utilizada en su funci√≥n de costo.

Mientras que k-Means utiliza la **media (mean)** de los puntos de un cluster como su centroide y minimiza la suma de los **cuadrados de las distancias euclidianas** (norma L2), k-Medians utiliza la **mediana (median)** de los puntos de un cluster como su "centro" y minimiza la **suma de las distancias absolutas** (norma L1 o distancia de Manhattan).

El algoritmo k-Medians opera de manera muy similar a k-Means:

1.  **Inicializaci√≥n:**
    * Se elige un n√∫mero predefinido de clusters, $k$.
    * Se inicializan $k$ **medianas** (puntos centrales de los clusters), a menudo de forma aleatoria.

2.  **Paso de Asignaci√≥n:**
    * Para cada punto de datos en el conjunto, se calcula su **distancia de Manhattan (L1)** a cada una de las $k$ medianas.
    * Cada punto de datos se **asigna al cluster cuya mediana es la m√°s cercana**.

3.  **Paso de Actualizaci√≥n:**
    * Para cada uno de los $k$ clusters, se **recalcula la posici√≥n de la mediana** como la mediana multivariada (componente por componente) de todos los puntos de datos que han sido asignados a ese cluster.

4.  **Iteraci√≥n:**
    * Los pasos de Asignaci√≥n y Actualizaci√≥n se repiten iterativamente.
    * El algoritmo converge cuando las asignaciones de los puntos a los clusters ya no cambian, o cuando las posiciones de las medianas no cambian significativamente.

**Ventajas clave de k-Medians sobre k-Means:**

* **Robustez a Outliers:** Al usar la mediana en lugar de la media, k-Medians es significativamente **m√°s robusto a los valores at√≠picos (outliers)**. Los outliers influyen fuertemente en la media (tirando de ella), pero tienen un impacto mucho menor en la mediana.
* **M√©trica de Distancia:** La distancia L1 es a veces m√°s apropiada que la L2 cuando las diferencias entre las caracter√≠sticas son m√°s importantes que sus valores al cuadrado, o cuando los datos no son necesariamente continuos o gaussianos.

**Limitaciones:**
* Requiere que el n√∫mero de clusters $k$ sea especificado de antemano.
* La mediana multivariada puede ser m√°s compleja de calcular que la media.
* Puede ser m√°s lento que k-Means en algunos escenarios.

**Aprendizaje Global vs. Local:**

Al igual que k-Means, **k-Medians es un modelo de aprendizaje global**.

* **Aspecto Global:** k-Medians busca una **partici√≥n global de todo el conjunto de datos** en $k$ clusters. El objetivo de la optimizaci√≥n (minimizar la suma de las distancias L1 a las medianas) se calcula sobre **todos los puntos de datos** y todos los clusters simult√°neamente. Las medianas, una vez convergidas, representan los "centros" robustos de los clusters en el espacio de caracter√≠sticas, y estos se utilizan para asignar cualquier nuevo punto a su cluster correspondiente. La soluci√≥n final es una asignaci√≥n de cada punto a un cluster que se aplica a nivel global.

* **Asignaciones Locales dentro de una Optimizaci√≥n Global:** Si bien en cada iteraci√≥n los puntos se asignan a su mediana "local" m√°s cercana, esta asignaci√≥n es parte de un proceso iterativo que busca optimizar un criterio global (la suma total de distancias L1). Las medianas mismas son influenciadas por todos los puntos asignados a su cluster, y la reubicaci√≥n de las medianas afecta las asignaciones de todos los puntos en la siguiente iteraci√≥n. El resultado son **fronteras de decisi√≥n lineales** (debido al uso de la distancia L1, similar a las fronteras de Voronoi), que son una caracter√≠stica de un modelo global que divide el espacio. Aunque es m√°s robusto a outliers, k-Medians todav√≠a tiende a encontrar clusters que son m√°s o menos "esf√©ricos" o convexos en la m√©trica L1, y puede tener dificultades con clusters de formas muy arbitrarias o no lineales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering por partici√≥n)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Variables num√©ricas (recomendado estandarizar)",
  "‚úÖ Agrupa observaciones seg√∫n distancia a la mediana (Manhattan)",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚úÖ M√°s robusto a valores at√≠picos que k-Means",
  "‚ö†Ô∏è No afecta directamente (no hay predictores ni multicolinealidad)",
  "‚úÖ Interpretaci√≥n clara de clusters y medianas",
  "‚ö†Ô∏è Algo m√°s lento que k-Means por c√°lculo de medianas",
  "‚ùå No se usa validaci√≥n cruzada, pero s√≠ √≠ndices de cluster (silhouette, etc.)",
  "‚ùå No funciona bien con clusters no esf√©ricos o tama√±os muy dispares"
)

detalles <- c(
  "M√©todo no supervisado que particiona datos en k clusters minimizando suma de distancias absolutas dentro de clusters.",
  "No busca predecir, sino encontrar grupos o clusters.",
  "Requiere variables num√©ricas; se recomienda estandarizaci√≥n para evitar sesgo por escala.",
  "Cada observaci√≥n se asigna al cluster con la mediana m√°s cercana usando distancia Manhattan.",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia.",
  "No requiere homoscedasticidad.",
  "M√°s robusto frente a outliers porque usa medianas en lugar de medias.",
  "Como t√©cnica de agrupamiento, no hay multicolinealidad entre variables.",
  "Medianas y clusters son f√°ciles de interpretar y visualizar.",
  "Computacionalmente puede ser un poco m√°s lento que k-Means.",
  "Se usan √≠ndices externos e internos para evaluar calidad del clustering, no validaci√≥n cruzada.",
  "No funciona bien si los clusters tienen formas complejas, tama√±os muy distintos o solapamientos."
)

tabla_kmedians <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_kmedians %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir k-medians",
             subtitle = "K - Medians")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Mean-Shift {-}   


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/Mean-Shift.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_mean_shift <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_mean_shift <- c(
  "‚úÖ No Supervisado (Clustering, Estimaci√≥n de Densidad)",
  "‚ùå No aplica directamente",
  "‚úÖ Num√©ricas",
  "‚úÖ Muy compleja (determina cl√∫steres de forma arbitraria)",
  "‚ùå No es requisito",
  "‚ùå No aplica directamente",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (pero puede formar cl√∫steres peque√±os para ellos)",
  "‚úÖ Maneja bien (basado en la densidad)",
  "‚úÖ Moderada (los centros son puntos de densidad)",
  "‚ö†Ô∏è Baja (costo computacional alto para N grande)",
  "‚ùå No aplica directamente (evaluaci√≥n de la coherencia del cl√∫ster)",
  "‚ùå Dimensionalidad muy alta, ruido excesivo, elecci√≥n de ancho de banda"
)

detalles_mean_shift <- c(
  "Algoritmo de clustering no supervisado que no requiere especificar el n√∫mero de cl√∫steres de antemano. Funciona iterativamente desplazando cada punto de datos hacia la media de los puntos dentro de una ventana de vecindad, hasta que converge a un pico de densidad.",
  "Mean-Shift no tiene una 'variable respuesta'. Su objetivo es identificar picos de densidad en el espacio de caracter√≠sticas, donde cada pico representa el centro de un cl√∫ster, y asignar puntos a esos cl√∫steres.",
  "Las variables de entrada deben ser num√©ricas para permitir el c√°lculo de distancias y la estimaci√≥n de densidad. Es importante escalar las caracter√≠sticas para asegurar que las distancias sean significativas.",
  "Capta relaciones muy complejas y no lineales al identificar autom√°ticamente cl√∫steres de forma arbitraria (no solo esf√©ricos o el√≠pticos), bas√°ndose en la concentraci√≥n de puntos de datos. Se adapta a la forma intr√≠nseca de los datos.",
  "No hace suposiciones sobre la normalidad de los residuos ni la distribuci√≥n subyacente de los datos, ya que es un m√©todo no param√©trico basado en la estimaci√≥n de la densidad de Kernel.",
  "El concepto de independencia de errores no aplica directamente. El algoritmo se basa en la convergencia de puntos hacia picos de densidad en el espacio de caracter√≠sticas.",
  "No asume homoscedasticidad. La forma de los cl√∫steres y la densidad pueden variar en diferentes regiones del espacio de caracter√≠sticas.",
  "S√≠, Mean-Shift puede ser sensible a los outliers. Un outlier muy aislado puede convertirse en un 'cl√∫ster' de un solo miembro si el ancho de banda es peque√±o, o puede influir en la posici√≥n de la media si est√° cerca del borde de un cl√∫ster. El ruido excesivo puede crear muchos picos de densidad espurios.",
  "Maneja bien la multicolinealidad, ya que su enfoque se basa en la densidad de puntos en el espacio. Las caracter√≠sticas correlacionadas simplemente contribuir√°n a la concentraci√≥n de puntos en ciertas regiones.",
  "La interpretabilidad es moderada. Los centros de los cl√∫steres son picos de densidad, que pueden ser interpretados como puntos representativos o focos de la concentraci√≥n de datos. Sin embargo, no son necesariamente puntos de datos reales como en Affinity Propagation.",
  "La velocidad y eficiencia son bajas. La complejidad computacional de Mean-Shift es t√≠picamente O(N^2) en el n√∫mero de muestras (N), lo que la hace poco pr√°ctica para datasets con m√°s de unos pocos miles de puntos. Las implementaciones optimizadas pueden reducir esto a O(N log N) en algunos casos.",
  "La validaci√≥n cruzada no se aplica directamente en el clustering. La evaluaci√≥n se realiza utilizando m√©tricas de coherencia de cl√∫ster (ej., coeficiente de silueta) o comparando con agrupaciones conocidas si se tienen etiquetas de referencia (√≠ndice de Rand ajustado, V-measure).",
  "No funciona bien si: 1) el **dataset es muy grande** debido a su alta complejidad computacional, 2) la **dimensionalidad de las caracter√≠sticas es muy alta** (la estimaci√≥n de densidad se vuelve poco fiable debido a la maldici√≥n de la dimensionalidad), 3) el **par√°metro de ancho de banda (bandwidth)** es dif√≠cil de determinar o no se elige correctamente, o 4) los datos tienen **ruido excesivo** que lleva a muchos peque√±os cl√∫steres falsos."
)

tabla_mean_shift <- data.frame(Criterio = criterios_mean_shift, Aplica = aplica_mean_shift, Detalles = detalles_mean_shift)

tabla_mean_shift %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Mean-Shift",
    subtitle = "Agrupamiento Basado en la Densidad y Desplazamiento de Medias"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Ordering Points To Identify the Clustering Structure (OPTICS) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/OPTICS.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_optics <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_optics <- c(
  "‚úÖ No Supervisado (Clustering Basado en Densidad, Jer√°rquico)",
  "‚ùå No aplica directamente",
  "‚úÖ Num√©ricas",
  "‚úÖ Muy compleja (detecta cl√∫steres de forma arbitraria y anidada)",
  "‚ùå No es requisito",
  "‚ùå No aplica directamente",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (pero puede identificarlos como ruido)",
  "‚úÖ Maneja bien (basado en la densidad local)",
  "‚úÖ Moderada a Alta (a trav√©s de la gr√°fica de alcanzabilidad)",
  "‚ö†Ô∏è Baja (costo computacional alto para N grande)",
  "‚ùå No aplica directamente (evaluaci√≥n de la coherencia del cl√∫ster)",
  "‚ùå Dimensionalidad muy alta, ruido excesivo, significado de distancias"
)

detalles_optics <- c(
  "Algoritmo de clustering no supervisado basado en la densidad, similar a DBSCAN, pero con una ventaja clave: produce una **estructura jer√°rquica** de cl√∫steres. Genera un 'diagrama de alcanzabilidad' que permite extraer cl√∫steres de diferentes densidades y granularidades.",
  "OPTICS no tiene una 'variable respuesta'. Su objetivo es identificar la estructura de agrupamiento de los datos, incluyendo cl√∫steres de diferentes densidades y puntos de ruido, sin predefinir el n√∫mero de cl√∫steres.",
  "Las variables de entrada deben ser num√©ricas para calcular las distancias. Es crucial escalar las caracter√≠sticas para asegurar que las m√©tricas de distancia sean significativas.",
  "Capta relaciones muy complejas y no lineales al identificar cl√∫steres de forma arbitraria (no solo esf√©ricos) y tambi√©n cl√∫steres **anidados**. Funciona encontrando los 'puntos centrales' de cl√∫steres de alta densidad y extendi√©ndose a partir de ellos.",
  "No hace suposiciones sobre la normalidad de los residuos ni la distribuci√≥n subyacente de los datos, ya que es un m√©todo no param√©trico basado en la densidad.",
  "El concepto de independencia de errores no aplica directamente. El algoritmo se basa en las distancias y densidades locales entre los puntos.",
  "No asume homoscedasticidad. Es capaz de descubrir cl√∫steres con densidades variables en diferentes regiones del espacio de datos.",
  "S√≠, OPTICS puede ser sensible a los outliers, especialmente si se encuentran en regiones de baja densidad. Sin embargo, una de sus fortalezas es que puede **identificar expl√≠citamente el ruido** (puntos que no pertenecen a ning√∫n cl√∫ster denso), lo que lo hace √∫til para la detecci√≥n de anomal√≠as.",
  "Maneja bien la multicolinealidad, ya que su enfoque se basa en la densidad de puntos en el espacio. Las caracter√≠sticas correlacionadas simplemente contribuir√°n a la concentraci√≥n de puntos en ciertas regiones.",
  "La interpretabilidad es moderada a alta. El **diagrama de alcanzabilidad** es una herramienta visual poderosa que permite entender la jerarqu√≠a de los cl√∫steres, sus densidades relativas y c√≥mo se anidan. Elegir los cl√∫steres finales del diagrama requiere cierta interpretaci√≥n manual o un algoritmo de extracci√≥n.",
  "La velocidad y eficiencia son bajas para datasets muy grandes. Su complejidad computacional es t√≠picamente O(N^2) en el n√∫mero de muestras (N) en el peor de los casos (similar a la construcci√≥n de un √°rbol de distancias), lo que la hace poco pr√°ctica para m√°s de unas pocas decenas de miles de puntos. Las optimizaciones con √≠ndices espaciales pueden mejorar a O(N log N).",
  "La validaci√≥n cruzada no se aplica directamente en el clustering. La evaluaci√≥n se realiza utilizando m√©tricas de coherencia de cl√∫ster (ej., coeficiente de silueta) o comparando con agrupaciones conocidas si se tienen etiquetas de referencia (√≠ndice de Rand ajustado, V-measure). La evaluaci√≥n visual del diagrama de alcanzabilidad es clave.",
  "No funciona bien si: 1) el **dataset es muy grande** debido a su alta complejidad computacional y requisitos de memoria, 2) la **dimensionalidad de las caracter√≠sticas es muy alta** (la densidad se vuelve menos significativa debido a la maldici√≥n de la dimensionalidad), 3) hay **ruido excesivo** que dificulta la identificaci√≥n de densidades significativas, o 4) la elecci√≥n de los par√°metros `eps` (radio de vecindad) y `min_samples` (n√∫mero m√≠nimo de puntos) es dif√≠cil para el dominio espec√≠fico."
)

tabla_optics <- data.frame(Criterio = criterios_optics, Aplica = aplica_optics, Detalles = detalles_optics)

tabla_optics %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Ordering Points To Identify the Clustering Structure (OPTICS)",
    subtitle = "Agrupamiento Basado en la Densidad con Estructura Jer√°rquica"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Spectral Clustering {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/Spectral Clustering.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_spectral <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_spectral <- c(
  "‚úÖ No Supervisado (Clustering)",
  "‚ùå No aplica directamente",
  "‚úÖ Num√©ricas",
  "‚úÖ Muy compleja (detecta formas de cl√∫ster no convexas)",
  "‚ùå No es requisito",
  "‚ùå No aplica directamente",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (la construcci√≥n del grafo y los autovectores pueden afectarse)",
  "‚úÖ Maneja bien (si la similitud es adecuada)",
  "‚ö†Ô∏è Baja (los nuevos espacios son abstractos)",
  "‚ö†Ô∏è Moderada a Baja (costo computacional alto para N grande)",
  "‚ùå No aplica directamente (evaluaci√≥n de la coherencia del cl√∫ster)",
  "‚ùå Datos inconexos, ruido excesivo, elecci√≥n del n√∫mero de cl√∫steres"
)

detalles_spectral <- c(
  "Algoritmo de clustering no supervisado que aborda el problema de agrupamiento transformando los datos en un problema de corte de grafo. Construye una matriz de similitud (o afinidad) entre los puntos, la transforma en una matriz Laplaciana, y luego realiza una reducci√≥n de dimensionalidad con los autovectores de esta matriz antes de aplicar k-Means.",
  "Spectral Clustering no tiene una 'variable respuesta'. Su objetivo es identificar grupos de puntos de datos que est√°n fuertemente conectados en un grafo de similitud, incluso si estos grupos no son convexos.",
  "Las variables de entrada deben ser num√©ricas para calcular las similitudes/distancias. Es crucial escalar las caracter√≠sticas y seleccionar una m√©trica de similitud apropiada (ej., RBF kernel) para construir el grafo.",
  "Capta relaciones muy complejas y no lineales al considerar la conectividad entre los puntos en un grafo. Puede encontrar cl√∫steres de formas arbitrarias y no convexas, algo que algoritmos como k-Means no pueden hacer directamente.",
  "No hace suposiciones sobre la normalidad de los residuos ni la distribuci√≥n subyacente de los datos, ya que no es un modelo param√©trico.",
  "El concepto de independencia de errores no aplica directamente. El algoritmo se basa en la estructura de conectividad y similitud entre los puntos en un grafo.",
  "No asume homoscedasticidad. La forma y la dispersi√≥n de los cl√∫steres se derivan de la conectividad en el grafo, no de varianzas uniformes.",
  "S√≠, Spectral Clustering es sensible a los outliers. Un outlier puede afectar la construcci√≥n de la matriz de similitud (especialmente si usa un kernel RBF muy ajustado) y, por lo tanto, distorsionar la estructura del grafo y los autovectores calculados. El ruido tambi√©n puede crear 'puentes' espurios entre cl√∫steres o aislar puntos.",
  "Maneja bien la multicolinealidad, siempre y cuando la m√©trica de similitud utilizada sea robusta a las caracter√≠sticas correlacionadas. La transformaci√≥n a un grafo y la reducci√≥n de dimensionalidad con autovectores ayudan a mitigar este problema.",
  "La interpretabilidad es baja. Las nuevas dimensiones obtenidas de los autovectores no suelen tener un significado f√≠sico o intuitivo directo en t√©rminos de las caracter√≠sticas originales, a diferencia de PCA. La interpretabilidad se limita a la visualizaci√≥n de los cl√∫steres finales.",
  "La velocidad y eficiencia pueden ser bajas para datasets grandes. La construcci√≥n de la matriz de similitud es O(N^2), y el c√°lculo de autovectores de una matriz N x N es computacionalmente intensivo (t√≠picamente O(N^3) o m√°s eficiente con m√©todos iterativos). Esto lo hace poco pr√°ctico para m√°s de unos pocos miles de puntos.",
  "La validaci√≥n cruzada no se aplica directamente en el clustering, ya que no hay etiquetas verdaderas para predecir. La evaluaci√≥n se realiza utilizando m√©tricas de coherencia de cl√∫ster (ej., coeficiente de silueta) o comparando con agrupaciones conocidas si se tienen etiquetas de referencia (√≠ndice de Rand ajustado, V-measure).",
  "No funciona bien si: 1) el **dataset es muy grande** debido a su alta complejidad computacional, 2) los datos est√°n **demasiado dispersos** o inconexos en el grafo de similitud, lo que hace que los cortes no sean significativos, 3) la **elecci√≥n de los par√°metros del kernel** (ej., ancho de banda de RBF) y el n√∫mero de cl√∫steres (K) es dif√≠cil y cr√≠tica, o 4) hay **ruido excesivo** que genera conexiones espurias o aislamientos incorrectos."
)

tabla_spectral <- data.frame(Criterio = criterios_spectral, Aplica = aplica_spectral, Detalles = detalles_spectral)

tabla_spectral %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Spectral Clustering",
    subtitle = "Agrupamiento Basado en la Estructura Espectral del Grafo"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


