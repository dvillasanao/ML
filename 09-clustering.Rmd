# üìè 9. Clustering (Aprendizaje No Supervisado) {-}  

**Ejemplos:** K-Means, DBSCAN, Agrupamiento Jer√°rquico.   
**Uso:** Excelente para **agrupar datos sin etiquetas previas**, permiti√©ndote descubrir **estructuras ocultas** o identificar **segmentos de mercado** dentro de tus conjuntos de datos. Es una herramienta clave en la exploraci√≥n de datos.   
**Ventajas:** Es incre√≠blemente √∫til para la **exploraci√≥n de datos** y para **reducir la complejidad** al encontrar patrones inherentes.  
**Limitaciones:** Generalmente, necesitas **elegir el n√∫mero de grupos** de antemano (excepto en DBSCAN), lo cual puede ser un desaf√≠o. Adem√°s, algunos algoritmos pueden ser **sensibles a la escala** de las caracter√≠sticas de tus datos.

---

## Affinity Propagation {-}   

## Agglomerative Clustering {-}   

## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/DBSCAN.png"))
```

**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** es un algoritmo de **agrupamiento (clustering) no supervisado** que se distingue de los algoritmos basados en centroides (como k-Means) por su capacidad para encontrar **clusters de formas arbitrarias** y para identificar **puntos de ruido (outliers)**. Su idea central es que los clusters son regiones densas de puntos en el espacio de caracter√≠sticas, separadas por regiones de baja densidad.

DBSCAN define tres tipos de puntos:

1.  **Punto N√∫cleo (Core Point):** Un punto es un punto n√∫cleo si, dentro de un radio especificado ($\epsilon$ o `eps`), contiene un n√∫mero m√≠nimo de otros puntos ( `MinPts`).
2.  **Punto Frontera (Border Point):** Un punto es un punto frontera si est√° dentro del radio $\epsilon$ de un punto n√∫cleo, pero no es un punto n√∫cleo en s√≠ mismo (no tiene `MinPts` vecinos dentro de su propio radio $\epsilon$).
3.  **Punto de Ruido (Noise Point):** Cualquier punto que no es un punto n√∫cleo ni un punto frontera. Estos puntos son considerados outliers.

El algoritmo de DBSCAN opera de la siguiente manera:

1.  **Inicializaci√≥n:** Selecciona un punto arbitrario del conjunto de datos que a√∫n no ha sido visitado.
2.  **Expansi√≥n de Cluster:**
    * Si el punto seleccionado es un **punto n√∫cleo**, se inicia un nuevo cluster. Todos sus vecinos dentro del radio $\epsilon$ se a√±aden al cluster.
    * Recursivamente, se visitan y a√±aden los vecinos de esos nuevos puntos. Si un vecino es tambi√©n un punto n√∫cleo, sus propios vecinos tambi√©n se a√±aden al cluster. Este proceso contin√∫a hasta que no se puedan a√±adir m√°s puntos al cluster (es decir, todos los puntos alcanzables por densidad han sido encontrados).
    * Si el punto seleccionado **no es un punto n√∫cleo**, se marca como ruido (o se deja para ser procesado m√°s tarde si es un punto frontera de otro cluster ya formado).
3.  **Iteraci√≥n:** El proceso se repite con otro punto no visitado hasta que todos los puntos han sido procesados.

DBSCAN es particularmente √∫til para encontrar clusters complejos en conjuntos de datos ruidosos y no requiere que el usuario especifique el n√∫mero de clusters de antemano. Sus dos hiperpar√°metros clave son `eps` (el radio de b√∫squeda de vecindad) y `MinPts` (el n√∫mero m√≠nimo de puntos para formar un n√∫cleo).

**Aprendizaje Global vs. Local:**

DBSCAN es un algoritmo de **agrupamiento inherentemente local**, aunque el resultado final es una partici√≥n global de los datos en clusters y ruido.

* **Aspecto Local:** El coraz√≥n de DBSCAN reside en la definici√≥n de densidad local y la conectividad. Las decisiones sobre si un punto es un n√∫cleo, un frontera o ruido, y si dos puntos pertenecen al mismo cl√∫ster, se basan **exclusivamente en la densidad de puntos en un vecindario muy localizado** definido por el radio $\epsilon$ y el `MinPts`. El algoritmo "expande" los cl√∫steres al moverse de un punto n√∫cleo a sus vecinos, y de estos a sus vecinos, y as√≠ sucesivamente. Esta capacidad de crecer y formar cl√∫steres org√°nicamente a partir de las densidades locales es lo que permite a DBSCAN descubrir formas arbitrarias y adaptarse a la estructura local de los datos. No hay una funci√≥n global o centroides predefinidos que gu√≠en la agrupaci√≥n; todo se deriva de las propiedades de densidad local.

* **Resultado Global (Partici√≥n):** Aunque el proceso es local, el resultado final es una **partici√≥n global del conjunto de datos** en varios cl√∫steres y un conjunto de puntos de ruido. Una vez que todos los puntos han sido procesados y los cl√∫steres expandidos, se obtiene una vista global de la estructura de agrupamiento.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (distancias euclidianas u otras m√©tricas)",
  "‚úÖ Detecta clusters basados en densidad, no forma lineal",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚úÖ Robusto a outliers (los detecta como ruido)",
  "‚ö†Ô∏è No afecta directamente (no hay predictores)",
  "‚ö†Ô∏è Clusters pueden ser arbitrarios, pero es intuitivo identificar ruido",
  "‚úÖ Razonablemente r√°pido para conjuntos medianos",
  "‚ùå No usa validaci√≥n cruzada cl√°sica; se eval√∫a con m√©tricas de clustering",
  "‚ùå No funciona bien con clusters de densidades muy diferentes o alta dimensionalidad"
)

detalles <- c(
  "Algoritmo de clustering basado en densidad que agrupa puntos cercanos y marca puntos aislados como ruido.",
  "No busca predecir, sino agrupar observaciones.",
  "Se basa en distancias; variables num√©ricas adecuadas; variables categ√≥ricas necesitan transformaci√≥n.",
  "No asume formas de clusters lineales ni convexas; puede detectar clusters arbitrarios.",
  "No genera residuos; no aplica normalidad.",
  "No hay modelo de error residual, no aplica independencia.",
  "No es un modelo predictivo, no aplica homoscedasticidad.",
  "Detecta outliers etiquet√°ndolos como ruido, siendo robusto frente a ellos.",
  "No hay predictores en sentido tradicional, por lo que multicolinealidad no afecta.",
  "Interpretaci√≥n basada en grupos densos y puntos aislados (ruido).",
  "Es eficiente, aunque su rendimiento puede disminuir en alta dimensionalidad.",
  "No utiliza validaci√≥n cruzada est√°ndar; evaluaci√≥n se basa en √≠ndices de clustering como Silhouette.",
  "Dificultades con clusters con diferentes densidades y cuando la dimensionalidad es muy alta."
)

tabla_dbscan <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_dbscan %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir DBSCAN",
             subtitle = "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Expectation Maximization (EM) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/EM.png"))
```

El algoritmo **Expectation-Maximization (EM)** es un m√©todo iterativo utilizado en estad√≠stica para encontrar las **estimaciones de m√°xima verosimilitud (MLE)** o las estimaciones de m√°xima a posteriori (MAP) de los par√°metros en modelos estad√≠sticos, especialmente cuando el modelo depende de **variables latentes (no observadas o "ocultas")** o cuando los datos est√°n "incompletos".

EM es particularmente √∫til para modelos de mezcla, donde se asume que los datos observados son una mezcla de varias distribuciones subyacentes, y la pertenencia de cada punto de datos a una distribuci√≥n espec√≠fica es la variable latente. El algoritmo consta de dos pasos principales que se alternan hasta la convergencia:

1.  **Paso E (Expectation Step - Paso de Expectativa):**
    * En este paso, dadas las estimaciones actuales de los par√°metros del modelo, se calculan las **probabilidades esperadas (o "responsabilidades")** de que cada punto de datos observado pertenezca a cada una de las componentes latentes (o de que las variables latentes tomen ciertos valores).
    * Esencialmente, se est√° haciendo una "suposici√≥n" sobre los valores de las variables latentes bas√°ndose en los par√°metros actuales del modelo y los datos observados.

2.  **Paso M (Maximization Step - Paso de Maximizaci√≥n):**
    * En este paso, utilizando las "responsabilidades" calculadas en el Paso E (trat√°ndolas como si fueran observaciones completas), se **re-estiman los par√°metros del modelo** para maximizar la verosimilitud esperada.
    * Esto es t√≠picamente un problema de optimizaci√≥n m√°s simple que el problema original de m√°xima verosimilitud con datos incompletos. Se ajustan los par√°metros (ej., medias, varianzas, pesos de mezcla) para que el modelo se ajuste mejor a los datos, considerando las asignaciones "blandas" a las variables latentes.

Los Pasos E y M se repiten iterativamente. La verosimilitud del modelo est√° garantizada para no disminuir en cada iteraci√≥n, y el algoritmo converge a un **m√°ximo local** de la funci√≥n de verosimilitud.

**Aplicaciones comunes:**
* **Modelos de Mezcla Gaussiana (GMMs):** Un uso protot√≠pico del EM para el clustering no supervisado.
* **Modelos Ocultos de Markov (HMMs):** Para problemas de reconocimiento de voz y bioinform√°tica.
* **Imputaci√≥n de datos faltantes:** Para estimar valores faltantes en un conjunto de datos.
* **An√°lisis de componentes latentes.**


**Aprendizaje Global vs. Local:**

El algoritmo Expectation-Maximization (EM) es un m√©todo de **aprendizaje global**, pero es importante entender un matiz sobre su convergencia.

* **Aspecto Global:** EM tiene como objetivo encontrar los **par√°metros de un modelo probabil√≠stico global** (como un GMM completo que describe la distribuci√≥n de todo el conjunto de datos) que maximicen la verosimilitud de los datos observados. Los par√°metros que se estiman (medias, covarianzas, pesos de mezcla en un GMM) son v√°lidos para todo el espacio de caracter√≠sticas. El algoritmo itera sobre todo el conjunto de datos en cada paso E y M para actualizar estos par√°metros globales. La soluci√≥n que busca EM es una representaci√≥n unificada y global de las distribuciones subyacentes de los datos.

* **Convergencia a M√°ximos Locales:** Aunque EM busca una soluci√≥n global, una limitaci√≥n cr√≠tica es que **solo est√° garantizado para converger a un m√°ximo local** de la funci√≥n de verosimilitud, no necesariamente al m√°ximo global. Esto significa que el resultado final puede depender de la **inicializaci√≥n** de los par√°metros del modelo. Si la funci√≥n de verosimilitud tiene m√∫ltiples "picos" (m√°ximos locales), EM puede quedar "atrapado" en uno de ellos. Para mitigar esto, es una pr√°ctica com√∫n ejecutar EM varias veces con diferentes inicializaciones aleatorias y seleccionar el resultado con la verosimilitud m√°s alta.

Por lo tanto, mientras que el objetivo de EM es aprender un modelo global que abarque todo el espacio de datos, su m√©todo iterativo de optimizaci√≥n lo hace susceptible a encontrar √≥ptimos locales en la funci√≥n de verosimilitud. La forma en que un modelo probabil√≠stico como un GMM puede modelar **relaciones no lineales** en los datos es que, al combinar m√∫ltiples distribuciones gaussianas (lineales), el modelo resultante puede capturar formas y densidades complejas y no lineales en el espacio de caracter√≠sticas. EM es el algoritmo que permite aprender estos componentes subyacentes.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (estimaci√≥n de par√°metros en modelos con datos incompletos o mixtos)",
  "‚ùå No aplica directamente (modelo probabil√≠stico)",
  "‚úÖ Variables num√©ricas o categ√≥ricas seg√∫n modelo",
  "‚úÖ Estima par√°metros m√°ximos de verosimilitud, puede manejar modelos complejos",
  "‚ö†Ô∏è Depende del modelo espec√≠fico usado con EM",
  "‚ö†Ô∏è Depende del modelo; errores independientes si asume modelo estad√≠stico cl√°sico",
  "‚ö†Ô∏è Depende del modelo estad√≠stico subyacente",
  "‚ö†Ô∏è Puede ser sensible a outliers dependiendo del modelo y datos",
  "‚ö†Ô∏è Depende del modelo y las variables involucradas",
  "‚ö†Ô∏è La interpretaci√≥n depende del modelo y par√°metros estimados",
  "‚ùå Puede ser lento si el modelo es complejo o datos muy grandes",
  "‚ùå Validaci√≥n cruzada depende del modelo, no es intr√≠nseco a EM",
  "‚ùå Puede converger a m√°ximos locales; requiere buen punto inicial y modelo adecuado"
)

detalles <- c(
  "Algoritmo iterativo para estimar par√°metros de modelos estad√≠sticos con datos faltantes o variables latentes.",
  "No genera predicciones directas, sino estima par√°metros para modelos probabil√≠sticos.",
  "Aplicable a datos num√©ricos o categ√≥ricos dependiendo del modelo (mezcla de Gaussianas, por ejemplo).",
  "Maximiza la funci√≥n de verosimilitud de manera iterativa, estimando variables latentes y par√°metros.",
  "La normalidad depende del modelo (por ejemplo, mezcla de Gaussianas asume normalidad).",
  "Si el modelo asume errores independientes, entonces s√≠; depende del modelo estad√≠stico usado.",
  "Homoscedasticidad depende del modelo estad√≠stico subyacente.",
  "Sensibilidad a outliers var√≠a seg√∫n la robustez del modelo y datos.",
  "Multicolinealidad afecta seg√∫n la estructura del modelo y variables involucradas.",
  "Interpretaci√≥n es sobre par√°metros estimados y variables latentes, no sobre coeficientes directos.",
  "Puede requerir muchas iteraciones, afectando velocidad en modelos complejos.",
  "La validaci√≥n cruzada depende del modelo aplicado tras la estimaci√≥n por EM.",
  "Puede quedarse atrapado en soluciones sub√≥ptimas; se recomienda m√∫ltiples inicios."
)

tabla_em <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_em %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir EM",
             subtitle = "Expectation Maximization (EM)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Gaussian Mixture Models (GMMs) {-}   


## Hierarchical Clustering (hclust) {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/hclust.png"))
```

El **Agrupamiento Jer√°rquico (Hierarchical Clustering)**, a menudo abreviado como **hclust**, es un m√©todo de **agrupamiento (clustering) no supervisado** que construye una **jerarqu√≠a de clusters** en lugar de una partici√≥n plana de los datos (como k-Means). El resultado de un agrupamiento jer√°rquico se visualiza com√∫nmente como un **dendrograma**, un diagrama en forma de √°rbol que muestra la secuencia de fusiones o divisiones de los clusters.

Existen dos tipos principales de agrupamiento jer√°rquico:

1.  **Agrupamiento Aglomerativo ("Bottom-Up"):** Es el tipo m√°s com√∫n.
    * Comienza tratando **cada punto de datos como un cluster individual**.
    * En cada paso, **fusiona los dos clusters m√°s cercanos** en un nuevo cluster.
    * Este proceso contin√∫a hasta que todos los puntos de datos pertenecen a un √∫nico cluster grande.
    * La "cercan√≠a" entre clusters se define por una **m√©trica de enlace (linkage)**. Las m√©tricas de enlace comunes incluyen:
        * **Enlace √önico (Single Linkage):** Distancia m√≠nima entre dos puntos en diferentes clusters. Tiende a formar clusters "largos" y "delgados".
        * **Enlace Completo (Complete Linkage):** Distancia m√°xima entre dos puntos en diferentes clusters. Tiende a formar clusters compactos.
        * **Enlace Promedio (Average Linkage):** Distancia promedio entre todos los pares de puntos en diferentes clusters.
        * **M√©todo de Ward:** Minimiza la varianza total dentro de los clusters despu√©s de la fusi√≥n. Tiende a formar clusters compactos de tama√±o similar.

2.  **Agrupamiento Divisivo ("Top-Down"):**
    * Comienza con **todos los puntos en un solo cluster grande**.
    * En cada paso, **divide el cluster actual en dos sub-clusters** m√°s peque√±os.
    * Este proceso contin√∫a hasta que cada punto de datos est√° en su propio cluster individual.
    * Es menos com√∫n en la pr√°ctica debido a su mayor complejidad computacional.

La principal ventaja de hclust es que no requiere especificar el n√∫mero de clusters de antemano; en cambio, el n√∫mero de clusters se puede determinar inspeccionando el dendrograma y "cort√°ndolo" a una altura apropiada. Tambi√©n es muy bueno para revelar la estructura anidada de los datos.


**Aprendizaje Global vs. Local:**

El Agrupamiento Jer√°rquico (hclust) es un algoritmo que se puede clasificar como de **aprendizaje local** en su construcci√≥n incremental, pero que al final revela una **estructura global** de los datos.

* **Aspecto Local (Proceso de Fusi√≥n/Divisi√≥n):** En cada paso del agrupamiento aglomerativo, la decisi√≥n de qu√© clusters fusionar se basa **exclusivamente en la distancia (o similitud) entre los clusters m√°s cercanos en ese momento**. Esta es una decisi√≥n puramente local, ya que solo se consideran los pares de clusters m√°s pr√≥ximos. El algoritmo construye la jerarqu√≠a fusionando iterativamente los vecinos m√°s cercanos, lo que le permite adaptarse a la forma y densidad local de los datos. Las fronteras de los cl√∫steres no est√°n predefinidas por un modelo global; en cambio, emergen de las relaciones de proximidad locales. Esto permite a hclust descubrir clusters de **formas arbitrarias** y **relaciones no lineales** que podr√≠an no ser detectadas por m√©todos que asumen formas espec√≠ficas de clusters (como k-Means con suposiciones esf√©ricas).

* **Aspecto Global (Dendrograma):** Aunque las decisiones de fusi√≥n son locales, el resultado final (el dendrograma) es una **representaci√≥n jer√°rquica global** de las relaciones de todos los puntos de datos. Proporciona una visi√≥n completa de c√≥mo todos los puntos se agrupan en diferentes niveles de granularidad, desde clusters individuales hasta un solo cluster grande. Esta estructura global revela patrones de anidamiento y relaciones a diferentes escalas.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering jer√°rquico)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Variables num√©ricas o categ√≥ricas (seg√∫n medida de distancia)",
  "‚úÖ Agrupa observaciones en base a similitud o distancia",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Sensible a valores at√≠picos que pueden distorsionar distancias",
  "‚ö†Ô∏è No afecta directamente (no hay predictores ni multicolinealidad)",
  "‚úÖ Dendrograma facilita interpretaci√≥n visual de grupos",
  "‚ö†Ô∏è Puede ser lento en datasets muy grandes",
  "‚ùå No se suele usar validaci√≥n cruzada, pero s√≠ m√©todos de evaluaci√≥n interna",
  "‚ùå Resultados muy sensibles a elecci√≥n de distancia y m√©todo de enlace"
)

detalles <- c(
  "M√©todo no supervisado para agrupar observaciones en una jerarqu√≠a basada en distancias.",
  "No busca predecir, sino identificar grupos o clusters.",
  "Puede trabajar con variables num√©ricas y categ√≥ricas si se define distancia adecuada.",
  "Construye dendrograma que muestra agrupamientos sucesivos desde observaciones individuales hasta un solo cluster.",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia de errores.",
  "No requiere homoscedasticidad.",
  "Valores at√≠picos pueden alterar significativamente la estructura del dendrograma.",
  "Como es una t√©cnica de agrupamiento, no existe multicolinealidad entre variables predictoras.",
  "Dendrograma permite interpretar las relaciones y agrupamientos entre observaciones.",
  "La complejidad aumenta r√°pido con el n√∫mero de observaciones (O(n^3)).",
  "Se eval√∫an √≠ndices de validaci√≥n de clusters (silhouette, Dunn, etc.) en lugar de CV.",
  "La elecci√≥n de m√©trica de distancia (Euclidiana, Manhattan) y m√©todo de enlace (completo, promedio, single) afecta mucho los resultados."
)

tabla_hclust <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_hclust %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir HClust",
             subtitle = "Hierarchical Clustering (hclust)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## k-Means  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/k-means.png"))
```

**k-Means** es uno de los algoritmos de **agrupamiento (clustering) no supervisado** m√°s populares y ampliamente utilizados. Su objetivo es particionar un conjunto de $n$ observaciones en $k$ grupos o "clusters", donde cada observaci√≥n pertenece al cluster cuyo centroide (media) es el m√°s cercano.

El algoritmo k-Means opera de la siguiente manera:

1.  **Inicializaci√≥n:**
    * Se elige un n√∫mero predefinido de clusters, $k$. Este es un **hiperpar√°metro** que debe ser especificado por el usuario.
    * Se inicializan $k$ **centroides** (puntos centrales de los clusters). Esto se puede hacer de forma aleatoria seleccionando $k$ puntos de datos al azar como centroides iniciales, o utilizando m√©todos m√°s sofisticados como k-Means++.

2.  **Paso de Asignaci√≥n (Expectation / E-step):**
    * Para cada punto de datos en el conjunto, se calcula su distancia (com√∫nmente euclidiana) a cada uno de los $k$ centroides.
    * Cada punto de datos se **asigna al cluster cuyo centroide es el m√°s cercano**.

3.  **Paso de Actualizaci√≥n (Maximization / M-step):**
    * Para cada uno de los $k$ clusters, se **recalcula la posici√≥n del centroide** como la media (promedio) de todos los puntos de datos que han sido asignados a ese cluster.

4.  **Iteraci√≥n:**
    * Los pasos de Asignaci√≥n y Actualizaci√≥n se repiten iterativamente.
    * El algoritmo converge cuando las asignaciones de los puntos a los clusters ya no cambian, o cuando las posiciones de los centroides no cambian significativamente entre iteraciones.

El objetivo del algoritmo es minimizar la **suma de los cuadrados de las distancias** de cada punto a su centroide asignado (tambi√©n conocida como la inercia del cluster o la suma de cuadrados dentro del cluster - WCSS).

**Ventajas:** Es simple de implementar, computacionalmente eficiente y escalable para grandes conjuntos de datos.
**Limitaciones:** Requiere que el n√∫mero de clusters $k$ sea especificado de antemano, es sensible a la inicializaci√≥n de los centroides, y tiende a formar clusters esf√©ricos de tama√±o similar, lo que puede ser una desventaja si los clusters tienen formas arbitrarias o densidades muy diferentes. Tambi√©n es sensible a los outliers.


**Aprendizaje Global vs. Local:**

k-Means es un modelo de **aprendizaje global**.

* **Aspecto Global:** k-Means busca una **partici√≥n global de todo el conjunto de datos** en $k$ clusters. El objetivo de la optimizaci√≥n (minimizar la suma de los cuadrados de las distancias a los centroides) se calcula sobre **todos los puntos de datos** y todos los clusters simult√°neamente. Los centroides, una vez convergidos, representan los "centros" de los clusters en el espacio de caracter√≠sticas, y estos se utilizan para asignar cualquier nuevo punto a su cluster correspondiente. La soluci√≥n final es una asignaci√≥n de cada punto a un cluster que se aplica a nivel global.

* **Asignaciones Locales dentro de una Optimizaci√≥n Global:** Aunque en cada iteraci√≥n los puntos se asignan a su centroide "local" m√°s cercano, esta asignaci√≥n es parte de un proceso iterativo que busca optimizar un criterio global (la inercia total del cluster). Los centroides mismos son influenciados por todos los puntos asignados a su cluster, y la reubicaci√≥n de los centroides afecta las asignaciones de todos los puntos en la siguiente iteraci√≥n. El resultado son **fronteras de decisi√≥n lineales (hiperplanos)** entre los clusters (cuyas combinaciones pueden formar pol√≠gonos de Voronoi), que son una caracter√≠stica de un modelo global que divide el espacio. Si los datos no se distribuyen linealmente y los clusters tienen formas no esf√©ricas o densidades muy diferentes, k-Means puede tener dificultades para descubrirlos, precisamente por su naturaleza global de optimizaci√≥n de la distancia euclidiana a un centroide.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering por partici√≥n)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Variables num√©ricas (recomendado estandarizar)",
  "‚úÖ Agrupa observaciones seg√∫n distancia a centroides",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Sensible a valores at√≠picos y centroides iniciales",
  "‚ö†Ô∏è No afecta directamente (no hay predictores ni multicolinealidad)",
  "‚úÖ F√°cil interpretaci√≥n de clusters y centroides",
  "‚úÖ R√°pido y eficiente para datasets grandes",
  "‚ùå No se usa validaci√≥n cruzada, pero s√≠ √≠ndices de cluster (silhouette, etc.)",
  "‚ùå No funciona bien con clusters no esf√©ricos o tama√±os muy dispares"
)

detalles <- c(
  "M√©todo no supervisado que particiona datos en k clusters minimizando suma de cuadrados dentro de clusters.",
  "No busca predecir, sino encontrar grupos o clusters.",
  "Requiere variables num√©ricas; es com√∫n estandarizarlas para evitar sesgos por escala.",
  "Cada observaci√≥n se asigna al cluster con el centroide m√°s cercano (distancia Euclidiana).",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia.",
  "No requiere homoscedasticidad.",
  "Los outliers pueden mover centroides y distorsionar clusters.",
  "Como t√©cnica de agrupamiento, no hay multicolinealidad entre variables.",
  "Centroides y clusters son f√°ciles de interpretar y visualizar.",
  "Algoritmo r√°pido, converge r√°pido en general.",
  "Se usan √≠ndices externos e internos para evaluar calidad del clustering, no validaci√≥n cruzada.",
  "No funciona bien si los clusters tienen formas complejas, tama√±os muy distintos o solapamientos."
)

tabla_kmeans <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_kmeans %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir k-means",
             subtitle = "K - Means")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## k-Medians  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Clustering/k-medians.png"))
```

**k-Medians** es un algoritmo de **agrupamiento (clustering) no supervisado** que es una variante de **k-Means**. Al igual que k-Means, su objetivo es particionar un conjunto de $n$ observaciones en $k$ grupos o "clusters". La principal diferencia radica en c√≥mo se calcula el "centro" de cada cluster y la m√©trica de distancia utilizada en su funci√≥n de costo.

Mientras que k-Means utiliza la **media (mean)** de los puntos de un cluster como su centroide y minimiza la suma de los **cuadrados de las distancias euclidianas** (norma L2), k-Medians utiliza la **mediana (median)** de los puntos de un cluster como su "centro" y minimiza la **suma de las distancias absolutas** (norma L1 o distancia de Manhattan).

El algoritmo k-Medians opera de manera muy similar a k-Means:

1.  **Inicializaci√≥n:**
    * Se elige un n√∫mero predefinido de clusters, $k$.
    * Se inicializan $k$ **medianas** (puntos centrales de los clusters), a menudo de forma aleatoria.

2.  **Paso de Asignaci√≥n:**
    * Para cada punto de datos en el conjunto, se calcula su **distancia de Manhattan (L1)** a cada una de las $k$ medianas.
    * Cada punto de datos se **asigna al cluster cuya mediana es la m√°s cercana**.

3.  **Paso de Actualizaci√≥n:**
    * Para cada uno de los $k$ clusters, se **recalcula la posici√≥n de la mediana** como la mediana multivariada (componente por componente) de todos los puntos de datos que han sido asignados a ese cluster.

4.  **Iteraci√≥n:**
    * Los pasos de Asignaci√≥n y Actualizaci√≥n se repiten iterativamente.
    * El algoritmo converge cuando las asignaciones de los puntos a los clusters ya no cambian, o cuando las posiciones de las medianas no cambian significativamente.

**Ventajas clave de k-Medians sobre k-Means:**

* **Robustez a Outliers:** Al usar la mediana en lugar de la media, k-Medians es significativamente **m√°s robusto a los valores at√≠picos (outliers)**. Los outliers influyen fuertemente en la media (tirando de ella), pero tienen un impacto mucho menor en la mediana.
* **M√©trica de Distancia:** La distancia L1 es a veces m√°s apropiada que la L2 cuando las diferencias entre las caracter√≠sticas son m√°s importantes que sus valores al cuadrado, o cuando los datos no son necesariamente continuos o gaussianos.

**Limitaciones:**
* Requiere que el n√∫mero de clusters $k$ sea especificado de antemano.
* La mediana multivariada puede ser m√°s compleja de calcular que la media.
* Puede ser m√°s lento que k-Means en algunos escenarios.

**Aprendizaje Global vs. Local:**

Al igual que k-Means, **k-Medians es un modelo de aprendizaje global**.

* **Aspecto Global:** k-Medians busca una **partici√≥n global de todo el conjunto de datos** en $k$ clusters. El objetivo de la optimizaci√≥n (minimizar la suma de las distancias L1 a las medianas) se calcula sobre **todos los puntos de datos** y todos los clusters simult√°neamente. Las medianas, una vez convergidas, representan los "centros" robustos de los clusters en el espacio de caracter√≠sticas, y estos se utilizan para asignar cualquier nuevo punto a su cluster correspondiente. La soluci√≥n final es una asignaci√≥n de cada punto a un cluster que se aplica a nivel global.

* **Asignaciones Locales dentro de una Optimizaci√≥n Global:** Si bien en cada iteraci√≥n los puntos se asignan a su mediana "local" m√°s cercana, esta asignaci√≥n es parte de un proceso iterativo que busca optimizar un criterio global (la suma total de distancias L1). Las medianas mismas son influenciadas por todos los puntos asignados a su cluster, y la reubicaci√≥n de las medianas afecta las asignaciones de todos los puntos en la siguiente iteraci√≥n. El resultado son **fronteras de decisi√≥n lineales** (debido al uso de la distancia L1, similar a las fronteras de Voronoi), que son una caracter√≠stica de un modelo global que divide el espacio. Aunque es m√°s robusto a outliers, k-Medians todav√≠a tiende a encontrar clusters que son m√°s o menos "esf√©ricos" o convexos en la m√©trica L1, y puede tener dificultades con clusters de formas muy arbitrarias o no lineales.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (clustering por partici√≥n)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Variables num√©ricas (recomendado estandarizar)",
  "‚úÖ Agrupa observaciones seg√∫n distancia a la mediana (Manhattan)",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚úÖ M√°s robusto a valores at√≠picos que k-Means",
  "‚ö†Ô∏è No afecta directamente (no hay predictores ni multicolinealidad)",
  "‚úÖ Interpretaci√≥n clara de clusters y medianas",
  "‚ö†Ô∏è Algo m√°s lento que k-Means por c√°lculo de medianas",
  "‚ùå No se usa validaci√≥n cruzada, pero s√≠ √≠ndices de cluster (silhouette, etc.)",
  "‚ùå No funciona bien con clusters no esf√©ricos o tama√±os muy dispares"
)

detalles <- c(
  "M√©todo no supervisado que particiona datos en k clusters minimizando suma de distancias absolutas dentro de clusters.",
  "No busca predecir, sino encontrar grupos o clusters.",
  "Requiere variables num√©ricas; se recomienda estandarizaci√≥n para evitar sesgo por escala.",
  "Cada observaci√≥n se asigna al cluster con la mediana m√°s cercana usando distancia Manhattan.",
  "No genera residuos ni modelo predictivo.",
  "No hay supuestos de independencia.",
  "No requiere homoscedasticidad.",
  "M√°s robusto frente a outliers porque usa medianas en lugar de medias.",
  "Como t√©cnica de agrupamiento, no hay multicolinealidad entre variables.",
  "Medianas y clusters son f√°ciles de interpretar y visualizar.",
  "Computacionalmente puede ser un poco m√°s lento que k-Means.",
  "Se usan √≠ndices externos e internos para evaluar calidad del clustering, no validaci√≥n cruzada.",
  "No funciona bien si los clusters tienen formas complejas, tama√±os muy distintos o solapamientos."
)

tabla_kmedians <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_kmedians %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir k-medians",
             subtitle = "K - Medians")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Mean-Shift {-}   

## Ordering Points To Identify the Clustering Structure (OPTICS) {-}   

## Spectral Clustering {-}   


