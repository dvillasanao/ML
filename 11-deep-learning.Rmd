# ü§ñ 4. Deep Learning {-}  

**Ejemplos:** CNN, RNN, Transformers.  
**Uso:** Ideal para **im√°genes**, **texto** y **series temporales**, especialmente con **grandes datos no estructurados**.  
**Ventajas:** Poderoso para datos complejos.   
**Limitaciones:** Exige **mucha data** y **computaci√≥n**; **poca interpretabilidad**.   

---



## Deep Boltzman Machine (DBM) {-}  


```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/DBM.png"))
```

El modelo de **Deep Boltzman Machine (DBM)** es un tipo de **red neuronal profunda generativa** que pertenece a la familia de los **modelos gr√°ficos probabil√≠sticos**. Se construye apilando m√∫ltiples **M√°quinas de Boltzmann Restringidas (RBMs)**, lo que le permite aprender representaciones jer√°rquicas y abstractas de los datos de entrada. Su principal objetivo es modelar la **distribuci√≥n de probabilidad conjunta** entre un conjunto de variables observables y m√∫ltiples capas de variables latentes (ocultas).

Las DBMs son modelos **no dirigidos** (las conexiones entre las neuronas son sim√©tricas y no tienen una direcci√≥n espec√≠fica) y est√°n compuestas por capas de unidades visibles (los datos de entrada) y varias capas de unidades ocultas. A diferencia de las RBMs simples que tienen una sola capa oculta, las DBMs tienen **m√∫ltiples capas ocultas**, lo que les permite capturar dependencias m√°s complejas y caracter√≠sticas de alto nivel en los datos. El proceso de aprendizaje en una DBM busca ajustar los pesos de las conexiones de manera que la red asigne una alta probabilidad a los datos de entrenamiento y una baja probabilidad a los datos que no son de entrenamiento.

Las caracter√≠sticas clave de las DBMs incluyen:

1.  **Representaci√≥n Jer√°rquica:** Cada capa oculta aprende representaciones progresivamente m√°s abstractas de los datos. Las primeras capas pueden capturar caracter√≠sticas de bajo nivel (ej., bordes en im√°genes), mientras que las capas superiores combinan estas para formar representaciones de alto nivel (ej., partes de objetos o conceptos).
2.  **Aprendizaje No Supervisado:** Las DBMs se entrenan t√≠picamente de forma **no supervisada**, lo que significa que no requieren etiquetas para el entrenamiento. Esto las hace valiosas para el pre-entrenamiento de modelos profundos en conjuntos de datos grandes y sin etiquetar, donde pueden aprender caracter√≠sticas √∫tiles que luego pueden ser utilizadas en tareas de aprendizaje supervisado (como la clasificaci√≥n).
3.  **Inferencia y Generaci√≥n:** Una vez entrenadas, las DBMs pueden ser utilizadas tanto para **inferencia** (estimar las representaciones ocultas dadas las entradas visibles) como para **generaci√≥n** (muestrear nuevas instancias de datos a partir de la distribuci√≥n aprendida del modelo).

Debido a su complejidad computacional en el entrenamiento exacto, las DBMs a menudo se entrenan utilizando un enfoque de **aprendizaje codicioso por capas** (entrenando RBMs individuales y apil√°ndolas) seguido de un ajuste fino de todo el modelo utilizando algoritmos como el **Contraste Divergente Aproximado (ACD)**.


**Aprendizaje Global vs. Local:**

El modelo de **M√°quina de Boltzmann Profunda (DBM)** es un modelo de **aprendizaje global**.

* **Aspecto Global:** Las DBMs construyen un **modelo probabil√≠stico unificado y global** de la distribuci√≥n de los datos. Los pesos de conexi√≥n en todas las capas de la red se ajustan para representar las dependencias y correlaciones en todo el espacio de entrada. No se crean modelos espec√≠ficos para subconjuntos locales de datos; en cambio, el modelo aprende una representaci√≥n coherente y jer√°rquica que se aplica a todos los puntos de datos. La funci√≥n de energ√≠a (o funci√≥n de coste) de la DBM se define sobre el espacio completo de variables visibles y ocultas, y el entrenamiento busca minimizar esta energ√≠a globalmente para que los datos de entrenamiento tengan una energ√≠a baja.

* **Impacto de la Estructura Jer√°rquica:** Aunque la DBM aprende representaciones en diferentes niveles de abstracci√≥n (jerarqu√≠as), estas representaciones contribuyen a un entendimiento cohesivo y global de los datos. La interacci√≥n entre las capas y las unidades es parte de una estructura probabil√≠stica interconectada que busca modelar la distribuci√≥n general de los datos. El proceso de inferencia y generaci√≥n, aunque implica pasar informaci√≥n a trav√©s de las capas, se basa en los par√°metros globales de la red para producir resultados consistentes y representativos de la distribuci√≥n aprendida. Esto contrasta con modelos locales que podr√≠an segmentar el espacio de entrada y construir modelos independientes para cada segmento.  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Generativo / Discriminativo (apilado, no supervisado/supervisado)",
  "‚úÖ Num√©rica continua o categ√≥rica (binaria, multiclase)",
  "‚úÖ Num√©ricas y/o Categ√≥ricas (requiere preprocesamiento)",
  "‚úÖ No lineal y Compleja",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Asume independencia condicional",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Robusto hasta cierto punto, pero datos muy ruidosos afectan",
  "‚úÖ Maneja bien multicolinealidad",
  "‚ö†Ô∏è Baja (tipo caja negra)",
  "‚ö†Ô∏è Lento en entrenamiento, r√°pido en inferencia",
  "‚úÖ √ötil para ajuste de hiperpar√°metros",
  "‚ùå Datos insuficientes; ‚ùå alta dimensionalidad sin suficiente regularizaci√≥n; ‚ùå problemas de escalabilidad en conjuntos muy grandes."
)

detalles <- c(
  "Modelo generativo y discriminativo de aprendizaje profundo. Se entrena capa por capa de forma no supervisada (como RBMs) y luego se ajusta con una capa supervisada.",
  "Puede usarse para regresi√≥n (respuesta continua) o clasificaci√≥n (respuesta categ√≥rica).",
  "Acepta diversos tipos de variables predictoras, pero requieren normalizaci√≥n/estandarizaci√≥n y codificaci√≥n (ej., one-hot encoding para categ√≥ricas).",
  "Capaz de modelar relaciones altamente no lineales y complejas entre las variables.",
  "Los DBMs no asumen ni requieren la normalidad de los residuos, ya que no son modelos estad√≠sticos lineales tradicionales.",
  "A nivel de las capas, los DBMs asumen independencia condicional entre las variables visibles eÈöêvariables latentes dada la otra capa.",
  "Al igual que la normalidad de residuos, la homoscedasticidad no es un supuesto directo para DBMs.",
  "Puede manejar cierto nivel de ruido, pero el rendimiento disminuye con outliers extremos que distorsionan el espacio latente.",
  "Debido a su naturaleza de aprendizaje de representaciones, los DBMs son inherentemente capaces de manejar la multicolinealidad entre predictores.",
  "Generalmente, los DBMs son modelos de 'caja negra' con baja interpretabilidad de las relaciones directas entre las entradas y salidas.",
  "El entrenamiento de DBMs puede ser computacionalmente costoso y lento, especialmente con grandes datasets y muchas capas. La inferencia es m√°s r√°pida.",
  "La validaci√≥n cruzada es esencial para seleccionar hiperpar√°metros como el n√∫mero de capas, unidades por capa, tasas de aprendizaje y regularizaci√≥n.",
  "Requiere una cantidad significativa de datos para entrenar de forma efectiva. Puede tener dificultades con conjuntos de datos peque√±os. La alta dimensionalidad sin una regularizaci√≥n adecuada puede llevar a sobreajuste o problemas de escalabilidad en conjuntos de datos muy grandes."
)

tabla_dbm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_dbm %>%
  gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Deep Boltzmann Machine (DBM)",
             subtitle = "Caracter√≠sticas y Consideraciones") %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia y principios de aprendizaje profundo") %>%
  tab_options(heading.title.font.size = 14,
              heading.subtitle.font.size = 12,
              table.font.names = "Century Gothic",
              table.font.size = 10,
              data_row.padding = px(1)) %>%
  tab_style(style = list(cell_text(align = "left",
                                   weight = 'bold')),
            locations = list(cells_title(groups = c("title")))) %>%
  tab_style(style = list(cell_text(align = "left")),
            locations = list(cells_title(groups = c("subtitle")))) %>%
  cols_width(starts_with("Detalles") ~ px(500),
             everything() ~ px(200)) %>%
  as_raw_html()
```


## Deep Belief Networks (DBNet) {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/DBNet.png"))
```

El **Deep Belief Network (DBN)** es un modelo de **red neuronal profunda generativa** que se construye apilando m√∫ltiples **M√°quinas de Boltzmann Restringidas (RBMs)**. Fue un avance significativo en el campo del aprendizaje profundo, particularmente en la superaci√≥n de los desaf√≠os de entrenamiento de redes neuronales con muchas capas ocultas. Las DBNs son modelos probabil√≠sticos que buscan aprender una **distribuci√≥n de probabilidad conjunta** sobre los datos de entrada y sus representaciones latentes (ocultas).   

La arquitectura de una DBN es una jerarqu√≠a de capas, donde cada capa es una RBM. La capa inferior es la capa visible (o de entrada), que recibe los datos. Las capas subsiguientes son capas ocultas, y la caracter√≠stica clave es que el resultado de la capa oculta de una RBM se convierte en la capa visible para la siguiente RBM en la pila. La conexi√≥n entre la capa superior m√°s alta (que es una RBM) es no dirigida y bidireccional, mientras que las conexiones entre las capas inferiores suelen ser dirigidas (de arriba hacia abajo) en la fase generativa despu√©s del entrenamiento.  

Las DBNs se caracterizan por:  
  
1.  **Construcci√≥n por Capas:** Se construyen apilando RBMs, donde cada RBM se entrena de forma independiente y no supervisada para aprender una representaci√≥n de su entrada.   
2.  **Pre-entrenamiento Codicioso por Capas:** La innovaci√≥n clave de las DBNs fue el algoritmo de pre-entrenamiento codicioso por capas. En lugar de intentar entrenar toda la red a la vez (lo que era dif√≠cil debido a problemas como los gradientes desvanecientes/explosivos y los m√≠nimos locales), cada RBM se entrena individualmente para aprender caracter√≠sticas √∫tiles de la entrada que recibe. La salida de la capa oculta de una RBM entrenada se utiliza como entrada para la capa visible de la siguiente RBM. Este proceso contin√∫a hasta que se entrenan todas las capas.  
3.  **Aprendizaje No Supervisado para Extracci√≥n de Caracter√≠sticas:** La fase de pre-entrenamiento es completamente no supervisada. Las RBMs aprenden a reconstruir sus entradas, lo que les permite extraer caracter√≠sticas relevantes y de alto nivel de los datos sin necesidad de etiquetas. Esto es especialmente valioso para conjuntos de datos grandes y no etiquetados.  
4.  **Ajuste Fino (Fine-tuning) Supervisado:** Despu√©s del pre-entrenamiento no supervisado, la DBN puede ser "desenrollada" y tratada como una red neuronal feed-forward para tareas supervisadas como la clasificaci√≥n. Se a√±ade una capa de salida (ej., softmax) en la parte superior, y toda la red se ajusta utilizando algoritmos de aprendizaje supervisado como la retropropagaci√≥n. El pre-entrenamiento act√∫a como una buena inicializaci√≥n de los pesos, ayudando a que el entrenamiento supervisado converja m√°s r√°pido y alcance mejores m√≠nimos.  
5.  **Generaci√≥n de Datos:** Dado que son modelos generativos, las DBNs pueden aprender la distribuci√≥n subyacente de los datos y, por lo tanto, pueden generar nuevas muestras de datos similares a las de entrenamiento.   

Las DBNs fueron fundamentales para demostrar la viabilidad del entrenamiento de redes profundas y abrieron el camino para el resurgimiento del aprendizaje profundo.   


  
**Aprendizaje Global vs. Local:**
  
El modelo **Deep Belief Network (DBN)** emplea un enfoque h√≠brido, pero en su fase de aprendizaje de caracter√≠sticas, se inclina hacia un **aprendizaje global** a trav√©s de una estrategia local y progresiva.

* **Aspecto Global (Objetivo Final y Representaci√≥n):** El objetivo general de una DBN es construir un **modelo probabil√≠stico jer√°rquico global** de los datos. Aunque el entrenamiento se realiza capa por capa, la intenci√≥n es que cada capa capture caracter√≠sticas que contribuyan a una comprensi√≥n m√°s abstracta y completa de la distribuci√≥n de los datos de entrada en su conjunto. Las caracter√≠sticas de bajo nivel aprendidas por las primeras RBMs se combinan en las capas superiores para formar representaciones m√°s complejas y de alto nivel, que son intr√≠nsecas a la estructura global de los datos. El modelo final, una vez que todas las RBMs est√°n entrenadas y se aplica el ajuste fino, opera como una red unificada que mapea entradas a salidas basadas en un entendimiento global de las relaciones en los datos.

* **Aspecto Local (Estrategia de Entrenamiento):** La fase de **pre-entrenamiento codicioso por capas** de las DBNs tiene un fuerte componente local. Cada **RBM individual** se entrena de manera local, optimizando sus propios pesos para modelar la relaci√≥n entre su capa visible y su capa oculta, **sin considerar expl√≠citamente las capas m√°s all√° de s√≠ misma en ese momento**. La entrada para cada RBM superior proviene de la activaci√≥n de la capa oculta de la RBM inferior ya entrenada. Este entrenamiento local y secuencial es lo que permite que las DBNs escalen a redes profundas y eviten problemas de optimizaci√≥n de modelos globales complejos desde cero. Sin embargo, esta "localidad" es solo en la etapa de entrenamiento por partes; el efecto acumulativo de estas optimizaciones locales es la construcci√≥n de una representaci√≥n jer√°rquica que eventualmente se une en un modelo global cuando se realiza el ajuste fino de toda la red.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Generativo / Discriminativo (apilado, no supervisado/supervisado)",
  "‚úÖ Num√©rica continua o categ√≥rica (binaria, multiclase)",
  "‚úÖ Num√©ricas y/o Categ√≥ricas (requiere preprocesamiento)",
  "‚úÖ No lineal y Compleja",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Asume independencia condicional entre capas",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Robusto hasta cierto punto, pero datos muy ruidosos afectan",
  "‚úÖ Maneja bien multicolinealidad",
  "‚ö†Ô∏è Baja (tipo caja negra)",
  "‚ö†Ô∏è Lento en entrenamiento, r√°pido en inferencia",
  "‚úÖ √ötil para ajuste de hiperpar√°metros",
  "‚ùå Datos insuficientes; ‚ùå alta dimensionalidad sin suficiente regularizaci√≥n; ‚ùå problemas de escalabilidad."
)

detalles <- c(
  "Modelo generativo y discriminativo que consiste en apilar varias Restricted Boltzmann Machines (RBMs) o componentes similares. Cada RBM se entrena de forma no supervisada, y luego la red completa puede ser ajustada de forma supervisada.",
  "Puede ser utilizado tanto para tareas de regresi√≥n (variables continuas) como de clasificaci√≥n (variables categ√≥ricas, incluyendo binarias y multiclase), especialmente despu√©s de un ajuste supervisado (fine-tuning).",
  "Acepta una variedad de tipos de variables de entrada. Las variables num√©ricas generalmente requieren normalizaci√≥n o estandarizaci√≥n, y las categ√≥ricas necesitan ser codificadas (ej. one-hot encoding).",
  "Las DBNs son extremadamente capaces de aprender y modelar relaciones complejas y no lineales entre las variables de entrada y salida, gracias a su arquitectura profunda y sus capas ocultas.",
  "Al igual que con los DBMs, las DBNs no se basan en supuestos de normalidad de los residuos, ya que no son modelos estad√≠sticos lineales tradicionales.",
  "La independencia condicional es un supuesto clave en la forma en que cada RBM dentro de la DBN procesa la informaci√≥n entre sus capas visible y oculta.",
  "La homoscedasticidad no es un supuesto inherente o un requisito directo para el entrenamiento o la aplicaci√≥n de las DBNs.",
  "Aunque pueden ser algo robustas al ruido en los datos, los valores at√≠picos extremos pueden afectar negativamente el proceso de aprendizaje de las representaciones en las capas ocultas.",
  "Su capacidad para aprender representaciones jer√°rquicas y de bajo nivel de los datos ayuda a mitigar los problemas causados por la multicolinealidad entre las variables predictoras.",
  "Las DBNs, como muchos modelos de aprendizaje profundo, son consideradas 'cajas negras'. Es dif√≠cil interpretar directamente c√≥mo las caracter√≠sticas de entrada se mapean a las decisiones de salida.",
  "El entrenamiento de una DBN puede ser muy lento y costoso computacionalmente, especialmente en conjuntos de datos grandes o con muchas capas. Sin embargo, una vez entrenadas, la fase de inferencia es generalmente r√°pida.",
  "La validaci√≥n cruzada es una t√©cnica crucial para la selecci√≥n y optimizaci√≥n de hiperpar√°metros importantes, como el n√∫mero de RBMs, el n√∫mero de unidades en cada capa, las tasas de aprendizaje y los coeficientes de regularizaci√≥n.",
  "Requieren grandes vol√∫menes de datos etiquetados (para el fine-tuning supervisado) o no etiquetados (para el pre-entrenamiento no supervisado) para aprender representaciones significativas. Pueden tener problemas de escalabilidad con conjuntos de datos masivos o arquitecturas muy profundas."
)

tabla_dbn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_dbn %>%
  gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Deep Belief Network (DBN)",
             subtitle = "Caracter√≠sticas y Consideraciones") %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia y principios de aprendizaje profundo") %>%
  tab_options(heading.title.font.size = 14,
              heading.subtitle.font.size = 12,
              table.font.names = "Century Gothic",
              table.font.size = 10,
              data_row.padding = px(1)) %>%
  tab_style(style = list(cell_text(align = "left",
                                   weight = 'bold')),
            locations = list(cells_title(groups = c("title")))) %>%
  tab_style(style = list(cell_text(align = "left")),
            locations = list(cells_title(groups = c("subtitle")))) %>%
  cols_width(starts_with("Detalles") ~ px(500),
             everything() ~ px(200)) %>%
  as_raw_html()
```

## Stacked Auto-Enconders {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Deep learning/SAE.png"))
```

Un **Autoencoder Apilado (Stacked Autoencoder - SAE)** es un tipo de **red neuronal profunda** que se construye apilando m√∫ltiples **autoencoders (AE)** simples. Al igual que los Autoencoders individuales, su prop√≥sito principal es el **aprendizaje de caracter√≠sticas no supervisado** y la **reducci√≥n de dimensionalidad**. La idea central es aprender representaciones compactas y de baja dimensionalidad (codificaciones) de los datos de entrada, que capturen las caracter√≠sticas m√°s importantes.

La arquitectura de un SAE se organiza en capas, donde cada capa es un autoencoder. Un autoencoder b√°sico consta de dos partes: un **codificador (encoder)** que mapea la entrada a una representaci√≥n de menor dimensi√≥n (el "c√≥digo" o "bottleneck"), y un **decodificador (decoder)** que reconstruye la entrada original a partir de esta representaci√≥n. En un autoencoder apilado:

1.  **Codificador y Decodificador:** Cada autoencoder en la pila tiene su propio codificador y decodificador.
2.  **Formaci√≥n de Capas:** La salida de la capa codificadora de un autoencoder se convierte en la entrada para la siguiente capa (el autoencoder superior). De esta manera, las capas progresivas aprenden representaciones de caracter√≠sticas cada vez m√°s abstractas y de alto nivel.

Las caracter√≠sticas clave de los Stacked Autoencoders incluyen:

1.  **Pre-entrenamiento Codicioso por Capas:** Similar a las DBNs, los SAEs se entrenan utilizando un enfoque de **pre-entrenamiento codicioso por capas**.
    * Primero, se entrena un autoencoder para aprender una representaci√≥n de la capa de entrada original.
    * Una vez entrenado, la capa del codificador de este AE se "congela" y sus salidas (las caracter√≠sticas aprendidas) se utilizan como entrada para el entrenamiento del siguiente autoencoder en la pila.
    * Este proceso se repite, entrenando un nuevo autoencoder sobre las representaciones aprendidas por el autoencoder anterior, construyendo as√≠ una jerarqu√≠a de caracter√≠sticas.
2.  **Aprendizaje No Supervisado:** Toda la fase de pre-entrenamiento es **no supervisada**, lo que significa que los SAEs pueden aprender representaciones poderosas de datos sin necesidad de etiquetas. Esto los hace muy √∫tiles en escenarios donde los datos etiquetados son escasos.
3.  **Reducci√≥n de Dimensionalidad y Extracci√≥n de Caracter√≠sticas:** El objetivo de cada autoencoder es encontrar una representaci√≥n de baja dimensionalidad que permita una buena reconstrucci√≥n de la entrada. Al apilar estos, el SAE aprende una jerarqu√≠a de caracter√≠sticas donde las capas m√°s profundas capturan abstracciones m√°s complejas y significativas de los datos.
4.  **Ajuste Fino (Fine-tuning) Supervisado:** Despu√©s del pre-entrenamiento no supervisado, el decodificador de cada autoencoder suele descartarse. Se toma la pila de codificadores como una red de extracci√≥n de caracter√≠sticas. A esta red se le a√±ade una capa de salida (ej., una capa softmax para clasificaci√≥n) y todo el modelo se ajusta finamente utilizando un algoritmo de aprendizaje supervisado (como retropropagaci√≥n con gradiente descendente) en una tarea espec√≠fica. El pre-entrenamiento act√∫a como una excelente inicializaci√≥n de los pesos, lo que ayuda a evitar m√≠nimos locales pobres y a acelerar la convergencia.

Los Stacked Autoencoders fueron un modelo popular antes del auge de las Redes Convolucionales y Recurrentes m√°s especializadas, y demostraron la efectividad del pre-entrenamiento no supervisado para inicializar redes profundas.



**Aprendizaje Global vs. Local:**

El modelo de **Autoencoder Apilado (Stacked Autoencoder - SAE)** es un modelo de **aprendizaje global** que se construye a trav√©s de una estrategia de entrenamiento local y secuencial.

* **Aspecto Global (Objetivo Final y Representaci√≥n):** El objetivo final de un SAE es aprender una **representaci√≥n global y jer√°rquica** de los datos. Cada capa codificadora en la pila extrae caracter√≠sticas de un nivel de abstracci√≥n creciente, contribuyendo a una comprensi√≥n m√°s profunda y compacta de toda la distribuci√≥n de los datos de entrada. La codificaci√≥n final producida por el SAE es una representaci√≥n de baja dimensionalidad que intenta encapsular la informaci√≥n m√°s relevante de los datos en su conjunto, permitiendo su reconstrucci√≥n. Cuando se utiliza para tareas posteriores (como clasificaci√≥n) despu√©s del ajuste fino, la red opera como un modelo unificado que aplica las caracter√≠sticas globales aprendidas a nuevas entradas.

* **Aspecto Local (Estrategia de Entrenamiento por Capas):** La fase de **pre-entrenamiento codicioso por capas** de los SAEs tiene un componente fuertemente local. Cada **autoencoder individual** en la pila se entrena de forma independiente para aprender una codificaci√≥n √≥ptima y una reconstrucci√≥n de su propia entrada. Esto significa que los pesos de cada autoencoder se optimizan localmente, en un momento dado, sin considerar directamente la optimizaci√≥n simult√°nea de toda la red. La entrada a cada autoencoder subsiguiente es la representaci√≥n codificada aprendida por el autoencoder anterior. Esta estrategia de entrenamiento "por partes" permite que las redes profundas sean entrenadas de manera m√°s eficiente y eficaz, ya que descompone un problema de optimizaci√≥n complejo en subproblemas m√°s manejables. Sin embargo, el resultado acumulado de estas optimizaciones locales es la construcci√≥n de una jerarqu√≠a de caracter√≠sticas que, en √∫ltima instancia, forma parte de un modelo global y unificado de los datos.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Aprendizaje de representaci√≥n (no supervisado) / Discriminativo (supervisado)",
  "‚úÖ Num√©rica continua o categ√≥rica (binaria, multiclase)",
  "‚úÖ Num√©ricas y/o Categ√≥ricas (requiere preprocesamiento)",
  "‚úÖ No lineal y Compleja",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è No es un supuesto directo, pero la compresi√≥n busca regularidad",
  "‚ùå No aplica directamente",
  "‚ö†Ô∏è Sensible, pueden afectar la calidad de las representaciones aprendidas",
  "‚úÖ Maneja bien multicolinealidad",
  "‚ö†Ô∏è Baja (tipo caja negra)",
  "‚ö†Ô∏è Lento en entrenamiento, r√°pido en inferencia",
  "‚úÖ √ötil para ajuste de hiperpar√°metros",
  "‚ùå Datos insuficientes; ‚ùå arquitectura inadecuada (cuello de botella); ‚ùå si las representaciones no son significativas para la tarea final."
)

detalles <- c(
  "Compuesto por m√∫ltiples autoencoders apilados. Cada autoencoder se entrena para aprender una representaci√≥n (codificaci√≥n) de la entrada de la capa anterior. Se usa para pre-entrenamiento no supervisado y luego fine-tuning supervisado para tareas espec√≠ficas.",
  "La capa de salida final puede adaptarse para tareas de regresi√≥n (variables continuas) o clasificaci√≥n (variables categ√≥ricas).",
  "Requiere preprocesamiento: variables num√©ricas estandarizadas/normalizadas y variables categ√≥ricas codificadas (ej. one-hot encoding).",
  "Excelente para capturar relaciones complejas, no lineales y de alta dimensionalidad en los datos a trav√©s de representaciones aprendidas.",
  "Los Stacked Autoencoders son modelos de aprendizaje autom√°tico no param√©tricos y no tienen supuestos sobre la normalidad de los residuos.",
  "No es un supuesto expl√≠cito, pero el objetivo de los autoencoders de reconstruir la entrada fomenta la captura de dependencias y regularidades en los datos, no necesariamente errores independientes.",
  "La homoscedasticidad no es una consideraci√≥n directa ni un requisito para el entrenamiento de Stacked Autoencoders.",
  "Pueden ser sensibles a valores at√≠picos, ya que estos pueden influir fuertemente en la forma en que se aprenden las representaciones latentes, potencialmente llevando a una reconstrucci√≥n deficiente o a caracter√≠sticas sesgadas.",
  "Al aprender representaciones de caracter√≠sticas de menor dimensionalidad, los SAEs son robustos frente a la multicolinealidad en las variables predictoras originales.",
  "Similar a otras redes neuronales profundas, los SAEs operan como 'cajas negras', haciendo dif√≠cil la interpretaci√≥n directa de las caracter√≠sticas latentes que aprenden.",
  "El entrenamiento capa por capa puede ser intensivo en tiempo y recursos computacionales, especialmente con grandes datasets y arquitecturas complejas. La fase de inferencia es r√°pida.",
  "La validaci√≥n cruzada es crucial para la selecci√≥n de hiperpar√°metros, como el n√∫mero de capas, el n√∫mero de unidades en cada capa latente, las tasas de aprendizaje y los t√©rminos de regularizaci√≥n.",
  "Requieren una cantidad sustancial de datos para aprender representaciones significativas y evitar el sobreajuste. Una arquitectura de cuello de botella mal dise√±ada puede limitar la capacidad de representaci√≥n. Si las caracter√≠sticas aprendidas por los autoencoders no son relevantes para la tarea final supervisada, el rendimiento puede ser pobre."
)

tabla_sae <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_sae %>%
  gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Stacked Autoencoders (SAE)",
             subtitle = "Caracter√≠sticas y Consideraciones") %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia y principios de aprendizaje profundo") %>%
  tab_options(heading.title.font.size = 14,
              heading.subtitle.font.size = 12,
              table.font.names = "Century Gothic",
              table.font.size = 10,
              data_row.padding = px(1)) %>%
  tab_style(style = list(cell_text(align = "left",
                                   weight = 'bold')),
            locations = list(cells_title(groups = c("title")))) %>%
  tab_style(style = list(cell_text(align = "left")),
            locations = list(cells_title(groups = c("subtitle")))) %>%
  cols_width(starts_with("Detalles") ~ px(500),
             everything() ~ px(200)) %>%
  as_raw_html()
```
