# üîç 8. Modelos Basados en Instancias {-}  

**Ejemplos:** K-Nearest Neighbors (KNN).  
**Uso:** Son ideales cuando tienes una **cantidad limitada de datos** y esperas que los patrones relevantes se encuentren en la **similitud local** entre casos. Se utilizan mucho cuando la **similitud directa** entre las observaciones es un factor clave.   
**Ventajas:** Su implementaci√≥n es **simple** y son bastante **eficaces** en problemas con pocas dimensiones.   
**Limitaciones:** **Escalan mal** con grandes vol√∫menes de datos debido a que necesitan almacenar y comparar cada instancia. Adem√°s, son **sensibles al ruido** en los datos.   

---

## Case-Based Reasoning (CBR) {-}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/CBR.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_cbr <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_cbr <- c(
  "‚úÖ Supervisado / Semi-supervisado",
  "‚úÖ Num√©rica o Categ√≥rica",
  "‚úÖ Mixtas (num√©ricas, categ√≥ricas, textuales, etc.)",
  "‚úÖ Compleja (analog√≠as, no lineal)",
  "‚ùå No es requisito",
  "‚ùå No aplica directamente",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (un caso at√≠pico puede generar una mala coincidencia)",
  "‚úÖ Maneja bien (si la similitud es robusta)",
  "‚úÖ Muy alta (se explica a partir de casos reales)",
  "‚ö†Ô∏è Moderada a Baja (b√∫squeda intensiva en grandes bases de casos)",
  "‚úÖ Compatible y muy √∫til",
  "‚ùå Poca diversidad de casos o alta dimensionalidad"
)

detalles_cbr <- c(
  "Paradigma de resoluci√≥n de problemas que se basa en la experiencia. Resuelve nuevos problemas adaptando soluciones a problemas similares resueltos en el pasado (casos). Es un ciclo de recuperar, reusar, revisar y retener.",
  "La 'variable respuesta' es la soluci√≥n o resultado del caso. Puede ser un valor num√©rico (ej., precio de venta), una categor√≠a (ej., diagn√≥stico), o una acci√≥n compleja (ej., plan de tratamiento).",
  "Las 'variables predictoras' son las caracter√≠sticas que describen el problema o el contexto del caso. Pueden ser de tipos muy diversos, incluyendo descripciones textuales, valores num√©ricos, categor√≠as, etc.",
  "La relaci√≥n entre las caracter√≠sticas del problema y la soluci√≥n es inherentemente compleja y no lineal. Se basa en la noci√≥n de 'similitud' o 'analog√≠a' entre el nuevo problema y los casos almacenados.",
  "No hace suposiciones estad√≠sticas sobre la normalidad de los residuos, ya que no es un modelo param√©trico basado en regresi√≥n.",
  "El concepto de independencia de errores no aplica directamente. La efectividad se basa en la calidad de la base de casos y la capacidad de encontrar casos relevantes. La 'independencia' se refiere m√°s a que cada caso es una instancia separada de experiencia.",
  "No asume homoscedasticidad. La soluci√≥n se deriva del caso m√°s similar, no de un promedio ponderado que dependa de varianzas.",
  "S√≠, CBR puede ser sensible a outliers si un caso at√≠pico se considera err√≥neamente muy similar a un nuevo problema. Sin embargo, si los outliers se identifican y gestionan adecuadamente en la base de casos, o si la m√©trica de similitud es robusta, la sensibilidad puede mitigarse.",
  "CBR puede manejar la multicolinealidad de manera natural si la funci√≥n de similitud es capaz de ponderar las caracter√≠sticas de forma inteligente o si las caracter√≠sticas redundantes no afectan la identificaci√≥n de casos realmente similares. No es tan susceptible como los modelos lineales.",
  "La interpretabilidad es una de sus mayores fortalezas. La soluci√≥n se proporciona junto con los casos pasados m√°s similares, lo que permite al usuario entender 'por qu√©' se lleg√≥ a esa soluci√≥n. Es un modelo transparente.",
  "La velocidad y eficiencia pueden variar significativamente. La fase de 'recuperaci√≥n' (buscar el caso m√°s similar) puede ser computacionalmente intensiva para bases de casos muy grandes o de alta dimensionalidad. Las t√©cnicas de indexaci√≥n son cruciales para la escalabilidad.",
  "Es compatible y muy √∫til. Se puede evaluar la precisi√≥n de la soluci√≥n predicha. La validaci√≥n cruzada puede ayudar a determinar la robustez del sistema y la efectividad de las funciones de similitud y adaptaci√≥n.",
  "No funciona bien si: 1) la base de casos es demasiado peque√±a o no es representativa del dominio del problema, 2) no hay suficientes casos similares al nuevo problema, 3) la dimensionalidad de las caracter√≠sticas es extremadamente alta y dificulta el c√°lculo de similitudes, o 4) la definici√≥n de 'similitud' es ambigua o dif√≠cil de cuantificar."
)

tabla_cbr <- data.frame(Criterio = criterios_cbr, Aplica = aplica_cbr, Detalles = detalles_cbr)

tabla_cbr %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Case-Based Reasoning (CBR)",
    subtitle = "Razonamiento a partir de la Experiencia Pasada"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```



## k - Nearest Neighbour (kNN)  {-}    

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/kNN.png"))
```


**k-Nearest Neighbour (kNN)** es un algoritmo de **Machine Learning no param√©trico** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**. Es considerado uno de los algoritmos m√°s simples y se basa en la idea de que los puntos de datos que est√°n cerca entre s√≠ en el espacio de caracter√≠sticas suelen tener propiedades similares. No es un algoritmo que "aprende" un modelo expl√≠cito durante la fase de entrenamiento, sino que es un **algoritmo perezoso (lazy learner)**.

**Funcionamiento de kNN:**

1.  **Entrenamiento:** En la fase de entrenamiento, kNN simplemente **almacena todo el conjunto de datos de entrenamiento**. No hay un proceso de "aprendizaje" de par√°metros o construcci√≥n de un modelo, como en la regresi√≥n lineal o las redes neuronales.
2.  **Predicci√≥n (para una nueva instancia):**
    * **Identificar Vecinos:** Para clasificar o predecir el valor de una nueva instancia, kNN calcula la **distancia** entre esta nueva instancia y *todas* las instancias en el conjunto de entrenamiento. La m√©trica de distancia m√°s com√∫n es la **distancia euclidiana**, pero se pueden usar otras (Manhattan, Minkowski, etc.).
    * **Seleccionar 'k' Vecinos M√°s Cercanos:** Se identifican los 'k' puntos de datos del entrenamiento que son m√°s cercanos a la nueva instancia. El valor de 'k' es un **hiperpar√°metro** que debe ser seleccionado por el usuario.
    * **Clasificaci√≥n:** Para tareas de clasificaci√≥n, la nueva instancia se asigna a la clase que es la **mayor√≠a entre sus 'k' vecinos m√°s cercanos** (votaci√≥n mayoritaria).
    * **Regresi√≥n:** Para tareas de regresi√≥n, el valor predicho para la nueva instancia es el **promedio (o mediana) de los valores de la variable de respuesta de sus 'k' vecinos m√°s cercanos**.

La elecci√≥n del valor de 'k' es crucial: un 'k' peque√±o puede hacer el modelo sensible al ruido (sobreajuste), mientras que un 'k' grande puede suavizar demasiado la predicci√≥n (subajuste) y las fronteras de decisi√≥n.

**Aprendizaje Global vs. Local:**

k-Nearest Neighbour (kNN) es el ejemplo por excelencia de un modelo de **aprendizaje puramente local**.

* **Aspecto Local:** La predicci√≥n para una nueva instancia depende **exclusivamente de los 'k' puntos de datos m√°s cercanos a ella en el espacio de caracter√≠sticas**. No se construye un modelo global que abarque todo el conjunto de datos. En cambio, para cada nueva consulta, el algoritmo "re-calcula" el vecindario relevante y realiza una predicci√≥n basada solo en la informaci√≥n de esa peque√±a regi√≥n local. Esto significa que la frontera de decisi√≥n (en clasificaci√≥n) o la funci√≥n de regresi√≥n (en regresi√≥n) se ajusta localmente a las caracter√≠sticas del vecindario del punto de consulta. Si los datos no se distribuyen linealmente y tienen estructuras complejas con patrones que var√≠an en diferentes regiones, kNN es muy efectivo porque puede adaptarse a estas variaciones locales al funcionar como una **"regresi√≥n (o clasificaci√≥n) ponderada localmente"**.

* **Sin Modelo Expl√≠cito Global:** Debido a su naturaleza de "aprendizaje perezoso", kNN no genera una funci√≥n matem√°tica expl√≠cita o un conjunto de coeficientes que describan la relaci√≥n global entre las variables. Todo el conocimiento del modelo est√° impl√≠cito en la base de datos de entrenamiento.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n y regresi√≥n)",
  "‚úÖ Num√©rica (regresi√≥n) o categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas (preferible), aunque puede adaptarse para categ√≥ricas",
  "‚ùå No asume ninguna forma funcional",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Muy sensible a valores at√≠picos",
  "‚ö†Ô∏è Problemas si hay predictores muy correlacionados",
  "‚ö†Ô∏è Dif√≠cil de interpretar (modelo basado en instancias)",
  "‚ùå Lento con grandes vol√∫menes (requiere calcular distancias)",
  "‚úÖ Crucial para elegir el mejor valor de *k*",
  "‚ùå No escala bien con datos grandes o con ruido"
)

detalles <- c(
  "Modelo no param√©trico que predice en funci√≥n de la cercan√≠a a ejemplos del conjunto de entrenamiento.",
  "Se usa tanto para clasificaci√≥n como para regresi√≥n seg√∫n el tipo de variable objetivo.",
  "Las variables deben estar en la misma escala; se recomienda estandarizar.",
  "No asume una relaci√≥n espec√≠fica entre variables; se basa en similitud.",
  "No se ajusta una funci√≥n, por lo tanto no hay residuos como tal.",
  "No se estiman errores independientes, ya que no hay funci√≥n de error expl√≠cita.",
  "No hay regresi√≥n residual, por lo tanto este supuesto no aplica.",
  "Outliers pueden alterar los vecinos m√°s cercanos y afectar la predicci√≥n.",
  "No requiere modelo expl√≠cito, pero predictores correlacionados pueden afectar el peso relativo en la distancia.",
  "Predicci√≥n se basa en instancias cercanas, dif√≠cil de resumir en una f√≥rmula.",
  "Requiere calcular distancia para cada predicci√≥n ‚Üí lento con grandes bases.",
  "Se suele usar validaci√≥n cruzada para encontrar el n√∫mero √≥ptimo de vecinos (*k*).",
  "Alta dimensi√≥n, ruido o escalas distintas entre variables afectan el rendimiento del modelo."
)

tabla_knn <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_knn %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir kNN",
             subtitle = "k - Nearest Neighbour (kNN)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Kernel Regression / Nadaraya-Watson Estimator   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/Kernel Regression.png"))
```

```{r}
library(gt)

criterios_kernel_reg <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_kernel_reg <- c(
  "‚úÖ Supervisado",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas",
  "‚úÖ Muy compleja (altamente no lineal)",
  "‚ùå No es requisito",
  "‚úÖ Necesaria",
  "‚ùå No es requisito (pero puede ser modelada)",
  "‚ö†Ô∏è S√≠ (muy)",
  "‚úÖ Maneja bien (si la distancia es apropiada)",
  "‚ö†Ô∏è Baja (no hay coeficientes fijos)",
  "‚ö†Ô∏è Baja a Moderada (costo computacional alto para N grande)",
  "‚úÖ Compatible y esencial",
  "‚ùå Dimensionalidad muy alta, ruido excesivo, datos escasos"
)

detalles_kernel_reg <- c(
  "M√©todo de regresi√≥n no param√©trico que estima el valor de la variable respuesta en un punto bas√°ndose en un promedio ponderado de las variables respuesta de los puntos vecinos en el conjunto de entrenamiento. La ponderaci√≥n se determina por una funci√≥n de kernel y un par√°metro de ancho de banda.",
  "La variable dependiente debe ser num√©rica y continua.",
  "Las variables predictoras deben ser num√©ricas para calcular distancias. Es crucial pre-procesar y escalar los datos adecuadamente.",
  "Es capaz de capturar relaciones altamente complejas y no lineales entre las variables, adapt√°ndose localmente a la forma de los datos sin asumir una forma funcional predefinida.",
  "No hace suposiciones sobre la normalidad de los residuos ni la distribuci√≥n subyacente de los datos, ya que es un m√©todo no param√©trico.",
  "Asume que los errores asociados a las observaciones son independientes entre s√≠.",
  "No asume homoscedasticidad. Puede modelar c√≥mo la varianza de la respuesta cambia en diferentes regiones del espacio de las caracter√≠sticas, ya que la estimaci√≥n es local.",
  "S√≠, la Regresi√≥n Kernel es muy sensible a los outliers. Un outlier puede tener una influencia desproporcionada en la estimaci√≥n de su vecindario local, especialmente si el ancho de banda es peque√±o. La presencia de ruido tambi√©n puede degradar el rendimiento.",
  "Maneja bien la multicolinealidad, ya que su enfoque se basa en las distancias entre puntos y en la ponderaci√≥n de vecinos, no en la inversi√≥n de matrices que se ve afectada por la colinealidad.",
  "La interpretabilidad es baja en el sentido tradicional. No produce coeficientes fijos o una ecuaci√≥n expl√≠cita. La 'explicaci√≥n' se basa en qu√© vecinos contribuyeron a la predicci√≥n, pero no es una relaci√≥n simple y global.",
  "La velocidad y eficiencia pueden ser un problema para datasets grandes. Para hacer una predicci√≥n para un nuevo punto, se necesita calcular la distancia a *todos* los puntos de entrenamiento, lo que resulta en una complejidad computacional de O(N) por predicci√≥n, donde N es el n√∫mero de muestras. El ajuste del ancho de banda tambi√©n es costoso.",
  "Es compatible y **esencial** para la Regresi√≥n Kernel. Permite seleccionar el ancho de banda √≥ptimo del kernel, que es un hiperpar√°metro cr√≠tico que controla el equilibrio entre el sesgo y la varianza del modelo.",
  "No funciona bien si: 1) la **dimensionalidad de las caracter√≠sticas es muy alta** (sufre la 'maldici√≥n de la dimensionalidad', donde los datos se vuelven escasos en espacios de alta dimensi√≥n, haciendo que los 'vecinos' sean menos representativos), 2) los datos son **muy ruidosos**, o 3) el **dataset es muy peque√±o** para capturar patrones locales significativos o **muy grande** para ser computacionalmente eficiente."
)

tabla_kernel_reg <- data.frame(Criterio = criterios_kernel_reg, Aplica = aplica_kernel_reg, Detalles = detalles_kernel_reg)

tabla_kernel_reg %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Kernel Regression",
    subtitle = "Regresi√≥n no Param√©trica Basada en la Proximidad Local"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Learning Vector Quantization (LVQ)  {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/LVQ.png"))
```

**Learning Vector Quantization (LVQ)** es un algoritmo de **clasificaci√≥n supervisada basado en prototipos**, desarrollado por Teuvo Kohonen. Puede ser visto como un tipo de **red neuronal artificial** que utiliza un enfoque de "ganador se lleva todo" (winner-take-all) para aprender a clasificar datos. LVQ es una alternativa al algoritmo k-Nearest Neighbour (kNN) que busca reducir la cantidad de informaci√≥n necesaria para almacenar los datos de entrenamiento, aprendiendo un conjunto m√°s peque√±o de **prototipos** que representan las clases.

La idea central de LVQ es la siguiente:

1.  **Representaci√≥n por Prototipos:** En lugar de memorizar todos los puntos de datos de entrenamiento (como kNN), LVQ aprende un conjunto de **vectores prototipo (o "codebook vectors")**. Cada prototipo est√° asociado a una clase espec√≠fica y representa una "regi√≥n" en el espacio de caracter√≠sticas que pertenece a esa clase.
2.  **Proceso de Aprendizaje (Entrenamiento Supervisado):**
    * Se inicializan los prototipos (a menudo aleatoriamente o con puntos de datos de entrenamiento).
    * Para cada instancia de entrenamiento:
        * Se encuentra el prototipo m√°s cercano (el "ganador") a esa instancia utilizando una m√©trica de distancia (com√∫nmente la distancia euclidiana).
        * Se ajusta la posici√≥n de este prototipo ganador:
            * Si el prototipo ganador tiene la **misma clase** que la instancia de entrenamiento, el prototipo se **mueve ligeramente m√°s cerca** de la instancia (recompensa).
            * Si el prototipo ganador tiene una **clase diferente** a la instancia de entrenamiento, el prototipo se **mueve ligeramente m√°s lejos** de la instancia (penalizaci√≥n).
    * Este proceso iterativo contin√∫a hasta que los prototipos convergen o se alcanza un n√∫mero m√°ximo de √©pocas. Las diferentes variantes de LVQ (LVQ1, LVQ2.1, LVQ3) tienen reglas de actualizaci√≥n ligeramente distintas.
3.  **Clasificaci√≥n (Predicci√≥n):** Para clasificar una nueva instancia, simplemente se encuentra el prototipo m√°s cercano a esa instancia en el espacio de caracter√≠sticas. La nueva instancia se asigna a la clase asociada con ese prototipo m√°s cercano. Es similar a un clasificador 1-NN que opera sobre los prototipos aprendidos.

LVQ es valorado por la interpretabilidad de sus prototipos (ya que son puntos en el espacio de caracter√≠sticas que representan una clase) y por su eficiencia una vez que los prototipos han sido aprendidos, ya que la predicci√≥n es mucho m√°s r√°pida que kNN en grandes conjuntos de datos.


**Aprendizaje Global vs. Local:**

Learning Vector Quantization (LVQ) es un modelo que exhibe caracter√≠sticas de **aprendizaje tanto global como local**.

* **Aspecto Local:** El coraz√≥n del aprendizaje en LVQ es la **adaptaci√≥n local de los prototipos**. En cada paso de entrenamiento, solo el prototipo m√°s cercano (o los dos prototipos m√°s cercanos en algunas variantes como LVQ2.1 y LVQ3) a una instancia de entrenamiento se ajusta. Esto significa que las reglas de aprendizaje operan en un **vecindario localizado** alrededor de la instancia de entrada. Los prototipos se mueven en el espacio de caracter√≠sticas para delimitar mejor las fronteras de clase, lo que refleja la estructura local de los datos. De esta manera, LVQ puede modelar **relaciones no lineales** y estructuras de clase complejas al ajustar las posiciones de estos "representantes" locales de las clases.

* **Aspecto Global:** Aunque el ajuste es local, el conjunto de todos los prototipos de LVQ, una vez entrenados, forma una **representaci√≥n global del espacio de caracter√≠sticas** que se utiliza para la clasificaci√≥n. Estos prototipos definen un mapa de clasificaci√≥n en todo el espacio de entrada, donde cada regi√≥n (celda de Voronoi) se asocia con una clase. Por lo tanto, el modelo final, que es la colecci√≥n de prototipos, se aplica de manera global para clasificar cualquier nueva observaci√≥n. El proceso de optimizaci√≥n para encontrar las posiciones de los prototipos, aunque iterativo y basado en actualizaciones locales, busca una configuraci√≥n global √≥ptima que minimice el error de clasificaci√≥n en todo el conjunto de entrenamiento.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (clasificaci√≥n)",
  "‚úÖ Categ√≥rica",
  "‚úÖ Num√©ricas (requiere c√°lculo de distancias)",
  "‚ùå No asume relaci√≥n funcional directa",
  "‚ùå No aplica (modelo de clasificaci√≥n, no regresi√≥n)",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Sensible a valores extremos (afectan los prototipos)",
  "‚ö†Ô∏è Variables correlacionadas pueden distorsionar distancias",
  "‚ö†Ô∏è Intermedio: prototipos ayudan pero no son tan interpretables como reglas",
  "‚úÖ R√°pido despu√©s del entrenamiento (dependiendo del n√∫mero de prototipos)",
  "‚úÖ √ötil para ajustar n√∫mero y posici√≥n de prototipos",
  "‚ùå Problemas si los datos no est√°n bien escalados o si hay clases muy desbalanceadas"
)

detalles <- c(
  "T√©cnica supervisada basada en instancias que usa prototipos para representar clases.",
  "Se usa para tareas de clasificaci√≥n en donde las clases est√°n etiquetadas.",
  "Requiere variables num√©ricas porque se basa en distancias euclidianas para asignaci√≥n de clases.",
  "No asume una relaci√≥n funcional, simplemente asigna una clase basada en el prototipo m√°s cercano.",
  "No se generan residuos, por tanto la normalidad no se eval√∫a.",
  "No hay error estructurado como en modelos de regresi√≥n, por lo tanto este supuesto no aplica.",
  "No se eval√∫a la varianza de errores ya que no es un modelo de regresi√≥n.",
  "Outliers pueden alterar la posici√≥n de los prototipos y generar errores de clasificaci√≥n.",
  "Variables altamente correlacionadas pueden sesgar las distancias, lo que afecta la clasificaci√≥n.",
  "Aunque los prototipos pueden ofrecer intuici√≥n sobre la clase, no son completamente transparentes.",
  "Despu√©s del ajuste de los prototipos, la clasificaci√≥n es eficiente.",
  "Es com√∫n usar validaci√≥n cruzada para seleccionar el n√∫mero y la distribuci√≥n de los prototipos.",
  "No es adecuado si las variables est√°n en escalas distintas o si no hay separaci√≥n clara entre clases."
)

tabla_lvq <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lvq %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LVQ",
             subtitle = "Learning Vector Quantization (LVQ)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Locally Weighted Learning (LWL)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/LWL.png"))
```

**Locally Weighted Learning (LWL)** es una clase de algoritmos de **aprendizaje supervisado no param√©trico** que se distingue por su enfoque en la **construcci√≥n de modelos locales** para cada nueva instancia de consulta, en lugar de aprender un √∫nico modelo global para todo el conjunto de datos. Es un tipo de **"aprendizaje perezoso" (lazy learning)**, lo que significa que la mayor parte del "trabajo" (c√°lculos) se realiza en el momento de la predicci√≥n, no durante una fase de entrenamiento expl√≠cita.

La idea central de LWL es que, para predecir la salida de una nueva instancia de consulta, se construye un modelo simple (a menudo lineal o polin√≥mico) utilizando solo las **instancias de entrenamiento que son "cercanas"** a la instancia de consulta. Adem√°s, a las instancias de entrenamiento m√°s cercanas se les asigna un **peso mayor** en la construcci√≥n de este modelo local.

El proceso de LWL (especialmente para regresi√≥n, conocida como **Regresi√≥n Lineal Ponderada Localmente - LWLR** o LOESS/LOWESS) implica:

1.  **Sin Fase de Entrenamiento expl√≠cita:** El algoritmo simplemente almacena todo el conjunto de datos de entrenamiento.
2.  **Para cada Instancia de Consulta (Predicci√≥n):**
    * **C√°lculo de Distancias:** Se calcula la distancia entre la instancia de consulta y todas las instancias de entrenamiento.
    * **Asignaci√≥n de Pesos:** Se aplica una **funci√≥n de kernel (funci√≥n de ponderaci√≥n)** a estas distancias para asignar un peso a cada instancia de entrenamiento. Las instancias m√°s cercanas a la consulta reciben un peso mayor, y los pesos disminuyen a medida que la distancia aumenta. Un hiperpar√°metro llamado **ancho de banda (bandwidth)** controla qu√© tan r√°pido disminuyen los pesos con la distancia (determina el "tama√±o del vecindario" influyente).
    * **Construcci√≥n del Modelo Local:** Se ajusta un modelo simple (ej., una regresi√≥n lineal) a las instancias de entrenamiento, pero esta vez, cada instancia se pondera seg√∫n el peso calculado. Esto es, se minimiza una suma de errores cuadrados ponderada.
    * **Predicci√≥n:** El valor predicho para la instancia de consulta se obtiene utilizando este modelo local reci√©n construido. El modelo local se descarta despu√©s de hacer la predicci√≥n para esa instancia.

LWL es muy efectivo para modelar relaciones **no lineales y complejas** en los datos porque puede adaptar la forma de la funci√≥n de predicci√≥n a las variaciones locales. Es una generalizaci√≥n de k-Nearest Neighbors (kNN) donde en lugar de solo promediar o votar, se ajusta un modelo ponderado.


**Aprendizaje Global vs. Local:**

Locally Weighted Learning (LWL) es el ep√≠tome del **aprendizaje puramente local**.

* **Aspecto Local:** LWL es intr√≠nsecamente local en su funcionamiento. Para **cada nueva predicci√≥n**, se construye un **modelo espec√≠fico y √∫nico** que solo es v√°lido en el **vecindario local** de la instancia de consulta. Los pesos asignados a las instancias de entrenamiento enfatizan las que est√°n m√°s cerca del punto de consulta, lo que significa que el modelo se "adapta" a la estructura de los datos en esa regi√≥n particular del espacio de caracter√≠sticas. Esto le permite manejar eficientemente relaciones no lineales y heterog√©neas, ya que la relaci√≥n puede ser diferente en distintas partes del dominio de los datos.

* **Sin Modelo Expl√≠cito Global:** No hay un conjunto fijo de par√°metros o una funci√≥n matem√°tica √∫nica que describa la relaci√≥n entre las entradas y las salidas para todo el conjunto de datos. En cambio, el "modelo" se genera din√°micamente para cada punto de consulta, utilizando solo la informaci√≥n relevante de su vecindario. La complejidad computacional de LWL aumenta con el n√∫mero de predicciones, ya que cada una requiere la construcci√≥n de un nuevo modelo local.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n o clasificaci√≥n)",
  "‚úÖ Num√©rica o categ√≥rica, seg√∫n tarea",
  "‚úÖ Num√©ricas o categ√≥ricas (requiere distancias)",
  "‚úÖ No lineal (ajustes locales en cada predicci√≥n)",
  "‚ö†Ô∏è Depende del modelo local usado (e.g., regresi√≥n lineal)",
  "‚ö†Ô∏è Puede no cumplir si hay dependencia local",
  "‚ö†Ô∏è Evaluada localmente, var√≠a seg√∫n vecindario",
  "‚úÖ S√≠, muy sensible a outliers en vecindarios locales",
  "‚ö†Ô∏è Puede causar inestabilidad en predicciones locales",
  "‚ö†Ô∏è Dif√≠cil de interpretar globalmente, clara localmente",
  "‚ùå Lento, necesita recalcular modelo para cada punto",
  "‚úÖ S√≠, especialmente leave-one-out o k-fold por zonas",
  "‚ùå Ineficiente con muchos datos o alta dimensi√≥n"
)

detalles <- c(
  "Modelo supervisado que ajusta un modelo distinto en cada punto de predicci√≥n usando los vecinos m√°s cercanos.",
  "Puede ser regresi√≥n (respuesta num√©rica) o clasificaci√≥n (respuesta categ√≥rica).",
  "Utiliza variables para calcular distancias a partir de un punto de consulta.",
  "Ajusta modelos simples en regiones locales, permitiendo capturar relaciones no lineales.",
  "En regresi√≥n local puede requerirse que los residuos sean normales si se desea inferencia.",
  "Los errores pueden no ser independientes si hay estructuras repetitivas locales.",
  "La varianza puede cambiar entre zonas del espacio, por lo que se revisa localmente.",
  "Los valores at√≠picos pueden sesgar el modelo local si caen cerca del punto de predicci√≥n.",
  "La multicolinealidad puede afectar si el modelo local es lineal, aunque su efecto se restringe localmente.",
  "La interpretaci√≥n es clara en zonas locales, pero no se generaliza a toda la muestra.",
  "Cada predicci√≥n entrena un nuevo modelo, lo que es computacionalmente costoso.",
  "La validaci√≥n cruzada ayuda a elegir par√°metros como el ancho del vecindario (kernel).",
  "El rendimiento cae en grandes vol√∫menes de datos o si los datos no presentan estructura local clara."
)

tabla_lwl <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lwl %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LWL",
             subtitle = "Locally Weighted Learning (LWL)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




## Self - Organizing Map (SOM)  {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/SOM.png"))
```

Una **Self-Organizing Map (SOM)**, tambi√©n conocida como **Mapa Autoorganizado de Kohonen** o **Mapa de Caracter√≠sticas Autoorganizado (SOFM)**, es un tipo de **red neuronal artificial no supervisada** utilizada principalmente para **reducci√≥n de dimensionalidad y visualizaci√≥n de datos**. Su objetivo es producir una representaci√≥n de baja dimensi√≥n (t√≠picamente bidimensional) de un conjunto de datos de alta dimensi√≥n, mientras **preserva la estructura topol√≥gica** de los datos originales. Esto significa que los puntos de datos que son similares en el espacio de alta dimensi√≥n se mapean a neuronas cercanas en el mapa de baja dimensi√≥n.

A diferencia de otras redes neuronales que utilizan el aprendizaje por retropropagaci√≥n y descenso de gradiente (aprendizaje basado en el error), las SOM utilizan un proceso de **aprendizaje competitivo**.

El funcionamiento de un SOM implica los siguientes pasos iterativos:

1.  **Inicializaci√≥n:** Se crea una cuadr√≠cula de "neuronas" (tambi√©n llamadas unidades o nodos) en el espacio de baja dimensi√≥n (ej., una cuadr√≠cula 2D). A cada neurona se le asigna un **vector de pesos** con la misma dimensionalidad que los datos de entrada. Estos vectores de pesos se inicializan aleatoriamente o de forma lineal.
2.  **Competencia:** Para cada vector de entrada (punto de datos) del conjunto de entrenamiento:
    * Se calcula la distancia (com√∫nmente euclidiana) entre el vector de entrada y el vector de pesos de cada neurona en la cuadr√≠cula.
    * La neurona con el vector de pesos m√°s cercano al vector de entrada se denomina **Unidad de Mejor Coincidencia (BMU - Best Matching Unit)**.
3.  **Cooperaci√≥n (Vecindad):** La BMU y sus neuronas **vecinas** (dentro de un radio definido en la cuadr√≠cula) son identificadas. El tama√±o de este radio de vecindad disminuye con el tiempo a medida que avanza el entrenamiento. La influencia del ajuste de los pesos disminuye con la distancia de la BMU dentro de esta vecindad (definido por una **funci√≥n de vecindad**, como una Gaussiana).
4.  **Adaptaci√≥n:** Los vectores de pesos de la BMU y sus neuronas vecinas se **ajustan ligeramente** para que se acerquen al vector de entrada original. La magnitud del ajuste est√° determinada por una **tasa de aprendizaje**, que tambi√©n disminuye con el tiempo. El ajuste es mayor para la BMU y menor para las neuronas m√°s alejadas dentro del radio de vecindad.
5.  **Iteraci√≥n:** Los pasos 2-4 se repiten para un gran n√∫mero de √©pocas (iteraciones) y para todos los vectores de entrada, hasta que los pesos de las neuronas convergen y la red se "autoorganiza".

Al final del entrenamiento, las neuronas en el mapa se han organizado de tal manera que las neuronas cercanas representan datos de entrada similares, creando un "mapa" donde las regiones con densidades de datos similares forman grupos o clusters.

**Aprendizaje Global vs. Local:**

Una Self-Organizing Map (SOM) es un modelo que **combina aspectos de aprendizaje global y local** de una manera muy particular, que evoluciona a lo largo del proceso de entrenamiento.

* **Aspecto Global (Fases Iniciales del Entrenamiento):** Al principio del entrenamiento, el radio de vecindad y la tasa de aprendizaje son grandes. Esto significa que cuando una BMU se ajusta, un **gran n√∫mero de neuronas circundantes en el mapa tambi√©n se ajustan**, incluso aquellas que est√°n relativamente lejos de la BMU. Este amplio ajuste permite que el mapa se "organice globalmente" para capturar la estructura general de los datos. La topolog√≠a general de la proyecci√≥n se establece en esta fase inicial. El mapa se estira y se contrae para abarcar la dispersi√≥n global de los datos, como si una "regresi√≥n ponderada localmente" de gran escala estuviera adaptando el mapa entero.

* **Aspecto Local (Fases Posteriores del Entrenamiento):** A medida que el entrenamiento avanza, el radio de vecindad y la tasa de aprendizaje **disminuyen gradualmente**. Esto hace que los ajustes a los pesos sean cada vez m√°s localizados. En las etapas finales, solo la BMU y sus vecinos m√°s cercanos (o incluso solo la BMU) se ajustan significativamente. Esta fase de "afinamiento" permite que el mapa capture los detalles m√°s finos y las **estructuras locales** dentro de los datos, refinando las fronteras entre los grupos y asegurando que los puntos similares se agrupen con alta precisi√≥n.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚ùå No supervisado (reducci√≥n de dimensionalidad y clustering)",
  "‚ùå No aplica (no hay variable respuesta)",
  "‚úÖ Num√©ricas (basado en distancias)",
  "‚úÖ No lineal (mapea datos de alta dimensi√≥n a una grilla)",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ùå No aplica",
  "‚ö†Ô∏è Puede ser sensible a outliers (afectan la topolog√≠a de la grilla)",
  "‚ö†Ô∏è No impacta directamente pero puede distorsionar distancias",
  "‚ö†Ô∏è Dif√≠cil de interpretar (requiere visualizaciones espec√≠ficas)",
  "‚ö†Ô∏è Entrenamiento puede ser lento en datasets grandes, luego eficiente",
  "‚ö†Ô∏è No es tradicional, pero se puede evaluar topolog√≠a y distorsi√≥n",
  "‚ùå Mal desempe√±o si los datos est√°n mal escalados o no hay estructura"
)

detalles <- c(
  "T√©cnica no supervisada que proyecta datos de alta dimensi√≥n a una grilla 2D preservando la topolog√≠a.",
  "No predice una variable, sino agrupa y organiza datos similares espacialmente.",
  "Basado en distancias euclidianas entre vectores de caracter√≠sticas; requiere variables num√©ricas.",
  "Preserva relaciones de vecindad: observaciones similares se ubican cerca en la grilla.",
  "No se generan residuos como en modelos de regresi√≥n o clasificaci√≥n.",
  "No hay modelo de error; no aplica este supuesto.",
  "No hay varianza de errores al no haber predicci√≥n.",
  "Outliers pueden alterar las posiciones en la grilla y afectar la interpretaci√≥n.",
  "La correlaci√≥n entre variables puede afectar las distancias y la formaci√≥n de grupos.",
  "Interpretaci√≥n se basa en visualizaci√≥n de mapas de componentes y distancias.",
  "Entrenamiento iterativo, m√°s lento que PCA pero √∫til para exploraci√≥n visual.",
  "No se usa validaci√≥n cruzada directa, pero se puede evaluar la topolog√≠a y mapas de distancia.",
  "Datos ruidosos, mal escalados o con muchas variables irrelevantes dificultan resultados √∫tiles."
)

tabla_som <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_som %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir SOM",
             subtitle = "Self - Organizing Map (SOM)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Prototype-Based Learning (General Concept) {-}     

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Intance-Based/PBL.png"))
```


```{r, echo = FALSE}
library(gt)

criterios_prototype_based <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_prototype_based <- c(
  "‚úÖ Supervisado / No Supervisado",
  "‚úÖ Num√©rica o Categ√≥rica (depende de la tarea)",
  "‚úÖ Num√©ricas (principalmente)",
  "‚úÖ Compleja (a trav√©s de la proximidad a prototipos)",
  "‚ùå No es requisito",
  "‚ùå No aplica directamente",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠ (la selecci√≥n de prototipos puede ser afectada)",
  "‚úÖ Maneja bien (la distancia es clave)",
  "‚úÖ Muy alta (transparente por naturaleza)",
  "‚ö†Ô∏è Moderada (depende del n√∫mero de prototipos y b√∫squeda)",
  "‚úÖ Compatible y √∫til",
  "‚ùå Datos de muy alta dimensionalidad o con ruido excesivo"
)

detalles_prototype_based <- c(
  "Clase de algoritmos de Machine Learning que construyen modelos identificando ejemplos representativos, llamados **prototipos**. Estos prototipos act√∫an como plantillas o res√∫menes de los datos. Se utiliza tanto en clasificaci√≥n como en clustering.",
  "La 'variable respuesta' es la etiqueta de clase (en clasificaci√≥n supervisada) o la pertenencia a un cl√∫ster (en clustering no supervisado). Para regresi√≥n, el prototipo puede estar asociado con un valor num√©rico.",
  "Las variables predictoras suelen ser num√©ricas, ya que la mayor√≠a de los algoritmos basados en prototipos se basan en m√©tricas de distancia (eucl√≠dea, manhattan, etc.). La escalada de caracter√≠sticas es a menudo crucial.",
  "Las relaciones entre las variables se capturan de forma no lineal a trav√©s de la proximidad a los prototipos. Un nuevo punto de datos se clasifica o agrupa seg√∫n su similitud con los prototipos existentes. La complejidad de la relaci√≥n depende de c√≥mo se definan y usen los prototipos.",
  "No hace suposiciones sobre la normalidad de los residuos ni la distribuci√≥n de los datos, ya que no es un modelo param√©trico que dependa de esas suposiciones.",
  "El concepto de independencia de errores no aplica directamente. El aprendizaje se basa en la representaci√≥n de datos a trav√©s de prototipos, no en la modelaci√≥n de errores residuales.",
  "No asume homoscedasticidad. La decisi√≥n se basa en la proximidad o la ponderaci√≥n local, que no dependen de la varianza constante del error.",
  "S√≠, el aprendizaje basado en prototipos puede ser sensible a los outliers. Si un outlier es seleccionado como prototipo, o si su presencia influye significativamente en la posici√≥n de los prototipos, puede sesgar las clasificaciones o agrupaciones. Algunas variantes tienen mecanismos para mitigar esto.",
  "Maneja bien la multicolinealidad. Dado que se basa en m√©tricas de distancia, las caracter√≠sticas correlacionadas simplemente contribuir√°n a la proximidad de manera consistente, sin los problemas de inestabilidad que se encuentran en los modelos lineales basados en la inversi√≥n de matrices.",
  "La interpretabilidad es **muy alta**. Los prototipos son puntos de datos reales (o representantes de cl√∫steres) que se pueden examinar y comprender. Esto permite entender 'por qu√©' se hizo una predicci√≥n o 'qu√©' representa un grupo de datos, se√±alando un ejemplo concreto.",
  "La velocidad y eficiencia son moderadas. La fase de aprendizaje puede ser r√°pida (si los prototipos son solo ejemplos del conjunto de datos) o m√°s lenta (si los prototipos se optimizan). La fase de predicci√≥n implica calcular distancias a todos los prototipos, lo que puede ser lento para muchos prototipos o alta dimensionalidad.",
  "Es compatible y √∫til. Se puede utilizar para evaluar la robustez del modelo y optimizar hiperpar√°metros como el n√∫mero de prototipos, la m√©trica de distancia o el ancho de banda del kernel (si aplica).",
  "No funciona bien si: 1) los datos tienen una **dimensionalidad extremadamente alta** (sufre la maldici√≥n de la dimensionalidad, haciendo que las distancias sean menos significativas), 2) los datos son **muy ruidosos** y los prototipos pueden ser seleccionados incorrectamente, o 3) no hay una estructura clara de 'prototipos' o 'representantes' dentro de los datos."
)

tabla_prototype_based <- data.frame(Criterio = criterios_prototype_based, Aplica = aplica_prototype_based, Detalles = detalles_prototype_based)

tabla_prototype_based %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir el Aprendizaje Basado en Prototipos",
    subtitle = "Modelos que Aprenden a partir de Ejemplos Representativos"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```

