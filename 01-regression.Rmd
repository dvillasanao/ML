# üîç **1. Regressi√≥n** {.unnumbered}

**Ejemplos:** Linear Regression, Ridge, Lasso   

**Cu√°ndo usarlo:**  

* Predicci√≥n de valores num√©ricos continuos (e.g. precios, temperaturas).
* Relaciones lineales entre variables.  

**Ventajas:** Simple, interpretable.  
**Limitaciones:** Mal desempe√±o con relaciones no lineales complejas.  

---  


## Ordinary Least Squares Regression (`OLSR`) {-}   


La **Regresi√≥n por M√≠nimos Cuadrados Ordinarios (OLS)** es una t√©cnica fundamental en estad√≠stica y Machine Learning para modelar la **relaci√≥n lineal** entre una **variable dependiente** (a predecir) y una o m√°s **variables independientes**. Su objetivo es encontrar la **l√≠nea (o hiperplano) que mejor se ajusta** a los datos, minimizando la **suma de los cuadrados de las diferencias** entre los valores reales y los predichos por el modelo. Es decir, busca los coeficientes que hacen que la distancia (al cuadrado) de los puntos a la l√≠nea sea m√≠nima.

Los coeficientes de OLS se pueden calcular directamente con una f√≥rmula matem√°tica, sin necesidad de procesos iterativos complejos, bajo ciertos supuestos como la linealidad de la relaci√≥n y la independencia de los errores.

En el contexto del **aprendizaje global vs. local**, OLS es un ejemplo claro de un modelo de **aprendizaje global**. OLS busca una **√∫nica ecuaci√≥n** o un conjunto de coeficientes que describan la relaci√≥n entre las variables para **todo el conjunto de datos**. La l√≠nea o hiperplano que encuentra es una soluci√≥n global que se aplica de manera uniforme en todo el espacio de caracter√≠sticas. Esto la hace muy interpretable y computacionalmente eficiente, pero limitada si la relaci√≥n real entre las variables no es estrictamente lineal en todo el dominio de los datos.


```{r, echo = FALSE}
# Datos de la tabla
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas y/o categ√≥ricas",
  "‚úÖ Lineal (supuesto clave)",
  "‚òëÔ∏è Deseable",
  "‚úÖ Necesaria",
  "‚úÖ Necesaria",
  "‚ö†Ô∏è S√≠",
  "‚ö†Ô∏è Problema com√∫n",
  "‚úÖ Alta",
  "‚úÖ Muy alta",
  "‚úÖ Compatible",
  "‚ùå Relaciones no lineales, outliers severos, colinealidad"
)
detalles <- c(
  "Se entrena con datos X ‚Üí y",
  "Ej. mpg, precio, ingresos",
  "Categor√≠as convertidas a dummies",
  "Se asume una relaci√≥n lineal entre X e Y",
  "Importante para intervalos de confianza v√°lidos",
  "Errores deben ser independientes",
  "Varianza de errores debe ser constante",
  "Outliers pueden influir mucho en el modelo",
  "Usar VIF para detectar problemas",
  "Modelo f√°cil de explicar",
  "R√°pido incluso con datos grandes",
  "Ayuda a prevenir overfitting",
  "Evitar si no hay linealidad o hay muchos outliers"
)
# Crear y mostrar tabla
tabla_olsr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_olsr %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir OLSR",
             subtitle = "Regresi√≥n de M√≠nimos Cuadrados Ordinarios (OLSR)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Linear Regression {.unnumbered}  

La **Regresi√≥n Lineal** es uno de los algoritmos m√°s fundamentales y ampliamente utilizados en el campo del **Machine Learning y la estad√≠stica**. Es un modelo **supervisado** que busca establecer una **relaci√≥n lineal** entre una **variable de respuesta (o dependiente)** continua y una o m√°s **variables predictoras (o independientes)**.

**Concepto y Ecuaci√≥n:**

La idea central de la regresi√≥n lineal es encontrar la **l√≠nea (o hiperplano en m√∫ltiples dimensiones)** que mejor se ajusta a los datos, de modo que se pueda predecir el valor de la variable dependiente bas√°ndose en los valores de las variables predictoras.

* **Regresi√≥n Lineal Simple:** Implica una √∫nica variable predictora. La ecuaci√≥n de la l√≠nea es:
    $Y = \beta_0 + \beta_1 X + \epsilon$
    Donde:
    * $Y$ es la variable de respuesta.
    * $X$ es la variable predictora.
    * $\beta_0$ es el **intercepto** (el valor de $Y$ cuando $X$ es 0).
    * $\beta_1$ es el **coeficiente de la pendiente** (cu√°nto cambia $Y$ por cada unidad de cambio en $X$).
    * $\epsilon$ es el **t√©rmino de error** o residual (la parte de $Y$ que el modelo no puede explicar).

* **Regresi√≥n Lineal M√∫ltiple:** Implica dos o m√°s variables predictoras. La ecuaci√≥n se extiende a:
    $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \epsilon$
    Donde:
    * $X_1, X_2, ..., X_p$ son las $p$ variables predictoras.
    * $\beta_1, \beta_2, ..., \beta_p$ son los coeficientes de cada variable predictora.

**C√≥mo Funciona (M√≠nimos Cuadrados Ordinarios - OLS):**

El m√©todo m√°s com√∫n para estimar los coeficientes ($\beta$s) en la regresi√≥n lineal es el de **M√≠nimos Cuadrados Ordinarios (OLS)**. OLS funciona minimizando la **suma de los cuadrados de los residuos**. Un residuo es la diferencia entre el valor real de $Y$ y el valor predicho por el modelo ($\hat{Y}$).

$\text{Minimizar: } \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} (Y_i - (\beta_0 + \beta_1 X_{i1} + ... + \beta_p X_{ip}))^2$

Al minimizar esta suma de cuadrados, OLS encuentra los coeficientes que hacen que la l√≠nea de regresi√≥n est√© lo m√°s cerca posible de la mayor√≠a de los puntos de datos.

**Supuestos Clave:**  

La validez de los resultados de la regresi√≥n lineal tradicional se basa en varios supuestos:

* **Linealidad:** La relaci√≥n entre las variables $X$ y $Y$ es lineal.
* **Independencia:** Las observaciones son independientes entre s√≠.
* **Normalidad de los Residuos:** Los residuos se distribuyen normalmente.
* **Homocedasticidad:** La varianza de los residuos es constante a lo largo de todos los niveles de las variables predictoras.
* **No Multicolinealidad Perfecta:** Las variables predictoras no deben estar perfectamente correlacionadas entre s√≠.

**Uso y Limitaciones:**

La regresi√≥n lineal es popular por su **simplicidad, interpretabilidad** y por ser un buen punto de partida para muchos problemas de predicci√≥n. Sin embargo, su principal limitaci√≥n es que solo puede modelar **relaciones lineales**. Si la relaci√≥n subyacente entre las variables es no lineal, una regresi√≥n lineal puede no capturarla adecuadamente y dar resultados inexactos.

**Aprendizaje Global vs. Local:**

La Regresi√≥n Lineal es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** La Regresi√≥n Lineal aprende un **√∫nico conjunto de coeficientes** que define una **l√≠nea (o hiperplano) global** que se aplica a todo el espacio de caracter√≠sticas. Esta l√≠nea busca representar la **relaci√≥n lineal promedio o general** entre las variables predictoras y la variable de respuesta a lo largo de todo el rango de los datos. La predicci√≥n para cualquier nueva instancia se realiza utilizando la misma ecuaci√≥n lineal, sin importar en qu√© parte del espacio de caracter√≠sticas se encuentre. No hay adaptaciones o modelos separados para diferentes subregiones de los datos; el modelo es una funci√≥n que describe una tendencia general y global.

* **Rigidez de la Linealidad:** Debido a su naturaleza global y lineal, la regresi√≥n lineal no puede capturar relaciones **no lineales o interacciones complejas** entre las variables predictoras de forma inherente. Si la relaci√≥n real en los datos es no lineal, el modelo lineal intentar√° ajustarla con la mejor l√≠nea recta posible, lo que podr√≠a llevar a un bajo rendimiento.


## Regresi√≥n Log√≠stica {.unnumbered}

La **Regresi√≥n Log√≠stica** es un modelo estad√≠stico usado principalmente para problemas de **clasificaci√≥n binaria**, donde el objetivo es predecir la **probabilidad** de que una instancia pertenezca a una de dos clases (por ejemplo, "s√≠" o "no", "0" o "1"). A pesar de su nombre, no predice un valor continuo, sino una probabilidad.

Este modelo utiliza una **funci√≥n sigmoide (o log√≠stica)** para transformar una combinaci√≥n lineal de las variables de entrada en un valor entre 0 y 1, que se interpreta como una probabilidad. Los coeficientes del modelo se aprenden maximizando la verosimilitud de observar los datos, generalmente a trav√©s de algoritmos como el descenso de gradiente.

En el contexto del **aprendizaje global vs. local**, la Regresi√≥n Log√≠stica es un modelo de **aprendizaje global**. Esto significa que busca un **√∫nico conjunto de coeficientes** que definen una frontera de decisi√≥n (un hiperplano) que se aplica a todo el espacio de caracter√≠sticas para separar las clases. Asume una relaci√≥n lineal entre las variables de entrada y el logaritmo de la probabilidad, y una vez entrenado, usa esta relaci√≥n global para hacer predicciones en cualquier parte del espacio de datos. Si bien es eficiente y muy interpretable, su naturaleza global puede limitar su rendimiento en casos donde las fronteras de decisi√≥n son inherentemente no lineales o muy complejas.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Lineal entre log-odds y predictores",
  "‚ùå No es requisito",
  "‚úÖ Necesaria",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠",
  "‚ö†Ô∏è Puede afectar",
  "‚úÖ Alta (coeficientes interpretables)",
  "‚úÖ Alta",
  "‚úÖ Compatible",
  "‚ùå Respuesta no binaria o multiclase sin ajuste"
)
detalles <- c(
  "Clasificaci√≥n binaria",
  "Ej. √©xito/fracaso, s√≠/no",
  "Convertir categ√≥ricas a dummies",
  "Relaci√≥n entre log(p/(1-p)) y X debe ser lineal",
  "No se exige normalidad en errores",
  "Independencia entre observaciones",
  "Idealmente varianza constante",
  "Outliers pueden alterar los coeficientes",
  "Usar VIF y regularizaci√≥n si hay problema",
  "Coeficientes en t√©rminos de odds/log-odds",
  "R√°pido y estable para datasets medianos",
  "K-fold funciona muy bien",
  "Evitar si hay multiclase sin ajuste"
)
tabla_logit <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

require(gt)
tabla_logit %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir logit",
             subtitle = "Regresi√≥n log√≠stica") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Locally Estimated Scatterplot Smoothing (`LOESS`) {.unnumbered}

**LOESS (Locally Estimated Scatterplot Smoothing)**, o LOWESS, es una t√©cnica de **regresi√≥n no param√©trica** para crear una curva suave que se ajusta a los datos en un diagrama de dispersi√≥n. Su gran ventaja es que **no asume una forma funcional global** espec√≠fica para la relaci√≥n entre las variables, lo que la hace muy flexible para identificar tendencias y patrones no lineales.

El principio de LOESS es simple: para estimar el valor suavizado en un punto, se seleccionan los **puntos de datos cercanos** (definido por un par√°metro de **"span"** o ancho de banda), se les asignan **pesos** (dando m√°s peso a los puntos m√°s cercanos), y luego se ajusta un **polinomio de bajo grado** (com√∫nmente lineal o cuadr√°tico) a esos puntos usando m√≠nimos cuadrados ponderados. Este proceso se repite para cada punto de inter√©s para construir la curva.

En el contexto del **aprendizaje global vs. local**, LOESS es un modelo de **aprendizaje puramente local**. Su flexibilidad reside en que **ajusta m√∫ltiples modelos simples y locales** (regresiones ponderadas) en diferentes vecindarios de los datos. No busca una √∫nica ecuaci√≥n global que describa la relaci√≥n en todo el conjunto de datos. Esto le permite adaptarse maravillosamente a las variaciones en las relaciones y curvaturas de los datos, lo que es especialmente √∫til cuando los datos no se distribuyen linealmente. Sin embargo, su naturaleza local implica que no produce una f√≥rmula expl√≠cita del modelo, y puede ser computacionalmente m√°s intensivo para conjuntos de datos muy grandes.  


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua",
  "‚úÖ Num√©ricas (usualmente 1 o 2 predictores)",
  "‚úÖ No lineal y suave",
  "‚ùå No necesaria",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è Muy sensible",
  "‚ùå No aplica (pocos predictores)",
  "‚úÖ Muy interpretable gr√°ficamente",
  "‚ö†Ô∏è Lento en grandes vol√∫menes de datos",
  "‚úÖ Puede usarse para elegir 'span'",
  "‚ùå Datos grandes o con ruido fuerte"
)
detalles <- c(
  "Modelo no param√©trico local",
  "Regresi√≥n para valores continuos",
  "Generalmente 1 o 2 variables num√©ricas",
  "Ajuste por vecindad, suaviza la curva",
  "No asume distribuci√≥n espec√≠fica",
  "Supuesto deseable si hay dependencias temporales",
  "Ideal si la varianza no cambia mucho localmente",
  "Altamente afectado por outliers locales",
  "No es una t√©cnica multivariable compleja",
  "La curva ajustada se interpreta visualmente",
  "Computacionalmente costoso con datos grandes",
  "Ayuda a seleccionar el mejor 'span'",
  "Poco eficaz en alta dimensi√≥n o datos muy dispersos"
)
tabla_loess <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
tabla_loess %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LOESS",
             subtitle = "Locally Estimated Scatterplot Smoothing (LOESS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Multivariate Adaptive Regression Splines (`MARS`) {.unnumbered}

**Multivariate Adaptive Regression Splines (MARS)** es un algoritmo de **regresi√≥n no param√©trica** que extiende los modelos lineales para manejar relaciones no lineales y complejas. Desarrollado por Jerome Friedman, MARS construye su modelo al **dividir el espacio de entrada en m√∫ltiples regiones y ajustar una funci√≥n lineal simple (o de orden superior) a cada regi√≥n**.

El proceso de MARS consta de dos fases: una **fase de adelante** que a√±ade iterativamente **funciones base** (pares de funciones "hinge" o bisagra) y **nudos** (puntos de corte) para capturar no linealidades e interacciones entre variables, y una **fase de atr√°s** que poda las funciones base menos significativas utilizando criterios como la **Validaci√≥n Cruzada Generalizada (GCV)** para prevenir el sobreajuste. Esto permite a MARS ser adaptable a las particularidades de los datos.

En el contexto del **aprendizaje global vs. local**, MARS se sit√∫a como un modelo de **aprendizaje adaptativo que combina aspectos globales y locales**. Es "local" en el sentido de que sus funciones base y nudos dividen el espacio de datos en regiones, y dentro de cada regi√≥n se aplica una relaci√≥n simple. Sin embargo, es "global" porque la suma de todas estas funciones base forma una √∫nica ecuaci√≥n que describe la relaci√≥n en todo el conjunto de datos y se aplica de forma consistente. Esto significa que si los datos no se distribuyen linealmente, MARS puede aprender y modelar estas relaciones complejas de forma adaptativa, encontrando autom√°ticamente d√≥nde y c√≥mo las relaciones cambian, ofreciendo una soluci√≥n que es tanto flexible como interpretable.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua o categ√≥rica (binaria con extensi√≥n)",
  "‚úÖ Num√©ricas (categ√≥ricas con dummies)",
  "‚úÖ No lineal (autom√°tico)",
  "‚ùå No requerida",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠ (aunque algo robusto)",
  "‚ö†Ô∏è Puede afectar",
  "‚ö†Ô∏è Media (modelo tipo caja negra)",
  "‚úÖ Razonable para tama√±os medianos",
  "‚úÖ Recomendado (ej. repeated k-fold)",
  "‚ùå Relaci√≥n puramente lineal o muchos factores irrelevantes"
)
detalles <- c(
  "Regresi√≥n flexible no lineal",
  "Ideal para regresi√≥n continua (tambi√©n clasificaci√≥n con `earth`)",
  "Crea autom√°ticamente 'splines' por variable",
  "Crea funciones por tramos con 'nudos'",
  "No exige distribuci√≥n espec√≠fica de errores",
  "Mejor si los datos no est√°n correlacionados temporalmente",
  "Idealmente errores con varianza constante",
  "Puede ser sensible a valores extremos",
  "Detecta interacciones, pero VIF sigue siendo √∫til",
  "Coeficientes no tan interpretables como OLS",
  "M√°s lento que OLS pero m√°s flexible",
  "CV ayuda a elegir n√∫mero √≥ptimo de t√©rminos",
  "Tiene riesgo de sobreajuste si no se controla bien"
)
tabla_mars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_mars %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MARS",
             subtitle = "Splines de Regresi√≥n Adaptativa Multivariante (MARS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Stepwise Regression {.unnumbered}

La **Regresi√≥n por Pasos (Stepwise Regression)** es una t√©cnica para construir un modelo de regresi√≥n lineal (o a veces otros modelos lineales generalizados) seleccionando las variables predictoras de forma iterativa y autom√°tica. Su objetivo es encontrar un subconjunto √≥ptimo de variables que mejore la capacidad predictiva del modelo sin incluir variables irrelevantes o redundantes. Esto ayuda a simplificar el modelo, mejorar la interpretabilidad y reducir el riesgo de sobreajuste.

Existen tres estrategias principales para la regresi√≥n por pasos:

1.  **Selecci√≥n Hacia Adelante (Forward Selection):**
    * Comienza con un modelo que no incluye ninguna variable predictora (solo el intercepto).
    * En cada paso, eval√∫a todas las variables predictoras disponibles que a√∫n no est√°n en el modelo.
    * A√±ade al modelo la variable que, al ser incluida, produce la mayor mejora estad√≠stica (generalmente medida por un valor p bajo, un R-cuadrado ajustado mayor, o un criterio de informaci√≥n como AIC o BIC).
    * El proceso contin√∫a hasta que ninguna de las variables restantes mejora el modelo por encima de un umbral predefinido.

2.  **Eliminaci√≥n Hacia Atr√°s (Backward Elimination):**
    * Comienza con un modelo que incluye **todas** las variables predictoras posibles.
    * En cada paso, eval√∫a las variables predictoras que actualmente est√°n en el modelo.
    * Elimina del modelo la variable que es menos significativa estad√≠sticamente (generalmente medida por un valor p alto, o una reducci√≥n en el R-cuadrado ajustado o un aumento en AIC/BIC).
    * El proceso contin√∫a hasta que la eliminaci√≥n de cualquier variable empeorar√≠a significativamente el modelo.

3.  **H√≠brida (Mixed / Bidirectional Stepwise):**
    * Combina la selecci√≥n hacia adelante y la eliminaci√≥n hacia atr√°s.
    * En cada paso, el algoritmo puede tanto a√±adir una variable si mejora el modelo, como eliminar una variable que ya est√° en el modelo si se vuelve redundante o no significativa. Esto permite que el modelo reconsidere variables que fueron a√±adidas o eliminadas en pasos anteriores. Es el enfoque m√°s com√∫n y robusto.

**Criterios de Selecci√≥n:**  

La decisi√≥n de a√±adir o eliminar una variable en cada paso se basa en criterios estad√≠sticos, siendo los m√°s comunes:
* **Valores p:** Umbrales para la significancia estad√≠stica de los coeficientes.
* **$R^2$ ajustado:** Mide la proporci√≥n de varianza explicada por el modelo, penalizando la inclusi√≥n de variables innecesarias.
* **Criterio de Informaci√≥n de Akaike (AIC):** Penaliza la complejidad del modelo (n√∫mero de par√°metros) en relaci√≥n con su bondad de ajuste.
* **Criterio de Informaci√≥n Bayesiano (BIC):** Similar al AIC, pero con una penalizaci√≥n m√°s fuerte por la complejidad.

**Ventajas y Desventajas:**

* **Ventajas:** Puede ayudar a construir modelos m√°s parsimoniosos, mejorar la interpretabilidad y reducir la multicolinealidad.
* **Desventajas:**
    * **Sobreajuste:** Puede llevar a sobreajuste si se usa de forma acr√≠tica, ya que el algoritmo se optimiza para los datos de entrenamiento.
    * **Problemas de Significancia Estad√≠stica:** Los valores p y otras m√©tricas pueden no ser confiables debido a la selecci√≥n de caracter√≠sticas basada en los datos.
    * **Inestabilidad:** El conjunto de variables seleccionadas puede ser muy sensible a peque√±as perturbaciones en los datos o a la elecci√≥n del criterio de selecci√≥n.
    * **Ignora el Conocimiento del Dominio:** Puede seleccionar variables que son estad√≠sticamente significativas pero que carecen de sentido pr√°ctico o causal.
    * **No Maneja Interacciones Complejas:** Es fundamentalmente un m√©todo para seleccionar variables para un modelo lineal y no est√° dise√±ado para descubrir relaciones no lineales o interacciones complejas.

Debido a sus desventajas, la regresi√≥n por pasos se utiliza con m√°s cautela hoy en d√≠a. A menudo se prefieren m√©todos de regularizaci√≥n (como Lasso o Elastic Net) para la selecci√≥n de caracter√≠sticas, ya que son m√°s estables y realizan la selecci√≥n de forma m√°s robusta.

**Aprendizaje Global vs. Local:**

La Regresi√≥n por Pasos es un modelo de **aprendizaje global**.

* **Aspecto Global:** La regresi√≥n por pasos construye un **√∫nico modelo de regresi√≥n lineal global** que busca explicar la relaci√≥n entre las variables predictoras y la respuesta en todo el conjunto de datos. La selecci√≥n de variables se realiza para optimizar el rendimiento de este modelo global. Los coeficientes finales que se obtienen definen una funci√≥n lineal que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar en qu√© parte del espacio de caracter√≠sticas se encuentre.

* **Proceso de Selecci√≥n (Global):** Aunque el proceso es iterativo y a√±ade/elimina variables, la decisi√≥n en cada paso se basa en c√≥mo esa adici√≥n/eliminaci√≥n afecta la bondad de ajuste o la complejidad del modelo en **todo el conjunto de datos**. No se ajustan modelos separados o locales para diferentes regiones.



## Support Vector Machine (SVM) {.unnumbered}

**Support Vector Machine (SVM)** es un potente y vers√°til algoritmo de **Machine Learning** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**, aunque es m√°s conocido por su aplicaci√≥n en clasificaci√≥n. Su objetivo principal es encontrar el **hiperplano √≥ptimo** que separe las clases en el espacio de caracter√≠sticas con el **margen** m√°s grande posible. Los puntos de datos m√°s cercanos a este hiperplano se llaman **vectores de soporte**, y son cruciales para definir la frontera de decisi√≥n.

Para manejar datos que no son linealmente separables, SVM utiliza el **"truco del kernel"**. Este truco permite a SVM mapear impl√≠citamente los datos a un espacio de mayor dimensi√≥n donde las clases podr√≠an ser linealmente separables, sin necesidad de calcular expl√≠citamente las coordenadas. Funciones kernel comunes como el **Radial Basis Function (RBF) o Gaussiano** permiten a SVM modelar fronteras de decisi√≥n no lineales complejas en el espacio original de baja dimensi√≥n.

En el contexto del **aprendizaje global vs. local**, SVM se clasifica principalmente como un modelo de **aprendizaje global**. Esto se debe a que busca un **√∫nico hiperplano √≥ptimo** (o una frontera de decisi√≥n no lineal definida por el kernel) que se aplica a la totalidad del espacio de caracter√≠sticas. Una vez entrenado, el modelo predice evaluando la posici√≥n de un nuevo punto con respecto a esta frontera global. Sin embargo, hay un matiz "local" en su funcionamiento: la determinaci√≥n de este hiperplano depende **cr√≠ticamente solo de los vectores de soporte**, que son los puntos de datos "m√°s dif√≠ciles" cercanos a la frontera. Los puntos que est√°n lejos del margen no influyen en la definici√≥n del modelo. Aunque la frontera de decisi√≥n es una funci√≥n global que se aplica en todas partes, su construcci√≥n est√° influenciada por estos puntos localmente relevantes, permitiendo a SVM adaptar su aproximaci√≥n incluso cuando las relaciones en los datos no se distribuyen linealmente, al encontrar la separaci√≥n √≥ptima en un espacio transformado.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas (categor√≠as deben codificarse)",
  "‚úÖ Capta relaciones no lineales (kernel)",
  "‚ùå No requiere",
  "‚úÖ Idealmente s√≠",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠, especialmente sin margen amplio",
  "‚úÖ Puede manejarla bien",
  "‚ùå Baja (modelo es una caja negra)",
  "‚ö†Ô∏è Lento con muchos datos o predictores",
  "‚úÖ Esencial para elegir kernel y par√°metros",
  "‚ùå Datos con mucho ruido o solapamiento entre clases"
)
detalles <- c(
  "Modelo supervisado que maximiza el margen entre clases",
  "Clasificaci√≥n binaria, multiclase o regresi√≥n (SVR)",
  "Requiere escalar o estandarizar las variables num√©ricas",
  "Puede usar kernel para resolver problemas no lineales",
  "No requiere supuestos cl√°sicos como normalidad",
  "Mejor si los datos son independientes",
  "Puede usarse aunque haya heterocedasticidad",
  "Los outliers cercanos al margen afectan el modelo",
  "Los kernels pueden reducir el efecto de multicolinealidad",
  "Dif√≠cil de explicar; es un modelo de tipo caja negra",
  "Puede ser costoso computacionalmente con datos grandes",
  "Par√°metros como C y gamma se ajustan v√≠a validaci√≥n cruzada",
  "No es ideal si hay ruido o datos mal etiquetados"
)
tabla_svm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_svm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir SVM",
             subtitle = "Support Vector Machine (SVM) ") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```
