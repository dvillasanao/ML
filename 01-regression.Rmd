# üîç **1. Regressi√≥n** {-}

**Ejemplos:** Linear Regression, Ridge, Lasso   
**Cu√°ndo usarlo:**

* Predicci√≥n de valores num√©ricos continuos (e.g. precios, temperaturas).  
* Relaciones lineales entre variables. 

**Ventajas:** Simple, interpretable.   
**Limitaciones:** Mal desempe√±o con relaciones no lineales complejas.

## Ordinary Least Squares Regression (`OLSR`) {-} 

**Regresi√≥n de M√≠nimos Cuadrados Ordinarios (OLSR)**: un m√©todo de regresi√≥n lineal para estimar los par√°metros desconocidos mediante la creaci√≥n de un modelo que minimizar√° la suma de los errores cuadrados entre los datos observados y los predichos (valores observados y valores estimados).   

```{r, echo = FALSE}
# Datos de la tabla
criterios <- c(
  "üîç Tipo de modelo",
  "üéØ Variable respuesta",
  "üî¢ Variables predictoras",
  "üìà Relaci√≥n entre variables",
  "üß™ Normalidad de residuos",
  "üîÅ Independencia de errores",
  "‚öñÔ∏è Homoscedasticidad",
  "‚ùó Sensible a outliers",
  "üîó Multicolinealidad entre predictores",
  "üß† Interpretabilidad",
  "üöÄ Velocidad y eficiencia",
  "üß™ Validaci√≥n cruzada",
  "‚ùå No funciona bien si..."
)

aplica <- c(
  "Supervisado",
  "Num√©rica continua",
  "Num√©ricas y/o categ√≥ricas",
  "Lineal (supuesto clave)",
  "Deseable",
  "Necesaria",
  "Necesaria",
  "S√≠",
  "Problema com√∫n",
  "Alta",
  "Muy alta",
  "Compatible",
  "Relaciones no lineales, outliers severos, colinealidad"
)

detalles <- c(
  "Se entrena con datos X ‚Üí y",
  "Ej. mpg, precio, ingresos",
  "Categor√≠as convertidas a dummies",
  "Se asume una relaci√≥n lineal entre X e Y",
  "Importante para intervalos de confianza v√°lidos",
  "Errores deben ser independientes",
  "Varianza de errores debe ser constante",
  "Outliers pueden influir mucho en el modelo",
  "Usar VIF para detectar problemas",
  "Modelo f√°cil de explicar",
  "R√°pido incluso con datos grandes",
  "Ayuda a prevenir overfitting",
  "Evitar si no hay linealidad o hay muchos outliers"
)

# Crear y mostrar tabla
tabla_olsr <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

require(gt)

tabla_olsr %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir OLSR") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```


## Linear Regression {-} 

**Regresi√≥n lineal** : se utiliza para estimar valores reales (costo de las casas, n√∫mero de visitas, ventas totales, etc.) basados en variables continuas.  


## Logistic Regression {-} 

**Regresi√≥n log√≠stica** : se utiliza para estimar valores discretos (valores binarios como 0/1, s√≠/no, verdadero/falso) basados en un conjunto dado de variables independientes. 



## Stepwise Regression {-} 

**Regresi√≥n por pasos** : a√±ade caracter√≠sticas al modelo una a una hasta encontrar la puntuaci√≥n √≥ptima para tu conjunto de caracter√≠sticas. La selecci√≥n por pasos alterna entre el avance y el retroceso, incorporando y eliminando variables que cumplen los criterios de entrada o eliminaci√≥n, hasta alcanzar un conjunto estable de variables.  

## Multivariate Adaptive Regression Splines (`MARS`) {-} 

**Splines de Regresi√≥n Adaptativa Multivariante (`MARS`)**: un m√©todo de regresi√≥n flexible que busca interacciones y relaciones no lineales que ayudan a maximizar la precisi√≥n predictiva. Este algoritmo es inherentemente no lineal (lo que significa que no es necesario adaptar el modelo a patrones no lineales en el datos agregando manualmente t√©rminos del modelo (squared terms, interaction effects).   

## Locally Estimated Scatterplot Smoothing (`LOESS`) {-} 

**Locally Estimated Scatterplot Smoothing (`LOESS`)**: un m√©todo para ajustar una curva suave entre dos variables o una superficie  entre un resultado y hasta cuatro variables predictoras. La idea es que, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n. Se puede aplicar regresi√≥n, lo que se denomina regresi√≥n ponderada localmente. Se puede aplicar LOESS cuando la relaci√≥n entre las variables independientes y dependientes no es lineal. Hoy en d√≠a, la mayor√≠a de los algoritmos (como las redes neuronales de propagaci√≥n hacia adelante cl√°sicas, las m√°quinas de vectores de soporte, los algoritmos del vecino m√°s cercano, etc.) son sistemas de aprendizaje global que se utilizan para minimizar las funciones de p√©rdida globales (por ejemplo, el error cuadr√°tico medio). Por el contrario, los sistemas de aprendizaje local dividen el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Esto generalmente se logra dividiendo la funci√≥n de costo en m√∫ltiples funciones de costo locales independientes. Una de las desventajas de los m√©todos globales es que, a veces, ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena. Pero entonces surge LOESS, una alternativa a la aproximaci√≥n de funciones globales.   


## Regression Ridge {-} 

**Regresi√≥n Ridge** es una extensi√≥n de la regresi√≥n lineal cl√°sica (`OLS`) que se usa cuando hay problemas de multicolinealidad o riesgo de sobreajuste. Aborda estos problemas introduciendo un t√©rmino de penalizaci√≥n a la funci√≥n de coste de la regresi√≥n lineal ordinaria (m√≠nimos cuadrados ordinarios, OLS).  

## Least Absolute Shrinkage and Selection Operator (`LASSO`) {-}

**Least Absolute Shrinkage and Selection Operator (`LASSO`)**: es otra t√©cnica de regularizaci√≥n utilizada en modelos de regresi√≥n lineal, similar a la Regresi√≥n Ridge, pero con una diferencia clave en el tipo de penalizaci√≥n que aplica en la funci√≥n de coste de la regresi√≥n lineal ordinaria.    


---