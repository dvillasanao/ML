# üîç 1. Regresi√≥n {-}   

**Ejemplos:** Regresi√≥n Lineal Simple, Regresi√≥n Ridge, Regresi√≥n Lasso.  
**Uso:** Ideal para **predecir valores num√©ricos continuos** (como precios o temperaturas) y cuando esperas **relaciones lineales** entre tus variables.   
**Ventajas:** Es un modelo **simple** de entender y altamente **interpretable**.  
**Limitaciones:** Su desempe√±o es bajo cuando las relaciones entre las variables son **no lineales** o muy complejas.  

---

## Ordinary Least Squares Regression (`OLSR`) {-}    

[Ordinary Least Squares Regression (`OLSR`) en R](https://dvillasanao.github.io/ML_Examples/Output/Regression/01_01.OLSR.html)  
[Ordinary Least Squares Regression (`OLSR`) en Python](https://dvillasanao.github.io/ML_Examples/R/Regression/01_01_OLSR_py.html)   

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/01_image_OLSR.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/OLSR.png"))
```


La **Regresi√≥n por M√≠nimos Cuadrados Ordinarios (OLS)** es una t√©cnica fundamental en estad√≠stica y Machine Learning para modelar la **relaci√≥n lineal** entre una **variable dependiente** (a predecir) y una o m√°s **variables independientes**. Su objetivo es encontrar la **l√≠nea (o hiperplano) que mejor se ajusta** a los datos, minimizando la **suma de los cuadrados de las diferencias** entre los valores reales y los predichos por el modelo. Es decir, busca los coeficientes que hacen que la distancia (al cuadrado) de los puntos a la l√≠nea sea m√≠nima.

Los coeficientes de OLS se pueden calcular directamente con una f√≥rmula matem√°tica, sin necesidad de procesos iterativos complejos, bajo ciertos supuestos como la linealidad de la relaci√≥n y la independencia de los errores.

En el contexto del **aprendizaje global vs. local**, OLS es un ejemplo claro de un modelo de **aprendizaje global**. OLS busca una **√∫nica ecuaci√≥n** o un conjunto de coeficientes que describan la relaci√≥n entre las variables para **todo el conjunto de datos**. La l√≠nea o hiperplano que encuentra es una soluci√≥n global que se aplica de manera uniforme en todo el espacio de caracter√≠sticas. Esto la hace muy interpretable y computacionalmente eficiente, pero limitada si la relaci√≥n real entre las variables no es estrictamente lineal en todo el dominio de los datos.


## Linear Regression {.unnumbered}   


La **Regresi√≥n Lineal** es uno de los algoritmos m√°s fundamentales y ampliamente utilizados en el campo del **Machine Learning y la estad√≠stica**. Es un modelo **supervisado** que busca establecer una **relaci√≥n lineal** entre una **variable de respuesta (o dependiente)** continua y una o m√°s **variables predictoras (o independientes)**.

**Concepto y Ecuaci√≥n:**

La idea central de la regresi√≥n lineal es encontrar la **l√≠nea (o hiperplano en m√∫ltiples dimensiones)** que mejor se ajusta a los datos, de modo que se pueda predecir el valor de la variable dependiente bas√°ndose en los valores de las variables predictoras.

* **Regresi√≥n Lineal Simple:** Implica una √∫nica variable predictora. La ecuaci√≥n de la l√≠nea es:
    $$Y = \beta_0 + \beta_1 X + \epsilon$$  
    
Donde:
    * $Y$ es la variable de respuesta.
    * $X$ es la variable predictora.
    * $\beta_0$ es el **intercepto** (el valor de $Y$ cuando $X$ es 0).
    * $\beta_1$ es el **coeficiente de la pendiente** (cu√°nto cambia $Y$ por cada unidad de cambio en $X$).
    * $\epsilon$ es el **t√©rmino de error** o residual (la parte de $Y$ que el modelo no puede explicar).

* **Regresi√≥n Lineal M√∫ltiple:** Implica dos o m√°s variables predictoras. La ecuaci√≥n se extiende a:   
    $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \epsilon$$  
    
Donde:
    * $X_1, X_2, ..., X_p$ son las $p$ variables predictoras.
    * $\beta_1, \beta_2, ..., \beta_p$ son los coeficientes de cada variable predictora.

**C√≥mo Funciona (M√≠nimos Cuadrados Ordinarios - OLS):**

El m√©todo m√°s com√∫n para estimar los coeficientes ($\beta$s) en la regresi√≥n lineal es el de **M√≠nimos Cuadrados Ordinarios (OLS)**. OLS funciona minimizando la **suma de los cuadrados de los residuos**. Un residuo es la diferencia entre el valor real de $Y$ y el valor predicho por el modelo ($\hat{Y}$).

$$\text{Minimizar: } \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} (Y_i - (\beta_0 + \beta_1 X_{i1} + ... + \beta_p X_{ip}))^2$$

Al minimizar esta suma de cuadrados, OLS encuentra los coeficientes que hacen que la l√≠nea de regresi√≥n est√© lo m√°s cerca posible de la mayor√≠a de los puntos de datos.

**Supuestos Clave:**  

La validez de los resultados de la regresi√≥n lineal tradicional se basa en varios supuestos:

* **Linealidad:** La relaci√≥n entre las variables $X$ y $Y$ es lineal.
* **Independencia:** Las observaciones son independientes entre s√≠.
* **Normalidad de los Residuos:** Los residuos se distribuyen normalmente.
* **Homocedasticidad:** La varianza de los residuos es constante a lo largo de todos los niveles de las variables predictoras.
* **No Multicolinealidad Perfecta:** Las variables predictoras no deben estar perfectamente correlacionadas entre s√≠.

**Uso y Limitaciones:**

La regresi√≥n lineal es popular por su **simplicidad, interpretabilidad** y por ser un buen punto de partida para muchos problemas de predicci√≥n. Sin embargo, su principal limitaci√≥n es que solo puede modelar **relaciones lineales**. Si la relaci√≥n subyacente entre las variables es no lineal, una regresi√≥n lineal puede no capturarla adecuadamente y dar resultados inexactos.

**Aprendizaje Global vs. Local:**

La Regresi√≥n Lineal es un modelo de **aprendizaje puramente global**.

* **Aspecto Global:** La Regresi√≥n Lineal aprende un **√∫nico conjunto de coeficientes** que define una **l√≠nea (o hiperplano) global** que se aplica a todo el espacio de caracter√≠sticas. Esta l√≠nea busca representar la **relaci√≥n lineal promedio o general** entre las variables predictoras y la variable de respuesta a lo largo de todo el rango de los datos. La predicci√≥n para cualquier nueva instancia se realiza utilizando la misma ecuaci√≥n lineal, sin importar en qu√© parte del espacio de caracter√≠sticas se encuentre. No hay adaptaciones o modelos separados para diferentes subregiones de los datos; el modelo es una funci√≥n que describe una tendencia general y global.

* **Rigidez de la Linealidad:** Debido a su naturaleza global y lineal, la regresi√≥n lineal no puede capturar relaciones **no lineales o interacciones complejas** entre las variables predictoras de forma inherente. Si la relaci√≥n real en los datos es no lineal, el modelo lineal intentar√° ajustarla con la mejor l√≠nea recta posible, lo que podr√≠a llevar a un bajo rendimiento.


## Regresi√≥n Log√≠stica (Logit) {.unnumbered}

```{r, echo = FALSE,out.width='50%', fig.align='center', fig.cap="Elaboraci√≥n propia"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Logit.png"))
```

La **Regresi√≥n Log√≠stica** es un modelo estad√≠stico usado principalmente para problemas de **clasificaci√≥n binaria**, donde el objetivo es predecir la **probabilidad** de que una instancia pertenezca a una de dos clases (por ejemplo, "s√≠" o "no", "0" o "1"). A pesar de su nombre, no predice un valor continuo, sino una probabilidad.

Este modelo utiliza una **funci√≥n sigmoide (o log√≠stica)** para transformar una combinaci√≥n lineal de las variables de entrada en un valor entre 0 y 1, que se interpreta como una probabilidad. Los coeficientes del modelo se aprenden maximizando la verosimilitud de observar los datos, generalmente a trav√©s de algoritmos como el descenso de gradiente.

En el contexto del **aprendizaje global vs. local**, la Regresi√≥n Log√≠stica es un modelo de **aprendizaje global**. Esto significa que busca un **√∫nico conjunto de coeficientes** que definen una frontera de decisi√≥n (un hiperplano) que se aplica a todo el espacio de caracter√≠sticas para separar las clases. Asume una relaci√≥n lineal entre las variables de entrada y el logaritmo de la probabilidad, y una vez entrenado, usa esta relaci√≥n global para hacer predicciones en cualquier parte del espacio de datos. Si bien es eficiente y muy interpretable, su naturaleza global puede limitar su rendimiento en casos donde las fronteras de decisi√≥n son inherentemente no lineales o muy complejas.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica binaria (0/1)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Lineal entre log-odds y predictores",
  "‚ùå No es requisito",
  "‚úÖ Necesaria",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠",
  "‚ö†Ô∏è Puede afectar",
  "‚úÖ Alta (coeficientes interpretables)",
  "‚úÖ Alta",
  "‚úÖ Compatible",
  "‚ùå Respuesta no binaria o multiclase sin ajuste"
)
detalles <- c(
  "Clasificaci√≥n binaria",
  "Ej. √©xito/fracaso, s√≠/no",
  "Convertir categ√≥ricas a dummies",
  "Relaci√≥n entre log(p/(1-p)) y X debe ser lineal",
  "No se exige normalidad en errores",
  "Independencia entre observaciones",
  "Idealmente varianza constante",
  "Outliers pueden alterar los coeficientes",
  "Usar VIF y regularizaci√≥n si hay problema",
  "Coeficientes en t√©rminos de odds/log-odds",
  "R√°pido y estable para datasets medianos",
  "K-fold funciona muy bien",
  "Evitar si hay multiclase sin ajuste"
)
tabla_logit <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

require(gt)
tabla_logit %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir logit",
             subtitle = "Regresi√≥n log√≠stica") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Generalized Linear Model (GLM) {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/GLM.png"))
```

```{r, echo = FALSE}
criterios_glm <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_glm <- c(
  "‚úÖ Supervisado",
  "‚úÖ Num√©rica, conteo, binaria, etc.",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ No lineal (transformada)",
  "‚ùå No es requisito",
  "‚úÖ Necesaria",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠",
  "‚ö†Ô∏è Puede afectar",
  "‚úÖ Alta (coeficientes interpretables)",
  "‚úÖ Alta",
  "‚úÖ Compatible",
  "‚ùå No cumple supuestos clave"
)

detalles_glm <- c(
  "Regresi√≥n o clasificaci√≥n (dependiendo de la distribuci√≥n y funci√≥n de enlace)",
  "Requiere que la respuesta pertenezca a la familia exponencial (e.g., Normal, Poisson, Binomial, Gamma)",
  "Las categ√≥ricas se convierten a variables dummy o indicadoras.",
  "La funci√≥n de enlace transforma la esperanza de la respuesta para ser lineal en los predictores.",
  "No asume normalidad en los errores, sino una distribuci√≥n espec√≠fica para la respuesta.",
  "Se asume que las observaciones son independientes.",
  "La varianza puede depender de la media a trav√©s de la funci√≥n de varianza.",
  "Pueden influir en la estimaci√≥n de los coeficientes, especialmente con algunas distribuciones.",
  "Puede generar inestabilidad en los coeficientes y dificultar la interpretaci√≥n.",
  "Los coeficientes se interpretan en la escala de la funci√≥n de enlace.",
  "Generalmente eficientes computacionalmente.",
  "T√©cnicas como k-fold cross-validation son aplicables.",
  "Por ejemplo, si la funci√≥n de enlace o la distribuci√≥n asumida no son adecuadas para los datos."
)

tabla_glm <- data.frame(Criterio = criterios_glm, Aplica = aplica_glm, Detalles = detalles_glm)

tabla_glm %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir GLM",
    subtitle = "Modelos Lineales Generalizados"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Least Angle Regression (LARS)  {-}  

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regularization/LARS.png"))
```

**Least Angle Regression (LARS)** es un algoritmo de **regresi√≥n lineal** desarrollado por Bradley Efron, Trevor Hastie, Iain Johnstone y Robert Tibshirani. Es particularmente interesante porque puede considerarse como una **versi√≥n m√°s eficiente y paso a paso de LASSO** (Least Absolute Shrinkage and Selection Operator) y es √∫til para **seleccionar caracter√≠sticas** y manejar datos de alta dimensi√≥n.

A diferencia de OLS, que calcula todos los coeficientes de una vez, o de Lasso, que requiere optimizaci√≥n m√°s compleja, LARS opera de manera incremental. Su idea central es avanzar los coeficientes de forma que su √°ngulo con el vector de residuos sea siempre el mismo y que sea el "m√°s peque√±o" posible.

El proceso de LARS se puede resumir as√≠:

1.  **Inicio:** Todos los coeficientes se inicializan en cero.
2.  **Identificaci√≥n del Predictor m√°s Correlacionado:** El algoritmo encuentra la variable predictora que est√° m√°s correlacionada con la variable de respuesta (o con el residuo actual).
3.  **Movimiento en la Direcci√≥n del Predictor:** El coeficiente de esa variable predictora se mueve gradualmente desde cero en la direcci√≥n del signo de su correlaci√≥n. A medida que el coeficiente se mueve, el residuo cambia.
4.  **Activaci√≥n de Nuevos Predictores:** Cuando otra variable predictora alcanza la misma correlaci√≥n con el residuo actual que la variable que ya est√° activa, el algoritmo cambia de direcci√≥n. Ahora, los coeficientes de *ambas* variables activas se mueven juntas en un "√°ngulo equiestad√≠stico" de tal manera que permanecen igualmente correlacionadas con el residuo.
5.  **Proceso Iterativo:** Este proceso contin√∫a, a√±adiendo nuevas variables al conjunto de variables "activas" (es decir, aquellas con coeficientes distintos de cero) a medida que estas alcanzan la misma correlaci√≥n con el residuo. Los coeficientes se mueven de forma coordinada.
6.  **Criterio de Parada:** El algoritmo se detiene cuando todos los predictores han sido incluidos en el modelo, o cuando se alcanza un n√∫mero predefinido de pasos o de variables.

**Relaci√≥n con otros modelos:**
* Si LARS se detiene cuando los coeficientes de las variables no activas son menores o iguales a la correlaci√≥n actual de las variables activas (y los coeficientes de las variables no activas se fijan en cero si su correlaci√≥n es menor), entonces genera la **soluci√≥n completa del camino de LASSO**.
* Tambi√©n puede generar el camino de soluciones para la **Ridge Regression** si se modifica ligeramente.

LARS es eficiente porque solo requiere un n√∫mero de pasos igual al n√∫mero de variables, o menos si se detiene antes.

**Aprendizaje Global vs. Local:**

Least Angle Regression (LARS) es un modelo de **aprendizaje global**.

* **Aspecto Global:** LARS construye un **modelo lineal global** paso a paso. Aunque el algoritmo a√±ade variables una por una y ajusta sus coeficientes de manera incremental, el modelo resultante en cada paso es una ecuaci√≥n de regresi√≥n lineal que se aplica a todo el conjunto de datos. La decisi√≥n de qu√© variable a√±adir y c√≥mo ajustar los coeficientes se basa en las correlaciones globales entre las variables predictoras y la respuesta (o el residuo). La finalidad es encontrar los coeficientes √≥ptimos para una funci√≥n de regresi√≥n que se aplica a todo el espacio de caracter√≠sticas.

* **Selecci√≥n de Caracter√≠sticas Globalmente:** La capacidad de LARS para realizar selecci√≥n de caracter√≠sticas (al igual que LASSO) es un proceso global. Se identifican las variables m√°s influyentes en el contexto de todo el conjunto de datos, y su inclusi√≥n en el modelo contribuye a la formaci√≥n de una relaci√≥n global entre los predictores y la respuesta. No se construyen modelos separados para diferentes subregiones de los datos; en cambio, se construye un √∫nico modelo global de manera progresiva.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado (regresi√≥n)",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (requiere estandarizaci√≥n)",
  "‚úÖ Lineal",
  "‚ö†Ô∏è Deseable para inferencia cl√°sica",
  "‚úÖ Necesaria",
  "‚úÖ Supuesto importante",
  "‚ö†Ô∏è Afectado por valores extremos",
  "‚úÖ Maneja bien multicolinealidad como LASSO",
  "‚úÖ Muy interpretable (secuencia de modelos anidados)",
  "‚úÖ Muy eficiente, especialmente con muchas variables",
  "‚úÖ √ötil para elegir n√∫mero de variables con validaci√≥n cruzada",
  "‚ùå Datos ruidosos o no lineales; ‚ùå si hay muchas interacciones no capturadas"
)

detalles <- c(
  "Algoritmo de regresi√≥n eficiente que selecciona variables secuencialmente como alternativa a LASSO.",
  "Busca predecir una variable continua usando m√∫ltiples predictores.",
  "Usa variables num√©ricas estandarizadas; es sensible a escala.",
  "Asume relaci√≥n lineal entre predictores y respuesta.",
  "No exige normalidad para predicci√≥n, pero s√≠ para inferencia estad√≠stica.",
  "Errores deben ser independientes entre s√≠.",
  "Supone varianza constante de los errores.",
  "Puede verse afectado si hay valores extremos en las variables.",
  "Muy √∫til cuando los predictores est√°n correlacionados; elige uno a la vez.",
  "Produce una ruta de modelos f√°cilmente interpretable con selecci√≥n progresiva.",
  "M√°s r√°pido que LASSO al generar trayectorias de coeficientes.",
  "Puede aplicarse validaci√≥n cruzada para seleccionar el mejor modelo en la secuencia.",
  "No captura relaciones no lineales o interacciones sin modificaci√≥n previa del modelo."
)

tabla_lars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_lars %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LARS",
             subtitle = "Least Angle Regression (LARS)")  %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Locally Estimated Scatterplot Smoothing (`LOESS`) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
knitr::include_graphics(paste0(here::here(), "/img/Regression/LOESS.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/LOESS_1.png"))
```

**LOESS (Locally Estimated Scatterplot Smoothing)**, o LOWESS, es una t√©cnica de **regresi√≥n no param√©trica** para crear una curva suave que se ajusta a los datos en un diagrama de dispersi√≥n. Su gran ventaja es que **no asume una forma funcional global** espec√≠fica para la relaci√≥n entre las variables, lo que la hace muy flexible para identificar tendencias y patrones no lineales.

El principio de LOESS es simple: para estimar el valor suavizado en un punto, se seleccionan los **puntos de datos cercanos** (definido por un par√°metro de **"span"** o ancho de banda), se les asignan **pesos** (dando m√°s peso a los puntos m√°s cercanos), y luego se ajusta un **polinomio de bajo grado** (com√∫nmente lineal o cuadr√°tico) a esos puntos usando m√≠nimos cuadrados ponderados. Este proceso se repite para cada punto de inter√©s para construir la curva.

En el contexto del **aprendizaje global vs. local**, LOESS es un modelo de **aprendizaje puramente local**. Su flexibilidad reside en que **ajusta m√∫ltiples modelos simples y locales** (regresiones ponderadas) en diferentes vecindarios de los datos. No busca una √∫nica ecuaci√≥n global que describa la relaci√≥n en todo el conjunto de datos. Esto le permite adaptarse maravillosamente a las variaciones en las relaciones y curvaturas de los datos, lo que es especialmente √∫til cuando los datos no se distribuyen linealmente. Sin embargo, su naturaleza local implica que no produce una f√≥rmula expl√≠cita del modelo, y puede ser computacionalmente m√°s intensivo para conjuntos de datos muy grandes.  


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua",
  "‚úÖ Num√©ricas (usualmente 1 o 2 predictores)",
  "‚úÖ No lineal y suave",
  "‚ùå No necesaria",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è Muy sensible",
  "‚ùå No aplica (pocos predictores)",
  "‚úÖ Muy interpretable gr√°ficamente",
  "‚ö†Ô∏è Lento en grandes vol√∫menes de datos",
  "‚úÖ Puede usarse para elegir 'span'",
  "‚ùå Datos grandes o con ruido fuerte"
)
detalles <- c(
  "Modelo no param√©trico local",
  "Regresi√≥n para valores continuos",
  "Generalmente 1 o 2 variables num√©ricas",
  "Ajuste por vecindad, suaviza la curva",
  "No asume distribuci√≥n espec√≠fica",
  "Supuesto deseable si hay dependencias temporales",
  "Ideal si la varianza no cambia mucho localmente",
  "Altamente afectado por outliers locales",
  "No es una t√©cnica multivariable compleja",
  "La curva ajustada se interpreta visualmente",
  "Computacionalmente costoso con datos grandes",
  "Ayuda a seleccionar el mejor 'span'",
  "Poco eficaz en alta dimensi√≥n o datos muy dispersos"
)
tabla_loess <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
tabla_loess %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir LOESS",
             subtitle = "Locally Estimated Scatterplot Smoothing (LOESS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Multivariate Adaptive Regression Splines (`MARS`) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/MARS.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/MARS_1.png"))
```

**Multivariate Adaptive Regression Splines (MARS)** es un algoritmo de **regresi√≥n no param√©trica** que extiende los modelos lineales para manejar relaciones no lineales y complejas. Desarrollado por Jerome Friedman, MARS construye su modelo al **dividir el espacio de entrada en m√∫ltiples regiones y ajustar una funci√≥n lineal simple (o de orden superior) a cada regi√≥n**.

El proceso de MARS consta de dos fases: una **fase de adelante** que a√±ade iterativamente **funciones base** (pares de funciones "hinge" o bisagra) y **nudos** (puntos de corte) para capturar no linealidades e interacciones entre variables, y una **fase de atr√°s** que poda las funciones base menos significativas utilizando criterios como la **Validaci√≥n Cruzada Generalizada (GCV)** para prevenir el sobreajuste. Esto permite a MARS ser adaptable a las particularidades de los datos.

En el contexto del **aprendizaje global vs. local**, MARS se sit√∫a como un modelo de **aprendizaje adaptativo que combina aspectos globales y locales**. Es "local" en el sentido de que sus funciones base y nudos dividen el espacio de datos en regiones, y dentro de cada regi√≥n se aplica una relaci√≥n simple. Sin embargo, es "global" porque la suma de todas estas funciones base forma una √∫nica ecuaci√≥n que describe la relaci√≥n en todo el conjunto de datos y se aplica de forma consistente. Esto significa que si los datos no se distribuyen linealmente, MARS puede aprender y modelar estas relaciones complejas de forma adaptativa, encontrando autom√°ticamente d√≥nde y c√≥mo las relaciones cambian, ofreciendo una soluci√≥n que es tanto flexible como interpretable.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua o categ√≥rica (binaria con extensi√≥n)",
  "‚úÖ Num√©ricas (categ√≥ricas con dummies)",
  "‚úÖ No lineal (autom√°tico)",
  "‚ùå No requerida",
  "‚úÖ Deseable",
  "‚úÖ Deseable",
  "‚ö†Ô∏è S√≠ (aunque algo robusto)",
  "‚ö†Ô∏è Puede afectar",
  "‚ö†Ô∏è Media (modelo tipo caja negra)",
  "‚úÖ Razonable para tama√±os medianos",
  "‚úÖ Recomendado (ej. repeated k-fold)",
  "‚ùå Relaci√≥n puramente lineal o muchos factores irrelevantes"
)
detalles <- c(
  "Regresi√≥n flexible no lineal",
  "Ideal para regresi√≥n continua (tambi√©n clasificaci√≥n con `earth`)",
  "Crea autom√°ticamente 'splines' por variable",
  "Crea funciones por tramos con 'nudos'",
  "No exige distribuci√≥n espec√≠fica de errores",
  "Mejor si los datos no est√°n correlacionados temporalmente",
  "Idealmente errores con varianza constante",
  "Puede ser sensible a valores extremos",
  "Detecta interacciones, pero VIF sigue siendo √∫til",
  "Coeficientes no tan interpretables como OLS",
  "M√°s lento que OLS pero m√°s flexible",
  "CV ayuda a elegir n√∫mero √≥ptimo de t√©rminos",
  "Tiene riesgo de sobreajuste si no se controla bien"
)
tabla_mars <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_mars %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir MARS",
             subtitle = "Splines de Regresi√≥n Adaptativa Multivariante (MARS)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

## Polynomial Regression {-} 

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Polynomial Regression.png"))
```

```{r, echo = FALSE}
criterios_poly_reg <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_poly_reg <- c(
  "‚úÖ Supervisado",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas (principalmente)",
  "‚úÖ No lineal (curvil√≠nea)",
  "‚úÖ Necesaria",
  "‚úÖ Necesaria",
  "‚úÖ Necesaria",
  "‚úÖ S√≠, muy",
  "‚ö†Ô∏è S√≠, alta",
  "‚ö†Ô∏è Moderada a baja",
  "‚úÖ Alta",
  "‚úÖ Compatible",
  "‚ùå No hay una relaci√≥n polin√≥mica clara"
)

detalles_poly_reg <- c(
  "Regresi√≥n para predecir un valor continuo.",
  "La variable dependiente debe ser continua.",
  "Funciona mejor con variables predictoras num√©ricas para crear t√©rminos polin√≥micos.",
  "Se ajusta una curva a los datos usando potencias de las variables predictoras (ej., x, x^2, x^3).",
  "Los errores deben seguir una distribuci√≥n normal con media cero.",
  "Las observaciones deben ser independientes entre s√≠.",
  "La varianza de los errores debe ser constante en todos los niveles de las variables predictoras.",
  "Los valores at√≠picos pueden distorsionar significativamente la curva ajustada.",
  "Los t√©rminos polin√≥micos (ej., x y x^2) suelen estar altamente correlacionados, requiriendo centrado o escalado.",
  "La interpretaci√≥n de los coeficientes puede ser compleja, ya que representan el cambio en la pendiente de la curva.",
  "R√°pido para ajustar y predecir, similar a la regresi√≥n lineal simple.",
  "Esencial para seleccionar el grado del polinomio y evaluar la generalizaci√≥n del modelo.",
  "Los datos no exhiben una tendencia curvil√≠nea, o si se elige un grado de polinomio inapropiado (muy bajo o muy alto)."
)

tabla_poly_reg <- data.frame(Criterio = criterios_poly_reg, Aplica = aplica_poly_reg, Detalles = detalles_poly_reg)

tabla_poly_reg %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Regresi√≥n Polin√≥mica",
    subtitle = "Extensi√≥n de la Regresi√≥n Lineal"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Quantile Regression {-}   

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/Quantile Regression.png"))
```

```{r, echo = FALSE}
library(gt)

criterios_quantile_reg <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica_quantile_reg <- c(
  "‚úÖ Supervisado",
  "‚úÖ Num√©rica continua",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ Lineal (en cada cuantil)",
  "‚ùå No es requisito",
  "‚úÖ Necesaria",
  "‚ùå No es requisito",
  "‚ùå Menos sensible",
  "‚ö†Ô∏è Puede afectar",
  "‚úÖ Alta (para cada cuantil)",
  "‚úÖ Moderada",
  "‚úÖ Compatible",
  "‚ùå Se busca solo la media condicional"
)

detalles_quantile_reg <- c(
  "Regresi√≥n para modelar la relaci√≥n entre predictores y un cuantil espec√≠fico (ej., mediana, Q1, Q3) de la variable respuesta.",
  "La variable dependiente debe ser continua.",
  "Puede manejar ambos tipos de variables, similar a la regresi√≥n lineal.",
  "La relaci√≥n es lineal en la escala de cada cuantil condicional (ej., la mediana o el percentil 90 de Y dado X).",
  "No asume normalidad en la distribuci√≥n de los errores, lo que la hace robusta a distribuciones no normales.",
  "Las observaciones deben ser independientes entre s√≠.",
  "No asume varianza constante de los errores; de hecho, puede modelar c√≥mo la varianza (o forma de la distribuci√≥n) cambia a trav√©s de los cuantiles.",
  "Utiliza la funci√≥n de p√©rdida de suma de valores absolutos ponderados, que es m√°s robusta a valores at√≠picos que el m√©todo de m√≠nimos cuadrados.",
  "La multicolinealidad puede generar coeficientes inestables, aunque es menos problem√°tica que en OLS para la inferencia.",
  "Los coeficientes se interpretan como el cambio en el cuantil especificado de la variable respuesta por unidad de cambio en el predictor, manteniendo los dem√°s constantes.",
  "Puede ser m√°s intensiva computacionalmente que la regresi√≥n de m√≠nimos cuadrados ordinarios (OLS) si se modelan muchos cuantiles.",
  "Aplicable para evaluar el rendimiento del modelo en la predicci√≥n de diferentes cuantiles.",
  "El objetivo es exclusivamente entender la media condicional de la respuesta, y no la distribuci√≥n completa o diferentes puntos de la distribuci√≥n."
)

tabla_quantile_reg <- data.frame(Criterio = criterios_quantile_reg, Aplica = aplica_quantile_reg, Detalles = detalles_quantile_reg)

tabla_quantile_reg %>%
  gt() %>%
  tab_header(
    title = "Gu√≠a r√°pida para elegir Regresi√≥n Cuant√≠lica",
    subtitle = "Modelado de cuantiles condicionales"
  ) %>%
  tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
  tab_options(
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    table.font.names = "Century Gothic",
    table.font.size = 10,
    data_row.padding = px(1)
  ) %>%
  tab_style(
    style = list(cell_text(align = "left", weight = 'bold')),
    locations = list(cells_title(groups = c("title")))
  ) %>%
  tab_style(
    style = list(cell_text(align = "left")),
    locations = list(cells_title(groups = c("subtitle")))
  ) %>%
  cols_width(
    starts_with("Detalles") ~ px(500),
    everything() ~ px(200)
  ) %>%
  as_raw_html()
```


## Stepwise Regression {.unnumbered}

```{r, echo = FALSE,out.width='50%', fig.align='center'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/StepW.png"))
```

La **Regresi√≥n por Pasos (Stepwise Regression)** es una t√©cnica para construir un modelo de regresi√≥n lineal (o a veces otros modelos lineales generalizados) seleccionando las variables predictoras de forma iterativa y autom√°tica. Su objetivo es encontrar un subconjunto √≥ptimo de variables que mejore la capacidad predictiva del modelo sin incluir variables irrelevantes o redundantes. Esto ayuda a simplificar el modelo, mejorar la interpretabilidad y reducir el riesgo de sobreajuste.

Existen tres estrategias principales para la regresi√≥n por pasos:

1.  **Selecci√≥n Hacia Adelante (Forward Selection):**
    * Comienza con un modelo que no incluye ninguna variable predictora (solo el intercepto).
    * En cada paso, eval√∫a todas las variables predictoras disponibles que a√∫n no est√°n en el modelo.
    * A√±ade al modelo la variable que, al ser incluida, produce la mayor mejora estad√≠stica (generalmente medida por un valor p bajo, un R-cuadrado ajustado mayor, o un criterio de informaci√≥n como AIC o BIC).
    * El proceso contin√∫a hasta que ninguna de las variables restantes mejora el modelo por encima de un umbral predefinido.

2.  **Eliminaci√≥n Hacia Atr√°s (Backward Elimination):**
    * Comienza con un modelo que incluye **todas** las variables predictoras posibles.
    * En cada paso, eval√∫a las variables predictoras que actualmente est√°n en el modelo.
    * Elimina del modelo la variable que es menos significativa estad√≠sticamente (generalmente medida por un valor p alto, o una reducci√≥n en el R-cuadrado ajustado o un aumento en AIC/BIC).
    * El proceso contin√∫a hasta que la eliminaci√≥n de cualquier variable empeorar√≠a significativamente el modelo.

3.  **H√≠brida (Mixed / Bidirectional Stepwise):**
    * Combina la selecci√≥n hacia adelante y la eliminaci√≥n hacia atr√°s.
    * En cada paso, el algoritmo puede tanto a√±adir una variable si mejora el modelo, como eliminar una variable que ya est√° en el modelo si se vuelve redundante o no significativa. Esto permite que el modelo reconsidere variables que fueron a√±adidas o eliminadas en pasos anteriores. Es el enfoque m√°s com√∫n y robusto.

**Criterios de Selecci√≥n:**  

La decisi√≥n de a√±adir o eliminar una variable en cada paso se basa en criterios estad√≠sticos, siendo los m√°s comunes:
* **Valores p:** Umbrales para la significancia estad√≠stica de los coeficientes.
* **$R^2$ ajustado:** Mide la proporci√≥n de varianza explicada por el modelo, penalizando la inclusi√≥n de variables innecesarias.
* **Criterio de Informaci√≥n de Akaike (AIC):** Penaliza la complejidad del modelo (n√∫mero de par√°metros) en relaci√≥n con su bondad de ajuste.
* **Criterio de Informaci√≥n Bayesiano (BIC):** Similar al AIC, pero con una penalizaci√≥n m√°s fuerte por la complejidad.

**Ventajas y Desventajas:**

* **Ventajas:** Puede ayudar a construir modelos m√°s parsimoniosos, mejorar la interpretabilidad y reducir la multicolinealidad.
* **Desventajas:**
    * **Sobreajuste:** Puede llevar a sobreajuste si se usa de forma acr√≠tica, ya que el algoritmo se optimiza para los datos de entrenamiento.
    * **Problemas de Significancia Estad√≠stica:** Los valores p y otras m√©tricas pueden no ser confiables debido a la selecci√≥n de caracter√≠sticas basada en los datos.
    * **Inestabilidad:** El conjunto de variables seleccionadas puede ser muy sensible a peque√±as perturbaciones en los datos o a la elecci√≥n del criterio de selecci√≥n.
    * **Ignora el Conocimiento del Dominio:** Puede seleccionar variables que son estad√≠sticamente significativas pero que carecen de sentido pr√°ctico o causal.
    * **No Maneja Interacciones Complejas:** Es fundamentalmente un m√©todo para seleccionar variables para un modelo lineal y no est√° dise√±ado para descubrir relaciones no lineales o interacciones complejas.

Debido a sus desventajas, la regresi√≥n por pasos se utiliza con m√°s cautela hoy en d√≠a. A menudo se prefieren m√©todos de regularizaci√≥n (como Lasso o Elastic Net) para la selecci√≥n de caracter√≠sticas, ya que son m√°s estables y realizan la selecci√≥n de forma m√°s robusta.

**Aprendizaje Global vs. Local:**

La Regresi√≥n por Pasos es un modelo de **aprendizaje global**.

* **Aspecto Global:** La regresi√≥n por pasos construye un **√∫nico modelo de regresi√≥n lineal global** que busca explicar la relaci√≥n entre las variables predictoras y la respuesta en todo el conjunto de datos. La selecci√≥n de variables se realiza para optimizar el rendimiento de este modelo global. Los coeficientes finales que se obtienen definen una funci√≥n lineal que se aplica de manera consistente a cualquier nueva observaci√≥n, sin importar en qu√© parte del espacio de caracter√≠sticas se encuentre.

* **Proceso de Selecci√≥n (Global):** Aunque el proceso es iterativo y a√±ade/elimina variables, la decisi√≥n en cada paso se basa en c√≥mo esa adici√≥n/eliminaci√≥n afecta la bondad de ajuste o la complejidad del modelo en **todo el conjunto de datos**. No se ajustan modelos separados o locales para diferentes regiones.



## Support Vector Machine (SVM) {.unnumbered} 

```{r echo=FALSE, fig.show="hold", out.width="48%"}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Regression/SVM.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/SVM_1.png"))
```


**Support Vector Machine (SVM)** es un potente y vers√°til algoritmo de **Machine Learning** que se utiliza tanto para tareas de **clasificaci√≥n** como de **regresi√≥n**, aunque es m√°s conocido por su aplicaci√≥n en clasificaci√≥n. Su objetivo principal es encontrar el **hiperplano √≥ptimo** que separe las clases en el espacio de caracter√≠sticas con el **margen** m√°s grande posible. Los puntos de datos m√°s cercanos a este hiperplano se llaman **vectores de soporte**, y son cruciales para definir la frontera de decisi√≥n.

Para manejar datos que no son linealmente separables, SVM utiliza el **"truco del kernel"**. Este truco permite a SVM mapear impl√≠citamente los datos a un espacio de mayor dimensi√≥n donde las clases podr√≠an ser linealmente separables, sin necesidad de calcular expl√≠citamente las coordenadas. Funciones kernel comunes como el **Radial Basis Function (RBF) o Gaussiano** permiten a SVM modelar fronteras de decisi√≥n no lineales complejas en el espacio original de baja dimensi√≥n.

En el contexto del **aprendizaje global vs. local**, SVM se clasifica principalmente como un modelo de **aprendizaje global**. Esto se debe a que busca un **√∫nico hiperplano √≥ptimo** (o una frontera de decisi√≥n no lineal definida por el kernel) que se aplica a la totalidad del espacio de caracter√≠sticas. Una vez entrenado, el modelo predice evaluando la posici√≥n de un nuevo punto con respecto a esta frontera global. Sin embargo, hay un matiz "local" en su funcionamiento: la determinaci√≥n de este hiperplano depende **cr√≠ticamente solo de los vectores de soporte**, que son los puntos de datos "m√°s dif√≠ciles" cercanos a la frontera. Los puntos que est√°n lejos del margen no influyen en la definici√≥n del modelo. Aunque la frontera de decisi√≥n es una funci√≥n global que se aplica en todas partes, su construcci√≥n est√° influenciada por estos puntos localmente relevantes, permitiendo a SVM adaptar su aproximaci√≥n incluso cuando las relaciones en los datos no se distribuyen linealmente, al encontrar la separaci√≥n √≥ptima en un espacio transformado.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas (categor√≠as deben codificarse)",
  "‚úÖ Capta relaciones no lineales (kernel)",
  "‚ùå No requiere",
  "‚úÖ Idealmente s√≠",
  "‚ùå No es requisito",
  "‚ö†Ô∏è S√≠, especialmente sin margen amplio",
  "‚úÖ Puede manejarla bien",
  "‚ùå Baja (modelo es una caja negra)",
  "‚ö†Ô∏è Lento con muchos datos o predictores",
  "‚úÖ Esencial para elegir kernel y par√°metros",
  "‚ùå Datos con mucho ruido o solapamiento entre clases"
)
detalles <- c(
  "Modelo supervisado que maximiza el margen entre clases",
  "Clasificaci√≥n binaria, multiclase o regresi√≥n (SVR)",
  "Requiere escalar o estandarizar las variables num√©ricas",
  "Puede usar kernel para resolver problemas no lineales",
  "No requiere supuestos cl√°sicos como normalidad",
  "Mejor si los datos son independientes",
  "Puede usarse aunque haya heterocedasticidad",
  "Los outliers cercanos al margen afectan el modelo",
  "Los kernels pueden reducir el efecto de multicolinealidad",
  "Dif√≠cil de explicar; es un modelo de tipo caja negra",
  "Puede ser costoso computacionalmente con datos grandes",
  "Par√°metros como C y gamma se ajustan v√≠a validaci√≥n cruzada",
  "No es ideal si hay ruido o datos mal etiquetados"
)
tabla_svm <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)
require(gt)
tabla_svm %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir SVM",
             subtitle = "Support Vector Machine (SVM) ") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```
