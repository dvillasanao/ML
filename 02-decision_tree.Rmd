# üå≤ **2. √Årboles de Decisi√≥n y Derivados** {-}  

**Ejemplos:** Decision Tree, Random Forest, Gradient Boosting  
**Cu√°ndo usarlo:**  
    * Problemas tabulares con relaciones no lineales y variables categ√≥ricas o num√©ricas.
    * Cuando interpretabilidad es importante.

**Ventajas:** Manejan datos heterog√©neos, f√°ciles de interpretar (√°rboles simples).   
**Limitaciones:** Sobreajuste en √°rboles simples; menor desempe√±o en datos muy ruidosos sin ensambles.

---


## C4.5  {-}   

**C4.5** es una extensi√≥n del algoritmo **ID3**, tambi√©n desarrollado por Ross Quinlan, y es uno de los algoritmos de **√°rboles de decisi√≥n** m√°s influyentes y ampliamente utilizados para tareas de **clasificaci√≥n**. Fue dise√±ado para abordar algunas de las limitaciones de su predecesor, ID3, y se ha convertido en un est√°ndar de facto en el aprendizaje autom√°tico para construir modelos predictivos interpretables.

Al igual que ID3, C4.5 construye un √°rbol de clasificaci√≥n seleccionando en cada nodo el atributo que mejor divide el conjunto de datos. Sin embargo, en lugar de usar solo la **ganancia de informaci√≥n**, C4.5 utiliza la **relaci√≥n de ganancia** (Gain Ratio). La relaci√≥n de ganancia normaliza la ganancia de informaci√≥n por la entrop√≠a intr√≠nseca del atributo, lo que ayuda a mitigar el sesgo de ID3 hacia atributos con muchos valores. Adem√°s, C4.5 introduce varias mejoras significativas:

* **Manejo de atributos continuos:** Puede discretizar atributos num√©ricos continuos dividiendo el rango en intervalos.
* **Manejo de valores faltantes:** Puede manejar datos con valores ausentes asignando una probabilidad fraccionada a cada rama posible.
* **Poda del √°rbol:** Implementa una t√©cnica de poda para reducir el sobreajuste, lo que implica eliminar ramas del √°rbol que no aportan significativamente a la clasificaci√≥n o que representan ruido en los datos.

En el contexto del **aprendizaje global vs. local**, C4.5, al igual que ID3 y CART, opera como un sistema de **aprendizaje local**. La construcci√≥n del √°rbol se logra a trav√©s de decisiones de divisi√≥n que se optimizan localmente en cada nodo, buscando la m√°xima homogeneidad o pureza en los subconjuntos resultantes. Esto le permite a C4.5 manejar eficazmente relaciones no lineales entre las variables independientes y dependientes. La idea es que, si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se puede aplicar de forma efectiva mediante esta **regresi√≥n ponderada localmente**, donde el algoritmo divide el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Al centrarse en divisiones √≥ptimas a nivel de subconjuntos, C4.5 ofrece una alternativa robusta a los m√©todos de aproximaci√≥n de funciones globales, que a veces pueden fallar en proporcionar una buena aproximaci√≥n cuando la relaci√≥n entre las variables no es lineal.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Categ√≥ricas y num√©ricas",
  "‚úÖ Captura relaciones no lineales",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es relevante",
  "‚ö†Ô∏è Moderadamente (puede hacer overfitting con ruido)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol interpretable)",
  "‚úÖ Relativamente r√°pido",
  "‚úÖ Recomendable para evitar sobreajuste",
  "‚ùå Demasiadas categor√≠as o ruido en datos"
)

detalles <- c(
  "Modelo supervisado tipo √°rbol de decisi√≥n",
  "Clasifica variables categ√≥ricas en ramas l√≥gicas",
  "Divide por puntos de corte para variables num√©ricas",
  "No asume forma funcional entre predictores y respuesta",
  "No necesita normalidad de errores",
  "Mejor si las observaciones son independientes",
  "No requiere varianzas constantes",
  "Datos ruidosos pueden afectar las ramas",
  "No se ve afectado por correlaciones entre predictores",
  "Salida f√°cil de visualizar y explicar",
  "Escala bien para tama√±os de muestra medianos",
  "Evita sobreajuste con poda y validaci√≥n",
  "Muchas clases con pocos datos pueden sobreajustar"
)

tabla_c45 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_c45 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir C4.5") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## C5.0  {-}  

**C5.0** es la versi√≥n m√°s reciente y avanzada de los algoritmos de √°rboles de decisi√≥n desarrollados por Ross Quinlan, sucediendo a ID3 y C4.5. Es un algoritmo propietario (aunque se ofrece una versi√≥n de c√≥digo abierto bajo ciertas licencias) y es ampliamente reconocido por su **rapidez**, **precisi√≥n** y **eficiencia** en la construcci√≥n de **√°rboles de decisi√≥n** y **reglas de clasificaci√≥n** para tareas de **clasificaci√≥n**.

Al igual que sus predecesores, C5.0 construye un √°rbol de clasificaci√≥n mediante la divisi√≥n recursiva de los datos en subconjuntos m√°s homog√©neos. Sin embargo, C5.0 incorpora mejoras significativas que lo hacen superior en muchos aspectos:

* **Velocidad y eficiencia:** Es notablemente m√°s r√°pido y m√°s eficiente en el uso de memoria que C4.5, lo que le permite manejar conjuntos de datos mucho m√°s grandes.
* **Impulso (Boosting):** C5.0 puede usar la t√©cnica de **boosting** (espec√≠ficamente, una variante de AdaBoost) para crear m√∫ltiples √°rboles de decisi√≥n y combinarlos para producir una predicci√≥n m√°s robusta y precisa. Esto reduce significativamente los errores de clasificaci√≥n y mejora la generalizaci√≥n.
* **Poda mejorada:** Ofrece t√©cnicas de poda m√°s sofisticadas para evitar el sobreajuste y producir √°rboles m√°s peque√±os y comprensibles.
* **Manejo de valores faltantes y atributos continuos:** Al igual que C4.5, maneja de manera efectiva valores faltantes y atributos num√©ricos continuos.
* **Generaci√≥n de reglas:** Adem√°s de √°rboles de decisi√≥n, C5.0 puede generar conjuntos de **reglas de clasificaci√≥n** concisas, que a menudo son m√°s f√°ciles de interpretar que un √°rbol completo.

En el contexto de la **regresi√≥n localmente ponderada**, C5.0, como los dem√°s algoritmos de √°rboles de decisi√≥n, opera bajo la premisa de un **aprendizaje local**. La construcci√≥n del √°rbol implica tomar decisiones de divisi√≥n √≥ptimas en cada nodo, bas√°ndose en la informaci√≥n local de ese subconjunto de datos. Si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n, que es su enfoque principal) se puede aplicar eficazmente al dividir el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples. Cada divisi√≥n en el √°rbol se puede ver como una forma de **regresi√≥n ponderada localmente**, donde el algoritmo se enfoca en aproximar la relaci√≥n dentro de un subespacio espec√≠fico del conjunto de datos. Esto convierte a C5.0 en una potente alternativa a los m√©todos de aproximaci√≥n de funciones globales, especialmente cuando la relaci√≥n entre las variables independientes y dependientes no es lineal y se busca un modelo interpretable y robusto.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Categ√≥ricas y num√©ricas",
  "‚úÖ Captura relaciones no lineales",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es relevante",
  "‚ö†Ô∏è Moderadamente (puede generar ramas excesivas)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol f√°cil de visualizar)",
  "‚úÖ Relativamente r√°pido en training",
  "‚úÖ Recomendable para evitar sobreajuste",
  "‚ùå Clases muy desbalanceadas sin ajuste"
)

detalles <- c(
  "Algoritmo de √°rbol de decisi√≥n avanzado basado en C4.5",
  "Clasifica en m√∫ltiples categor√≠as (tambi√©n multiclase)",
  "Divide autom√°ticamente variables num√©ricas con puntos de corte",
  "No asume funci√≥n lineal: usa ganancia de informaci√≥n y boosting",
  "No exige normalidad de errores",
  "Mejor si las instancias son independientes",
  "No requiere varianzas constantes",
  "Los valores extremos pueden influir en ramas profundas",
  "No se ve afectado por correlaci√≥n alta entre predictores",
  "Salida clara con reglas y pesos de boosting",
  "M√°s r√°pido que C4.5 y con opciones de boosting",
  "Usar k-fold o repeated CV para determinar par√°metros √≥ptimos",
  "Muchos atributos irrelevantes pueden generar sobreajuste"
)

tabla_c50 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_c50 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir C5.0") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

``` 


## Classification and Regression Tree (CART)  {-} 

**Classification and Regression Tree (CART)** es un m√©todo no param√©trico que se utiliza para construir **√°rboles de decisi√≥n** tanto para problemas de **clasificaci√≥n** como de **regresi√≥n**. La idea central es dividir recursivamente el espacio de las caracter√≠sticas en regiones m√°s peque√±as y manejables, creando as√≠ un modelo con forma de √°rbol que es f√°cil de interpretar.

A diferencia de los modelos lineales o algunos algoritmos de aprendizaje global, CART no asume una relaci√≥n lineal entre las variables. En su lugar, el algoritmo identifica los mejores **puntos de divisi√≥n** en las variables predictoras para maximizar la **homogeneidad** de las respuestas dentro de cada regi√≥n resultante. Para problemas de clasificaci√≥n, esto se mide com√∫nmente con m√©tricas como la **impureza Gini** o la **ganancia de informaci√≥n**, mientras que para la regresi√≥n, se busca minimizar la **suma de los cuadrados de los residuos**.

Mientras que muchos algoritmos (como las redes neuronales cl√°sicas o las m√°quinas de vectores de soporte) son sistemas de **aprendizaje global** que buscan minimizar una funci√≥n de p√©rdida √∫nica para todo el conjunto de datos, CART se puede considerar m√°s como un sistema de **aprendizaje local**. Construye el modelo tomando decisiones de divisi√≥n locales en cada nodo del √°rbol, lo que le permite capturar relaciones complejas y no lineales en los datos. Esto es particularmente √∫til cuando una aproximaci√≥n de funci√≥n global √∫nica podr√≠a no ser suficiente para modelar la relaci√≥n entre las variables. Una de las ventajas de CART es su capacidad para manejar diferentes tipos de datos (num√©ricos y categ√≥ricos) y su interpretabilidad, ya que la ruta desde la ra√≠z hasta una hoja del √°rbol representa un conjunto de reglas de decisi√≥n.



```{r, echo =FALSE}

criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica o Continua",
  "‚úÖ Num√©ricas y Categ√≥ricas",
  "‚úÖ No lineal y con interacciones",
  "‚ùå No requiere",
  "‚ö†Ô∏è Puede verse afectado",
  "‚ö†Ô∏è No necesario pero deseable",
  "‚ö†Ô∏è S√≠, en puntos de corte",
  "‚úÖ No se ve afectado",
  "‚úÖ Alta (gr√°fico del √°rbol)",
  "‚úÖ R√°pido en datasets medianos",
  "‚úÖ Muy usado para poda y ajuste",
  "‚ùå Muy profundo (overfitting), datos muy ruidosos"
)

detalles <- c(
  "Algoritmo basado en divisiones binarias",
  "Puede predecir clases o valores continuos",
  "Acepta todo tipo de variables predictoras",
  "Captura relaciones complejas y no lineales",
  "No requiere distribuci√≥n normal",
  "Idealmente los errores deben ser independientes",
  "La varianza constante mejora resultados",
  "Puede generar divisiones extremas por valores at√≠picos",
  "No necesita preocuparse por colinealidad",
  "F√°cil de entender, especialmente con √°rboles peque√±os",
  "Escalable pero no √≥ptimo en grandes vol√∫menes sin poda",
  "Usa poda y validaci√≥n cruzada para evitar sobreajuste",
  "Tiende al sobreajuste si no se poda o se regulariza"
)

tabla_cart <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

require(gt) 

tabla_cart %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CART",
             subtitle = "Classification and Regression Tree (CART)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Chi-squared Automatic Interaction Detection (CHAID)  {-}    

**Chi-squared Automatic Interaction Detection (CHAID)** es un algoritmo de **√°rboles de decisi√≥n** utilizado principalmente para tareas de **clasificaci√≥n** y, en menor medida, para la **regresi√≥n** (aunque se aplica m√°s com√∫nmente a variables dependientes categ√≥ricas). La idea fundamental de CHAID es construir un √°rbol de decisi√≥n al encontrar las mejores divisiones en las variables predictoras que maximicen la significancia estad√≠stica de la relaci√≥n con la variable dependiente.

A diferencia de ID3, C4.5 o CART, que utilizan medidas de impureza como la entrop√≠a o el √≠ndice Gini, CHAID se basa en pruebas estad√≠sticas de **chi-cuadrado ($\chi^2$)** para identificar las divisiones √≥ptimas. Cuando la variable dependiente es nominal o ordinal, CHAID eval√∫a cada variable predictora para encontrar la combinaci√≥n de categor√≠as que sea m√°s significativamente diferente de otras combinaciones en t√©rminos de la distribuci√≥n de la variable dependiente. El algoritmo fusiona las categor√≠as de una variable predictora si no son significativamente diferentes, y luego selecciona la variable predictora y la divisi√≥n que resultan en el valor m√°s bajo de $p$ (es decir, la mayor significancia estad√≠stica) de la prueba $\chi^2$. Para variables dependientes continuas, se utiliza una prueba F.

En el contexto del **aprendizaje global vs. local**, CHAID opera como un sistema de **aprendizaje local**. La construcci√≥n del √°rbol es un proceso iterativo y recursivo donde las decisiones de divisi√≥n se toman en cada nodo bas√°ndose en la significancia estad√≠stica local de la interacci√≥n entre las variables predictoras y la variable dependiente. Esto le permite a CHAID descubrir relaciones complejas y no lineales en los datos. La idea es que, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n (o clasificaci√≥n) de manera efectiva mediante lo que se denomina **regresi√≥n ponderada localmente**. Esto se logra al dividir el problema de aprendizaje global en m√∫ltiples problemas de aprendizaje m√°s peque√±os y simples, donde cada rama del √°rbol representa una regi√≥n del espacio de caracter√≠sticas donde las interacciones son evaluadas y modeladas localmente. Esto hace de CHAID una alternativa robusta a los m√©todos de aproximaci√≥n de funciones globales, especialmente cuando se busca un modelo interpretable y se quieren identificar las interacciones entre las variables de una manera estad√≠sticamente rigurosa.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n multinivel)",
  "‚úÖ Categ√≥ricas nativas (num√©ricas requieren binarizaci√≥n o discretizaci√≥n)",
  "‚úÖ No lineal (explora interacciones autom√°ticas con œá¬≤)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente (outliers categ√≥ricos pueden crear nodos muy peque√±os)",
  "‚úÖ Robusto a multicolinealidad (usa œá¬≤, no varianzas)",
  "‚úÖ Media (√°rboles con muchos nodos pueden resultar complejos)",
  "‚ö†Ô∏è Razonablemente r√°pido en datasets moderados, lento si hay muy altas cardinalidades",
  "‚úÖ Recomendable para determinar profundidad y grado de interacci√≥n",
  "‚ùå Variable objetivo continua o muchos niveles con pocas observaciones"
)

detalles <- c(
  "Construye un √°rbol de decisi√≥n usando pruebas œá¬≤ para detectar interacciones entre predictores y variable objetivo.",
  "Dise√±ado para clasificar en m√∫ltiples categor√≠as sin orden; puede manejar targets con m√°s de dos niveles.",
  "Funciona mejor con predictores categ√≥ricos; las variables num√©ricas deben transformarse en categor√≠as mediante binning.",
  "No asume ninguna forma funcional; detecta autom√°ticamente relaciones complejas basadas en œá¬≤.",
  "No depende de supuestos de normalidad de errores ni de forma de distribuci√≥n de residuos.",
  "Las instancias deben ser independientes; no es ideal para datos con fuerte dependencia temporal sin procesar.",
  "Homoscedasticidad no se eval√∫a, ya que no se basa en un t√©rmino de error param√©trico como OLS.",
  "Los valores extremos en variables categ√≥ricas con pocas observaciones pueden crear ramas muy espec√≠ficas, pero CHAID maneja cardinalidades moderadas.",
  "Al basarse en œá¬≤, CHAID no se ve afectado directamente por colinealidad, aunque variables muy correlacionadas pueden crear redundancia en las divisiones.",
  "Cada divisi√≥n se basa en pruebas de œá¬≤; el √°rbol resultante puede interpretarse visualmente, pero muchos niveles pueden reducir claridad.",
  "La creaci√≥n recursiva de nodos por agrupaci√≥n de categor√≠as es eficiente para conjuntos de datos moderados; puede volverse lento si hay muchas categor√≠as de predictores.",
  "Se usa validaci√≥n cruzada para podar el √°rbol y elegir el nivel √≥ptimo de interacci√≥n, equilibrando sesgo y varianza.",
  "No es adecuado si la variable objetivo es continua (sin discretizar) o si hay demasiados niveles con muy pocos casos en cada uno."
)

tabla_chaid <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)


tabla_chaid %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir CHAID",
             subtitle = "Chi-squared Automatic Interaction Detection (CHAID)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```

## Conditional Decision Trees (Conditional Inference Trees - CITs)  {-}   

**Conditional Decision Trees**, often referred to as **Conditional Inference Trees (CITs)**, represent a class of **√°rboles de decisi√≥n** que abordan una limitaci√≥n importante de los algoritmos de √°rboles de decisi√≥n tradicionales como CART, ID3, y C4.5: el **sesgo en la selecci√≥n de variables**. Mientras que los algoritmos tradicionales pueden favorecer variables predictoras con muchas categor√≠as o valores continuos (debido a que estas variables tienen m√°s "oportunidades" de generar una divisi√≥n que parezca √≥ptima), los CITs emplean un enfoque basado en **pruebas estad√≠sticas** para seleccionar la mejor divisi√≥n.

La idea fundamental de los Conditional Decision Trees es que cada divisi√≥n en el √°rbol se basa en la **significancia estad√≠stica** de la asociaci√≥n entre las variables predictoras y la variable de respuesta. En lugar de seleccionar el atributo que maximiza una medida de impureza (como la ganancia de informaci√≥n o la impureza Gini), los CITs realizan una serie de **pruebas de inferencia condicional** (t√≠picamente **pruebas de permutaci√≥n**).

El algoritmo opera de la siguiente manera:
1.  En cada nodo, se eval√∫a una **hip√≥tesis nula** de independencia entre cada variable predictora y la variable de respuesta.
2.  Se calcula el valor de $p$ para cada variable predictora.
3.  La variable predictora con el valor de $p$ m√°s peque√±o (es decir, la asociaci√≥n m√°s estad√≠sticamente significativa) es seleccionada para la divisi√≥n, siempre y cuando este valor de $p$ sea menor que un umbral de significancia predefinido.
4.  Una vez seleccionada la variable, se encuentra la mejor divisi√≥n binaria (generalmente) dentro de esa variable para ese nodo.
5.  Este proceso se repite recursivamente hasta que no haya m√°s variables significativas para dividir o se alcance un criterio de parada.

En el contexto del **aprendizaje global vs. local**, los Conditional Decision Trees se pueden considerar como un enfoque de **aprendizaje local** con un fuerte respaldo estad√≠stico. Aunque el √°rbol resultante es un modelo global, cada decisi√≥n de divisi√≥n se toma localmente bas√°ndose en la inferencia estad√≠stica sobre la relaci√≥n entre las variables en ese subconjunto de datos. Esto significa que si los datos no se distribuyen linealmente, el concepto de regresi√≥n (o clasificaci√≥n) se aplica de forma efectiva mediante lo que se denomina **regresi√≥n ponderada localmente**. Al utilizar pruebas de significancia para las divisiones, los CITs evitan el problema de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en una √∫nica aproximaci√≥n global, ya que las divisiones se determinan por la evidencia estad√≠stica local. Esto los convierte en una alternativa robusta que ofrece una selecci√≥n de variables menos sesgada y modelos con una mayor interpretabilidad estad√≠stica.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua (regresi√≥n) o Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Num√©ricas y categ√≥ricas",
  "‚úÖ No lineal, usa tests condicionales para particionar",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No relevante",
  "‚ö†Ô∏è Moderadamente (consume tests basados en permutaciones)",
  "‚úÖ Robusto a colinealidad",
  "‚úÖ Alta (cada divisi√≥n est√° basada en criterios estad√≠sticos claros)",
  "‚ö†Ô∏è M√°s lento que CART en datasets grandes",
  "‚úÖ Recomendable para podar y evitar sobreajuste",
  "‚ùå Datos muy peque√±os por nodo o variables irrelevantes"
)

detalles <- c(
  "Construye √°rboles basados en test de independencia condicional (ctree).",
  "Permite tanto regresi√≥n (valor continuo) como clasificaci√≥n multinivel.",
  "Acepta variables num√©ricas y categ√≥ricas sin necesidad de dummies.",
  "Detecta relaciones complejas y no lineales usando tests basados en permutaciones.",
  "No exige que los residuos sigan una distribuci√≥n espec√≠fica.",
  "Ideal si las observaciones no est√°n correlacionadas en el tiempo.",
  "No requiere homoscedasticidad porque no se basa en un modelo param√©trico de error.",
  "Los outliers pueden afectar el c√°lculo de los tests, aunque es m√°s robusto que CART.",
  "El algoritmo ctree no se ve afectado por predictores altamente correlacionados.",
  "Los √°rboles generados son f√°ciles de visualizar y explicar.",
  "Para cada divisi√≥n realiza m√∫ltiples tests, por lo que es m√°s lento en datos muy grandes.",
  "Usar k-fold o repeated CV para elegir la profundidad y evitar sobreajuste.",
  "No es apto si tienes muy pocas observaciones en cada parto o muchas variables irrelevantes."
)

tabla_ctree <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_ctree %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir ctree",
             subtitle = "Conditional Decision Trees") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 

```


## Decision Stump  {-}  

Un **Decision Stump** es el tipo de **√°rbol de decisi√≥n** m√°s simple y fundamental, compuesto por un **√∫nico nodo de decisi√≥n (la ra√≠z)** que se conecta directamente a los **nodos hoja**. La idea es que un *decision stump* toma una decisi√≥n de clasificaci√≥n o regresi√≥n bas√°ndose en una sola caracter√≠stica o atributo de entrada.

Aunque parece demasiado simple, la l√≥gica es que, a pesar de su simplicidad, un *decision stump* identifica el mejor umbral o categor√≠a dentro de una √∫nica variable para separar los datos de la manera m√°s efectiva posible. Para problemas de clasificaci√≥n, esto significa encontrar la caracter√≠stica que, por s√≠ sola, maximice alguna medida de **pureza** (como la ganancia de informaci√≥n, la impureza Gini, o la significancia chi-cuadrado) o minimice el error de clasificaci√≥n. Para regresi√≥n, buscar√° el punto de divisi√≥n en una sola caracter√≠stica que minimice la suma de los cuadrados de los errores.

En el contexto del **aprendizaje local vs. global**, un *decision stump* es inherentemente un sistema de **aprendizaje local**. Su "aprendizaje" se limita a encontrar la mejor divisi√≥n dentro de una √∫nica variable, lo que es una forma extrema de **regresi√≥n ponderada localmente**. Si los datos no se distribuyen linealmente, un *decision stump* no puede por s√≠ mismo modelar relaciones complejas. Sin embargo, su valor no reside en ser un modelo predictivo robusto por s√≠ mismo, sino en ser un **"clasificador d√©bil"** o **"regresor d√©bil"** que puede ser combinado en **conjuntos de modelos (ensembles)** m√°s potentes. Por ejemplo, los *decision stumps* son los bloques de construcci√≥n m√°s comunes para algoritmos de **boosting** como **AdaBoost**. En estos casos, m√∫ltiples *decision stumps* se entrenan secuencialmente, cada uno enfoc√°ndose en los errores que cometieron los *stumps* anteriores, sumando sus "aprendizajes locales" para formar un modelo global m√°s preciso. Esto contrarresta la limitaci√≥n de que "a veces ning√∫n valor de par√°metro puede proporcionar una aproximaci√≥n suficientemente buena" en un solo modelo.


```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n) o Continua (regresi√≥n simplificada)",
  "‚úÖ Num√©ricas y/o categ√≥ricas",
  "‚ö†Ô∏è Captura solo una divisi√≥n (muy simple, un solo nodo interno)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No relevante",
  "‚úÖ Relativamente robusto (poca complejidad)",
  "‚úÖ Ignora colinealidad (usa solo una variable)",
  "‚úÖ Muy alta (un solo umbral para dividir)",
  "‚úÖ Extremadamente r√°pido",
  "‚úÖ Se puede usar k-fold para evaluar estabilidad",
  "‚ùå No funciona bien si la relaci√≥n es compleja o no hay un buen umbral √∫nico"
)

detalles <- c(
  "Modelo de √°rbol con un solo nivel de decisi√≥n (un umbral en una sola variable).",
  "En clasificaci√≥n predice una clase binaria; en regresi√≥n, un valor medio para cada divisi√≥n.",
  "Selecciona la mejor variable con el punto de corte que maximiza ganancia (clasificaci√≥n) o reduce varianza (regresi√≥n).",
  "Solo ajusta un umbral, por lo que no modela interacciones ni no linealidades complejas.",
  "No hay supuestos param√©tricos de distribuci√≥n de errores.",
  "Mejor si las instancias no est√°n correlacionadas (por ejemplo, no aplica a series de tiempo sin agrupar).",
  "La varianza constante no se eval√∫a, pues el modelo es no param√©trico y muy simple.",
  "Un solo punto de corte es menos sensible a outliers en comparaci√≥n con √°rboles profundos, pero a√∫n puede verse afectado si un outlier define el umbral.",
  "Como solo usa una variable, no se ve afectado por correlaciones altas entre predictores.",
  "El modelo entero es resumido en un √∫nico umbral; f√°cil de explicar.",
  "Muy r√°pido de entrenar y predecir, pues solo se eval√∫a un umbral en un predictor.",
  "Es √∫til para comprobar si hay una √∫nica variable con gran poder predictivo; k-fold ayuda a validar que el umbral se mantenga estable.",
  "No sirve si el problema requiere varias divisiones, interacciones o relaciones no lineales profundas."
)

tabla_stump <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_stump %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir Decision Stump",
             subtitle = "Decision Stump") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## Iterative Dichotomiser 3 (ID3)  {-}    
**Iterative Dichotomiser 3 (ID3)** es un algoritmo cl√°sico para construir **√°rboles de decisi√≥n**, dise√±ado principalmente para tareas de **clasificaci√≥n**. Fue uno de los primeros algoritmos de √°rboles de decisi√≥n desarrollados por Ross Quinlan. La idea central de ID3 es construir un √°rbol de clasificaci√≥n seleccionando en cada nodo del √°rbol el atributo que mejor divide el conjunto de datos en subconjuntos m√°s puros y homog√©neos.

ID3 opera de forma **iterativa** y **dicot√≥mica** (aunque puede manejar atributos con m√°s de dos categor√≠as), dividiendo el conjunto de datos en cada paso bas√°ndose en el atributo m√°s informativo. La selecci√≥n del "mejor" atributo se basa en m√©tricas de **teor√≠a de la informaci√≥n**, principalmente la **ganancia de informaci√≥n** (Information Gain). La ganancia de informaci√≥n mide la reducci√≥n en la **entrop√≠a** (una medida de la impureza o desorden de un conjunto de datos) que se logra al dividir los datos seg√∫n un atributo particular. El atributo con la mayor ganancia de informaci√≥n es elegido como el nodo de decisi√≥n en cada nivel del √°rbol.

A diferencia de los sistemas de aprendizaje global que buscan minimizar funciones de p√©rdida globales (como el error cuadr√°tico medio), ID3 es un algoritmo de **aprendizaje local** en el sentido de que toma decisiones de divisi√≥n √≥ptimas en cada nodo bas√°ndose en la informaci√≥n disponible en ese subconjunto de datos. Aunque la construcci√≥n del √°rbol es un proceso global, cada paso de la divisi√≥n se optimiza localmente para maximizar la pureza de los subconjuntos resultantes. Esto le permite a ID3 capturar relaciones no lineales entre las variables, ya que no asume una distribuci√≥n lineal de los datos. En esencia, si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n (o clasificaci√≥n, en este caso) de manera ponderada localmente al dividir el espacio de caracter√≠sticas en regiones m√°s manejables. Sin embargo, una desventaja de ID3 es que tiende a favorecer atributos con muchos valores y puede ser propenso al sobreajuste.



```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Categ√≥rica (clasificaci√≥n)",
  "‚úÖ Principalmente categ√≥ricas (num√©ricas requieren discretizaci√≥n)",
  "‚úÖ No lineal (basado en ganancia de informaci√≥n)",
  "‚ùå No requiere",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No aplica",
  "‚ö†Ô∏è Moderadamente (valores at√≠picos pueden generar ramas poco representativas)",
  "‚úÖ Robusto a multicolinealidad",
  "‚úÖ Alta (√°rbol simple de interpretar)",
  "‚úÖ R√°pido con datos moderados y discretizados",
  "‚úÖ Recomendable para equilibrar datos y evitar overfitting",
  "‚ùå Respuesta continua, muchos valores faltantes o ruido elevado"
)

detalles <- c(
  "Construye un √°rbol de decisi√≥n dividiendo por ganancia de informaci√≥n (entrop√≠a).",
  "Clasifica muestras en categor√≠as discretas, ej. S√≠/No, A/B/C.",
  "Mejor con variables categ√≥ricas nativas; las num√©ricas deben transformarse en rangos.",
  "No asume ninguna relaci√≥n funcional: usa particiones basadas en criterios de informaci√≥n.",
  "No hay residuos en el sentido param√©trico; no exige distribuci√≥n normal.",
  "Las instancias deben ser independientes; no orientado a series temporales.",
  "No requiere varianzas constantes porque no hay t√©rmino de error param√©trico.",
  "Los outliers categ√≥ricos pueden crear nodos muy peque√±os no representativos.",
  "ID3 ignora correlaciones altas, pero demasiadas variables correlacionadas pueden ralentizar la b√∫squeda de mejores divisiones.",
  "Cada nodo muestra la regla de divisi√≥n; el √°rbol global es f√°cil de visualizar para pocos niveles.",
  "La construcci√≥n recursiva es eficiente para datos discretizados; se vuelve lento si hay muchas categor√≠as o atributos.",
  "Se usa para podar y seleccionar profundidad √≥ptima del √°rbol, equilibrando sesgo y varianza.",
  "No es recomendable si la variable objetivo es continua o si hay mucho ruido sin transformar."
)

tabla_id3 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)  

tabla_id3 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir IDE3",
             subtitle = "Iterative Dichotomiser 3 (ID3)") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```



## M5 (Model Tree) {-}  

**M5**, a menudo referida como **M5'** o **M5P** (su implementaci√≥n en el software Weka), es un algoritmo de **√°rboles de decisi√≥n** espec√≠ficamente dise√±ado para **tareas de regresi√≥n**, es decir, para predecir valores num√©ricos continuos. Desarrollado por Ross Quinlan en 1992 y luego mejorado por Wang y Witten en 1997, M5 se destaca de los √°rboles de regresi√≥n tradicionales (como los de CART que solo tienen valores constantes en las hojas) al incorporar **modelos de regresi√≥n lineal** en sus nodos hoja.

La idea fundamental de M5 es combinar la interpretabilidad de un √°rbol de decisi√≥n con la capacidad predictiva de los modelos de regresi√≥n lineal. Funciona en dos etapas principales:

1.  **Construcci√≥n del √Årbol:** M5 construye un √°rbol de decisi√≥n de forma recursiva, similar a otros algoritmos de √°rboles. Sin embargo, en lugar de usar medidas de impureza para clasificaci√≥n, utiliza la **reducci√≥n de la desviaci√≥n est√°ndar (SDR)** como criterio de divisi√≥n. El algoritmo selecciona el atributo y el punto de divisi√≥n que maximizan la reducci√≥n de la desviaci√≥n est√°ndar del valor objetivo en los subconjuntos resultantes. Este proceso contin√∫a hasta que el n√∫mero de instancias en un nodo es muy peque√±o o la desviaci√≥n est√°ndar es muy baja.

2.  **Poda y Suavizado:** Una vez construido el √°rbol inicial, M5 lo **poda** para evitar el sobreajuste. En lugar de reemplazar los nodos con un valor constante, los nodos hoja (y a veces nodos internos) son reemplazados por **modelos de regresi√≥n lineal multivariados**. Estos modelos lineales se construyen utilizando los atributos relevantes para esa rama del √°rbol. Adem√°s, M5 aplica un proceso de **suavizado** para compensar las discontinuidades bruscas que podr√≠an surgir entre las predicciones de modelos lineales adyacentes. Este suavizado ajusta el valor predicho en una hoja bas√°ndose en las predicciones de los modelos en los nodos a lo largo de la ruta desde la ra√≠z hasta esa hoja.

En el contexto del **aprendizaje global vs. local**, M5 es un h√≠brido interesante. Por un lado, la construcci√≥n del √°rbol se basa en decisiones de divisi√≥n **locales**, buscando la mejor reducci√≥n de la desviaci√≥n est√°ndar en cada nodo. Esto permite a M5 modelar relaciones no lineales, ya que "si los datos no se distribuyen linealmente, se puede aplicar el concepto de regresi√≥n de manera ponderada localmente". El √°rbol divide el problema de regresi√≥n global en m√∫ltiples subproblemas m√°s peque√±os. Por otro lado, al tener **modelos de regresi√≥n lineal** en las hojas, M5 incorpora un componente de **aproximaci√≥n de funci√≥n local** m√°s sofisticado que un simple valor constante. Estos modelos lineales son "locales" para la regi√≥n de datos que representa esa hoja, pero internamente son modelos globales para esa subregi√≥n. Esto permite a M5 ofrecer una alternativa potente a las aproximaciones de funciones puramente globales, especialmente cuando las relaciones entre las variables son complejas y se benefician de una combinaci√≥n de particionamiento del espacio y modelado lineal dentro de esas particiones.

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relaci√≥n entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validaci√≥n cruzada",
  "No funciona bien si..."
)

aplica <- c(
  "‚úÖ Supervisado",
  "‚úÖ Continua",
  "‚úÖ Num√©ricas (categ√≥ricas procesar como dummies)",
  "‚úÖ Lineal por segmentos (√°rbol + regresi√≥n en hojas)",
  "‚ùå No requiere estrictamente",
  "‚úÖ Deseable, pero no obligatorio",
  "‚ùå No es requisito",
  "‚ö†Ô∏è Moderadamente (outliers pueden distorsionar regresiones locales)",
  "‚úÖ Relativamente robusto (regresi√≥n en hojas mitiga algo la colinealidad)",
  "‚ö†Ô∏è Media (√°rbol complejo, hojas lineales m√°s interpretables)",
  "‚ö†Ô∏è Moderado (depende de n√∫mero de nodos y atributos)",
  "‚úÖ Recomendable para optimizar n√∫mero de nodos y hojas",
  "‚ùå Muchos nodos con pocos casos o ruido elevado"
)

detalles <- c(
  "Modelo de √°rbol de regresi√≥n con ajustes lineales en cada hoja.",
  "Predice valores continuos, p. ej., precio, consumo, etc.",
  "Requiere que variables categ√≥ricas se conviertan a indicadores antes de ajuste.",
  "Combina particiones basadas en atributos con regresiones m√∫ltiples en hojas.",
  "No exige que los residuos en cada hoja sean normales, aunque mejora inferencia.",
  "Ideal si las observaciones son independientes; en series de tiempo hay que agrupar.",
  "La varianza constante no es cr√≠tica, cada hoja ajusta localmente.",
  "Los extremos pueden afectar las regresiones locales; poda puede mitigar esto.",
  "El m√©todo divide el espacio antes de ajustar, reduciendo efectos de colinealidad.",
  "El √°rbol completo puede ser grande, pero cada hoja contiene una funci√≥n lineal clara.",
  "Construcci√≥n y poda del √°rbol m√°s costosas que OLS, pero razonables para tama√±os medianos.",
  "Ayuda a determinar n√∫mero √≥ptimo de hojas y complejidad del √°rbol.",
  "Si hay muy pocas observaciones por hoja o ruido demasiado alto, las regresiones locales fallan."
)

tabla_m5 <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

tabla_m5 %>%
 gt() %>%
  tab_header(title = "Gu√≠a r√°pida para elegir M5",
             subtitle = "M5 model tree algorithm") %>%
   tab_footnote(footnote = "Fuente: Elaboraci√≥n propia") %>%
     tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html() 
```




